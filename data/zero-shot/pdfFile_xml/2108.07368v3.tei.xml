<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CaraNet: Context Axial Reverse Attention Network for Segmentation of Small Medical Objects</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ange</forename><surname>Lou</surname></persName>
							<email>ange.lou@vanderbilt.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Vanderbilt University</orgName>
								<address>
									<settlement>Nashville</settlement>
									<region>TN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyue</forename><surname>Guan</surname></persName>
							<email>frankshuyueguan@gwu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">The George Washington University</orgName>
								<address>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanseok</forename><surname>Ko</surname></persName>
							<email>hsko@korea.ac.kr</email>
							<affiliation key="aff2">
								<orgName type="institution">Korea University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murray</forename><surname>Loew</surname></persName>
							<email>loew@gwu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">The George Washington University</orgName>
								<address>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CaraNet: Context Axial Reverse Attention Network for Segmentation of Small Medical Objects</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Small object segmentation</term>
					<term>Brain tumor</term>
					<term>Colonoscopy</term>
					<term>Attention</term>
					<term>Context axial reverse</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Segmenting medical images accurately and reliably is important for disease diagnosis and treatment. It is a challenging task because of the wide variety of objects' sizes, shapes, and scanning modalities. Recently, many convolutional neural networks (CNN) have been designed for segmentation tasks and achieved great success. Few studies, however, have fully considered the sizes of objects, and thus most demonstrate poor performance for small objects segmentation. This can have a significant impact on the early detection of diseases. This paper proposes a Context Axial Reserve Attention Network (CaraNet) to improve the segmentation performance on small objects compared with several recent state-of-the-art models. We test our CaraNet on brain tumor (BraTS 2018) and polyp (Kvasir-SEG, CVC-ColonDB, CVC-ClinicDB, CVC-300, and ETIS-LaribPolypDB) segmentation datasets. Our CaraNet achieves the top-rank mean Dice segmentation accuracy, and results show a distinct advantage of CaraNet in the segmentation of small medical objects. Codes available: https://github.com/AngeLouCN/CaraNet</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Deep learning has had a tremendous impact on various fields in science. Our focus of the current study in deep learning is on one of the most critical areas of computer vision: medical image segmentation. Recently, various convolutional neural networks (CNNs) have shown great performance on medical image segmentation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. Those CNNs have been introduced for various medical imaging modalities, including X-ray, visible-light imaging, magnetic resonance imaging (MRI), positron emission tomography (PET), and computerized tomography (CT). They all achieved excellent performance on medical image segmentation challenges from different modalities, like BraTS <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>, KiTS19 <ref type="bibr" target="#b7">[8]</ref>, and COVID19-20 <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. To obtain more accurate segmentation results, many works introduced improvements of network architectures. Those improvements are mostly attributed to exploring new neural architectures by designing networks with varying depths (ResNet <ref type="bibr" target="#b10">[11]</ref>), widths (ResNeXt <ref type="bibr" target="#b11">[12]</ref>), connectivity (DenseNet <ref type="bibr" target="#b12">[13]</ref> and GoogLeNet <ref type="bibr" target="#b13">[14]</ref>), or new types of components (pyramid scene <ref type="bibr" target="#b14">[15]</ref> and atrous convolution <ref type="bibr" target="#b15">[16]</ref>). Although those new architectures improve the overall segmentation results, they are less sensitive to detecting small medical objects. And it is very common in medical image segmentation that the anatomy of interest occupies only a very small portion of the image <ref type="bibr" target="#b16">[17]</ref>. Most extracted features belong to the background, while these small lesion areas are important for early detection and diagnosis. For example, the survival rate decreases with the growing size of a brain tumor <ref type="bibr" target="#b17">[18]</ref>. Thus, it has clinical significance to build an effective network to detect tiny medical objects.</p><p>The attention mechanism plays a dominant role in neural network research. It can effectively use information transferred from several subsequent feature maps to detect the salience features <ref type="bibr" target="#b18">[19]</ref>. Many attention methods such as self-attention and multi-head attention have been verified to have high performance in applications of natural language processing <ref type="bibr" target="#b19">[20]</ref> and computer vision <ref type="bibr" target="#b20">[21]</ref>. Those attention methods also have been successfully used for medical image segmentation; for example, the Medical Transformer <ref type="bibr" target="#b21">[22]</ref> (MedT) used a gated axial self-attention layer to build a Local-Global (LoGo) network for ultrasound and microscopy image segmentation, TransUNet <ref type="bibr" target="#b22">[23]</ref> stacked self-attention as a transformer in the encoder for CT image segmentation, and CoTr <ref type="bibr" target="#b23">[24]</ref> bridged two CNN encoder and decoder by the transformer encoder for multi-organ segmentation. All those attention-based segmentations achieve significant improvement compared with purely convolutional neural networks like U-Net <ref type="bibr" target="#b0">[1]</ref> and FCN <ref type="bibr" target="#b24">[25]</ref>.</p><p>Although those new types of neural networks show good performance on many medical segmentation tasks, they seldom consider the small object segmentation, especially in the medical image area. We propose here a novel attention-based deep neural network, called Context Axial Reverse Attention Network (CaraNet). The contribution of the paper can be summarized as follows: 1) We propose a novel neural network -CaraNet --to solve the problem of segmentation of small medical objects. 2) We introduce a method to evaluate the network's performance on small medical objects. 3) Our experiments show that CaraNet outperforms most current models (e.g., PraNet from MICCAI '20) and</p><p>advances the state-of-the-art by a large margin, both overall and on small objects, in segmentation performance on polyps. <ref type="figure" target="#fig_0">Figure 1</ref> shows the architecture of our CaraNet, which uses a parallel partial decoder <ref type="bibr" target="#b25">[26]</ref> to generate the highlevel semantic global map and a set of context and axial reverse attention operations to detect global and local feature information. We will introduce each component in the following subsections. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Backbone</head><p>Transfer learning provides a feasible method for alleviating the challenge of data-hunger, and it has been widely applied to the field of computer vision <ref type="bibr" target="#b26">[27]</ref>. Benefiting from the strong visual knowledge distributed in ImageNet <ref type="bibr" target="#b27">[28]</ref>, the pre-trained CNNs can be fine-tuned with a small amount of task-specific data and can perform well on downstream tasks. Since Res2Net <ref type="bibr" target="#b28">[29]</ref> can construct hierarchical residual-like connections within one single residual block that has stronger multi-scale representation ability, we applied the pre-trained Res2Net as the backbone of CaraNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Partial decoder</head><p>Existing state-of-the-art segmentation networks rely on aggregating multi-level features from the encoder (e.g., U-Net aggregates all level features extracted from an encoder). Compared to the high-level features, however, low-level features contribute less to performance but have higher computational cost because of their larger spatial resolution <ref type="bibr" target="#b29">[30]</ref>. Thus, we applied the parallel partial decoder <ref type="bibr" target="#b25">[26]</ref> as shown in <ref type="figure">Figure 2</ref> to aggregate high-level features. We feed the original image which size is ? ? ? (?, , and represent the height, width, and channel) into Res2Net, and we can get five different level features { , = 1, ? , 5} with resolution { ? 2 ?1 , 2 ?1 } . We aggregated the high-level features { 3 , 4 , 5 } from Res2Net by using the partial decoder with a parallel connection. Then, we can get a global map = ( 3 , 4 , 5 ).</p><formula xml:id="formula_0">Figure 2.</formula><p>Overview of partial decoder with parallel connection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Channel-wise feature pyramid module</head><p>The Feature Pyramid (FP) has been widely used in deep learning models for computer vision tasks due to its ability to represent multi-scale features. For example, PSPNet <ref type="bibr" target="#b30">[31]</ref> builds a pyramid pooling module with different sizes' pooling layers to extract multi-scale features, and the Feature Pyramid Network (FPN) <ref type="bibr" target="#b31">[32]</ref> takes different strides with convolution kernels to obtain a FP. Although those FP-based methods perform well in the computer vision area, they cannot avoid using large numbers of parameters, which consume a large amount of computation resources. Alternatively, our previous works <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref> proposed a lightweight Channel-wise Feature Pyramid (CFP) module and successfully applied it to both nature and medical image segmentation. The architecture of this CFP module is shown in <ref type="figure" target="#fig_1">Figure 3</ref>. <ref type="figure" target="#fig_1">Figure 3</ref>(a) shows the architecture of the CFP module; it contains total channels and each channel has its own dilation rate . Typically, we choose the = 4 for CaraNet and the dilation rates for each channel { 1 , 2 , 3 , 4 } = {1,2,4,8}; thus, each channel's dimension is /4. Then, we applied hierarchical feature fusion <ref type="bibr" target="#b34">[35]</ref> (HFF) to sum the outputs of all channels step by step. For the FP channel, we provide two versions with regular convolution and asymmetric convolution as shown in <ref type="figure" target="#fig_1">Figure 3</ref>(b) and (c). We connected the outputs of each convolutional module by using skip connection, and thus each channel can be considered as a sub-pyramid. We selected the regular convolution as FP channel for CaraNet. The overall FP is obtained from concatenating those sub-pyramids from the hierarchical feature fusion operation. The final FP contains four levels of feature stacks as shown in <ref type="figure" target="#fig_2">Figure 4</ref>. These four levels of feature stacks { , = 1, ? ,4} are computed by:</p><formula xml:id="formula_1">{ 1 = 1 2 = 1 + 2 3 = 2 + 3 4 = 3 + 4 (1)</formula><p>And the final FP is computed by ? .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Axial reverse attention module</head><p>The previous partial decoder that generates the global map (Sec. 2.2) could roughly locate the position of medical objects, and the CFP module extracted only multi-scale features from the pre-trained model. To obtain more accurate feature information, we designed the Axial Reverse Attention (A-RA) module to analyze localization information and multi-scale features. The overview and detail of the A-RA module can be seen in <ref type="figure" target="#fig_0">Figure 1</ref> and <ref type="figure">Figure 5</ref>, respectively. The input of the top line is the multi-scale feature maps ? from the CFP module and we used axial attention to analyze the salience information. The axial attention is based on self-attention, which maps a query and a set of key-value pairs to an output and the operation:</p><formula xml:id="formula_2">( , , ) = ( ? )<label>(2)</label></formula><p>where , , , and represent query, key, value, and dimension of key, respectively. However, self-attention consumes great computational resources, especially when the spatial dimension of the input is large <ref type="bibr" target="#b35">[36]</ref>. Therefore, we applied axial attention, which factorizes 2D attention into two 1D attention along height and width axes. Here we replace the softmax activation function with a sigmoid, based on the experiments. For the second line, we applied the reverse operation <ref type="bibr" target="#b36">[37]</ref> to detect the salience features from the side-output , which is obtained from the output of the previous A-RA module. The reverse operation is:</p><formula xml:id="formula_3">= 1 ? ( ).<label>(3)</label></formula><p>The total axial reverse attention operation is:</p><formula xml:id="formula_4">= ? (4)</formula><p>where ? is element-wise multiplication, and the is feature from the axial attention route. <ref type="figure">Figure 5</ref>. Structure of Axial Reverse Attention module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Deep supervision</head><p>We apply weighted intersection over union (IoU) and weighted binary cross-entropy (BCE) in our loss function: ? = ? + ? to calculate the global loss and local (pixel-level) loss, respectively. To train CaraNet, we apply deep supervision for the three side-outputs ( 1 , 2 , 3 ) and the global map . Before calculating the loss, we up-sampled them to the same size as ground truth . Thus, the total loss:</p><formula xml:id="formula_5">? = ?( , ) + ? ?( ,<label>) 5 =3</label></formula><p>(5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Small object segmentation analysis</head><p>Since the size of all images input to the segmentation models must be fixed, the size of an object is determined by the number of pixels in the object m and the number of total pixels in the image N. Thus, we consider the object's size using the size ratio (proportion) = m/N. Then, we evaluate the performance of segmentation models according to the sizes of objects. Especially, we mainly focus on the small areas whose size ratios are smaller than 5%. For a segmentation model, we first obtain the mean-Dice coefficients and size ratios of segmentations from the test dataset. Similar to computing the histogram, we plot the results in a curve whose y-axis is mean-Dice coefficients and xaxis is increasingly sorted size ratios. To smooth the curve, we take interval-averaged mean-Dice coefficients by sorted size ratios: we divide the entire range of size ratios into a consecutive, non-overlapping, and of equal length series of intervals, and then calculate the average mean-Dice coefficients of size ratios in each interval. The interval-averaged coefficients have a smooth curve and are more stable in the presence of noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Implementation details</head><p>We implemented our model in PyTorch accelerated by the NVIDIA RTX GPU. We resized input images to 352 ? 352 for polyp segmentation and 256 ? 256 for brain tumor segmentation, and employed a multi-scale training strategy {0.75, 1.0, 1.25} instead of data augmentation. We used Adam optimizer with the initial learning rate 1 ?4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dataset</head><p>We test our CaraNet on five polyp segmentation datasets: ETIS <ref type="bibr" target="#b37">[38]</ref>, CVC-ClinicDB <ref type="bibr" target="#b38">[39]</ref>, CVC-ColonDB <ref type="bibr" target="#b39">[40]</ref>, EndoScene <ref type="bibr" target="#b40">[41]</ref>, and Kvasir <ref type="bibr" target="#b41">[42]</ref>. The first four are standard benchmarks, and the last one is the largest dataset, which was released recently. We also test our model on a brain tumor segmentation dataset (BraTS 2018), which contains more extremely small medical objects. <ref type="table" target="#tab_0">Table 1</ref> shows the details of these datasets: image size, scale of testing set, and size ratios of medical objects. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Baseline</head><p>We compared CaraNet with six medical image segmentation models, including state-of-the-art models: U-Net <ref type="bibr" target="#b0">[1]</ref>, U-Net++ <ref type="bibr" target="#b1">[2]</ref>, ResUNet-mod <ref type="bibr" target="#b42">[43]</ref>, ResUNet++ <ref type="bibr" target="#b2">[3]</ref>, SFA <ref type="bibr" target="#b43">[44]</ref>, and PraNet <ref type="bibr" target="#b26">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training and measurement metrics</head><p>We randomly split 80% of images from Kvasir and CVC-ClinicDB as training set and the remainder as a testing dataset. In addition to mean Dice and mean IoU, we also apply four other measurement metrics: weighted dice metric , MAE, enhanced alignment metric ? <ref type="bibr" target="#b44">[45]</ref>, and structural measurement <ref type="bibr" target="#b45">[46]</ref>. <ref type="table" target="#tab_1">Table 2</ref> shows the polyp segmentation on the five datasets. The weighted dice metric is used to amend the "equal importance flaw" in dice. The MAE is used to measure the pixel-to-pixel accuracy. The recently released enhanced alignment metric ? is utilized to evaluate the pixel-level and global-level similarity. And is used to measure the structure similarity between predictions and ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Results</head><p>We also show some polyp segmentation results in <ref type="figure">Figure 6</ref>. For the five polyp datasets, CaraNet not only outperforms the compared models in overall performance, but also on samples with small polyps. <ref type="figure">Figure 7</ref> shows the segmentation performance of CaraNet and PraNet for small objects (proportions ? 5%). For the extremely small object segmentation analysis on the BraTS 2018 dataset, we compare only CaraNet with PraNet because PraNet has the closest performance to ours, and the overall accuracies of the other segmentation models are clearly lower than those of CaraNet and PraNet. (Note: the fluctuations with size in colonoscopy datasets are caused by types and boundary of polyps, and quality of imaging) 0.791 n/a n/a n/a n/a n/a ResUNet++ 0.813 0.793 n/a n/a n/a n/a SFA 0. 0.779 n/a n/a n/a n/a n/a ResUNet++ 0.796 0.796 n/a n/a n/a n/a     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CVC-300</head><p>CaraNet (Ours)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PraNet</head><p>To further evaluate the effectiveness of CaraNet for small-object segmentation, we conducted another experiment using the brain tumor dataset (BraTS 2018). The polyp datasets lack extremely small objects (the minimum is about 0.11%) and do not have enough small samples (like Kvasir and CVC-ClinicDB, in <ref type="figure">Figure 7</ref>, there are fewer samples in the range of small sizes). The brain tumor dataset was created from the BraTS 2018 database by slicing 2D images from the "T1ce" source with "NET" type labels. We randomly select 60% of the images as the training set and the remainder as the testing dataset. Altogether, 3231 images with proportions of tumor sizes ranging from 0.01% -4.91% were in the testing dataset. <ref type="table" target="#tab_4">Table 3</ref>, <ref type="figure" target="#fig_4">Figure 8</ref>, and <ref type="figure" target="#fig_5">Figure 9</ref> show the comparison result. We compared CaraNet with only PraNet for the same reason stated above. Clearly, our CaraNet performed better, especially for the extremely small cases (range 0.01% -0.1% in <ref type="figure" target="#fig_4">Figure 8</ref> and the red area indicates that the Dice value of CaraNet is greater than that of PraNet; blue area shows the opposite. Values on the right show the summations of all red and blue differential values).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">DISCUSSION</head><p>We propose a novel deep-learning based segmentation model -CaraNet, by combining the Axial Reverse Attention and Channel-wise Feature Pyramid (CFP) modules. This new method can help improve the performance of the segmentation of small medical objects. Through the experiments, we show that CaraNet outperforms the most famous models by a large margin overall for six measurement metrics. As shown by the polyp segmentation results, CaraNet not only produces high quality segmentation on samples of large polyps, but also performs well for small and multi smallobject segmentation. <ref type="figure" target="#fig_5">Figure 9</ref> shows some results of extremely small tumor segmentation from the BraTS 2018 dataset. The advantage of the CaraNet in segmenting small single-and multi-objects is evident. In addition, compared with the recent state-of-the-art network, PraNet, CaraNet provides a more precise prediction for the most-challenging cases.</p><p>Although CaraNet achieves good improvement on medical image segmentation tasks, there are still some limitations and potentials to optimize the model. For example, using bilinear interpolation to up-sample feature maps cannot avoid the loss of some useful information and lead to a coarse boundary. It can be improved by applying a deconvolutional layer. In addition, the backbone of CaraNet is pre-trained on ImageNet, which contains natural images that are very different from medical images. Moreover, the sliced brain MRI data also cause loss of spatial information between the voxels; that may influence the accuracy of small tumor detection. In our future work, we will use the Model Genesis <ref type="bibr" target="#b46">[47]</ref> as a 3-D backbone to replace the Res2Net (2-D backbone) and adjust the CaraNet to employ it on 3-D medical imaging segmentation to build a 3-D version CaraNet for more accurate CT or MRI image segmentation. Segmentation of 3-D medical images is of growing interest; we believe CaraNet will address that problem successfully.</p><p>We also introduce the process to evaluate segmentation models according to the size of objects. We consider the object's size using the size ratio including the sizes of objects and the whole image. In this study, we assume the size ratios of "small objects" are less than 5%. However, we did not discuss specific reasons to choose the threshold of 5%, and there is not a clear definition of the "small objects". Since few studies have fully considered the sizes of objects and the smallobject problems in medical imaging, we will further study this question in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>We have proposed a novel neural network, CaraNet, for small medical object segmentation. From the overall segmentation accuracy, we can find that CaraNet outperforms all state-of-the-art approaches by at least 2% (mean dice accuracy). For the early diagnosis dataset (ETIS) which contains many small polyps, however, CaraNet can reach 74.7% mean dice accuracy, which is about 12% higher than PraNet. For the extremely-small object segmentation dataset (BraTS 2018), CaraNet can achieve 3% higher than PraNet. When evaluating segmentation models according to the size of objects, CaraNet outperforms PraNet for small objects in all six datasets we used.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Overview of CaraNet, which contains pretrained backbone, partial decoder, channel-wise feature pyramid (CFP) module and axial reverse attention (A-RA) module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>(a) CFP module, (b) FP channel with regular convolution, (c) FP channel with asymmetric convolution</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Final feature pyramid obtained from CFP module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>Polyp segmentation results Performance vs. Size on the five polyp datasets. The x-axis is the proportion size (%) of polyp; y-axis is the averaged mean Dice coefficient. Blue is for our CaraNet and orange is for the PraNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 .</head><label>8</label><figDesc>Performance vs. Size on brain tumor datasets. The x-axis is the proportion size (%) of tumor; the y-axis is the difference between the averaged mean Dice coefficient results of CaraNet and PraNet (CaraNet meanDice -PraNet meanDice). Red areas show CaraNet outperforms PraNet, blue areas show the opposite. The summation of red areas is 0.763 and summation of blue areas is 0.236; that means CaraNet overall outperforms PraNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 .</head><label>9</label><figDesc>Brain tumor segmentation results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Details of datasets</figDesc><table><row><cell></cell><cell>Image size</cell><cell cols="2">Number of test samples Object size ratio</cell></row><row><cell>ETIS</cell><cell>966 ? 1225</cell><cell>196</cell><cell>0.11% -29.05%</cell></row><row><cell>CVC-ClinicDB</cell><cell>288 ? 384</cell><cell>62</cell><cell>0.34 % -45.88%</cell></row><row><cell>CVC-ColonDB</cell><cell>500 ? 574</cell><cell>380</cell><cell>0.30% -63.15%</cell></row><row><cell>CVC-300</cell><cell>500 ? 574</cell><cell>60</cell><cell>0.55% -18.42%</cell></row><row><cell>Kvasir</cell><cell>1070 ? 1348</cell><cell>100</cell><cell>0.79% -62.13%</cell></row><row><cell>BraTS 2018</cell><cell>256 ? 256</cell><cell>3231</cell><cell>0.01% -4.91%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative results on Kvasir, CVC-ClinicDB, CVC-ColonDB, ETIS, and CVC-T (test dataset of EndoScene).</figDesc><table><row><cell></cell><cell>Methods</cell><cell>mean Dice</cell><cell>mean IoU</cell><cell></cell><cell></cell><cell></cell><cell>MAE</cell></row><row><cell></cell><cell>UNet</cell><cell>0.818</cell><cell>0.746</cell><cell>0.794</cell><cell>0.858</cell><cell>0.893</cell><cell>0.055</cell></row><row><cell></cell><cell>UNet++</cell><cell>0.821</cell><cell>0.743</cell><cell>0.808</cell><cell>0.862</cell><cell>0.910</cell><cell>0.048</cell></row><row><cell>Kvasir</cell><cell>ResUNet-mod</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Quantitative results on brain tumor (BraTS 2018) dataset.</figDesc><table><row><cell>Methods</cell><cell cols="2">mean Dice mean IoU</cell><cell>MAE</cell></row><row><cell>CaraNet (Ours)</cell><cell>0.631</cell><cell>0.507</cell><cell>0.629 0.786 0.927 0.003</cell></row><row><cell>PraNet (MICCAI'20)</cell><cell>0.619</cell><cell>0.494</cell><cell>0.606 0.776 0.920 0.003</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015-10" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Unet++: A nested u-net architecture for medical image segmentation. In Deep learning in medical image analysis and multimodal learning for clinical decision support</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M R</forename><surname>Siddiquee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="3" to="11" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Resunet++: An advanced architecture for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Smedsrud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>De Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Johansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Symposium on Multimedia (ISM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-12" />
			<biblScope unit="page" from="225" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pranet: Parallel reverse attention network for polyp segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-10" />
			<biblScope unit="page" from="263" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The multimodal brain tumor image segmentation benchmark (BRATS)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jakab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Farahani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Van Leemput</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1993" to="2024" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Advancing the cancer genome atlas glioma MRI collections with expert segmentation labels and radiomic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Akbari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sotiras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rozycki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Davatzikos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Identifying the best machine learning algorithms for brain tumor segmentation, progression assessment, and overall survival prediction in the BRATS challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jakab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rempfler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Crimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Jambawalikar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1811.02629</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The kits19 challenge data: 300 kidney tumor cases with clinical context, ct semantic segmentations, and surgical outcomes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sathianathen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kalapara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Walczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kaluzniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Weight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00445</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Rapid Artificial Intelligence Solutions in a Pandemic-The COVID-19-20 Lung</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">T</forename><surname>Diez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zember</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Molto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Linguraru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
		<respStmt>
			<orgName>CT Lesion Segmentation Challenge</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">CT Images in Covid-19 [Data set]. The Cancer Imaging Archive</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Turkbey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sanford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Amalou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
		<idno type="DOI">10.7937/tcia</idno>
		<ptr target="https://doi.org/10.7937/tcia" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep learning techniques for medical image segmentation: achievements and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Hesamian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of digital imaging</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="582" to="596" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-task learning for small brain tumor segmentation from MRI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page">7790</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep semantic segmentation of natural and medical images: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Taghanaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Abhishek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohen-Adad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hamarneh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="137" to="178" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Medical transformer: Gated axial-attention for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M J</forename><surname>Valanarasu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Oza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hacihaliloglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10662</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Transunet: Transformers make strong encoders for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04306</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03024</idno>
		<title level="m">CoTr: Efficiently Bridging CNN and Transformer for 3D Medical Image Segmentation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pranet: Parallel reverse attention network for polyp segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-10" />
			<biblScope unit="page" from="263" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Context-dependent pre-trained deep neural networks for largevocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on audio, speech, and language processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="42" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Res2net: A new multi-scale backbone architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cascaded partial decoder for fast and accurate salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3907" to="3916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">CFPNet: Channel-wise Feature Pyramid for Real-Time Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loew</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.12212</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loew</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04075</idno>
		<title level="m">CFPNet-M: A Light-Weight Encoder-Decoder Based Network for Multimodal Biomedical Image Real-Time Segmentation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Espnet: Efficient spatial pyramid of dilated convolutions for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the european conference on computer vision (ECCV)</title>
		<meeting>the european conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="552" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Axial-deeplab: Stand-alone axialattention for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-08" />
			<biblScope unit="page" from="108" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Reverse attention for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="234" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Toward embedded detection of polyps in wce images for early diagnosis of colorectal cancer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Histace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Romain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Granado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer assisted radiology and surgery</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="283" to="293" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">WM-DOVA maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>S?nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fern?ndez-Esparrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rodr?guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vilari?o</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="99" to="111" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Automated polyp detection in colonoscopy videos using shape and context information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Gurudu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="630" to="644" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A benchmark for endoluminal scene segmentation of colonoscopy images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>V?zquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>S?nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fern?ndez-Esparrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of healthcare engineering</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Kvasir-seg: A segmented polyp dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Smedsrud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>De Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Johansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia Modeling</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-01" />
			<biblScope unit="page" from="451" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Road extraction by deep residual u-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="749" to="753" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Selective feature aggregation network with areaboundary constraints for polyp segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Y</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-10" />
			<biblScope unit="page" from="302" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Enhanced-alignment measure for binary foreground map evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10421</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Structure-measure: A new way to evaluate foreground maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4548" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Models genesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Gotway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page">101840</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
