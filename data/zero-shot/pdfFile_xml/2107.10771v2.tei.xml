<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EAN: Event Adaptive Network for Enhanced Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Tian</surname></persName>
							<email>eetianyuan@sjtu.edu.cn.y.yanis</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Image Communication and Network Engineering</orgName>
								<orgName type="institution">Shang-hai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Yan</surname></persName>
							<email>yanyichao@sjtu.edu.cn.g.guoiswithbaidu.e-mail:guoguodong01@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Image Communication and Network Engineering</orgName>
								<orgName type="institution">Shang-hai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangtao</forename><surname>Zhai</surname></persName>
							<email>zhaiguangtao@sjtu.edu.cn.y.yanis</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Image Communication and Network Engineering</orgName>
								<orgName type="institution">Shang-hai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Image Communication and Network Engineering</orgName>
								<orgName type="institution">Shang-hai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Gao</surname></persName>
							<email>zhiyong.gao@sjtu.edu.cn.y.yanis</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Image Communication and Network Engineering</orgName>
								<orgName type="institution">Shang-hai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">with the AI Institute</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhai</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">with the AI Institute</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">with the AI Institute</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EAN: Event Adaptive Network for Enhanced Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Received: 9 September 2021 / Accepted: 20 July 2022</note>
					<note>International Journal of Computer Vision manuscript No. (will be inserted by the editor) Codes are available at: https: //github.com/tianyuan168326/EAN-Pytorch.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Action recognition ? Dynamic neural networks ? Vision Transformers ? Motion representation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Efficiently modeling spatial-temporal information in videos is crucial for action recognition. To achieve this goal, state-of-the-art methods typically employ the convolution operator and the dense interaction modules such as non-local blocks. However, these methods cannot accurately fit the diverse events in videos. On the one hand, the adopted convolutions are with fixed scales, thus struggling with events of various scales. On the other hand, the dense interaction modeling paradigm only achieves sub-optimal performance as action-irrelevant parts bring additional noises for the final prediction. In this paper, we propose a unified action recognition framework to investigate the dynamic nature of video content by introducing the following designs. First, when extracting local cues, we generate the spatial-temporal kernels of dynamic-scale to adaptively fit the diverse events. Second, to accurately aggregate these cues into a global video representation, we propose to mine the interactions only among a few selected foreground objects by a Transformer, which yields a sparse paradigm. We call the proposed framework as Event Adaptive Network (EAN) because both key designs are adaptive to the input video content. To exploit the shortterm motions within local segments, we propose a novel and efficient Latent Motion Code (LMC) module, further improving the performance of the framework. Extensive experiments on several large-scale video datasets, e.g., Somethingto-Something V1&amp;V2, Kinetics, and Diving48, verify that our models achieve state-of-the-art or competitive performances at low FLOPs. . denotes the corresponding author.</p><p>(a) Pushing something so that it falls off the table (b) Moving something towards the camera <ref type="figure">Fig. 1</ref>: Two examples from the Something-Something dataset <ref type="bibr" target="#b20">[21]</ref>. The objects (i.e., can, box, and hand) and events have diverse spatial-temporal scales in different videos. Therefore, convolution kernels with adaptive scales can better fit them. Moreover, the interactions among them are naturally sparse, which can be accurately and efficiently modeled by a dedicated sparse model. The objects, object interactions, and key events are indicated by the colored boxes, pink dotted arrows, and gray dotted arrows, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract Efficiently modeling spatial-temporal information in videos is crucial for action recognition. To achieve this goal, state-of-the-art methods typically employ the convolution operator and the dense interaction modules such as non-local blocks. However, these methods cannot accurately fit the diverse events in videos. On the one hand, the adopted convolutions are with fixed scales, thus struggling with events of various scales. On the other hand, the dense interaction modeling paradigm only achieves sub-optimal performance as action-irrelevant parts bring additional noises for the final prediction. In this paper, we propose a unified action recognition framework to investigate the dynamic nature of video content by introducing the following designs. First, when extracting local cues, we generate the spatial-temporal kernels of dynamic-scale to adaptively fit the diverse events. Second, to accurately aggregate these cues into a global video representation, we propose to mine the interactions only among a few selected foreground objects by a Transformer, which yields a sparse paradigm. We call the proposed framework as Event Adaptive Network (EAN) because both key designs are adaptive to the input video content. To exploit the shortterm motions within local segments, we propose a novel and efficient Latent Motion Code (LMC) module, further improving the performance of the framework. Extensive experiments on several large-scale video datasets, e.g., Somethingto-Something V1&amp;V2, Kinetics, and Diving48, verify that our models achieve state-of-the-art or competitive performances at low FLOPs. Codes are available at: https: //github.com/tianyuan168326/EAN-Pytorch.</p><p>Y. Tian, G. Zhai, and Z. <ref type="bibr">Gao</ref>   <ref type="figure">Fig. 1</ref>: Two examples from the Something-Something dataset <ref type="bibr" target="#b20">[21]</ref>. The objects (i.e., can, box, and hand) and events have diverse spatial-temporal scales in different videos. Therefore, convolution kernels with adaptive scales can better fit them. Moreover, the interactions among them are naturally sparse, which can be accurately and efficiently modeled by a dedicated sparse model. The objects, object interactions, and key events are indicated by the colored boxes, pink dotted arrows, and gray dotted arrows, respectively.</p><p>Keywords Action recognition ? Dynamic neural networks ? Vision Transformers ? Motion representation 1 Introduction</p><p>Video action recognition is an open challenge in computer vision, drawing increasing attention in both research and industrial communities, because of its fundamental role for tremendous applications, e.g., human behavior monitoring <ref type="bibr" target="#b8">[9]</ref> [7] <ref type="bibr" target="#b50">[51]</ref>, video surveillance <ref type="bibr" target="#b16">[17]</ref>, anomaly events analysis <ref type="bibr" target="#b1">[2]</ref> [ <ref type="bibr" target="#b34">35]</ref>, to name a few. It goes beyond the recognition performed on single images and depends on comprehensively arXiv:2107.10771v2 [cs.CV] 9 Aug 2022 modeling both (1) the local spatial-temporal cues and (2) the global object interactions in videos.</p><p>Many previous methods <ref type="bibr" target="#b59">[60]</ref> [32] <ref type="bibr" target="#b69">[70]</ref> [53] <ref type="bibr" target="#b5">[6]</ref> [54] <ref type="bibr" target="#b13">[14]</ref> achieve promising performance by only modeling the local spatial-temporal cues. However, these networks are typically built with convolutions, whose scales are usually empirically determined and kept fixed for different input videos. Indeed, by designing multi-scale networks, such as in ResNet <ref type="bibr" target="#b22">[23]</ref>, Inception networks <ref type="bibr" target="#b47">[48]</ref>, and Res2Net <ref type="bibr" target="#b17">[18]</ref>, the models are equipped with convolution kernels of diverse scales. But, these architectures are still static, not adapting to the various events within videos. We illustrate this challenge in <ref type="figure">Fig. 1</ref>. There naturally arises a question -can we design a dynamic architecture that adaptively fits the events in each video?</p><p>Additionally, recognizing the actions in videos needs to reason about the interactions among the objects. Although the local interactions can be well captured by the convolutions, there are always some non-local interactions that can only be observed from a global view. For example, in <ref type="figure">Fig. 1</ref> (a), the key interaction is "the can is moved towards the ground across several frames". Modeling interactions like this requires global reasoning capability, which is beyond the function of convolution. To model the global information, dense interaction models <ref type="bibr" target="#b60">[61]</ref>  <ref type="bibr" target="#b3">[4]</ref>  <ref type="bibr" target="#b12">[13]</ref> calculate the paired correlations at all positions, which inevitably introduce the background noise signals. In contrast, the sparse models <ref type="bibr" target="#b61">[62]</ref>  <ref type="bibr" target="#b38">[39]</ref> are more accurate because they only target the action-relevant regions. Nevertheless, the inefficiency and the error accumulation caused by their embedded object detector are nontrivial to resolve. Moreover, the utilized heavy detector hinders the end-to-end training of the whole system. Therefore, there arises another question -can we model the global object interactions sparsely without relying on a heavy object detector?</p><p>In this paper, we answer both questions with yes, by carefully designing several spatial-temporal modeling modules. First, we propose an Event Adaptive Block (EAB) to enhance the convolution operators with scale-adaptive modeling capability. Particularly, this block perceives the scale information of the key events within the input video, and then dynamically synthesizes the spatial-temporal kernel. Since the scale of the kernel is not fixed, it is unfeasible to represent it with a single trainable tensor. Instead, we reformulate it as a soft fusion of several fixed-scale spatial-or temporal-convolution kernels. Since the synthesized kernel is customized to the input video, the local event cues within the video are better modeled. Moreover, the prevalent architectures, e.g., R(2+1)D CNNs <ref type="bibr" target="#b53">[54]</ref> and Inception-Nets <ref type="bibr" target="#b47">[48]</ref> can be viewed as a special case of the proposed EAB. Second, we propose a Sparse Object Interaction Transformer (SOI-Tr) to build sparse interaction graphs by adaptively selecting the most important objects involved in the actions. Concretely, given the deep video features, an embedded ob-ject localization network first outputs several saliency maps, each of which corresponds to an object. Then, a shallow Transformer <ref type="bibr" target="#b54">[55]</ref> is used to model the long-range interactions among this small number of objects. Thanks to the feature-level detection scheme, this module gets rid of the heavy detector and is end-to-end trainable, which is more effective and efficient than the previous models <ref type="bibr" target="#b61">[62]</ref>  <ref type="bibr" target="#b38">[39]</ref>.</p><p>In addition to the two spatial-temporal modeling modules above, we further propose a novel Latent Motion Code (LMC) module to efficiently exploit the short-term motion information within local video segments. Specifically, the low-level motion cues within each segment, i.e., RGB differences, are first encoded into a compact latent space. Then, the high-order motion information is reasoned in this space. The motion information further facilitates the discriminating capability of our method for some hard action cases.</p><p>We incorporate the proposed three modules into a unified ConvNet called Event Adaptive Network (EAN). By following a series of efficient network designs, the proposed EAN is highly efficient. The whole framework can be jointly optimized following the sparse sampling strategy proposed in TSN <ref type="bibr" target="#b59">[60]</ref>. We emphasize our contributions as follows:</p><p>-A novel Event Adaptive Block (EAB) is proposed to generate the video-adaptive spatial-temporal convolution kernel of dynamic scale, demonstrating superior local spatial-temporal modeling capability. Moreover, our approach is the very first work to generate dynamic spatialtemporal convolution kernels for video data. -A Sparse Object Interaction Transformer (SOI-Tr) is developed to accurately reason the global interactions among the sparse foreground objects, without relying on bounding box annotations or external object detectors. -A novel and efficient Latent Motion Code (LMC) module is devised to capture the short-term motion information within local video segments in a latent space. -By incorporating the proposed EAB, SOI-Tr, and LMC into the off-the-shelf 2D CNNs, i.e., 2D ResNet, we build up a strong yet efficient video action recognition framework called Event Adaptive Network (EAN). Our models achieve state-of-the-art or competitive results on several large-scale video datasets, i.e., Something-Something V1&amp;V2 <ref type="bibr" target="#b20">[21]</ref>, Kinetics <ref type="bibr" target="#b5">[6]</ref>, and Diving48 <ref type="bibr" target="#b30">[31]</ref>. [50] enhance the 2D CNNs with various temporal modules and achieve promising results. To simultaneously learn the temporal dynamics along with the spatial representations in videos, 3D networks, e.g., C3D network <ref type="bibr" target="#b52">[53]</ref>, I3D <ref type="bibr" target="#b5">[6]</ref>, 3D- Our framework aims to simultaneously model local spatial-temporal information and global object interactions by incorporating two novel modules, i.e., Event Adaptive Block (EAB) and Sparse Object Interaction Transformer (SOI-Tr), into the 2D ResNet backbone CNN. The EAB first perceives the scale of local events and then dynamically synthesizes video-adaptive spatial-temporal kernels from spatial (S) or temporal (T) kernels of fixed scales. The SOI-Tr specializes in global interactions among sparse foreground objects by leveraging a Transformer. Besides, a latent motion code (LMC) module is adopted to efficiently exploit short-term motion information within local segments. Our proposed framework is an end-to-end hybrid model that uses both convolution and self-attention.</p><p>ResNet <ref type="bibr" target="#b21">[22]</ref> [49], R(2+1)D CNNs <ref type="bibr" target="#b53">[54]</ref>, and Slowfast networks <ref type="bibr" target="#b13">[14]</ref>, have also recently gained much attention. Our framework is built upon the 2D CNNs due to their better efficiency.  <ref type="bibr" target="#b7">[8]</ref> in image tasks attempt to dynamically generate aggregation weights and use them to combine a set of convolutional kernels.</p><p>More recently, TANet <ref type="bibr" target="#b33">[34]</ref> generalizes this idea to temporal modeling for the video recognition task. However, the generated temporal kernel is shared across all channels, demonstrating limited modeling capability and performance. In contrast, our method generates the full spatial-temporal kernel, whose parameters are specified for each channel.  <ref type="bibr" target="#b54">[55]</ref> to image/video tasks by unfolding the visual signal or its feature map to a sequence of tokens. Although their global modeling capability is inherently superior to the convolution-based methods, these models are computationallyexpensive due to the dense self-attention mechanism. Similar to us, Girdhar et al. <ref type="bibr" target="#b18">[19]</ref> and Plizzari et al. <ref type="bibr" target="#b39">[40]</ref> also leverage a lightweight Transformer architecture to model the interactions among the selected key regions of the input video. However, they either rely on an object region proposal network (RPN) to produce dense proposal regions or an external computationally-heavy keypoint extractor to locate the human keypoints. In contrast, our object representation is sparse and is produced by a lightweight three-layer CNN.  <ref type="bibr" target="#b57">[58]</ref> propose several lightweight modules to produce task-specific short-term motion representations, which can be jointly optimized with the action recognition network. For example, PAN <ref type="bibr" target="#b66">[67]</ref> proposes to use the difference of the low-level features between the adjacent frames as a novel motion cue named Persistence of Appearance (PA). More recently, TDN <ref type="bibr" target="#b57">[58]</ref> uses a short-term module to map the RGB difference motion signals into compact features and fuse the features with that produced by the backbone network. In contrast, our proposed latent motion code (LMC) module exploits high-order motion information in a latent space, which is more effective and also efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>In this work, we propose a novel video action recognition framework called Event Adaptive Network (EAN), as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. The network is built by inserting several Event Adaptive Blocks (EABs) and a Sparse Object Interaction Transformer (SOI-Tr) into different stages of the 2D ResNet backbone CNN. Moreover, a Latent Motion Code (LMC) module is adopted to exploit the short-term motion information within local video segments. All components in our framework are differentiable and the proposed EAN is endto-end trainable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Event Adaptive Block</head><p>Event Adaptive Block (EAB) aims to generate the spatialtemporal kernel to adaptively model the local cues within the input video, as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. We start from an approximated formulation of the optimal kernel for the video, and then implement the formulation as an efficient block. An approximated formulation for the optimal kernel. Formally, given an input video feature X l with channel number C, which is the output of the l-th (l ? <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref>) stage of the backbone CNN. We first assume that there exists an optimal spatial-temporal kernelF that accurately fits the key elements (i.e., objects and events) of the video. This kernel transforms the input X l into an output tensor Y l of the same shape by convolution:</p><formula xml:id="formula_0">Y l = X l * F .<label>(1)</label></formula><p>Both the scale and the parameters of theF can adapt to videos with different contents. Because the accurate shape ofF is unknown, we cannot easily implement it as a trainable fixed-scale convolution kernel. Instead, we propose to solve the surrogate problem, i.e., approximating the produced Y l . We achieve this by leveraging a group of fixed-scale spatial or temporal convolutions:</p><formula xml:id="formula_1">Y l = ? G i=1 {M ? ? G j=1 X l j * F (2j?1) s } i * F (2i?1) t ,<label>(2)</label></formula><p>where F (2j?1) s and F (2i?1) t represent the spatial convolution with kernel size (2j?1)?(2j?1) and the temporal convolution with kernel size 2i?1, respectively. Each convolution is performed on a group of features for reducing the computation cost. G denotes the group number, ? denotes the channel concatenating operation, and X l j = X l [j ? c : (j + 1) ? c], where c = C/G. ? denotes the channel-wise broadcasting matrix multiplication operation. M denotes the fusion matrix that relates the spatial and temporal convolutions, and (b) A zoom-in of ESP-Net. "S:3?3, d=2" represents a 2D spatial convolution with kernel size 3 and dilation size 2. "T" indicates a 1D temporal convolution. "Max:3" denotes the 3D max-pooling operator with kernel size 3. "Avg", ? and ? denote the average pooling, the element-wise addition, and the broadcasting channel-only matrix multiplication, respectively. "FC" denotes a fully connected layer.</p><p>is estimated by the Event Scale Perceiving Network (ESP-Net):</p><formula xml:id="formula_2">M = ESP-Net(X l ), M ? R C?C .<label>(3)</label></formula><p>As formulated in Eq. (2), M dynamically gates the spatial information flowed into each temporal convolution. By choosing different M , we can mimic the previous handcrafted video architectures. For example, by only activating the matrix elements connecting the spatial and temporal kernels with the same size, the proposed formulation degenerates to the (2+1)D convolutions. Moreover, the multi-scale spatial-only or temporal-only convolutions are also the special cases of it.</p><p>Event Scale Perceiving Network (ESP-Net). It is well known that the scale information is embodied in the spatialtemporal context, which encodes the rich semantics w.r.t the shapes of objects and the dynamics of events. Thus, ESP-Net is implemented as a lightweight 3D network with a small channel number but a large receptive field, as shown in <ref type="figure" target="#fig_1">Fig. 3</ref> (b). Specifically, a 1?1?1 3D convolution layer is first adopted to reduce feature channels of the input tensor X l by 16 times. Then, the video context features are extracted with two 3D convolutions with kernel size 5 ? 5 ? 5 and stride size 2 ? 2 ? 2. Subsequently, the average pooling operation is utilized to only reserve the channel dimension of the tensor, and globally aggregate the event scale information of the input video. Finally, a linear transformation layer followed by a reshaping operation is utilized to produce M . Implementation of EAB. We wrap the above procedure into an Event Adaptive Block (EAB). This block is defined as: Z l = Y l + X l , where Y l is given in Eq. (2) and "+ X l " denotes a residual connection <ref type="bibr" target="#b22">[23]</ref>. The residual connection allows us to insert the proposed block into any pre-trained model such as ResNet, without breaking its initial behavior (e.g., when the weights of the last Conv layer in EAB are initialized as zeros). An example of EAB with maximum receptive field size 5 ? 5 ? 5 is illustrated in <ref type="figure" target="#fig_1">Fig. 3 (a)</ref>. Bottleneck design is introduced for reducing the computation complexity, i.e., we first reduce the feature channel number by four times through 1?1?1 convolutions. The spatial convolutions are followed by batch normalization (BN) and ReLU non-linearity. We also introduce a max-pooling branch as a complement for convolution. To further reduce the parameter and the computational complexity, we replace the convolution of large kernel size with dilated convolution.</p><p>Further discussion with dynamic convolution. Dynamic Convolution <ref type="bibr" target="#b7">[8]</ref> proposes to decouple the dynamic convolution as the attentions over several static convolutions. Nevertheless, our method is dedicated to video data while they are only for image data. In addition to that, there are several other significant differences between our method and them. First, our method is not merely context-adaptive but also scale-adaptive. More concretely, during the kernel generation procedure, dynamic convolution uses a global average pooling (GAP) operation to extract the global context information as the first step. In contrast, we reserve the additional spatial-temporal dimensions and utilize the 3D convolutions to extract the scale information of the objects and events. Second, the convolutions adopted in our method are with various kernel sizes to adapt to the events of various scales, while that in dynamic convolution are with the same kernel size. Third, the element of M in our method is specified for each channel, while the attention weight of dynamic convolution is shared across all channels of the convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sparse Object Interaction Transformer</head><p>The proposed EAB only captures the local information of the video, lacking the global modeling capability. Therefore, we propose a Sparse Object Interaction Transformer (SOI-Tr) to aggregate the local action cues into a global representation, as shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. To make the modeling procedure more accurate for the specific input video, we only mine the interactions among the foreground objects in each frame, which are localized on the fly in the feature space.</p><p>Given the output feature of the 5-th stage of the backbone CNN X 5 ? R C?T ?W ?H , where C denotes the channel number, T denotes the temporal length, W ?H represent the spatial scales, we model SOI-Tr as follows:</p><p>(1) Localizing the foreground objects. The location of each object is represented as a two-dimensional saliency map, whose spatial scale is equivalent to that of X 5 , i.e., W ? H. We use a small fully convolutional network (FCN) termed Saliency-Net to regress the object saliency maps in parallel:</p><formula xml:id="formula_3">O = Saliency-Net(X 5 ), O ? R N ?T ?W ?H ,<label>(4)</label></formula><p>where N denotes the maximum number of the foreground objects in one frame. We empirically set N = 4 as most actions only involve less than four objects. O <ref type="bibr">(2,t)</ref> represents the saliency map for the second object in the t-th frame. E t denotes the positional embedding. We illustrate the feature map as the original RGB frame for better intuitive understanding.</p><p>(2) Pooling the object features. We first denote the saliency map for the n-th object in the t-th frame as O (n,t) ? R W ?H . Then, the feature representation of the object is produced by spatially weighting the input video feature with the saliency map:</p><formula xml:id="formula_4">F (n,t) = SSUM((E t + X 5 t ) O (n,t) ), F (n,t) ? R C ,<label>(5)</label></formula><p>where denotes the broadcasting element-wise multiplication, SSUM denotes the summation across spatial dimensions. E t denotes a learnable spatially positional embedding with the same shape as X 5 t ? R C?W ?H . (3) Modeling the object interactions. With the objectlevel features, we model the global interactions among them using a Transformer:</p><formula xml:id="formula_5">F = Transformer(F ), F ? R N ?T ?C .<label>(6)</label></formula><p>The produced F is with the same shape as F .</p><p>(4) Enhancing the global video representation. Finally, we perform average-pooling on the original video features and the interaction features, yielding the global video representation X . It should be mentioned that the SOI-Tr module also adopts the bottleneck designing with a channel compressing factor of four.</p><p>Saliency-Net. This network is implemented as a lightweight four-layer CNN followed by a spatial Softmax layer, where the first layer reduces the the input channel number by a factor of eight. The second layer is a 3D convolution with kernel size 3?1?1, which detects the moving objects with obvious motions. The third layer is a 2D convolution with a larger spatial kernel size 5?5, which localizes objects more accurately by considering the context information.</p><p>Transformer. The transformer architecture used in our framework is built by stacking two residual blocks, where each block includes a multi-head QKV self-attention module. Different from the vanilla Transformer, we mainly remove the classification token and replace the Layer Normalization with Batch Normalization, following the practices proposed in <ref type="bibr" target="#b43">[44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Latent Motion Code Module</head><p>The proposed EAB and SOI-Tr modules can already extract the action cues from the input video clip effectively. We insert several EABs and a SOI-Tr into the 2D ResNet backbone, building the EAN RGB model, as shown in <ref type="figure" target="#fig_3">Fig. 5</ref> (a). EAN RGB already recognizes the actions from the sparsely sampled video clip effectively. Nevertheless, some subtle action cues are inevitably lost during the sampling procedure. To alleviate this issue, we sample the video clip more densely, and introduce a novel Latent Motion Code (LMC) module to efficiently mine the motion cues within the local segments of this dense clip, as shown in <ref type="figure" target="#fig_3">Fig. 5 (c)</ref>. When equipping EAN RGB with the LMC module, we build an improved EAN RGB+LMC model, as shown in <ref type="figure" target="#fig_3">Fig. 5 (b)</ref>. Although the EAN RGB+LMC takes more frames as input, it is also with high efficiency, due to the adopted latent motion modeling scheme and early feature fusion strategy.</p><p>Frame Sampling Strategy. Following the previous works <ref type="bibr" target="#b58">[59]</ref> [67] <ref type="bibr" target="#b57">[58]</ref>, we uniformly divide the original long video into several groups and then select a segment from each group. Specifically, the input video is first divided into N groups with equal length. N is 8 or 16 for different computational budgets. During the training procedure, five adjacent frames are randomly chosen from each group as a 5-frame segment. The N segments form a dense clip {S t } N t=1 . The first frames of each segment form a sparse clip {S t <ref type="bibr" target="#b0">[1]</ref> } N t=1 .</p><p>Latent Motion Code Module (LMC). This module aims to transform the short-term motion information within each local segment into a single compact motion feature, as shown in <ref type="figure" target="#fig_3">Fig. 5</ref> (c). Given an input segment S t , we model the motion information within it as follows:</p><p>(1) Calculating RGB difference maps. We obtain the lowlevel motion cue, i.e., RGB difference map, by subtracting every two consecutive frames:</p><formula xml:id="formula_6">d t[i] = S t[i] ? S t[i?1] , d t[i] ? R 3?224?224 , i ? [2, 5]. (7)</formula><p>(2) Encoding motion from RGB to latent space. Due to the high redundancy between the consecutive frames, the produced difference map is naturally sparse and contains many near-zero values. To simultaneously improve the compactness of the signal and also filter out the task-unrelated motion information, we use a learnable encoder to transform it into a high-dimensional latent space. Specifically, we divide d t[i] into 7 ? 7 = 49 patches, where each patch is of shape 3 ? 32 ? 32. Then, we compress these threedimensional patches into 128-element latent vectors, and the vectors form a latent map of size 7 ? 7:</p><formula xml:id="formula_7">v t[i] = H e (d t[i] ), v t[i] ? R 128?7?7 ,<label>(8)</label></formula><p>where the encoder H e is implemented as a linear layer that is shared by all patches. The input and output dimensions of the layer are 3 ? 32 ? 32 = 3072 and 128, respectively.</p><p>(3) Modeling high-order motion in latent space. The latent map is with low resolution and thus can be efficiently processed by 3D convolutions:</p><formula xml:id="formula_8">{c t[i] } 5 t=2 = H m ({v t[i] } 5 t=2 ), c t[i] ? R 128?7?7 ,<label>(9)</label></formula><p>where H m is implemented as two stacking 3D convolutions with kernel size 3 and group size 16. We call the produced c t[i] as latent motion code (LMC) because it captures the high-order motion information in the latent space.</p><p>(4) Decoding motion from latent to feature space. Through another linear transformation, LMCs can be decoded into the feature space. Following TDN <ref type="bibr" target="#b57">[58]</ref>, we align the dimensions of the decoded features with the features from the Conv1 stage. Concretely, we decode the vector in each spatial position of LMC into a feature patch of size 16 ? 8 ? 8, and these 7 ? 7 patches form a motion feature map of shape 16 ? 56 ? 56:</p><formula xml:id="formula_9">m t[i] = H d (c t[i] ), m t[i] ? R 16?56?56 ,<label>(10)</label></formula><p>where the decoder H d is implemented as a linear layer with the input dimension of 128 and the output dimension of 16? 8 ? 8, respectively. Finally, the motion feature for segment S t is constructed by stacking the motion feature maps along the channel dimension: <ref type="bibr" target="#b1">[2]</ref> ; m t <ref type="bibr" target="#b2">[3]</ref> ; m t <ref type="bibr" target="#b3">[4]</ref> ; m t <ref type="bibr" target="#b4">[5]</ref> ], m t ? R 64?56?56 . (11)</p><formula xml:id="formula_10">m t = [m t</formula><p>EAN RGB+LMC architecture. As shown in <ref type="figure" target="#fig_3">Fig. 5 (b)</ref>, for each segment S t , we add the motion feature m t produced by the LMC module to the Conv1 feature of the first frame S t <ref type="bibr" target="#b0">[1]</ref> :</p><formula xml:id="formula_11">f t = m t + Conv1(S t[1] ).<label>(12)</label></formula><p>Then, the fused features of each segment are fed to the remained stages of EAN for predicting the action category score:</p><formula xml:id="formula_12">action = Stage2-5({f t } N 1 ),<label>(13)</label></formula><p>where Conv1 and Stage2-5 are indicated in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Datasets. We evaluate our method on several large-scale video datasets with different properties, requiring our models to understand different aspects of action recognition task.</p><p>Something-Something includes V1 <ref type="bibr" target="#b20">[21]</ref> and V2 <ref type="bibr" target="#b37">[38]</ref> versions, which are two large-scale crowd-sourcing video datasets for action recognition. There are about 110k (V1) and 220k (V2) videos covering 174 fine-grained action categories with diverse objects and scenes, focusing on humans performing pre-defined basic actions. In this dataset, the actions are performed with different objects so that models are required to understand the basic actions instead of recognizing the appearance of the objects or the background scenes. Moreover, the spatial and the temporal scales of the objects and the events vary hugely across different videos, as shown in <ref type="figure">Fig. 1</ref>, which is suitable for verifying the flexible spatialtemporal modeling ability of the proposed method.</p><p>Kinetics <ref type="bibr" target="#b5">[6]</ref> is a challenging human action recognition dataset, which contains 400 and 600 human action classes. This dataset includes human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands and hugging. Compared to the temporal reasoning required by the actions in Something-Something, the actions in this dataset heavily rely on the appearance of the objects. We evaluate our models on the trimmed version to evaluate its capacity in modeling the appearances and the interaction among objects. The experiments are conducted on the validation set of Kinetics-400 <ref type="bibr" target="#b5">[6]</ref> because there are many well-known baseline methods.</p><p>Diving48 <ref type="bibr" target="#b30">[31]</ref> includes more than 18K video clips for 48 unambiguous diving classes. This proves to be a challenging task for modern action recognition systems as dives include three stages (takeoff, flight, entry) and thus require modeling of long-term temporal dynamics. This requires both multi-scale temporal modeling and the perceiving of longrange dependencies. Therefore, we conduct experiments on this dataset to verify the multi-scale spatial-temporal modeling ability of our method. We report the accuracy on the first version of the official validation split, which has been adopted by several previous methods. <ref type="bibr" target="#b0">1</ref> Implementation Detail We implement our model in Pytorch, and we adopt ResNet50 <ref type="bibr" target="#b22">[23]</ref> pretrained on ImageNet <ref type="bibr" target="#b10">[11]</ref> as the backbone. Following previous works <ref type="bibr" target="#b28">[29]</ref> [30], we also insert temporal convolutions with kernel size 3 and the motion excitation (ME) module proposed in <ref type="bibr" target="#b29">[30]</ref> before each 3?3 convolutions of bottleneck layers of the original ResNet50, aiming to enhance its basic temporal modeling ability. We also incorporate these changes into all baselines in the ablation study for a fair comparison. The parameters within the EABs and SOI-Tr are randomly initialized. For the spatial dimension of the sampled clips, the short-side of the frames are resized to 256 and then cropped to 224 ? 224. We perform random cropping and flipping as data augmentation during training. It's worth mentioning that we do not perform horizontal flipping on the moving direction related action classes such as "moving something from left to right". We train the network with a batch size of 64 and optimize it using SGD with an initial learning rate of 0.01 for 40 epochs, and decay it by a factor of 10 for every 10 epochs. The total training epochs are about 70. The dropout ratio is set to 0.5. The weight decay is set to 5e ?4 and 1e ?4 for Something/Diving48 and Kinetics-400, respectively. <ref type="bibr" target="#b1">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison with State-of-the-Arts</head><p>Something V1 and V2. We first compare our method with the other state-of-the-art approaches on Something V1 and Something V2 datasets, as shown in Tab. 1. The previous approaches are divided into four groups: 3D CNNs, object interaction modeling enhanced 3D CNNs, 2D CNNs, and 2D CNNs enhanced with short-term motion representation.</p><p>Our method outperforms all methods built with 3D convolutions and meanwhile achieves higher efficiency. For ex- <ref type="table">Table 1</ref>: Comparison to state-of-the-arts on Something-Something V1&amp;V2 datasets. Following TDN <ref type="bibr" target="#b57">[58]</ref>, we adopt the 1clip and center-crop inference scheme where only a center crop of 224?224 from a single clip is used for evaluation. <ref type="bibr">8F</ref> and 16F indicate the sampling segment number of the input video is 8 and 16, respectively. The result of EAN En(RGB) is produced by averaging the predicted action scores from the EAN 8F(RGB) and EAN 16F(RGB) models, which follows TSM <ref type="bibr" target="#b31">[32]</ref>. ? indicates the paper didn't provide the results. We also compare our method with the two methods <ref type="bibr" target="#b38">[39]</ref> [62] that first detect the objects of the input frames in the RGB space and then model the object interactions. Although we do not use the pretrained object detector or extra object bounding box annotations to get the proposal regions, our method still significantly outperforms them. Specifically, our improvements over GCN + Non-local <ref type="bibr" target="#b61">[62]</ref> and I3D + STIN + OIE <ref type="bibr" target="#b38">[39]</ref> are 11.1% (on Something V1) and 8.6% (on Something V2), respectively, in terms of the Top1 recogni-tion accuracy. This proves the superiority of the end-to-end object detection scheme and the Transformer architecture adopted in our method.</p><p>As for the 2D CNN-based methods, we compare our EAN RGB architecture with them for a fair comparison, where only one frame is sampled from each segment. Our models achieve the best performance under all settings of different input frame numbers. The performances of TSN and TRN are relatively inferior to other methods because both the two methods only model the temporal information upon the highest-level feature maps from the backbone CNN. TEA is superior to all other 2D CNNs because it explores multi- <ref type="table">Table 2</ref>: Comparison to state-of-the-arts on Kinetics-400 dataset. Following TDN <ref type="bibr" target="#b57">[58]</ref>, we adopt the 10-clip and 3-crop inference scheme where three crops of 256?256 frames and 10 clips are used for testing. Therefore, the computational cost of the same model here is 30? heavier than that on Something datasets. ? indicates the paper didn't provide the results. scale spatial-temporal information. Compared with TEA, our method outperforms it consistently with the different input frame numbers. When using 8 and 16 input frames, the improvements are 3.0% and 1.5% on Something V1 dataset. The reason is that the multi-scale architecture of TEA is based on the hand-crafted Res2Net, which is static and not adaptive to the video. In contrast, the spatial-temporal modeling architecture of our method is dynamic and adaptive.</p><p>We further compare the improved EAN RGB+LMC architecture with the other recent 2D CNNs that also take advantage of the short-term motion information, where 5 adjacent frames are sampled from each segment. Compared with the optical flow-based methods, i.e., TRN RGB+Flow and TSM RGB+Flow , our smallest model EAN 8F(RGB+LMC) already outperforms them by 11.2% and 0.6%, respectively. It's worth noting that the computational complexity of our LMC motion feature produced from the input video of 40 frames is only 1.1 GFLOPs, while the computational complexity of FlowNet2.0 <ref type="bibr" target="#b23">[24]</ref> is 2006 GFLOPs for the same video. In other words, the proposed LMC module is 1823? more efficient than optical flow modality, while achieving better performance for the action recognition task.</p><p>By averaging the predictions from EAN 8F(RGB+LMC) and EAN 16F(RGB+LMC) , the resulted model EAN <ref type="bibr">En(RGB+LMC)</ref> boosts the action recognition performance to a new state-ofthe-art level, i.e., 57.2% ( +2.1%) on Something V1 and 68.8% ( +1.8%) on Something V2, when using the recent method TDN as the anchor. Compared with the PAN model, the improvement of our method is 1.9% on Something V1, even though that PAN adopts a much heavier backbone network, i.e., 2D-ResNet101. Furthermore, we summarize the key differences among our adopted ResNet baseline, our variant models and other recent relevant methods, as shown in Tab. 3. Particularly, TDN also uses a short-term temporal difference module (SDM) to exploit short-term motion information in the low-level feature space, and fuse the motion features into the backbone in the early stage. Nevertheless, our method outperforms TDN by 1.1% on Something V1. The consistent improvements of our method over the other methods strongly justify the superiority of the proposed event scale adaptive spatial-temporal modeling paradigm by EAB, sparse object interaction modeling scheme by SOI-Tr, and high-order motion representation in latent space by LMC.</p><p>Kinetics-400. To verify that our method also effectively captures rich object appearance cues and the interactions among them, we compare our method with other state-ofthe-art results on the Kinetics-400, as shown in Tab. 2. When <ref type="table">Table 3</ref>: Comparison of different models in terms of input clip setting and their key modules, i.e., local spatialtemporal modeling module, global aggregation module and short-term motion modeling module. PA, TSM and ME indicate the appearance of persistence module <ref type="bibr" target="#b66">[67]</ref>, the temporal shift module <ref type="bibr" target="#b31">[32]</ref> and the motion excitation module <ref type="bibr" target="#b29">[30]</ref>, respectively. L/SDM represents the long/short-term temporal difference modules in TDN <ref type="bibr" target="#b57">[58]</ref>. AVG indicates the spatial-temporal average pooling operation. The model performances on Something V1 are also reported. compared with the methods based on 2D CNNs, our method outperforms all of them when using the same backbone network, and demonstrates a better trade-off between the action recognition accuracy and the computational complexity. For example, when equipped with the same ResNet-50 backbone, our method outperforms the recent method TDN by 0.6%. When adopting the ResNet-101 backbone, TDN shows the strongest result among all 2D CNNs. Nevertheless, this also increases the computation cost of TDN, which is even close to the 3D CNN method, i.e., SlowFast + NL network. Our EAN models achieve the best complexity performance trade-off among all state-of-the-art methods.</p><p>Diving48. To prove that our method can model subtle fine-grained motion cues, we test our method on Diving48. This dataset requires modeling the subtle body motions in long-short terms and includes much fewer videos compared with Something-Something and Kinetics. We input 16 frames <ref type="table">Table 4</ref>: Comparison to state-of-the-arts on Diving48. We adopt the single clip or twice clips inference schemes where a center crop of 224?224 from a single clip or twice clips is used. ? indicates the paper didn't provide the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Pre to the network and sample two clips from the video during inference. The results are shown in Tab. 4. Our method outperforms the recent state-of-the-art GST <ref type="bibr" target="#b35">[36]</ref> when using single clip ( +1.6%) or twice clips ( +2.9%) as the input videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Studies for EAN</head><p>We conduct extensive ablation studies on Something V1 <ref type="bibr" target="#b20">[21]</ref> dataset to demonstrate the superiority of the proposed framework by answering the following questions. The variant models in this section are derived from the EAN 8F(RGB) model. The input clip is always with 8 frames. Q1: Are the proposed EAB and SOI-Tr effective and necessary? As mentioned in Sec. 3, in our framework, the EAB extracts more accurate local spatial-temporal representation and the SOI-Tr derives global object interaction representation from the video. To confirm that both two representations are effective and necessary for a high-performance action recognition framework, we conduct ablation experiments. Specifically, we equip ResNet baseline with the two proposed modules separately and analyze their impact on the performance. As shown in Tab. 5, both the two modules demonstrate strong video modeling capability. When the ResNet baseline is enhanced with the EABs, the Top1 accuracy is significantly improved by 2.2%. The reason is that the features extracted by the ResNet baseline are not accurate enough, and the proposed EABs can refine the features with the dynamic spatial-temporal kernel. For a more intuitive understanding, we will visualize the refined feature maps by our method in section 4.3. Then, we observe that the ResNet + SOI-Tr baseline also outperforms the original ResNet baseline by 0.7% in terms of Top1 accuracy, while only introducing an extra 0.7 GFLOPs computation cost. Finally, simultaneously using EAB and SOI-Tr boosts the performance to 51.9%, which proves the complementarity of the two proposed modules.</p><p>We also plot the top 5 classes that are significantly improved after introducing the SOI-Tr. As shown in <ref type="figure">Fig. 6</ref>, we find that the most improved instances can be roughly divided into two groups: (a) The instances that require tracking the state of a certain object over the whole clip, such as the <ref type="figure">Fig. 6</ref>: The top 5 action classes that are significantly improved after introducing the SOI-Tr module. <ref type="figure">Fig. 7</ref>: Visualization of one video clip from the most improved category by the SOI-Tr. The input clip is first processed by EABs to obtain the spatial-temporal feature map X 5 . Then, SOI-Tr calculates the saliency map O of the most concentrated object and the interactions of this object across the temporal axis. We take the 4th frame as the anchor and show the attention vector ?.</p><p>videos of "Lifting a surface ... " and "Pulling two ends ... ". (b) The instances that contain multiple objects and the interactions between them, such as the videos of "Pretending to put ... ". This is aligned with the motivation of introducing SOI-Tr, i.e., accurately modeling the long-range object interactions benefits the recognition of some complex actions.</p><p>To systematically understand how the EABs and SOI-Tr improve the recognition performance, we randomly select one video from the category "Lifting a surface with something on it but not enough for it to slide down" and visualize it. In <ref type="figure">Fig. 7</ref>, we can clearly see that the original feature X 5 before global modeling concentrates on the background or the board, omitting the main object, i.e., the small sliding box. This makes sense because both the spatial area and the motion magnitude of the board are more obvious than the small box. After introducing the SOI-Tr, the object detector first finds the main object. Then, the Transformer model builds the long-range dependencies across the whole clip. We also notice that the board in the first frame is also detected. But, this background object will be neglected in the self-attention model because its weight is only 0.11. Q2: Where to insert the proposed modules? We perform an ablation study on which stage to use local operator (EAB) and global operator (SOI-Tr). The results are shown in Tab. 6. From these results, we see that adding more EABs into the main network only slightly increases the computational cost due to the high efficiency of the bottleneck designing and group convolution. When some EABs are replaced with the SOI-Tr, the performance decreases consistently. This implies that the local spatial-temporal information is crucial for action recognition, which cannot be substituted by the high-level object interaction information. We also try to build the network only with EABs, the result is also inferior to the original hybrid model (convolution+selfattention). The setting of using EAB after stage 1?4 and SOI-Tr after stage 5 obtains the best recognition accuracy and is with reasonable complexity.</p><p>Q3: Is the prior assumption of SOI-Tr reasonable? To prove the end-to-end foreground object detector and the sparsity assumption for object interactions are both important for SOI-Tr, we train other variant models where we replace our detected object regions with the same number of the fixed regions or the regions detected by a pre-trained Faster RCNN <ref type="bibr" target="#b41">[42]</ref> model. When the number of the boxes output from Faster RCNN is too small, we pad it with the central region of the frames. The performances of the models are compared in Tab. 7. First, we notice that building the interaction model upon the fixed regions already slightly improves the performance, proving that the interaction modeling is beneficial to the action recognition. Then we use the Faster RCNN detector to predict more accurate foreground regions. Surprisingly, the performance improvement is negligible. This may be ascribed to the fact that most frames only contain one or two objects, which cover fewer regions compared with the "fixed region" scheme. In contrast, the Saliency-Net embedded in our method always detects enough salient regions in an end-to-end manner and obtains the best performance, i.e., 51.9%. Also, it is computationally efficient due to the shared feature extractor with the other parts of the framework. We emphasize that our embedded Saliency-Net outperforms Faster RCNN by 0.8% while running 118? faster.</p><p>We further try to leverage all positions to build a dense interaction model, as shown in the penultimate row of Tab. 7. However, the performance is obviously inferior to our method. This strongly supports our assumption that most regions are only background noises for the final prediction and leveraging all of them will deteriorate the final performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Further Studies for EAB</head><p>In this section, we make further studies on the aspects that impact the effectiveness of EAB.</p><p>Large receptive field and multi-scale modeling are important. To verify this, we introduce the following baselines: <ref type="figure">Fig. 8</ref>: Illustrations of the baseline spatial-temporal blocks. The representation signs are the same meaning as <ref type="figure" target="#fig_1">Fig. 3</ref>.</p><p>(1) S-Block. It is implemented with a (2+1)D convolution with kernel size 3 ? 3 ? 3 and group size 3, as shown in <ref type="figure">Fig. 8 (a)</ref>, which only captures single-scale features with a small receptive field.</p><p>(2) L-Block. It is implemented with a (2+1)D convolution with kernel size 3 ? 3 ? 3, group size 3, and dilation size 2 ? 2 ? 2, as shown in <ref type="figure">Fig. 8 (b)</ref>, which captures singlescale features with a larger receptive field.</p><p>(3) Incep-Block. It is implemented with a group of (2+1)D convolutions in an Inception-style, as shown in <ref type="figure">Fig. 8 (c)</ref>, which captures multi-scale features with a larger receptive field. The only difference between this baseline and EAB is that the M is replaced with an identity mapping operation.</p><p>We compare our method with the proposed baseline methods in Tab. 8. First, it can be seen that EAB outperforms the S-Block baseline by a large margin (51.9% vs. 49.6%). The improvement is originated from two aspects: (1) The large spatial-temporal kernel within EAB enables the larger receptive field and aggregates more local information. <ref type="formula" target="#formula_1">(2)</ref> The explicit multi-scale modeling introduces richer feature representation. It is necessary to validate the independent contribution from the two aspects. We first compare the S-Block baseline with the L-Block baseline. L-Block has a larger spatial-temporal receptive field size but the same number of parameters. We can see that the recognition accuracy is improved by 0.7%. Then, we build the Incep-Block baseline by enhancing the L-Block baseline with multi-scale modeling capability. This improvement further improves the recognition accuracy. From the comparisons above, we verify that both the two aspects facilitate the action task, and multiscale architecture fully exploits the large receptive field. <ref type="figure">Fig. 9</ref>: Comparison of the features from Stage4 of EAN and the evolution of the prediction scores. The proposed EAB can discover more semantically-aligned regions for actions, and also suppress the noisy information from backgrounds to yield correct prediction. Zoom in for better visualization.</p><p>We randomly select one video from Something V1 dataset and visualize the 8?14?14 feature map output from Stage4 (this is before the inserting of the SOI-Tr), as shown in <ref type="figure">Fig. 9</ref>. It clearly demonstrates that our method can discover more semantically consistent regions for actions, and in the meantime reduce noisy backgrounds for correct prediction. Moreover, the feature activation heatmaps of our method are better spatially aligned with the target object (see the water). We also show the state evolution process in <ref type="figure">Fig. 9</ref>. Interestingly, our method detects the start and end points of actions although only trained with classification labels.</p><p>Dynamic architecture matters. From Tab. 8, we notice that the performance gap between the Incep-Block baseline and EAB is still rather large, i.e., 1.1% Top1 accuracy. We conjecture this is due to the dynamic architecture of EAB.</p><p>As mentioned in section 3.1, the inference pathway for EAB is determined by the kernel fusion matrix M . For more detailed analysis, we propose two kernel fusion strategies: (1) Channel Shuffle. We replace M with a conventional fusion method, i.e., Channel Shuffle operation <ref type="bibr" target="#b67">[68]</ref>, which enables the communication of the features of different groups.</p><p>(2) Static Matrix. The M is a learnable matrix during training. But, it's a fixed matrix during inference. Both the above two baselines belong to the static architecture but they are similar to the EAB in terms of the network details, which are perfect for studying the impact of dynamic modeling. We compare their performances in Tab. 9. We observe that the performance improves consistently with a more complex kernel fusion strategy, i.e., Identity ?Channel Shuffle ?Static Matrix ?Dynamic Matrix. The Dynamic fusion matrix adopted by EAB shows the best performance (51.9%) with negligible extra cost.</p><p>Kernel visualization. To verify that the dynamic kernel fusion matrix M of EAB is indeed adaptive to the scales of the main objects and the key events within different videos, we conduct a group of experiments by augmenting one anchor video and observing the change of the weights of the fixed-scale kernels. The augmented videos and the kernel weight changing procedure are illustrated in <ref type="figure">Fig. 10</ref>. We first see, both the weight distributions along the temporal axis or the spatial axis are not sparse, i.e., all kernels are activated. This supports our assumption that the optimal spatialtemporal kernel for the video is with an unknown complex shape and cannot be accurately replaced by one kernel of fixed-scale. Also, the distributions do not follow some simple distributions such as Uniform or Gaussian, indicating that the kernel weights cannot be trivially hand-crafted and are required to be learned from data. When we spatially zoom in the anchor video by 1.6?, the main objects in the video, i.e., the hand and the stick, are easier to be discovered, we see that EAB is more inclined to exploit the spatial convolutions of small kernels such as that of size 1?1 instead of that of size 5?5 . Similarly, when we sample the frames with 2? higher frame-rate, the object motions become slower and the small temporal convolutions such as that with kernel size 1 are fully used.</p><p>Studies on EAB details. In this part, we conduct experiments to verify whether all the designs of EAB contribute <ref type="figure">Fig. 10</ref>: Visualizing the dynamic kernel fusion matrix M of the proposed EAB via the kernel weights. For each spatial or temporal kernel, its weight is computed by summing the matrix values connected to it. In the first row, we give an anchor video. Below it, we show the impact to the kernel weights by changing the spatial or temporal scales of the anchor video. The kernel weights of anchor, spatially-, and temporally-augmented videos are indicated by gray, green, and brown bars, respectively. "S-3"&amp;"T-3" denotes the fixed-size spatial&amp;temporal kernel of size 3?3&amp;3. to the final performance. As shown in Tab. 10, the max pooling operation significantly improves the performance (1.3% w.r.t Top1 accuracy), and meanwhile our method is not sensitive to specific implementation of this operation. Both average pooling and max pooling operators achieve excellent performance. Max pooling demonstrates a slight advantage over average pooling because the regions of the key objects and frames related to action only cover a small proportion of the input video data. Also, we find that the extra nonlinearity introduced by the intermediate ReLU operations between spatial-and temporal-filters also benefit the performance, which is consistent with the conclusion from previous work <ref type="bibr" target="#b53">[54]</ref>. Besides, we demonstrate that the dilated convolution achieves comparable performance with the or-dinary convolution while it is much more efficient. Finally, we also try to decompose the 2D spatial convolution into two stacking 1D convolutions. But this brings a slight performance drop. To summarize, the extensive experiments in this section prove the necessity of detailed designs in EAB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Erroneous Cases and Limitations</head><p>Although the quantitative results on standard benchmarks and the extensive analysis above have verified the effectiveness of the proposed framework, it inevitably has some limitations, which lead to erroneous recognition results. One limitation is caused by the simple architecture of ESP-Net within EAB. ESP-Net is responsible for perceiving the event scales within the input video, composed of two convolution layers followed by a global average pooling operation. Although this simple "average" operation is lightweight in terms of the computational cost, it also makes the statistical results of the video biased to the large objects. As shown in <ref type="figure" target="#fig_4">Fig. 11</ref>, the feature activations are dominated by the large-area human hand shadow, neglecting the real objects (the human hand and the charger) involved in the action plugging something into something. Another limitation is originated from the proposed SOI-Tr. The adaptiveness of SOI-Tr lies in detecting different foreground objects for different input videos. Nevertheless, the adaptiveness may be limited by the representation of the objects, i.e., points in the feature map, which correspond to fixed-size regions within the input video. Therefore, the granularity and the scale of the detected foreground objects are not flexible enough. As shown in <ref type="figure" target="#fig_0">Fig. 12</ref>, only small parts of the towel can be detected. Therefore, the global state folding of the towel can not be perceived. Instead, the local states of the wrongly attended objects, i.e., the human hand and the partial towel, contribute to the wrong prediction touching part of something. <ref type="figure" target="#fig_0">Fig. 12</ref>: Recognition error caused by SOI-Tr. We visualize the foreground object distribution maps, where only parts of the towel are detected in this case. Therefore, the global state folding of the towel can not be perceived, resulting the biased action recognition result touching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Works</head><p>To model the spatial-temporal scale variances and the longrange object interactions in videos, we propose to dynamically generate the video-adaptive kernels from the input video and model the interactions among the objects with a Transformer. Moreover, we design a novel short-term motion representation to further enhance the performance of our method. We perform extensive evaluations to study the effectiveness of the proposed approach on video action recognition task, and the results demonstrate that our models achieve impressive performances on Something-Something V1/V2, Kinetics-400, and Diving48 datasets. In the future, we will explore how to better approximate the video-adaptive kernel. As for the network architecture, we will investigate more powerful backbone networks. We also plan to extend the proposed framework to more downstream video tasks such as the spatial-temporal action localization task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Event Adaptive Network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>(a) EAB of maximum receptive filed size 5?5?5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>The architecture of the proposed Sparse Object Interaction Transformer (SOI-Tr).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>(a) EAN RGB model only takes the sparsely sampled clip as the input. (b) EAN RGB+LMC model takes both the densely sampled clip and the sparse clip as the input. (c) Zooming-in of the Latent Motion Code (LMC) module, which transforms a local video segment S t to a compact motion feature m t .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 11 :</head><label>11</label><figDesc>Recognition error caused by EAB. The shadow of the human hand instead of the real hand and the charger is attended, causing the recognition result changing from plugging something into something to moving part of something.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>tianyuan,zhaiguangtao,zhiyong.gao}@sjtu.edu.cn. Y. Yan is with the AI Institute, Shanghai Jiao Tong University, Shanghai, China. E-mail: yanyichao@sjtu.edu.cn. G. Guo is with Baidu. E-mail: guoguodong01@baidu.com. denotes the corresponding author. (a) Pushing something so that it falls off the table (b) Moving something towards the camera</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">are with the Institute of</cell></row><row><cell cols="7">Image Communication and Network Engineering, Shang-</cell></row><row><cell>hai</cell><cell>Jiao</cell><cell>Tong</cell><cell>University,</cell><cell>Shanghai,</cell><cell>China.</cell><cell>E-mail:</cell></row></table><note>{ee</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>8% higher Top1 accuracy (44.4% vs. 53.2% on Something V1) with only ? 11% computational cost.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="3">Pre-train Frames GFLOPs</cell><cell cols="4">Something V1 Top1 (%) Top5 (%) Top1 (%) Top5 (%) Something V2</cell></row><row><cell>3D CNNs:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>I3D [6]</cell><cell>3D-ResNet50</cell><cell>Kinetics</cell><cell>32?2</cell><cell>306</cell><cell>41.6</cell><cell>72.2</cell><cell>-</cell><cell>-</cell></row><row><cell>Non-local I3D [61]</cell><cell>3D-ResNet50</cell><cell>Kinetics</cell><cell>32?2</cell><cell>336</cell><cell>44.4</cell><cell>76.0</cell><cell>-</cell><cell>-</cell></row><row><cell>ECO(En) [71]</cell><cell cols="2">BNInc+3D-ResNet18 Kinetics</cell><cell>92</cell><cell>267</cell><cell>46.4</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>S3D-G [64]</cell><cell>InceptionV1</cell><cell>ImageNet</cell><cell>64</cell><cell>71</cell><cell>48.2</cell><cell>78.7</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">3D CNNs + Object interaction:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GCN + Non-local [62]</cell><cell>3D-ResNet50</cell><cell>Kinetics</cell><cell>32?2</cell><cell>606</cell><cell>46.1</cell><cell>76.8</cell><cell>-</cell><cell>-</cell></row><row><cell>I3D + STIN + OIE [39]</cell><cell>I3D</cell><cell>Kinetics</cell><cell>32</cell><cell>154</cell><cell>-</cell><cell>-</cell><cell>60.2</cell><cell>84.4</cell></row><row><cell>2D CNNs:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>TSN [59]</cell><cell>BN-Inception</cell><cell>ImageNet</cell><cell>8</cell><cell>16</cell><cell>19.5</cell><cell>-</cell><cell>33.4</cell><cell>-</cell></row><row><cell>MultiScale TRN [70]</cell><cell>BN-Inception</cell><cell>ImageNet</cell><cell>8</cell><cell>16</cell><cell>34.4</cell><cell>63.2</cell><cell>48.8</cell><cell>77.6</cell></row><row><cell>TSM 8F [32]</cell><cell>ResNet-50</cell><cell>Kinetics</cell><cell>8</cell><cell>33</cell><cell>45.6</cell><cell>74.2</cell><cell>58.8</cell><cell>85.4</cell></row><row><cell>TSM 16F [32]</cell><cell>ResNet-50</cell><cell>Kinetics</cell><cell>16</cell><cell>65</cell><cell>47.2</cell><cell>77.1</cell><cell>63.4</cell><cell>88.5</cell></row><row><cell>TANet 8F [34]</cell><cell>ResNet-50</cell><cell>ImageNet</cell><cell>8</cell><cell>33</cell><cell>46.5</cell><cell>75.8</cell><cell>60.5</cell><cell>86.2</cell></row><row><cell>TANet 16F [34]</cell><cell>ResNet-50</cell><cell>ImageNet</cell><cell>16</cell><cell>66</cell><cell>47.6</cell><cell>77.7</cell><cell>62.5</cell><cell>87.6</cell></row><row><cell>TANet En [34]</cell><cell>ResNet-50</cell><cell>ImageNet</cell><cell>8+16</cell><cell>99</cell><cell>50.6</cell><cell>79.3</cell><cell>-</cell><cell>-</cell></row><row><cell>TEINet 8F [33]</cell><cell>ResNet-50</cell><cell>ImageNet</cell><cell>8</cell><cell>33</cell><cell>47.4</cell><cell>-</cell><cell>61.3</cell><cell>-</cell></row><row><cell>TEINet 16F [33]</cell><cell>ResNet-50</cell><cell>ImageNet</cell><cell>16</cell><cell>66</cell><cell>49.9</cell><cell>-</cell><cell>62.1</cell><cell>-</cell></row><row><cell>TEINet En [33]</cell><cell>ResNet-50</cell><cell>ImageNet</cell><cell>8+16</cell><cell>99</cell><cell>52.5</cell><cell>-</cell><cell>65.5</cell><cell>89.8</cell></row><row><cell>STM [26]</cell><cell>ResNet-50</cell><cell>ImageNet</cell><cell>8?30</cell><cell>990</cell><cell>49.2</cell><cell>79.3</cell><cell>62.3</cell><cell>88.8</cell></row><row><cell>STM [26]</cell><cell>ResNet-50</cell><cell cols="2">ImageNet 16?30</cell><cell>2010</cell><cell>50.7</cell><cell>80.4</cell><cell>64.2</cell><cell>89.8</cell></row><row><cell>GST 8F [36]</cell><cell>ResNet-50</cell><cell>ImageNet</cell><cell>8</cell><cell>29</cell><cell>47.0</cell><cell>76.1</cell><cell>-</cell><cell>-</cell></row><row><cell>GST 16F [36]</cell><cell>ResNet-50</cell><cell>ImageNet</cell><cell>16</cell><cell>59</cell><cell>48.6</cell><cell>77.9</cell><cell>62.6</cell><cell>87.9</cell></row><row><cell>TEA 8F [30]</cell><cell>ResNet-50</cell><cell>ImageNet</cell><cell>8</cell><cell>35</cell><cell>48.9</cell><cell>78.1</cell><cell>-</cell><cell>-</cell></row><row><cell>TEA 16F [30]</cell><cell>ResNet-50</cell><cell>ImageNet</cell><cell>16</cell><cell>70</cell><cell>51.9</cell><cell>80.3</cell><cell>-</cell><cell>-</cell></row><row><cell>TEA [30]</cell><cell>ResNet-50</cell><cell cols="2">ImageNet 16?30</cell><cell>2100</cell><cell>52.3</cell><cell>81.9</cell><cell>65.1</cell><cell>89.9</cell></row><row><cell>EAN 8F(RGB) (Ours)</cell><cell>ResNet-50</cell><cell>ImageNet</cell><cell>8</cell><cell>36</cell><cell>51.9</cell><cell>79.5</cell><cell>63.5</cell><cell>88.2</cell></row><row><cell>EAN 16F(RGB) (Ours)</cell><cell>ResNet-50</cell><cell>ImageNet</cell><cell>16</cell><cell>72</cell><cell>53.4</cell><cell>81.4</cell><cell>64.6</cell><cell>89.1</cell></row><row><cell>EAN En(RGB) (Ours)</cell><cell>ResNet-50</cell><cell>ImageNet</cell><cell>8+16</cell><cell>108</cell><cell>55.8</cell><cell>83.1</cell><cell>66.6</cell><cell>89.9</cell></row><row><cell cols="2">2D CNNs + Short-term motion:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>TRN RGB+Flow [70]</cell><cell>BN-Inception</cell><cell>ImageNet</cell><cell>8?7</cell><cell>-</cell><cell>42.0</cell><cell>-</cell><cell>55.5</cell><cell>83.1</cell></row><row><cell>TSM RGB+Flow [32]</cell><cell>ResNet-50</cell><cell>ImageNet</cell><cell>16?7</cell><cell>-</cell><cell>52.6</cell><cell>81.9</cell><cell>66.0</cell><cell>90.5</cell></row><row><cell>PAN 8F(RGB+PAN) [67]</cell><cell>ResNet-50</cell><cell>ImageNet</cell><cell>8?5</cell><cell>68</cell><cell>50.5</cell><cell>79.2</cell><cell>63.8</cell><cell>88.6</cell></row><row><cell>PAN [67]</cell><cell>ResNet-101</cell><cell cols="2">ImageNet (8?5)?2</cell><cell>503</cell><cell>55.3</cell><cell>82.8</cell><cell>66.5</cell><cell>90.6</cell></row><row><cell>TDN 8F(RGB+SDM) [58]</cell><cell>ResNet-50</cell><cell>ImageNet</cell><cell>8?5</cell><cell>36</cell><cell>52.3</cell><cell>80.6</cell><cell>64.0</cell><cell>88.8</cell></row><row><cell>TDN 16F(RGB+SDM) [58]</cell><cell>ResNet-50</cell><cell>ImageNet</cell><cell>16?5</cell><cell>72</cell><cell>53.9</cell><cell>82.1</cell><cell>65.3</cell><cell>89.5</cell></row><row><cell>TDN En(RGB+SDM) [58]</cell><cell>ResNet-50</cell><cell cols="2">ImageNet (8+16)?5</cell><cell>108</cell><cell>55.1</cell><cell>82.9</cell><cell>67.0</cell><cell>90.3</cell></row><row><cell>EAN 8F(RGB+LMC) (Ours)</cell><cell>ResNet-50</cell><cell>ImageNet</cell><cell>8?5</cell><cell>37</cell><cell>53.4</cell><cell>81.1</cell><cell>65.2</cell><cell>89.4</cell></row><row><cell>EAN 16F(RGB+LMC) (Ours)</cell><cell>ResNet-50</cell><cell>ImageNet</cell><cell>16?5</cell><cell>74</cell><cell>54.7</cell><cell>82.3</cell><cell>66.6</cell><cell>90.3</cell></row><row><cell>EAN En(RGB+LMC) (Ours)</cell><cell>ResNet-50</cell><cell cols="2">ImageNet (8+16)?5</cell><cell>111</cell><cell>57.2</cell><cell>83.9</cell><cell>68.8</cell><cell>91.4</cell></row><row><cell cols="3">ample, compared with Non-local I3D [61], our EAN 8F(RGB+LMC)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>model achieves 8.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Comparison of the performance of using different spatial-temporal modeling modules.</figDesc><table><row><cell>Method</cell><cell cols="2">Param FLOPs</cell><cell cols="2">Something V1 Top1 (%) Top5 (%)</cell></row><row><cell>ResNet baseline</cell><cell>24.0M</cell><cell>33.1G</cell><cell>48.6</cell><cell>77.5</cell></row><row><cell>ResNet+EABs</cell><cell>29.5M</cell><cell>35.3G</cell><cell>50.8</cell><cell>78.4</cell></row><row><cell>ResNet+SOI-Tr</cell><cell>30.3M</cell><cell>33.8G</cell><cell>49.3</cell><cell>77.9</cell></row><row><cell cols="2">ResNet+EABs+SOI-Tr 36.0M</cell><cell>36.1G</cell><cell>51.9</cell><cell>79.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Study on the location of EAB and SOI-Tr.</figDesc><table><row><cell>EAB</cell><cell>SOI-Tr</cell><cell cols="2">Param FLOPs</cell><cell cols="2">Something V1 Top1 (%) Top5 (%)</cell></row><row><cell cols="3">Stage 1?2 Stage 3?5 35.6M</cell><cell>35.8G</cell><cell>49.4</cell><cell>78.2</cell></row><row><cell cols="3">Stage 1?3 Stage 4?5 34.8M</cell><cell>35.9G</cell><cell>50.4</cell><cell>79.2</cell></row><row><cell>Stage 1?4</cell><cell>Stage 5</cell><cell>36.0M</cell><cell>36.1G</cell><cell>51.9</cell><cell>79.5</cell></row><row><cell>Stage 1?5</cell><cell>-</cell><cell>65.8M</cell><cell>36.2G</cell><cell>50.8</cell><cell>79.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Study on various object region sampling strategies.</figDesc><table><row><cell>Regions</cell><cell>FLOPs</cell><cell cols="2">Something V1 Top1 (%) Top5 (%)</cell></row><row><cell>None</cell><cell>35.3G</cell><cell>50.8</cell><cell>78.4</cell></row><row><cell>Fixed</cell><cell>35.8G</cell><cell>50.9</cell><cell>78.4</cell></row><row><cell>Faster RCNN</cell><cell>71.3G</cell><cell>51.1</cell><cell>78.7</cell></row><row><cell>All</cell><cell>36.4G</cell><cell>51.3</cell><cell>79.1</cell></row><row><cell>Our Det-Net</cell><cell>36.1G</cell><cell>51.9</cell><cell>79.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Comparison of different spatial-temporal blocks. RFS denotes the receptive field size.</figDesc><table><row><cell>Models</cell><cell>RFS</cell><cell>Multi scale?</cell><cell>Param FLOPs</cell><cell cols="2">Something V1 Top1 (%) Top5 (%)</cell></row><row><cell>Only SOI-Tr</cell><cell>-</cell><cell>-</cell><cell>30.3M 33.8G</cell><cell>49.3</cell><cell>77.9</cell></row><row><cell>+S-Block</cell><cell>3 ? 3 ? 3</cell><cell>-</cell><cell>30.9M 36.1G</cell><cell>49.6</cell><cell>78.1</cell></row><row><cell>+L-Block</cell><cell>5 ? 5 ? 5</cell><cell>-</cell><cell>30.9M 36.1G</cell><cell>50.3</cell><cell>78.4</cell></row><row><cell cols="2">+Incep-Block 5 ? 5 ? 5</cell><cell></cell><cell>30.9M 36.0G</cell><cell>50.8</cell><cell>78.8</cell></row><row><cell>+EAB</cell><cell>5 ? 5 ? 5</cell><cell></cell><cell>36.0M 36.1G</cell><cell>51.9</cell><cell>79.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>Comparison of different feature fusion methods.</figDesc><table><row><cell>Methods</cell><cell cols="2">Param FLOPs</cell><cell cols="2">Something V1 Top1 (%) Top5 (%)</cell></row><row><cell cols="2">Identity (Incep-Block) 30.9M</cell><cell>36.0G</cell><cell>50.8</cell><cell>78.8</cell></row><row><cell>Channel Shuffle</cell><cell>30.9M</cell><cell>35.9G</cell><cell>51.1</cell><cell>78.9</cell></row><row><cell>Static Matrix</cell><cell>30.9M</cell><cell>36.0G</cell><cell>51.3</cell><cell>79.2</cell></row><row><cell>Dynamic Matrix</cell><cell>36.0M</cell><cell>36.1G</cell><cell>51.9</cell><cell>79.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc>Ablation on detailed designs of EAB.</figDesc><table><row><cell>Design</cell><cell>Param</cell><cell>FLOPs</cell><cell cols="2">Something V1 Top1 (%) Top5 (%)</cell></row><row><cell>Without Max Pool</cell><cell>36.0M</cell><cell>36.1G</cell><cell>50.6</cell><cell>78.4</cell></row><row><cell>Avg Pool</cell><cell>36.0M</cell><cell>36.1G</cell><cell>51.4</cell><cell>79.8</cell></row><row><cell>Without inter ReLU</cell><cell>36.0M</cell><cell>36.1G</cell><cell>50.9</cell><cell>79.0</cell></row><row><cell>Without dilation</cell><cell>37.2M</cell><cell>37.5G</cell><cell>51.9</cell><cell>79.8</cell></row><row><cell>(1+1+1)D</cell><cell>35.7M</cell><cell>35.7G</cell><cell>51.2</cell><cell>79.2</cell></row><row><cell>Ours</cell><cell>36.0M</cell><cell>36.1G</cell><cell>51.9</cell><cell>79.5</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.svcl.ucsd.edu/projects/resound/Diving48 {train/test}.json<ref type="bibr" target="#b1">2</ref> We adopt the same hyper-parameter settings as the official codebase of TDN for a fair comparison.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work was supported by NSFC (61831015), National Key R&amp;D Program of China (2021YFE0206700), NSFC (U19B2035), Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102), and CAAI-Huawei MindSpore Open Fund.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vivit: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6836" to="6846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Spatiotemporal deformable prototypes for motion anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bensch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Scherf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huisken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="502" to="523" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning discriminative motion features through detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04172</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05095</idno>
		<title level="m">Is space-time attention all you need for video understanding? arXiv preprint</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Space-time mixing attention for video transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Perez Rua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sudhakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sportscap: Monocular 3d human motion capture and fine-grained understanding in challenging sports videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2846" to="2864" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dynamic convolution: Attention over convolution kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">39</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Second-order temporal pooling for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="340" to="362" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spatial-temporal transformer for dynamic scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ackermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="372" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.11227</idno>
		<title level="m">Multiscale vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep insights into convolutional networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="420" to="437" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional twostream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visual surveillance for moving vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Ferryman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Worrall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="187" to="197" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Res2net: A new multi-scale backbone architecture. IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="652" to="662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Video action transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="244" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Anticipative video transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="505" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The&quot; something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller-Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal features with 3d residual networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3154" to="3160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2462" to="2470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="667" to="675" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Stm: Spatiotemporal and motion encoding for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2000" to="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attentive spatio-temporal representation learning for diving classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kanojia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Raman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semantic image networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Khowaja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Motionsqueeze: Neural motion feature learning for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="345" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Tea: Temporal excitation and aggregation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="909" to="918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Resound: Towards action recognition without representation bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="513" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7083" to="7093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Teinet: Towards an efficient architecture for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="669" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.06803</idno>
		<title level="m">Tam: Temporal adaptive module for video recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fast abnormal event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="993" to="1011" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Grouped spatial-temporal aggregation for efficient action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5512" to="5521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attend and interact: Higher-order object interactions for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Melvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alregib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peter Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6790" to="6800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mahdisoltani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gharbieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09235</idno>
		<title level="m">Fine-grained video classification and captioning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Something-else: Compositional action recognition with spatial-temporal interaction networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1049" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Skeleton-based action recognition via spatial and temporal transformer networks. Computer Vision and Image Understanding 208</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Plizzari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cannici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matteucci</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page">219</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Optical flow estimation using a spatial pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4161" to="4170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11605</idno>
		<title level="m">Bottleneck transformers for visual recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8934" to="8943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-first AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Self-supervised motion representation via scattering local motion cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="71" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Self-conditioned probabilistic learning of video rescaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Video-based early asd detection via temporal pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Multimedia and Expo</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="272" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Video modeling with correlation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="352" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Appearance-and-relation networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1430" to="1439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Tdn: Temporal difference networks for efficient action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.10071</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Temporal segment networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="2740" to="2755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision</title>
		<meeting>the European conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="399" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A coarse-to-fine framework for resource efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="305" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04971</idno>
		<title level="m">Condconv: Conditionally parameterized convolutions for efficient inference</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime tv-l 1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint pattern recognition symposium</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Pan: Towards fast action recognition via learning persistence of appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.03462</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Vidtr: Video transformer without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Brattoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Marsic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="577" to="590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="803" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Eco: Efficient convolutional network for online video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision</title>
		<meeting>the European conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="695" to="712" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
