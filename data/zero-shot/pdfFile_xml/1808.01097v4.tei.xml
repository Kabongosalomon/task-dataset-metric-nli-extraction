<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CurriculumNet: Weakly Supervised Learning from Large-Scale Web Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Malong Artificial Intelligence Research Center</orgName>
								<orgName type="institution">Malong Technologies</orgName>
								<address>
									<settlement>Shenzhen, Shenzhen</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
							<email>whuang@malong.com</email>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Malong Artificial Intelligence Research Center</orgName>
								<orgName type="institution">Malong Technologies</orgName>
								<address>
									<settlement>Shenzhen, Shenzhen</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Zhang</surname></persName>
							<email>haozhang@malong.com</email>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Malong Artificial Intelligence Research Center</orgName>
								<orgName type="institution">Malong Technologies</orgName>
								<address>
									<settlement>Shenzhen, Shenzhen</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfan</forename><surname>Zhuang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Malong Artificial Intelligence Research Center</orgName>
								<orgName type="institution">Malong Technologies</orgName>
								<address>
									<settlement>Shenzhen, Shenzhen</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengke</forename><surname>Dong</surname></persName>
							<email>dongdk@malong.com</email>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Malong Artificial Intelligence Research Center</orgName>
								<orgName type="institution">Malong Technologies</orgName>
								<address>
									<settlement>Shenzhen, Shenzhen</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
							<email>mscott@malong.com</email>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Malong Artificial Intelligence Research Center</orgName>
								<orgName type="institution">Malong Technologies</orgName>
								<address>
									<settlement>Shenzhen, Shenzhen</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinglong</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Malong Artificial Intelligence Research Center</orgName>
								<orgName type="institution">Malong Technologies</orgName>
								<address>
									<settlement>Shenzhen, Shenzhen</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CurriculumNet: Weakly Supervised Learning from Large-Scale Web Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Curriculum learning ? weakly supervised ? noisy data ? large- scale ? web images</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a simple yet efficient approach capable of training deep neural networks on large-scale weakly-supervised web images, which are crawled raw from the Internet by using text queries, without any human annotation. We develop a principled learning strategy by leveraging curriculum learning, with the goal of handling a massive amount of noisy labels and data imbalance effectively. We design a new learning curriculum by measuring the complexity of data using its distribution density in a feature space, and rank the complexity in an unsupervised manner. This allows for an efficient implementation of curriculum learning on large-scale web images, resulting in a highperformance CNN model, where the negative impact of noisy labels is reduced substantially. Importantly, we show by experiments that those images with highly noisy labels can surprisingly improve the generalization capability of the model, by serving as a manner of regularization. Our approaches obtain state-of-the-art performance on four benchmarks: WebVision, ImageNet, Clothing-1M and Food-101. With an ensemble of multiple models, we achieved a top-5 error rate of 5.2% on the WebVision challenge [18] for 1000-category classification. This result was the top performance by a wide margin, outperforming second place by a nearly 50% relative error rate. Code and models are available at: https://github.com/MalongTech/CurriculumNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep convolutional networks have rapidly advanced numerous computer vision tasks, providing state-of-the-art performance on image classification <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b7">8]</ref>, object detection <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b19">20]</ref>, sematic segmentation <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10]</ref>, etc. They produce strong visual features by training the networks in a fully-supervised manner using large-scale manually annotated datasets, such as ImageNet <ref type="bibr" target="#b4">[5]</ref>, MS-COCO <ref type="bibr" target="#b20">[21]</ref> and PASCAL VOC <ref type="bibr" target="#b5">[6]</ref>. Full and clean human annotations are of crucial <ref type="figure">Fig. 1</ref>. Image samples of the WebVision dataset <ref type="bibr" target="#b18">[19]</ref> from the categories of Carton, Dog, Taxi and Banana. The dataset was collected from the Internet by using text queries generated from the 1, 000 semantic concepts of the ImageNet benchmark <ref type="bibr" target="#b4">[5]</ref>. Each category includes a number of mislabeled images as shown on the right. importance to achieving a high-performance model, and better results can be reasonably expected if a larger dataset is provided with noise-free annotations. However, obtaining massive and clean annotations are extremely expensive and time-consuming, rendering the capability of deep models unscalable to the size of collected data. Furthermore, it is particularly hard to collect clean annotations for tasks where expert knowledge is required, and labels provided by different annotators are possibly inconsistent.</p><p>An alternative solution is to use the web as a source of data and supervision, where a large amount of web images can be collected automatically from the Internet by using input queries, such as text information. These queries can be considered as natural annotations of the images, providing weak supervision of the collected data, which is a cheap way to increase the scale of the dataset near-infinitely. However, such annotations are highly unreliable, and often include a massive amount of noisy labels. Past work has shown that these noisy labels could significantly affect the performance of deep neural networks on image classification <ref type="bibr" target="#b38">[39]</ref>. To address this problem, recent approaches have been developed by proposing robust algorithms against noisy labels <ref type="bibr" target="#b29">[30]</ref>. Another solution is to develop noise-cleaning methods that aim to remove or correct the mislabelled examples in training data <ref type="bibr" target="#b31">[32]</ref>. However, the noise-cleansing methods often suffer from the main difficulty in distinguishing mislabeled samples from hard samples, which are critical to improving model capability. Besides, semi-supervised methods have also been introduced by using a small subset of manually-labeled images, and then the models trained on this subset are generalized to a larger dataset with unlabelled or weakly-labelled data <ref type="bibr" target="#b35">[36]</ref>. Unlike these approaches, we do not aim to propose a noise-cleaning, noise-robust or semi-supervised algorithm. Instead, we investigate improving model capability of standard neural networks by introducing a new training strategy.</p><p>In this work, we study the problem of learning convolutional networks from large-scale images with a massive amount of noisy labels, such as the WebVision challenge <ref type="bibr" target="#b17">[18]</ref>, which is a 1000-category image classification task having the same categories as ImageNet <ref type="bibr" target="#b4">[5]</ref>. The labels are provided by simply using the queries text generated from the 1,000 semantic concepts of ImageNet <ref type="bibr" target="#b4">[5]</ref>, without any manual annotation. Several image samples are presented in <ref type="figure">Fig. 1</ref>. Our goal is to provide a solution able to handle massive noisy labels and data imbalance effectively. We design a series of experiments to investigate the impact of noisy labels on the performance of deep networks, when the amount of training images is sufficiently large. We develop a simple but surprisingly efficient training strategy that allows for improving model generalization and overall capability of the standard deep networks, by leveraging highly noisy labels. We observe that training a CNN from scratch using both clean and noisy data is more effective than just using the clean one. The contributions of this work are three-fold: -We propose CurriculumNet by developing an efficient learning strategy with curriculum learning. This allows us to train high-performance CNN models from large-scale web images with massive noisy labels, which are obtained without any human annotation. -We design a new learning curriculum by ranking data complexity using distribution density in an unsupervised manner. This allows for an efficient implementation of curriculum learning tailored for this task, by directly exploring highly noisy labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-We conduct extensive experiments on a number of benchmarks, including</head><p>WebVision <ref type="bibr" target="#b18">[19]</ref>, ImageNet <ref type="bibr" target="#b4">[5]</ref>, Clothing1M <ref type="bibr" target="#b38">[39]</ref> and Food101 <ref type="bibr" target="#b1">[2]</ref>, where the proposed CurriculumNet obtains state-of-the-art performance. The Curricu-lumNet, with an ensemble of multiple models, archived the top performance with a top-5 error rate of 5.2%, on the WebVision Challenge at CVPR 2017, outperforming the other results by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>We give a brief review on recent studies developed for dealing with noisy annotations on image classification. For a comprehensive overview of label noise taxonomy and noise robust algorithms we refer to <ref type="bibr" target="#b6">[7]</ref>. Recent approaches to learn from noisy web data can be roughly classified into two categories. (1) Methods aim to directly learn from noisy labels. This group of approaches mainly focus on noise-robust algorithms <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b24">25]</ref>, and label cleansing methods which aim to remove or correct mislabeled data <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15]</ref>. However, they generally suffer from the main challenge of identifying mislabeled samples from hard training samples, which are crucial to improve model capability. (2) Semi-supervised learning approaches have also been developed to handle these shortcomings, by combining the noisy labels with a small set of clean labels  [40, <ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b37">38]</ref>. A transfer learning approach solves the label noise by transferring correctness of labels to other classes <ref type="bibr" target="#b16">[17]</ref>. The models trained on this subset are generalized to a larger dataset with unlabelled or weakly-labelled data <ref type="bibr" target="#b35">[36]</ref>. Unlike these approaches, we do not propose a noise-cleansing or noise-robust or semi-supervised algorithm. Instead, we investigate improving model capability of the standard neural networks, by introducing a new training strategy that alleviates negative impact of the noisy labels.</p><p>Convolutional neural networks have recently been applied to training a robust model with noisy data <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b14">15]</ref>. Xiao et al. <ref type="bibr" target="#b38">[39]</ref> introduced a general framework to train CNNs with a limited amount of human annotation, together with noisy data sets containing millions of images. A behavior of CNNs on the training set with highly noisy labels was studied in <ref type="bibr" target="#b29">[30]</ref>. MentorNet <ref type="bibr" target="#b14">[15]</ref> improved the performance of CNNs trained on noisy data, by learning an additional network that weights the training examples. Our method differs from these approaches by directly considering the mislabelled samples in our training process, and we show by experiments that with an efficient training scheme, a standard deep network is strongly robust against the highly noisy labels.</p><p>Our work is closely related to the work of <ref type="bibr" target="#b12">[13]</ref>, which is able to model noise arising from missing, but visually present labels. The method in <ref type="bibr" target="#b12">[13]</ref> is conditioned on the input image, and was designed for multiple labels per image. It does not take advantage of cleaning labels, and the focus is on missing labels, while our approach works reliably on the highly noisy labels, without any cleaned (manual annotation). Our learning curriculum is designed in a completely unsupervised manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we present details of the proposed CurriculumNet motivated by human learning, in which the model starts from learning easier aspects of a concept, and then gradually includes more complicated tasks into the learning process <ref type="bibr" target="#b0">[1]</ref>. We introduce a new method to design a learning curriculum in an unsupervised manner. Then CNNs are trained by following the designed curriculum, where the amount of noisy labels is increased gradually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>Pipeline of CurriculumNet is described in <ref type="figure" target="#fig_0">Fig. 2</ref>. It contains three main steps: (i) initial features generation, (ii) curriculum design and (iii) curriculum learning. First, we use all training data to learn an initial model which is then applied to computing a deep representation (e.g., fully-convolutional (fc) features) from each image in the training set. Second, the initial model aims to roughly map all training images into a feature space where the underlying structure and relationship of the images in each category can be discovered, providing an efficient approach that defines the complexity of the images. We explore the defined complexity to design a learning curriculum where all images in each category are split into a number of subsets ordered by complexity. Third, based on the designed curriculum, we employ curriculum learning which starts training CNNs from an easy subset which combines the easy subsets over all categories. It is assumed to have more clean images with correct labels in the easy subset. Then the model capability is improved gradually by continuously adding the data with increasing complexity into the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Curriculum Design</head><p>Curriculum learning was originally proposed in <ref type="bibr" target="#b0">[1]</ref>. It was recently applied to dealing with noise and outliers. One of the main issues to deliver advances of this learning idea is to design an efficient learning curriculum that is specific for our task. The designed curriculum should be able to discover meaningful underlying local structure of the large-scale noisy data in a particular feature space, and our goal is to design a learning curriculum able to rank the training images from easy to complex in an unsupervised manner. We apply a density based clustering algorithm that measures the complexity of training samples using data distribution density. Unlike previous approaches which were developed to handle noisy labels in small-scale or moderate-scale datasets, we design a new learning curriculum that allows our training strategy with a standard CNN to work practically on large-scale datasets, e.g., the WebVision database which contains over 2,400,000 web images with massive noisy labels.</p><p>Specifically, we aim to split the whole training set into a number of subsets, which are ranked from an easy subset having clean images with more reliable labels, to a more complex subset containing massive noisy labels. Inspired by recent clustering algorithms described in <ref type="bibr" target="#b28">[29]</ref>, we conduct the following procedures in each category. First, we train an initial model from the whole training set by using an Inception v2 architecture <ref type="bibr" target="#b13">[14]</ref>. Then all images in each category are projected into a deep feature space, by using the fc-layer features of the initial model, P i ? f (P i ) for each image P i . Then we calculate a Euclidean distance matrix D ? R n?n as,</p><formula xml:id="formula_0">D ij = f (P i ) ? f (P j ) 2<label>(1)</label></formula><p>where n is the number of images in current category, and D ij indicates a similarity value between P i and P j ( A smaller D ij means higher similarity between P i and P j ). We first calculate a local density (? i ) for each image:</p><formula xml:id="formula_1">? i = j X(D ij ? d c )<label>(2)</label></formula><p>where</p><formula xml:id="formula_2">X(d) = 1 d &lt; 0 0 other</formula><p>where d c is determined by sorting n 2 distances in D ? R n?n from small values to large ones, and select a number which is ranked at k%. This result is insensitive to the value of k between 50 and 70, and we empirically set k = 60 in all our experiments. ? i is the number of samples whose distances to i is smaller than d c . It is natural to assume that a group of clean images with correct labels often have relatively similar visual appearance, and these images are projected closely to each other, leading to a large value of local density. By contrast, noisy images often have a significant visual diversity, resulting in a sparse distribution with a smaller value of the density. Then we define a distance (? i ) for each image:</p><formula xml:id="formula_3">? i = min j:?j &gt;?i (D ij ) if ?j s.t. ? j &gt; ? i max(D ij ) otherwise<label>(3)</label></formula><p>If there exists an image I j having ? j &gt; ? i , ? i is D i? where? is the sample nearest to i among the data. Otherwise, if ? i is the largest one among all density, ? j is the distance between i and the data point which is farthest from i. Then a data point with the highest local density has the maximum value of ?, and is selected as cluster center for this category.</p><p>As we have computed a cluster center for the category, a closer data point to the cluster center, has a higher confidence to have a correct label. Therefore, we simply proceed with the k-mean algorithm to divide data points into a number of clusters, according to their distances to the cluster center, D cj , where c is the cluster center. <ref type="figure" target="#fig_1">Fig. 3</ref> (left) is an ? ? ? figure for all images in the category of cat from the WebVision dataset.</p><p>We generate three clusters in each category, and simply use the images within each cluster as a data subset. As each cluster has a density value measuring data distribution within it, and relationship between different clusters. This provides a natural way to define the complexity of the subsets, giving a simple rule for designing a learning curriculum. A subset with a high density value means all images are close to each other in feature space, suggesting that these images have a strong similarity. We define this subset as a clean one, by assuming most of the labels are correct. The subset with a small density value means the images have a large diversity in visual appearance, which may include more irrelevant images with incorrect labels. This subset is considered as noisy data. Therefore, we generate a number of subsets in each category, arranged from clean, noisy, to highly noisy ones, which are ordered with increasing complexity. Each category has the same number of subsets, and we combine them over all categories, which form our final learning curriculum that implements training sequentially on the clean, noisy and highly noisy subsets. <ref type="figure" target="#fig_1">Fig. 3 (left)</ref> show data distribution of the three subsets in the category of "cat" from the WebVision dataset, with a number of sample images. As seen, images from the clean subset have very close visual appearance, while the highly noisy subset contains a number of random images which are completely different from those in the clean subset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Curriculum Learning</head><p>The Learning process is performed by following the nature of the underlying data structure. That is, the designed curriculum is able to discover the underlying data structure based on visual appearance, in an unsupervised manner. We design a learning strategy which relies on intuition -tasks are ordered by increasing difficulty, and training is proceeded sequentially from easier tasks to harder ones. We develop a multi-stage learning process that trains a standard neural network more efficiently with the enhanced capability for handling massive noisy labels.</p><p>Training details are described in <ref type="figure" target="#fig_1">Fig. 3 (right)</ref>, where a convolutional model is trained through three stages by continuously mixing training subsets from the clean subset to the highly noisy one. Firstly, a standard convolutional architecture, such as Inception v2 <ref type="bibr" target="#b13">[14]</ref>, is used. The model is trained by only using the clean data, where images within each category have close visual appearance. This allows the model to learn basic but clear visual information from each category, serving as the fundamental features for the following process. Secondly, when the model trained in the first stage converges, we continue the learning process by adding the noisy data, where images have more significant visual diversity, allowing the model to learn more meaningful and discriminative features from harder samples. Although the noisy data may include incorrect labels, it roughly preserves the main structure of the data, and thus leads to performance improvement. Thirdly, the model is further trained by adding the highly noisy data which contains a large number of visually irrelevant images with incorrect labels. The deep features learned by following the first two-stage curriculum are able to capture the main underlying structure of the data. We observe that the highly noisy data added in the last stage does not impact negatively to the learned data structure. By contrast, it improves the generalization capability of the model, and allows the model to avoid over-fitting over the clean data, by providing a manner of regularization. A final model is obtained when the training converges in the last stage, where the three subsets are all combined. In addition, when samples from different subsets are combined in the second and third stages, we set different loss weights to the training samples of different subsets as 1, 0.5 and 0.5 for the clean, noisy and highly noisy subsets, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Implementation Details</head><p>Training Details: The scale of WebVision data <ref type="bibr" target="#b18">[19]</ref> is significantly larger than that of ImageNet <ref type="bibr" target="#b4">[5]</ref>, it is important to consider the computational cost when extensive experiments are conducted in evaluation and comparisons. In our experiments, we employ the inception architecture with batch normalization (bninception) <ref type="bibr" target="#b13">[14]</ref> as our standard architecture. The bn-inception model is trained by adopting the proposed density-ranking curriculum leaning. The network weights are optimized with mini-batch stochastic gradient decent (SGD), where the batch size is set to 256, and Root Mean Square Propagation (RMSprop) algorithm <ref type="bibr" target="#b13">[14]</ref> is adopted. The learning rate starts from 0.1, and decreases by a factor of 10 at the iterations of 30 ? 10 4 , 50 ? 10 4 , 60 ? 10 4 , 65 ? 10 4 , 70 ? 10 4 . The whole training process stop at 70 ? 10 4 iterations. To reduce the risk of over-fitting, we use common data augmentation technologies which include random cropping , scale jittering, and ratio jittering. We also add a dropout operation with a ratio of 0.2 after the global pooling layer. Selective Data Balance: By comparing with ImageNet, another challenge of the WebVision data <ref type="bibr" target="#b17">[18]</ref> is that the training images in different categories are highly unbalanced. For example, a large-scale category can have over 10,000 images, while a small-scale category only contains less than 400 images. CNN models, directly trained with random sampling on such unbalanced classes, will have a bias towards the large categories. To alleviate this problem, we develop a two-level data balance approach: subset-level balance and category-level balance. In the subset-level balance, training samples are selected in each min-batch as follows: (256, 0, 0), (128, 128, 0) and (128, 64, 64) for stage 1-3, respectively.</p><p>For the category-level balance, in each mini-batch, we first random select 256 (in stage 1) or 128 (in stage 2 and 3) categories from the 1000 classes, and then we randomly select only one sample from each selected category. Notice that the category-level balance is only implemented on the clean subset. The performance was dropped down when we applied it to the noisy or highly noisy subset. Because we randomly collect a single sample from each category in the categorylevel balance, it is possible to obtain a single but completely irrelevant sample from the noisy or highly noisy subset, which would negatively affect the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-scale convolutional kernels:</head><p>We also apply multi-scale convolutional kernels in the first convolutional layer, with three different kernel sizes: 5 ? 5, 7?7 and 9?9. Then we concatenate three convolutional maps generated by three types of filters, which form the final feature maps of the first convolutional layer. The multi-scale filters enhance the low-level features in the first layer, leading to about 0.5% performance improvements on top-5 errors on the WebVision data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results and Comparisons</head><p>The proposed CurriculumNet is evaluated on four benchmarks: WebVision <ref type="bibr" target="#b18">[19]</ref>, ImageNet <ref type="bibr" target="#b4">[5]</ref>, Clothing1M <ref type="bibr" target="#b38">[39]</ref> and Food101 <ref type="bibr" target="#b1">[2]</ref>. Particularly, we investigate the learning capability on large-scale web images without human annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>WebVision dataset <ref type="bibr" target="#b18">[19]</ref> is an object-centric dataset, and is larger than Ima-geNet <ref type="bibr" target="#b4">[5]</ref> for object recognition and classification. The images are crawled from both Flickr and Google images search, by using queries generated from the 1, 000 semantic concepts of the ILSVRC 2012. Meta information along with those web images (e.g., title, description, tags, etc.) are also crawled. The dataset for the WebVision 2017 contains 1,000 object categories (the same with the Ima-geNet). The training data contains 2,439,574 images in total, but without any human annotation. It includes massive noisy labels, as shown in <ref type="figure">Fig. 1</ref>. There are 50,000 manually-labeled images are used as validation set, and another 50,000 manually-labeled images for testing. The evaluation measure is based on top-5 error, where each algorithm provides a list of at most 5 object categories to match the ground truth.</p><p>Clothing1M dataset <ref type="bibr" target="#b38">[39]</ref> is a large-scale fashion dataset, which includes 14 clothes categories. It contains 1 million noisy labeled images and 74,000 manually annotated images. We call the annotated images the clean set, which is divided into training data, validation data and testing data, with numbers of 50,000, 14,000, and 10,000 images, respectively. There are some images that overlap between the clean set and the noisy set. The dataset was designed for learning robust models from noisy data without human supervision. Food-101 dataset <ref type="bibr" target="#b1">[2]</ref> is a standard benchmark to evaluate recognition accuracy of food visuals. It contains 101 classes, with 101,000 real-world food images in total. The numbers of training and testing images are 750 and 250 per category, respectively. This is a clean dataset with full manual annotations provided. To conduct experiments with noisy data, we manually add 20% noisy images into the training set, which are randomly collected from the training set of ImageNet <ref type="bibr" target="#b4">[5]</ref>, and each image is randomly assigned a label from 101 categories from the Food-101.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiments and Comparisons</head><p>We conducted extensive experiments to evaluate the efficiency of the proposed approaches. We compare various training schemes by using the BN-Inception.</p><p>On training strategy. We evaluate four different training strategies by using a standard Inception v2 architecture, resulting in four models, which are described as follows.</p><p>-Model-A: the model is trained by directly using the whole training set.</p><p>-Model-B: the model is trained by only using the clean subset.</p><p>-Model-C: the model is trained by using the proposed learning strategy, with a 2-subset curriculum: clean and noisy subsets. -Model-D: the model is trained by using the proposed learning strategy, with a 3-subset curriculum: clean, noisy and highly noisy subsets.</p><p>Test loss of four models (on the validation set of WebVision) are compared in <ref type="figure" target="#fig_2">Fig. 4</ref>, where the proposed CurriculumNet with a 2-subset curriculum and a 3-subset curriculum (Model-C and Model-D) have better convergence rates. Top 1 and Top 5 results of four models on the validation set of WebVision are reported in <ref type="table" target="#tab_1">Table 1</ref>. The results are mainly consistent with the test loss presented in <ref type="figure" target="#fig_2">Fig. 4</ref>. The proposed method, with 3-subset curriculum learning, On highly noisy data or training labels. We further investigate the impact of highly noisy data to the proposed learning strategy. We used different percentages of data from the highly noisy subset for 3-subset curriculum learning, ranging from 0% to 100%. Results are reported in <ref type="table" target="#tab_2">Table 2</ref>. As shown, the best results on both Top 1 and Top 5 are achieved at 50% of the highly noisy data. This suggests that, by using the proposed training method, even the highly noisy data can improve model generalization capability, by increasing the amount of the training data with more significant diversity, demonstrating the efficiency of the proposed approach. Increasing the amount of highly noisy data further did not improvement the performance, but with very limited negative affect.</p><p>To provide more insights and give deeper analysis on the impact of label noise, we applied the most recent ImageNet-trained SEnet <ref type="bibr" target="#b11">[12]</ref> (which has a Top 5 error of 4.47% on ImageNet) to classify all images from the training set of the WebVision data. We assume the output label of each image by SEnet is correct, and compute the rate of correct labels in each category. We observed that the average noise rate over the whole training set of the WebVision data is high to 52% (Top 1), indicating that a large amount of incorrect labels is included. We further compute the average noise rates for three subsets of the designed learning curriculum, which are 65%, 39% and 15%, respectively. These numbers are consistent with the increasing complexity of the three subsets, and suggest that most of the images in the third subset are highly noisy.</p><p>We calculate the number of categories in 10 different intervals of the correct rates of the training labels, as shown in <ref type="figure" target="#fig_3">Fig. 5 (left)</ref>. There are 12 categories having a correct rate that is lower than 10%. We further compute the average performance gain in each interval, as show in <ref type="figure" target="#fig_3">Fig. 5 (right)</ref>. We found that the categories with lower correct rates (e.g., &lt; 40%) have larger performance gains (&gt; 4%), and the most significant improvement happens in the interval of 10%-20%, which has an improvement of 7.7%.</p><p>On different clustering algorithms. The proposed clustering based curriculum learning can generalize well to other clustering algorithms. We verify it by comparing our density based curriculum design with K-means based clustering on the proposed 3-subset CurriculumNet. As shown in <ref type="figure" target="#fig_2">Fig. 4 (right)</ref>, the Model-B* which is trained using the clean subset by K-means has a significantly lower performance, which means that training without the proposed curriculum learning is highly sensitive to the quality. By adopting the proposed method, Model-D* significantly improves the performance, from 16.6% to 11.5% (Top 5), which is comparable to Model-D. These results demonstrate the strong robustness of the proposed CurriculumNet, allowing for various qualities of the data generated by different algorithms.</p><p>Final results on the WebVision challenge. We further evaluate the performance of CurriculumNet (Model-D) by using various network architectures, including Inception v2 <ref type="bibr" target="#b13">[14]</ref>, Inception v3 <ref type="bibr" target="#b34">[35]</ref>, Inception v4 <ref type="bibr" target="#b32">[33]</ref> and Inception resnet v2 <ref type="bibr" target="#b32">[33]</ref>. Results are reported in <ref type="table">Table 3</ref>. As can be found, the Inception v3 outperforms the Inception v2 substantially, from 10.82% to 7.88% on the Top 5, while a more complicated model, such as Inception v4 and Inception resnet v2, only has similar performance with a marginal performance gain obtained.</p><p>Our final results were obtained with ensemble of six models. We had the best performance at a Top 5 error of 5.2% on the WebVision challenge 2017 <ref type="bibr" target="#b17">[18]</ref>. It outperforms the 2nd one by a margin of about 2.5%, which is about 50% relative error, and thus is significant for this challenging task. The 5.2% Top 5 error is also comparable to human performance on the ImageNet, but our method obtained this result by using weakly-supervised training data without any human annotation.</p><p>Comparisons with the state-of-the-art methods. Our method is evaluated by comparing it with recent state-of-the-art approaches developed specifically for  <ref type="bibr" target="#b24">[25]</ref>. Experiments and comparisons are conducted on four benchmarks: WebVision <ref type="bibr" target="#b18">[19]</ref>, ImageNet <ref type="bibr" target="#b4">[5]</ref>, Clothing1M <ref type="bibr" target="#b38">[39]</ref> and Food101 <ref type="bibr" target="#b1">[2]</ref>. Model-D with Inception v2 is used in all our experiments. By following <ref type="bibr" target="#b16">[17]</ref>, we use the training set of WebVision to train the models, and test on the validation sets of the WebVision and ILSVRC, both of which has the same 1000 categories. On the Clothing1M, we conduct two groups of experiments by following <ref type="bibr" target="#b16">[17]</ref>, we first apply our curriculum-based training method to one million noisy data, and then use 50K clean data to fine-tune the trained model. We compare both results against CleanNet <ref type="bibr" target="#b16">[17]</ref> and the approach of Patrini et. al. <ref type="bibr" target="#b24">[25]</ref>. Full results are presented in <ref type="table">Table 4</ref>. CurriculumNet improves the performance of our baseline significantly in all four databases. Furthermore, our results compare favorably against recent CleanNet on all datasets, with consistent improvements ranged from about 1.5% to 3.3%. Particularly, Curriculum-Net reduces Top 5 error of the CleanNet from 12.2% to 10.8% on the Web-Vision data. In addition, CurriculumNet also outperforms Patrini et. al.'s approach (19.6%?18.5%) <ref type="bibr" target="#b24">[25]</ref> on the Clothing1M. On the Food101, Curriculum-Net, trained with 20% additional noise data with completely random labels, achieved substantial improvements over both CleanNet (16.0%?12.7%) and FoodNet (27.9%?12.7%) <ref type="bibr" target="#b23">[24]</ref>. These remarkable improvements confirm the advances of CurriculumNet, demonstrating strong capability for learning from massive ammounts of noisy labels.</p><p>Train with more clean data: WebVision+ImageNet. We evaluate the performance of CurriculumNet by increasing the amount of clean data in the training set of WebVision. Since ImageNet data is fully cleaned and manually annotated, a straightforward approach is to simply combine the training sets of WebVision and ImageNet data. We implement CurriculumNet with Inception v2 by considering ImageNet data as an additional clean subset, and test the results on the validation sets of both databases. Results are reported in <ref type="table">Table 5</ref>.</p><p>We summarize key observations as follows. (i) By combining WebVision data into ImageNet data, the performance is generally improved due to the increased amount of training data. (ii) Performance of the proposed CurriculumNet is improved significantly on both validation sets by increasing the amount of the clean data (ImageNet), such as 10.8%?8.5% on WebVision, and 15.1%?7.1% on ImageNet. (iii) By using both WebVision and ImageNet as training data, CurriculumNet is able to improve the performance on both validation sets. For example, it reduces the Top 5 error of WebVision from 9.0% to 8.5% with a same training set. (iv) On ImageNet, CurriculumNet boosts the performance from a Top 5 error of 8.6% to 7.1%, by leveraging additional noisy data (e.g., WebVision). This performance gain is significant on ImageNet, which further confirms the strong capability of CurriculumNet on learning from noisy data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented CurriculumNet -a new training strategy able to train CNN models more efficiently on large-scale weakly-supervised web images, where no human annotation is provided. By leveraging the idea of curriculum learning, we propose a novel learning curriculum by measuring data complexity using cluster density. We show by experiments that the proposed approaches have strong capability for dealing with massive, noisy sets of labels. They not only reduce the negative affect of noisy labels, but also, notably, improve the model generalization ability by using the highly noisy data. The proposed CurriculumNet achieved state-of-the-art performance on the Webvision, ImageNet, Clothing-1M and Food-101 benchmarks. With an ensemble of multiple models, it obtained a Top 5 error of 5.2% on the Webvision Challenge 2017, which outperforms the other submissions by a large margin of about 50% relative error rate.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Pipeline of the proposed CurriculumNet. The training process includes three main steps initial features generation, curriculum design and curriculum learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Left: the sample of the cat category with three subsets. Right: learning process with designed curriculum.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Testing loss of four different models with BN-Inception architecture, (left) Density-based curriculum, and (right) K-mean based curriculum.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Numbers of categories (left), and performance improvements (right) in 10 different rate intervals of the training labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Top-1 and Top-5 errors (%) of four different models with BN-Inception architecture on validation set. The models are trained on WebVision training set and tested on the WebVision and ILSVRC validation sets under various models.</figDesc><table><row><cell>Mothed</cell><cell cols="2">WebVision Top-1 Top-5</cell><cell cols="2">ImageNet Top-1 Top-5</cell></row><row><cell>Model-A</cell><cell>30.16</cell><cell>12.43</cell><cell>36.00</cell><cell>16.20</cell></row><row><cell>Model-B</cell><cell>30.28</cell><cell>12.98</cell><cell>37.09</cell><cell>16.42</cell></row><row><cell>Model-C</cell><cell>28.44</cell><cell>11.38</cell><cell>35.66</cell><cell>15.24</cell></row><row><cell>Model-D</cell><cell>27.91</cell><cell>10.82</cell><cell>35.24</cell><cell>15.11</cell></row><row><cell cols="5">significantly outperforms the model trained on all data, with improvements of</cell></row><row><cell cols="5">30.16% ? 27.91% and 12.43% ? 10.82% on Top 1 and Top 5 errors, respectively.</cell></row><row><cell cols="5">These improvements are significant on such a large-scale challenge. Consistent</cell></row><row><cell cols="5">improvements are obtained on the validation set of ImageNet, where the models</cell></row><row><cell cols="5">were trained on the WebVision data. In all 1000 categories, our approaches</cell></row><row><cell cols="5">lead to performance improvements on 668 categories, while only 195 categories</cell></row><row><cell cols="5">reduced their Top 5 results, and the results of remaining 137 categories were</cell></row><row><cell>unchanged.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Performance (%) of model-D by using various percentages of data from the highly noisy subset.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Table 3. Performance (%) of model-D by</cell></row><row><cell></cell><cell></cell><cell></cell><cell>using various networks.</cell><cell></cell><cell></cell></row><row><cell cols="2">Noise data(%) Top1</cell><cell>Top5</cell><cell>Networks</cell><cell>Top1</cell><cell>Top5</cell></row><row><cell>0</cell><cell>28.44</cell><cell>11.38</cell><cell>Inception v2</cell><cell>27.91</cell><cell>10.82</cell></row><row><cell>25%</cell><cell>28.17</cell><cell>10.93</cell><cell>Inception v3</cell><cell>22.21</cell><cell>7.88</cell></row><row><cell>50%</cell><cell>27.91</cell><cell>10.82</cell><cell>Inception v4</cell><cell>21.97</cell><cell>6.64</cell></row><row><cell>75%</cell><cell>28.48</cell><cell>11.07</cell><cell cols="2">Inception resnet v2 20.70</cell><cell>6.38</cell></row><row><cell>100%</cell><cell>28.33</cell><cell>10.94</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">learning from label noise, such as CleanNet [17], FoodNet [24] and Patrini et. al.'s</cell></row><row><cell>approach</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>Comparisons with most recent results on the Webvision, ImageNet, Clothes-1M and Food101 databases. For the Webvision and ImageNet, the models are trained on WebVision training set and tested on WebVision and ILSVRC validation sets. Performance on the validation sets of ImageNet and WebVision. Models are trained on the training set of ImageNet, WebVision or ImageNet+WebVision.</figDesc><table><row><cell>Mothed</cell><cell>WebVision Top-1(Top-5)</cell><cell cols="2">ImageNet Top-1(Top-5)</cell><cell cols="2">Clothing1M Food101 Top-1 Top-1</cell></row><row><cell>Baseline[17]</cell><cell>32.2(14.2)</cell><cell cols="2">41.1(20.2)</cell><cell>24.8</cell><cell>18.3</cell></row><row><cell>CleanNet [17]</cell><cell>29.7(12.2)</cell><cell cols="2">36.6(15.4)</cell><cell>20.1</cell><cell>-</cell></row><row><cell>MentorNet [15]</cell><cell>29.2(12.0)</cell><cell cols="2">37.5(17.0)</cell><cell>-</cell><cell>-</cell></row><row><cell>Our Baseline</cell><cell>30.3(13.0)</cell><cell cols="2">37.1(16.4)</cell><cell>24.2</cell><cell>15.0</cell></row><row><cell>CurriculumNet</cell><cell>27.9(10.8)</cell><cell cols="2">35.2(15.1)</cell><cell>18.5</cell><cell>12.7</cell></row><row><cell>Traning Data</cell><cell></cell><cell></cell><cell cols="2">WebVision Top-1 Top-5</cell><cell>ImageNet Top-1 Top-5</cell></row><row><cell>ImageNet</cell><cell></cell><cell></cell><cell>32.8</cell><cell>13.9</cell><cell>26.9</cell><cell>8.6</cell></row><row><cell cols="2">ImageNet+WebVision</cell><cell></cell><cell>25.3</cell><cell>9.0</cell><cell>25.6</cell><cell>7.4</cell></row><row><cell cols="2">CurriculumNet(WebVision)</cell><cell></cell><cell>27.9</cell><cell>10.8</cell><cell>35.2</cell><cell>15.1</cell></row><row><cell cols="3">CurriculumNet(WebVision+ImageNet)</cell><cell>24.7</cell><cell>8.5</cell><cell>24.8</cell><cell>7.1</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>ACM</publisher>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Food-101-mining discriminative components with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="446" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Identifying mislabeled training data. CoRR, abs/1106</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Brodley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Friedl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page">219</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno>abs/1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The pascal visual object classes challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html" />
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">results</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Classification in the presence of label noise: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fr?nay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Verleysen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="845" to="869" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Locally-supervised deep hybrid model for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="808" to="820" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<title level="m">Deep residual learning for image recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Decoupled deep neural network for semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1495" to="1503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Seeing through the human reporting bias: Visual classifiers from noisy human-centric labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>abs/1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Mentornet: Regularizing very deep neural networks on corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno>abs/1712.05055</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Design of robust neural network classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nonboe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hintz-Madsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Hansen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>ICASSP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Cleannet: Transfer learning for scalable image classifier training with label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<idno>abs/1711.07131</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Webvision challenge: Visual learning and understanding with web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno>abs/1705.05640</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Webvision database: Visual learning and understanding from web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno>abs/1708.02862</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection pp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Foodnet: Recognizing foods using ensemble of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Deepthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Puhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1758" to="1762" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Making deep neural networks robust to label noise: a loss correction approach pp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1944" to="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Semi-supervised learning in gigantic image collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS. pp</title>
		<imprint>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Clustering by fast search and find of density peaks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Laio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">344</biblScope>
			<biblScope unit="issue">6191</biblScope>
			<biblScope unit="page" from="1492" to="1496" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Deep learning is robust to massive label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rolnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shavit</surname></persName>
		</author>
		<idno>abs/1705.10694</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning from noisy labels with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno>CoRR abs/1406.2080</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>AAAI</publisher>
			<biblScope unit="page" from="4278" to="4284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR. pp</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Learning from noisy large-scale datasets with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Knowledge guided disambiguation for large-scale scene classification with multi-resolution cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2055" to="2068" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Extracting visual knowledge from web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2691" to="2699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Semi-supervised learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<idno>abs/1106.0219</idno>
		<imprint>
			<date type="published" when="2005" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
