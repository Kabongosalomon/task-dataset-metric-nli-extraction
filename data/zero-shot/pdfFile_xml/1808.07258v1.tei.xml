<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Escaping from Collapsing Modes in a Constrained Space</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Che</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National Tsing Hua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chieh</forename><forename type="middle">Hubert</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National Tsing Hua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Che-Rung</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National Tsing Hua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google AI, Mountain View</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wei</surname></persName>
							<email>wewei@google.com</email>
							<affiliation key="aff2">
								<orgName type="department">Google Cloud AI, Mountain View</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwann-Tzong</forename><surname>Chen</surname></persName>
							<email>htchen@cs.nthu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National Tsing Hua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Escaping from Collapsing Modes in a Constrained Space</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generative adversarial networks (GANs) often suffer from unpredictable mode-collapsing during training. We study the issue of mode collapse of Boundary Equilibrium Generative Adversarial Network (BEGAN), which is one of the state-of-the-art generative models. Despite its potential of generating high-quality images, we find that BEGAN tends to collapse at some modes after a period of training. We propose a new model, called BEGAN with a Constrained Space (BEGAN-CS), which includes a latent-space constraint in the loss function. We show that BEGAN-CS can significantly improve training stability and suppress mode collapse without either increasing the model complexity or degrading the image quality. Further, we visualize the distribution of latent vectors to elucidate the effect of latent-space constraint. The experimental results show that our method has additional advantages of being able to train on small datasets and to generate images similar to a given real image yet with variations of designated attributes on-the-fly.</p><p>* Indicates equal contribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The main goal of this paper is to provide new insights into the problem of mode collapse in training Generative Adversarial Networks (GANs) <ref type="bibr" target="#b0">[1]</ref>. GANs have shown great potential in generating new data based on real samples and have been applied to various vision tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. Our study points out a simple but effective approach that can be used to improve the stability of training GANs for generating high-quality images with respect to disentangled representations.</p><p>GANs comprise two core components: generator G and discriminator D. The two components are optimized with respect to two spaces. One is the latent space Z for the generator, and the other is the data space X associated with a real data distribution p real (x) for training data x ? X. The objective of the generator is to find a mapping G : Z ? X that maximizes the probability of the discriminator mistakenly accepting a generated image G(z), z ? Z as from p real (x). On the contrary, the discriminator's objective is to distinguish whether any given x ? X belongs to p real (x). During training, the generator only learns from the information provided by the discriminator, and aims to estimate a good mapping such that p model (G(z)) is similar to p real (x).</p><p>Compared with auto-encoders <ref type="bibr" target="#b9">[10]</ref>, GANs can generate sharper images owing to the adversarial loss. However, a downside of adopting the adversarial loss is that it makes the training of GANs unstable. The performance is strongly dependent on hyper-parameters selection, and the generated images tend to have weaker structural coherence.</p><p>Boundary Equilibrium Generative Adversarial Network (BEGAN) <ref type="bibr" target="#b10">[11]</ref> introduced by Berthelot et al . suggests several modifications on the architecture and loss designs, which significantly improve the quality of generated images and the training stability. Another contribution of BEGAN is providing an approximation of convergence for the class of energy-based GANs.</p><p>Despite the promising improvements of BEGAN, we empirically observe that BEGAN still unavoidably runs into mode collapses after certain epochs of training. In the meanwhile, neither the approximation of convergence nor the loss functions of BEGAN is able to detect the sudden mode collapses. In our experiments, the exact time when mode collapsing happens is highly related to target image resolution and dataset size. In addition to the typical drawbacks of mode collapsing, this unpredictable behavior also makes BEGAN's intended contribution to providing "global measure of convergence" incomplete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Contributions</head><p>We propose a new constraint loss toward addressing the mode collapsing problem. We find that the mode-collapsing problem is suppressed after adding the constraint loss. This new loss term does not increase model complexity and is computationally low-cost. Furthermore, it does not introduce any trade-off regarding image quality and diversity. The proposed model is called BEGAN with a Constrained Space (BEGAN-CS).</p><p>We visualize the latent vectors produced in training phase using Principal Component Analysis (PCA) <ref type="bibr">[12]</ref>. In section 3.1, we analyze the effect of the constraint loss and explain why this loss term makes training process stable.</p><p>Since BEGAN-CS is more stable during training, it performs consistently well even when the size of training dataset is ten-times smaller than the normal setting, in which BEGAN fails to obtain acceptable results. In section 4.3, our experiment shows that the proposed BEGAN-CS can eventually converge to a better state, while BEGAN ends up at mode collapsing in an early stage.</p><p>We further discover that BEGAN is able to learn strong and high-quality disentangled representations in an unsupervised setting. The learned disentangled representations could be used to modify the underlying attributes of generated images. In the meanwhile, owing to the constraint loss, BEGAN-CS can accomplish approximation Enc(x * ) z * on-the-fly for any given real image x * , where G(z * ) is an approximate image to x * under the fixed generator weights. By leveraging the z * approximation and the disentangled representations, BEGAN-CS can generate on the fly a set of images conditioning on a real image x * . The generated images are visually similar to the given real image and are able to exhibit the adjustable disentangled attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Deep Convolutional Generative Adversarial Network (DCGAN) <ref type="bibr" target="#b11">[13]</ref> improves the original GAN <ref type="bibr" target="#b0">[1]</ref> by employing a convolutional architecture to achieve better stability of training and enhanced quality of generated images. Salimans et al . further present several practical techniques for training GANs <ref type="bibr" target="#b12">[14]</ref>. Nevertheless, avoiding mode collapsing while keeping the quality of generated images is still a challenging issue in practice.</p><p>Energy-Based Generative Adversarial Network (EBGAN) <ref type="bibr" target="#b13">[15]</ref> introduces another perspective for formulating GANs. EBGAN implements the discriminator as an auto-encoder with per-pixel error. Boundary Equilibrium Generative Adversarial Network (BEGAN) <ref type="bibr" target="#b10">[11]</ref> shares the same discriminator setting as EBGAN and makes several improvements on the designs of architecture and loss function. One of BEGAN's core contributions is introducing the equilibrium concept, which balances the power between the generator and the discriminator. With these improvements, BEGAN provides fast and stable training convergence, and is capable of generating high visual-quality images. Another contribution of BEGAN is providing an approximate measure of convergence. The earlier class of GANs lacks convergence measurement. Not until later, a new class of GANs exemplified by Wasserstein Generative Adversarial Network (WGAN) <ref type="bibr" target="#b14">[16]</ref> introduces a new loss metric, which correlates with the generator's convergence. To our knowledge, BEGAN yields an alternative class of GANs that also has a loss correlated with convergence measurement.</p><p>Apart from the class of energy-based GANs, Progressive Growing of Generative Adversarial Networks (PGGANs) <ref type="bibr" target="#b15">[17]</ref> is another approach to generating high-quality images. By changing the training procedure without modifying the original GAN loss, PGGANs are able to increase training stability and to produce diverse yet high-resolution (up to 1024 ? 1024 pixels) images.</p><p>The z * approximation property of BEGAN-CS is similar to another class of bijective GANs, which constructs a bijection between the latent space Z and the data space X. This class of models includes ALI <ref type="bibr" target="#b16">[18]</ref>, BiGAN <ref type="bibr" target="#b17">[19]</ref>, VEE-GAN <ref type="bibr" target="#b18">[20]</ref> and <ref type="bibr" target="#b19">[21]</ref>. These four methods share a similar characteristic, requiring additional effort to optimize an extended network. VEEGAN introduces an extra reconstructor network F ? , which maps real data distribution p(x) to a Gaussian. ALI/BiGAN both introduce an additional encoder network in the generator, and try to build up a bijection function. For <ref type="bibr" target="#b19">[21]</ref>, the loss term L s (Eq. (9) in <ref type="bibr" target="#b19">[21]</ref>) has a pre-requirement that the generator must include the real images in its latent space. They introduce an extra encoder network in generator to fulfill this requirement.</p><p>In comparison, BEGAN-CS introduces a light-weight loss that utilizes the built-in mechanism of BEGAN without a need of extra networks. This makes the latent space inverting function jointly optimizable with the discriminator. Also, the constraint loss is a very strong indicator, detecting and protecting the model from mode collapsing. We also include further experimental comparisons with the class of bijective GANs in section 4.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>Mode collapse is a phenomenon that the generated images get stuck in or oscillate between a few modes. This phenomenon under BEGAN's setting has a unique characteristic. Since every sample shares the same encoder in the discriminator of BEGAN, the generated images that collapse at the same mode will share similar latent vectors as encoded by the encoder.</p><p>By leveraging this property, we propose the latent-space constraint loss (L c ), or the constraint loss for short. It constrains the norm of the difference between the latent vector z and the internal state of encoder Enc(G(z)), where Enc is the encoder within the discriminator. During the training process, the constraint loss is only optimized with respect to the discriminator. Although the mode-collapsing problem happens on the generator side, adding the constraint loss directly to the generator would expose too much information to the generator about how to exploit the discriminator, and thus turns out accelerating the occurrence of mode collapse. The constraint loss can also be viewed as a regularizer, which guides the function Enc(G(?)) to be an identity function, and forces the encoder of the discriminator to retain the diversity and uniformity of randomly sampled z ? Z. <ref type="figure" target="#fig_0">Fig. 1</ref> is an overview of the full-architecture of BEGAN-CS. The objective function of BEGAN-CS is mostly similar to BEGAN, except the additional constraint loss. The full objective of BEGAN-CS includes</p><formula xml:id="formula_0">L G = L(G(z G ; ? G ); ? D ) , for ? G<label>(1)</label></formula><p>and</p><formula xml:id="formula_1">L D = L(x real ; ? D ) ? k t ? L(G(z D ; ? G ); ? D ) + ? ? L c , for ? D (2) with L c = z D ? Enc(G(z D )) , (the constraint loss) k t+1 = k t + ?(?L(x; ? D ) ? L(G(z G ; ? G ); ? D )) , for each epoch .<label>(3)</label></formula><p>The total loss L G of the generator and the total loss L D of the discriminator are optimized to solve for the parameters ? G and ? D , respectively. The function L(x; ? D ) = x ? D(x) associated with ? D computes the norm of the difference between any given image x and its reconstructed image D(x) by the decoder of </p><formula xml:id="formula_2">(G(z D ; ? G ); ? D ).</formula><p>The hyper-parameter ? ? [0, 1] balances between the real-image reconstruction loss L(x; ? D ) and the generated-image discrimination loss L(G(z G ; ? G ); ? D ). The hyper-parameter ? is a weighting factor for constraint loss. The constraint loss L c is to enforce Enc(G(?)) to be an identity function for z D .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Latent Space Analysis</head><p>For further illustrating the effectiveness of our method and analyzing the root cause of mode collapsing, we visualize the latent space through time with and without the constraint loss. We take PCA as our choice of dimensionality reduction method, and project the latent vectors onto two-dimensional space. Another common choice of dimensionality reduction for visualization is t-Distributed Stochastic Neighbor Embedding (t-SNE) <ref type="bibr" target="#b20">[22]</ref>. For the latent space, we are more interested in the density and distribution of the points rather than the relative nearness between points or clusters. As a result, PCA is more suitable for our analysis. <ref type="figure" target="#fig_2">Fig. 2</ref> shows a preliminary analysis of BEGAN and BEGAN-CS. We train both models on the CelebA dataset <ref type="bibr" target="#b21">[23]</ref>. The 64-dimensional latent vectors of generated images (Enc(G(z))) and real images (Enc(x)) are projected onto twodimensional space via PCA.</p><p>In this experiment, BEGAN gets into mode collapse at epoch 23. In addition to the obvious change in the shape of distribution after BEGAN mode-collapsing, our empirical analysis also shows two strong patterns. First, in comparison with BEGAN, the latent-vector distribution (in red) of images generated by BEGAN-CS can better fit the real images' latent-vector distribution (in blue). The latent vectors of BEGAN-CS scatter more uniformly across all epochs.</p><p>Second, for BEGAN without adding the constraint loss, both the variance of real images' latent vectors (Var(real)) and the variance of generated images' latent vectors (Var(gen)) grow rapidly as the number of epochs increases. Our hypothesis is that the latent spaces of real images and generated images both expand too rapidly and non-uniformly. Since the number of training data is fixed, as the latent space of real images expands, the density of real images decreases. In the end, the generator of BEGAN reaches a low-density area in the latent space where there is only a few latent vectors of real images nearby. The generator of BEGAN then gets stuck in that area. In contrast, BEGAN-CS has the latentspace constraint as a regularizer, which restricts the latent spaces of real images and generated images expand incautiously. In other words, the constraint loss limits the distribution of Enc(G(z)) to be similar to uniform distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Obtaining Optimal z * in One-Shot</head><p>Given an image x * , finding an optimal latent vector z * such that G(z * ) ? x * &lt; for some small is a challenging problem for GANs. Traditionally, z * can be obtained by back-propagation for solving the optimization min z * ( G(z * ) ? x * ). We name this optimization process as z * -search. However, z * -search is time-consuming and needs to run for each inference individually, and thus is impractical for real-world applications.</p><p>In the case of BEGAN-CS, the constraint loss works as a regularizer, guiding the composite function Enc(G(z)) z to be similar to an identity function. Consider the definition of z * , where G(z * ) = x * . We know that Enc(G(z * )) should be close to z * due to the identity property. This implies that we may take x * and obtain Enc(x * ) as an approximation to z * after a single pass through the encoder Enc(x * ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Disentangled Representation Learning and Application</head><p>We find that BEGAN is able to learn strong and high-quality disentangled representations in an unsupervised setting. The direction of any vector within latent space Z has a universally meaningful semantic, such as mixture of gender, age, smile and hair-style. These learned representations can be combined with vector arithmetic operations to generate images with multiple designated representations.</p><p>However, these disentangled representations are only effective for latent vectors, which is a strong restriction that forbids many GAN models to use the disentangled representation for practical applications, since obtaining the latent vectors via z * -search is computation-demanding. In the meanwhile, as we have shown in section 3.2, BEGAN-CS is able to produce the approximation of z * onthe-fly. By adding multiple selected representation vectors to the approximated z * with respect to any given real image x * , we can generate images that are visually similar to x * and comprise the selected representations at the same time. We demonstrate this idea with a real example produced by BEGAN-CS in <ref type="figure" target="#fig_3">Fig 3.</ref> In this example, the generated image of <ref type="figure" target="#fig_3">Fig 3d acquires</ref> both hair-styles shown <ref type="figure" target="#fig_3">Fig 3b &amp; Fig 3c.</ref> For BEGAN, which lacks the ability of estimating z * directly, the same effect may be forcibly achieved through time-consuming z * -search to obtain suitable z * . Unfortunately, z * -search causes the major bottleneck at inference time and is therefore hard to use in real-world scenarios.</p><p>Similar applications can also be achieved using Variational Auto-Encoder (VAE) based models <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b24">26]</ref> or other task-specific GAN models, such as In-foGAN <ref type="bibr" target="#b25">[27]</ref>. However, the images generated by VAE-based models tend to be blurry, while InfoGAN cannot generate high-quality results as BEGAN does. In comparison, our results are more promising in terms of stability and quality.   </p><formula xml:id="formula_3">G( Enc( ) ) =? (a) G( Enc( ) + styleA ) =? (b) G( Enc( ) + styleB ) =? (c) G( Enc( ) + styleA + styleB ) =? (d)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We train BEGAN-CS using the CelebA dataset for all the experiments presented in this paper. BEGAN-CS does not adopt the learning rate decay technique described in BEGAN's original paper, since the training process of BEGAN-CS is already very stable. The hyper-parameter ? that controls the importance of the constraint loss is set to 0.1 as the default value. We use L2-norm in</p><formula xml:id="formula_4">L(x; ? D ) = x ? D(x)</formula><p>throughout the experiments, while in practice, L1-norm can also be used. For any hyper-parameter that is not mentioned, we choose the same value as in BEGAN's original setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Effectiveness of the Constraint Loss</head><p>In <ref type="figure" target="#fig_4">Fig. 4</ref>, we validate the effectiveness of the constraint loss. We show the generated images at specific epochs during the training of BEGAN and BEGAN-CS on the CelebA dataset. The image resolution is 64 ? 64 and the batch size is 64. BEGAN-CS can continuously be trained up to 100 epochs without any evidence of mode collapsing, loss of diversity, or reduction in quality. In contrast, BEGAN encounters mode collapse at the 25th epoch (i.e., the time-step B in <ref type="figure" target="#fig_4">Fig. 4</ref>). In addition to the advantage of preventing from mode collapse, the proposed BEGAN-CS model also maintains a very good performance in generating high-quality images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Observing the Sudden Mode Collapsing</head><p>An interesting finding during our experiments is the timing of mode collapsing. As is mentioned in <ref type="bibr" target="#b10">[11]</ref>, the global measure of convergence can be used by BEGAN to determine whether the network has reached the final state or if the model has collapsed. However, in practice we are not able to observe significant evidence of mode collapsing directly from the value of the convergence measure.</p><p>Instead, the evidence of mode collapse are more often to be observed from the k value. The k value in BEGAN controls how much attention is paid on L(G(z)). According to our observation, every time the k value suddenly drops, BEGAN is going to collapse shortly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Better Convergence on Small Datasets</head><p>The dataset size is also an important factor for the timing of mode collapse. Under a setting of reducing the training dataset CelebA to 1/10 of its original size, BEGAN collapses earlier than training on full dataset. The early occurrence of mode collapse keeps BEGAN from converging to an optimal state. The timestep A in <ref type="figure" target="#fig_6">Fig. 6</ref> is the best state that BEGAN can achieve during its training on the down-sized CelebA dataset. On the other hand, BEGAN-CS has a more stable training process. In <ref type="figure" target="#fig_6">Fig. 6</ref>, BEGAN-CS can continuously optimize on the 1/10 down-sized CelebA dataset without encountering mode collapse, and eventually converges to a better state than BEGAN. For the quantitative comparison to demonstrate the effectiveness of the proposed constraint loss, we accordingly report "Fr?chet Inception Distance" (FID) <ref type="bibr" target="#b26">[28]</ref> score through time of BEGAN and BEGAN-CS in <ref type="figure" target="#fig_5">Fig. 5</ref>. The experiments are conducted at 64 ? 64 resolution. It can be seen in <ref type="figure" target="#fig_5">Fig. 5</ref> that, during training, the FID of BEGAN-CS does not increase drastically as BEGAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">FID Score Curve Comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Obtaining Optimal z * in One-Shot</head><p>In section 3.2, we have shown that BEGAN-CS can approximate optimal z * with Enc(x * ). Appendix A shows the experimental results of interpolation with obtained z * from z * -search using different GAN architectures. The experiments may serve as proofs of concept for comparing the well-known GANs architectures, including FisherGAN <ref type="bibr" target="#b27">[29]</ref>, PGGAN <ref type="bibr" target="#b15">[17]</ref>, and BEGAN. The experimental results show that the obtained G(Enc(x * )) of BEGAN-CS is visually similar to x * . In contrast, the original BEGAN and other state-of-the-art GANs require time-consuming z * -search for 10,000 iterations to obtain competitive results. It would take 340 seconds to 3,970 seconds depending on the network architecture.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Comparison with Bijective Models</head><p>VEEGAN runs experiments on a synthetic toy dataset which consists of 25 independent Gaussian distributions, and observes better stable and higher diversity than other GANs. We accordingly run the similar experiment and provide comparisons in <ref type="figure">Fig. 7</ref> for VEEGAN, ALI, BEGAN, and BEGAN-CS. We find that the vanilla BEGAN can already fit most of the modes of the real data distribution, though it requires extensive hyper-parameters tuning. Furthermore, BEGAN-CS can stabilize the training and converge to a final state of higher quality. Although VEEGAN can fit to all modes, the distribution is relatively blurry and less similar to the real data distribution. Lastly, ALI fails to fit to the real data distribution.</p><p>The hyper-parameters we used for BEGAN and BEGAN-CS on the toy dataset are ? = 0.1, ?=25, ?=1e-4. We use Adam <ref type="bibr" target="#b28">[30]</ref> optimizer with lr d =1e-4, lr g =5e-4, ? 1 =0.5 and ? 2 =0.999. The latent dimension of Z is set to 32. Both the generator and discriminator are consist of 2 layers of feed-forward network with 128 nodes and ReLU activation. We also set the weight initialization function to be a uniform-random sampler in range [? 9/n, 9/n], which n is the number of layer input.</p><p>We also present qualitative comparisons on image reconstruction with BEGAN-CS and ALI in <ref type="figure">Fig. 8</ref>. We find that the loss functions used by all three methods, ALI, BiGAN, and BEGAN-CS, do not guarantee that the reconstruction results are identical to the real images. BEGAN-CS is better at retaining some of the important features, such as hair color, skin color, gaze, and head pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">On-the-Fly Representation Manipulation</head><p>In section 3.3, we demonstrate a new application of BEGAN-CS with the disentangled representations. By obtaining the approximation of z * with Enc(x * ) and applying the selected disentangled representations, BEGAN-CS can generate images that are visually similar to x * and exhibit the selected representations at the same time. As a proof of concept, we visualize the process of adding single representation in <ref type="figure">Fig. 9</ref> and multiple representations in <ref type="figure" target="#fig_0">Fig. 10</ref>.</p><p>In <ref type="figure">Fig. 9</ref>, we first obtain the approximation of z * from Enc(x * ). Then for each dimension i, we linearly interpolate and replace the value of latent vector z * at its ith dimension by a grid value in [?5, 5] with step size 1, and thus can generate a series of images based on the modified latent vectors. The images show that each dimension of the latent space Z represents a universal disentangled representation. We can perform similar visual transformations to any z ? Z. <ref type="figure">Fig. 9</ref> shows some of the interesting disentangled representations. The full visualization across the 64 dimensions is displayed in Appendix C.</p><p>The learned disentangled representations can also be used to perform multiple vector arithmetic operations on latent vectors. This property enables us to control multiple attributes of a fixed image at the same time by adjusting multiple dimension values on the corresponding latent vector. We visualize the results of combining two different representations in <ref type="figure" target="#fig_0">Fig. 10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We identify that BEGAN suffers from the unpredictable mode-collapsing problem. The precise time when mode collapsing happens is non-deterministic, highly related to the resolution of generated images and the size of training dataset. We propose BEGAN with a Constrained Space (BEGAN-CS) toward addressing the mode-collapsing problem and visualize the effect of constraint loss in the latent space. We experimentally show that the model-collapsing problem is suppressed after adding the constraint loss. BEGAN-CS performs particularly better than BEGAN when the size of training dataset is ten-times smaller than the normal setting. These advantages enable the class of energy-based GANs to move on to the next challenge of generating even higher resolution images.</p><p>We also discover that BEGAN can learn salient and high-quality disentangled representations in an unsupervised setting. Combined with the particular property that BEGAN-CS is able to approximate z * on-the-fly, BEGAN-CS can generate images that are visually similar to the given real image and able to exhibit the adjustable disentangled properties. "Obtaining z * in one-shot" and "adjustable image attributes" are two interesting properties that have various potential applications, such as style manipulation and attribute-based editing.   <ref type="figure" target="#fig_0">Fig. 11</ref>: Interpolation between two real images in latent space using different GAN models. Other state-of-the-art GANs require time-consuming z * -search for 10,000 iterations to obtain competitive results, taking several minutes. Nevertheless, the quality of the z * -search results is still not as good as the quality of the images generated on-the-fly by BEGAN-CS. <ref type="figure" target="#fig_0">Fig. 12</ref>: (a-c) We perform z * -search from a random starting point z ? Z for FisherGAN, PGGAN, and BEGAN. (d) BEGAN-CS starts from Enc(x). We also show the result of G(Enc(x * )) for BEGAN and BEGAN-CS. It can be seen that only for BEGAN-CS the result of G(Enc(x * )) can be considered as a good approximation of z * .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>An overview of BEGAN-CS. the discriminator. The latent vectors z D and z G are randomly sampled from Z. The variable k t ? [0, 1] controls how much emphasis to put on L</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>We visualize the distributions of latent vectors of BEGAN and BEGAN-CS over epochs. Both BEGAN and BEGAN-CS are trained on CelebA dataset under 64 ? 64 resolution and batch size 64. Each graph consists of 6,400 random real images' latent vectors, i.e. Enc(x), and 6,400 generated images' latent vectors, i.e. Enc(G(z)). The upper five graphs are generated by BEGAN, while the bottom five graphs are produced by BEGAN-CS. PCA is performed separately at each epoch based on the latent vectors of the real images. Each blue point represents a latent vector of a real image after applying PCA, and the red points correspond to the latent vectors of the generated images. The text under each graph lists the variance of real images' latent vectors (Var(real)) and the variance of generated images' latent vectors (Var(gen)). During the training of BEGAN, the variances of the distributions of latent vectors keep growing. Note that most of the graphs are created with a fixed interval of 10 epochs, except the bottomright graph directly skips to the 101st epoch to highlight the effectiveness of BEGAN-CS. BEGAN has already collapsed before the 41st epoch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>An example of disentangled representations. The "styleA" and "styleB" are two learned disentangled representations. Note that these representations are universal and can be applied to any latent vector z for generating images G(z + style) with designated attributes. (a) Approximate z * by G(Enc(x * )) in oneshot. (b) &amp; (c) The learned disentangled representations can be combined with G(Enc(x * )). (d) Vector arithmetic with multiple disentangled representations. In this case, the generated image has both hair-styles shown in styleA and styleB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>We validate the effectiveness of the constraint loss by showing the generated images at specific epochs during the training of BEGAN and BEGAN-CS on the CelebA dataset. The image resolution is 64 ? 64 and the batch size is 64. Note that BEGAN fails to reach epoch C since it already collapses at epoch B. In contrast, BEGAN-CS survives after epoch C. Furthermore, BEGAN-CS maintains a very good performance in generating high-quality images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>FID through time. (Left) Full CelebA. (Right) 1/10 CelebA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>Better convergence of BEGAN-CS on small datasets. We show the generated images at selected epochs during training BEGAN and BEGAN-CS on a 1/10 sized subset of CelebA. Training images are of 128 ? 128 resolution and the batch size is 24. BEGAN-CS is stable and converges to a particularly better state than BEGAN. The best state of BEGAN is at time-step A with degraded quality, while BEGAN-CS can generate higher-quality results at time-step C.However, the quality of the z * -search result is still unstable and the searched image frequently looks quite different to the given real image, such as wrong gender or incorrect head pose. More examples on z * -search with different GAN models and different numbers of optimization iterations are shown in Appendix B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 :Fig. 8 :</head><label>78</label><figDesc>Experimental results on the synthetic dataset introduced by VEEGAN.(a) Real images (b) ALI (c) BEGAN-CS Image reconstruction results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Hair and skin color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 :Fig. 10 :</head><label>910</label><figDesc>Selected disentangled representations produced by BEGAN-CS at 64 ? 64 resolution. For each series of images, the left-most image is the fixed real image x * . In each sub-figure, we first obtain approximation of z * using Enc(x * ). For each dimension i, we linearly interpolate and replace the ith dimension of z * by a value in [?5, 5] with step size 1, and then generate the image set {G(z * i )}.?: gender : hair and skin color ?: gender : age ?: age : hairstyle Two-dimensional combinations of disentangled representations.</figDesc></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-08" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised pixel-level domain adaptation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="95" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards diverse and natural image descriptions via a conditional GAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="2989" to="2998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Weakly supervised generative adversarial networks for 3d reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno>abs/1705.10904</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Photo-realistic single image superresolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generative face completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="5892" to="5900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning from simulated and unsupervised images through adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="2242" to="2251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semi supervised semantic segmentation using generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Souly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="5689" to="5697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="2962" to="2971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BEGAN: boundary equilibrium generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schumm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<idno>abs/1703.10717</idno>
	</analytic>
	<monogr>
		<title level="m">Principal component analysis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno>abs/1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05" />
			<biblScope unit="page" from="2226" to="2234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Energy-based generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno>abs/1609.03126</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno>abs/1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno>abs/1710.10196</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adversarially learned inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<idno>abs/1606.00704</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>abs/1605.09782</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">VEEGAN: reducing mode collapse in gans using implicit variational learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Valkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">U</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09" />
			<biblScope unit="page" from="3310" to="3320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adversarial training of variational autoencoders for high fidelity image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision, WACV 2018</title>
		<meeting><address><addrLine>Lake Tahoe, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-03-12" />
			<biblScope unit="page" from="1312" to="1320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV</title>
		<meeting>International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>abs/1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">beta-vae: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Autoencoding beyond pixels using a learned similarity metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B L</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>S?nderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning</title>
		<meeting>the 33nd International Conference on Machine Learning<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-19" />
			<biblScope unit="page" from="1558" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05" />
			<biblScope unit="page" from="2172" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09" />
			<biblScope unit="page" from="6629" to="6640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gan</forename><surname>Fisher</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09" />
			<biblScope unit="page" from="2510" to="2520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoshida</surname></persName>
		</author>
		<idno>abs/1802.05957</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
