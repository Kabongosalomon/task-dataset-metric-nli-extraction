<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Low-light Image Enhancement via Breaking Down the Darkness</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Hu</surname></persName>
							<email>huqiming@tju.edu.cnxj.max.guo@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">College of Intelligence and Computing</orgName>
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<postCode>300350</postCode>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Intelligence and Computing</orgName>
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<postCode>300350</postCode>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Low-light Image Enhancement via Breaking Down the Darkness</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Images captured in low-light environment often suffer from complex degradation. Simply adjusting light would inevitably result in burst of hidden noise and color distortion. To seek results with satisfied lighting, cleanliness, and realism from degraded inputs, this paper presents a novel framework inspired by the divide-and-rule principle, greatly alleviating the degradation entanglement. Assuming that an image can be decomposed into texture (with possible noise) and color components, one can specifically execute noise removal and color correction along with light adjustment. Towards this purpose, we propose to convert an image from the RGB space into a luminance-chrominance one. An adjustable noise suppression network is designed to eliminate noise in the brightened luminance, having the illumination map estimated to indicate noise boosting levels. The enhanced luminance further serves as guidance for the chrominance mapper to generate realistic colors. Extensive experiments are conducted to reveal the effectiveness of our design, and demonstrate its superiority over state-ofthe-art alternatives both quantitatively and qualitatively on several benchmark datasets. Our code is publicly available at https://github.com/mingcv/Bread.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Capturing high-quality images under less controlled conditions is challenging especially using mobile devices. Often, people are shooting images in unsatisfactory light environment. For instance, we might take a photo against light source (please see <ref type="figure">Fig. 1</ref>); or a surveillance camera may be monitoring a place in the nighttime. In such cases, the images will suffer from poor visibility. To obtain high-quality images, a few solutions can be applied like one can extend exposure time to receive more information, but if the target scene is dynamic, the blur effect likely appears in captured images; another possible way is to set a flash for light compensation, which however frequently introduces unexpected <ref type="bibr">Figure 1</ref>. Visual comparison on a sample from the VV dataset. Our method obtains striking improvement over the other competitors, e.g., the sky tone and the realism of human skin. highlights and unbalanced lighting into photos. Hence, instead of upgrading hardware, developing effective low-light enhancement techniques is highly desired for practical use.</p><p>Low-light image enhancement is not a solo problem of light adjustment, which also has troubles of noise burst and color distortion concealed in the darkness because of the limited capability of photographing devices. A number of methods follow the Retinex theory <ref type="bibr" target="#b12">[13]</ref> through decomposing an image I into its reflectance R and illumination L in the form of I = R ? L, where ? designates the Hadamard product operator. Because the reflectance component reflects the intrinsic property of material, it is constant against variant illuminations. Ideally, once the illumination is estimated or given, the reflectance can be immediately obtained. One can adjust illumination according to different demands. However, due to limited quality of sensors, the noise factor N will be always an annoying resident in im-  <ref type="figure">Figure 2</ref>. An image from the LOL dataset, which is in low-light before being linearly enhanced. As can be observed in its RGB, HSV and YCbCr channels, the chrominance components in the YCbCr space are obviously less affected by noise than others.</p><p>Average PSNR Average SSIM <ref type="figure">Figure 3</ref>. The illuminations of low-light images in the LOL training dataset are first aligned with their references and converted to different color-spaces. We replace one of their channels with the ground-truth one, and then compare them with references, obtaining average metrics that indicate to what extent a single channel in different spaces represents the major information of an image.</p><p>ages, thus the model turns out to be I = R?L+N . A simple algebraic transformation leads to I = (R +? ) ? L, wher? N = N/L with element-wise division. We can see from above that the noise will be also amplified along with light enhancement, which becomes spatially-correlated with L as discussed in <ref type="bibr" target="#b29">[30]</ref>. Furthermore, in low-light conditions, sensors (either CCD or CMOS) are sensitive and non-linear to insufficient photons of different light spectrum, which brings color distortion even with illumination and noise being properly handled. Therefore, for the sake of producing high-quality results from degraded inputs, a qualified algorithm should remedy the highly entangled illness of dim light, noise, and color distortion, simultaneously. This work studies how to handle the complex degradation in the darkness from a dividing and ruling perspective. Assuming that an image can be disassembled into texture (together with the main body of noise) and color components, the operations including noise removal and color correction along with light adjustment could be executed specifically. In Figs. 2 and 3, we visualize how the noise distributes in different color-spaces and make statistics on the effect of the single-channel restoration on data pairs from the LOL training dataset <ref type="bibr" target="#b25">[26]</ref>. The results suggest that the YCbCr space is a "good" candidate to do the job of texture-color decomposition. As aforementioned, the principle behind our design is that "mind your own business". In other words, a solver is customized for coping with one type of degradation intently. By the luminancechrominance separation, we are thereby possible to treat the degradation in the texture and color components individually, say focusing on the noise issue in the texture and the color distortion in the other part. Please notice that, although several methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b28">29]</ref> have been recently proposed to take care of noise and color shift issues along with light enhancement without distinction on the recovered reflectance component, they barely consider decomposing the degradation from a texture-color point of view to further ease the problem.</p><p>This paper develops an effective low-light image enhancement framework via breaking down the darkness (Bread for short) based on the above analysis. The major contributions of this paper are summarized as follows:</p><p>1. To the best of our knowledge, this is a pioneering attempt to decouple the entanglement of noise and color distortion, further mitigating the difficulty of low-light enhancement with complex degradation.</p><p>2. We present an effective noise synthesis strategy under the guidance of illumination, significantly improving the suppression quality of amplified and spatiallycorrelated noise in the luminance.</p><p>3. To tackle the color distortion issue left in lightenhanced images, we design a novel color adaption network, which can properly deal with the chrominance according to given luminance.</p><p>4. Extensive comparisons together with ablation studies are provided to verify the efficacy of our method, and reveal its advance over other state-of-the-art methods both qualitatively and quantitatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Numerous low-light image enhancement methods have been proposed over last decades, which can be roughly grouped into traditional and deep learning-based methods.</p><p>Traditional Methods. The simplest and most intuitive way is to linearly adjust the value range or execute a nonlinear Gamma correction on inputs. Further, global and local histogram-based methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18]</ref> are introduced to expand the dynamic range of images. In spite of their ease of use, the enhancement quality is hardly guaranteed, due to the content-blindness. Derived from the Retinex theory <ref type="bibr" target="#b12">[13]</ref>, Single-scale Retinex (SSR) <ref type="bibr" target="#b11">[12]</ref> first uses the Gaussian blurred input as its illumination map, and then removes the estimated illumination from the input as its final result. Multi-scale Retinex (MSR) <ref type="bibr" target="#b10">[11]</ref> extends SSR by fusing the results of multiple Gaussian blur functions with different variances. Besides the above mentioned attempts, NPE <ref type="bibr" target="#b22">[23]</ref> takes local maxima assumption to predict the illumination, which is manipulated by a mapper to act as the enhanced illumination and merged with the reflectance component. LIME <ref type="bibr" target="#b8">[9]</ref> proposes to refine the initial illumination obtained by the Max-RGB assumption and structure preserving constraint. Although these methods can somewhat brighten low-light images, they barely take other hidden degradation, like noise and color distortion, into consideration. To suppress the noise effect, SRIE <ref type="bibr" target="#b6">[7]</ref> further imposes the sparsity on the recovered reflectance. Similarly, RRM <ref type="bibr" target="#b14">[15]</ref> integrates noise estimation into the main driving optimization to eliminate the noise in reflectance. However, the applicability of these optimization-based methods is limited because of the expensive computation, sensitivity to hyper-parameters, and unsatisfied enhancement quality.</p><p>Deep Learning-based Methods. Recently, methods based on deep learning have dominated the target task. For instance, MSR-Net <ref type="bibr" target="#b19">[20]</ref> integrates the MSR mechanism into a neural network and uses the BM3D <ref type="bibr" target="#b4">[5]</ref> for denoising. It gains improvement in visual quality, but still suffers from the drawbacks like over-enhancement as the traditional MSR and lacks an embedded denoising functionality. LLNet <ref type="bibr" target="#b15">[16]</ref> synthesizes training pairs by randomly applying Gamma adjustment and adding synthetic noise to clean images. An auto-encoder network is then employed to learn the mapping function. However, the relationship between real-world illumination and noise is not touched, thus residual noise and over-smoothing problems show up. Cai et al. <ref type="bibr" target="#b1">[2]</ref> exploit to construct references from multiexposure sequences for single image enhancement. However, its performance is upper-bounded by involved MEF methods. DUPE <ref type="bibr" target="#b21">[22]</ref> and GLAD <ref type="bibr" target="#b24">[25]</ref> learn the illumination map for image retouching. Despite reasonable results, they cannot effectively alleviate noise and color distortion issues. DRD <ref type="bibr" target="#b25">[26]</ref> and KinD++ <ref type="bibr" target="#b28">[29]</ref> resort to the layer decomposition strategy that is beneficial to both illumination adjustment and reflectance refinement. DRBL <ref type="bibr" target="#b26">[27]</ref> develops a deep recursive band network for semi-supervised low-light enhancement. DLN <ref type="bibr" target="#b20">[21]</ref> introduces lighten-darken tradeoff and feature aggregation blocks to ameliorate results. However, they frequently have troubles in over-exposure and color distortion. In unsupervised settings, Enlighten-GAN <ref type="bibr" target="#b9">[10]</ref> attempts to take advantage of larger-scale unpaired training data through employing the GAN mechanism, while Zero-DCE <ref type="bibr" target="#b7">[8]</ref> alternatively learns a set of non-reference loss functions. Although relieving the requirement of paired data, they mainly focus on the light factor, and thus have insufficient abilities to remedy other defects. Despite a progress made toward addressing the problem, effective and efficient designs for handling complex and multientangled degradation are desirable for practical use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Analysis &amp; Motivation</head><p>As previously discussed, low-light image enhancement encounters complex degradation mixed up by dim light, noise, and color distortion. Most previous methods tend to directly restore the reflectance from illumination-adjusted input under the guidance of ground-truth data, which in nature ignores the noise-amplification process by the illumination adjustment operation. In other words, the defects hidden in the darkness will burst in the RGB channels of the reflectance map, which are correlated to the spatially-variant illumination. This fact is tricky for both end-to-end networks like <ref type="bibr" target="#b24">[25]</ref> and post-processing methods like <ref type="bibr" target="#b19">[20]</ref>, resulting in artifacts in texture and/or color. Motivated by the principle of divide-and-rule, we propose to convert images from the RGB colorspace into a luminance-chrominance one for decoupling the light and noise factors from the color degradation. Among various colorspaces, YCbCr seems to be a good choice supported by the evidence given in <ref type="figure">Fig. 3</ref>. To handle the dim light and noise in the luminance, we denote the low-light luminance as Y low = Y ? L low + N , and the normal-light reference as Y high = Y ? L high . The connection between Y low and Y high can be found as follows:</p><formula xml:id="formula_0">Y high = (Y low ? N ) ? L high L low = Y lo? L ? N L ,<label>(1)</label></formula><p>whereL = L low /L high represents the relative difference between illuminations of low and normal-light images, and the division is element-wise. We propose a solver, i.e. illumination adjustment network, to seek</p><formula xml:id="formula_1">Y IA = Y low /L. From Eq. (1), we can see that the noise term? = N/L from Y low remains in Y IA .</formula><p>Even if N is simple,? will become much more complicated due to the correlation with spatially-variant illumination. Thus, it is natural to adopt L as an indicator for the denoising. Barely previous methods take into account this property. KinD <ref type="bibr" target="#b29">[30]</ref> is limited to simply concatenate L low and R together to restore the reflectance, which may not sufficiently exploit the guidance information from the illumination, and the single restoration network has to deal with all the degradation simultaneously. Inspired by the above, we alternatively synthesize noisy images that have the same illumination with references but are corrupted with stimulated amplified noise guided by A = ?(L), where ?(?) is a function that reflects the relationship between illuminations and noise levels. By this means, we obtain a noise-suppression solver, the solution of which are constrained by A. Though the relative noise level map is determined, its overall scale is inaccessible due to the differences in photographing devices. To robustly remove the noise, we further fuse the denoised luminance Y N S under different suppression strengths to obtain the expected luminance map Y N F . Having Y N F estimated, it is reasonable to employ it as guidance for mapping chrominance (color correction) from the input to an adjusted/target light levels, which is achieved by our color adaption network. <ref type="figure" target="#fig_1">Figure 4</ref> shows the overall architecture of our method, which comprises an illumination adjustment net (IAN), an adaptive noise suppression net (ANSN), and a color adaption net (CAN). As can be seen from <ref type="figure" target="#fig_1">Fig. 4</ref> A, we firstly convert the input from the RGB to the YCbCr colorspace, obtaining the luminance Y low and chrominance C b low , C r low components. Then, Y low is fed into the IAN to predict the relative illumination difference mapL, yielding the adjusted Y IA . The ANSN takes in Y IA and ?(L) (denoted as A) to produce Y N S . As discussed in 3.1, we introduce a noise fusion module (NFM) into the noise suppression stage. Relative noise level maps {A 1 , ..., A k } with k different suppression strengths are used to generate k denoised luminance maps {Y N S1 , ..., Y N S k }, which are then fused as Y N F . The obtained luminance Y N F is consequently utilized as the guidance of the chrominance enhancement. Given the chrominance components of the lowlight image, and Y N F , the CAN outputs C b CA and C r CA . Finally, we combine Y N F , C b CA and C r CA , and convert them back to the RGB colorspace.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Overall Network Design</head><p>To largely exclude influence from other factors and verify the main claim, all the sub-networks follow the shape shown in <ref type="figure" target="#fig_1">Fig. 4</ref> B. There are three down-sampling layers and three up-sampling layers in each sub-network, and two convolutional layers with ReLU activations are inserted be-fore each scaling layer. The inner channels of each block are doubled after each down-sampling block and halved after each up-sampling layer, changing between 32 and 128. All the convolution layers employ 3 ? 3 kernel size, stride = 1 and padding = 1. All the sub-networks are with a Sigmoid activation at their tails, except for the ANSN which is merely the convolution. Note that our sub-networks are optimized individually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">IAN: Illumination Adjustment Network</head><p>IAN is proposed to estimate the relative difference of illumination maps between low-light luminance Y low and the normal-light Y high counterpart. The difference map is adopted to adjust the low-light input into the normal-light and further contributes to the noise suppression. The following loss function is used for IAN:</p><formula xml:id="formula_2">L IA = Y low /(L + ) ? Y high 2 2 + ? W ? ?L 1 + ? ?L ? ?Y low 1 ,<label>(2)</label></formula><p>where ? 1 and ? 2 respectively stand for the 1 and 2 norms, and is a small constant for avoiding zero denominator. In addition, ? and ? are two coefficients to balance the importance of different terms.L = ? IAN (Y low ) ? L low /L high is the predicted relative difference of illumination between Y low and Y high . The second term is to constrain the illumination to be piece-wise smooth with W = 1/(?Y low + ) as the weight map. The last term is to preserve the similarity between Y low andL in the gradient domain for reducing halo artifacts. ? designates the first-order derivative filter. OnceL is acquired, it is used to generate the output of the first stage, following Y IA = Y low /(L + ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">ANSN: Adaptive Noise Suppression Network</head><p>According to Eq. (1), we obtain the adjusted luminance Y IA = Y high +? , in which the noise in Y low is also amplified. Moreover, possible errors in the estimated illumination should not flow to the subsequent process. Therefore, Input LIME <ref type="bibr" target="#b8">[9]</ref> NPE <ref type="bibr" target="#b22">[23]</ref> DUPE <ref type="bibr" target="#b21">[22]</ref> GLAD <ref type="bibr" target="#b24">[25]</ref> DRBL <ref type="bibr" target="#b26">[27]</ref> KinD++ <ref type="bibr" target="#b29">[30]</ref> EG <ref type="bibr" target="#b9">[10]</ref> Zero-DCE <ref type="bibr" target="#b7">[8]</ref> Bread Bread-ME GT Input LIME <ref type="bibr" target="#b8">[9]</ref> NPE <ref type="bibr" target="#b22">[23]</ref> DUPE <ref type="bibr" target="#b21">[22]</ref> GLAD <ref type="bibr" target="#b24">[25]</ref> DRBL <ref type="bibr" target="#b26">[27]</ref> KinD++ <ref type="bibr" target="#b28">[29]</ref> EG <ref type="bibr" target="#b9">[10]</ref> Zero-DCE <ref type="bibr" target="#b7">[8]</ref> Bread Bread-ME GT <ref type="figure">Figure 5</ref>. Visual comparison of state-of-the-art methods and ours on samples from the LOL dataset. we simulate amplified noise? on normal-light references Y high without changing their illumination. Regions originally under darker light should have more intense noise than those under brighter conditions, thus relative illumination mapL estimated previously is natural to be adopted to in-dicate noise levels. The choices of noise pattern are investigated in LLNet <ref type="bibr" target="#b15">[16]</ref>, which says that Gaussian and Poisson noises are competent. We take the simple Gaussian model for noise synthesis. Modified from the traditional AWGN model <ref type="bibr" target="#b27">[28]</ref>, we reach the following for noise synthesis:</p><formula xml:id="formula_3">Y N = Y high + N (0, A),<label>(3)</label></formula><p>where A can be viewed as an attention map in inverse proportion toL. Moreover, the following simple loss function is used for ANSN:</p><formula xml:id="formula_4">L N S = ? N S (Y N , A) ? N (0, A) 2 2 .<label>(4)</label></formula><p>With the trained ANSN, the amplified noise in Y IA can be removed through</p><formula xml:id="formula_5">Y N S = Y IA ? ? N S (Y IA , A).</formula><p>Though ANSN is already able to produce noise-free enhancement results in most cases, the solo usage of it may still leads to over-smoothed luminance for some samples. The reason is that the overall scale of the noise level map is inaccessible due to the differences in photographing devices. To robustly remove the noise, we develop a noise fusion module (NFM) to merge the luminance maps {Y N S1 , Y N S2 , ..., Y N S k } with k different denoising strengths of {A 1 , A 2 , ..., A k }. Moreover, the fusion process is expected to further remedy errors caused by the previous stages. To be concluded, the following learning problem is introduced:</p><formula xml:id="formula_6">L N F = Y N F ? Y high 2 2 + SSIM(Y N F , Y high ) (5) where Y N F = ? N F (Y N S 1...k , A 1...k )</formula><p>, and SSIM(?, ?) is the structural similarity loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">CAN: Color Adaption Network</head><p>Having obtained the Y N S , the color components are expected to be adapted accordingly.</p><p>Given the luminance and chrominance components of the original input (Y low , C b low , C r low ) and the reference (Y high , C b high , C r high ), the loss function for CAN is designed as follows:</p><formula xml:id="formula_7">L CA = C b CA ? C b high 2 2 + C r CA ? C r high 2 2 ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_8">(C b CA , C r CA ) = ? CA (Y low , C b low , C r low , Y high ).</formula><p>Note that, following the divide-and-rule principle, we use Y high rather than Y N F during training to avoid the influence from possible errors left in previous stages. Once the network is trained, Y high is replaced with Y N F for testing.</p><p>Considering the low-saturation problem of the references in the LOL <ref type="bibr" target="#b25">[26]</ref> training data as shown in <ref type="figure">Fig. 6</ref>, we can alternatively introduce multi-exposure data for the training of CAN, which often covers a wide range of exposure and aplenty color patterns. The following learning problem is desired:</p><formula xml:id="formula_9">(C b M E , C r M E ) = ? M E (Y e1 , C be1 , C re1 , Y e2 ).<label>(7)</label></formula><p>Image pairs under different exposures, denoted as (Y e1 , C be1 , C re1 ) and (Y e2 , C be2 , C re2 ), are randomly selected GLAD <ref type="bibr" target="#b24">[25]</ref> DRBL <ref type="bibr" target="#b26">[27]</ref> GT (train)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bread</head><p>Bread-ME GT (eval) <ref type="figure">Figure 6</ref>. The ground truth of training data in the LOL dataset typically has less vivid color than that of evaluation.</p><p>from the sequences of multi-exposure photographs. The objective function of this module shares the same form with Eqn. <ref type="bibr" target="#b5">(6)</ref>. It forces the enhanced chrominance components to fit the exposure condition with respect to a certain luminance guided by real multi-exposure data. Our approach also answers a key question that arises in the enhancement -to what extent the enhanced color should be. In most, if not all, of the cases, we expect it to be natural, under a certain exposure. The framework with multi-exposure data introduced is denoted as Bread-ME. We show results of both Bread and Bread-ME in quantitative experiments for fair comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>All of our models are implemented in PyTorch and optimized with Adam optimizer, the parameters of which are set as ? 1 = 0.9, and ? 2 = 0.999. All the learning rates are fixed as 10 ?3 , except for the finetuning of the color adaption network based on multi-exposure Data, which is set as 10 ?4 . ? and ? in Eqn. (2) are set as 4.0 and 0.5, respectively. We set A = exp(?L), k = 3, A 1,2,3 = {0, 0.05A, 0.1A} for the ANSN and NFM.</p><p>We adopt the LOL dataset as the training data, which contains 485 low/normal light pairs for training and 15 for evaluation. To imitate various exposures in real-world photographs, we synthesize 8 images with different exposures for each low-light image. The magnifications of the exposures are uniformly distributed from one to making 25% pixels over-exposed at most. For the training of the color adaption network based on multi-exposure data, we first select the SICE dataset <ref type="bibr" target="#b1">[2]</ref> for training, then append the original 485 pairs of LOL dataset into the training data for finetuning. Input LIME <ref type="bibr" target="#b8">[9]</ref> SRIE <ref type="bibr" target="#b6">[7]</ref> MSRNet <ref type="bibr" target="#b19">[20]</ref> DUPE <ref type="bibr" target="#b21">[22]</ref> DRD <ref type="bibr" target="#b25">[26]</ref> GLAD <ref type="bibr" target="#b24">[25]</ref> DRBL <ref type="bibr" target="#b26">[27]</ref> KinD++ <ref type="bibr" target="#b28">[29]</ref> EG <ref type="bibr" target="#b9">[10]</ref> Zero-DCE [8] Ours <ref type="figure">Figure 7</ref>. Visual comparison between different methods on the DICM testing dataset. Please zoom in for details.</p><p>Input GLAD <ref type="bibr" target="#b24">[25]</ref> KinD++ <ref type="bibr" target="#b28">[29]</ref> DRBL <ref type="bibr" target="#b26">[27]</ref> EG <ref type="bibr" target="#b9">[10]</ref> Zero-DCE [8] Ours <ref type="figure">Figure 8</ref>. Visual comparison between different methods on the VV (top two rows) and MEF-DS <ref type="bibr" target="#b5">[6]</ref> (the last row). Please zoom in for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Performance Evaluation</head><p>To verify the effectiveness of our proposed method, several public datasets are used for evaluation, including LOL <ref type="bibr" target="#b25">[26]</ref>, DICM <ref type="bibr" target="#b13">[14]</ref>, NPE <ref type="bibr" target="#b22">[23]</ref> and VV 1 . Representative stateof-the-art methods, including LIME <ref type="bibr" target="#b8">[9]</ref>, SIRE <ref type="bibr" target="#b6">[7]</ref>, NPE <ref type="bibr" target="#b22">[23]</ref>, RRM <ref type="bibr" target="#b14">[15]</ref>, MSRNet <ref type="bibr" target="#b19">[20]</ref>, DUPE <ref type="bibr" target="#b21">[22]</ref>, GLAD <ref type="bibr" target="#b24">[25]</ref>, DRD <ref type="bibr" target="#b25">[26]</ref>, DRBL <ref type="bibr" target="#b26">[27]</ref>, KinD <ref type="bibr" target="#b29">[30]</ref>, EG <ref type="bibr" target="#b9">[10]</ref> and Zero-DCE <ref type="bibr" target="#b7">[8]</ref> are adopted for comparisons. Image quality assessment metrics, including PSNR, SSIM, NIQE <ref type="bibr" target="#b16">[17]</ref>, DeltaE <ref type="bibr" target="#b18">[19]</ref> and LOE <ref type="bibr" target="#b23">[24]</ref>, are employed to measure these methods.</p><p>The quantitative comparison between Bread(-ME) and other competitors on the LOL evaluation dataset is reported in <ref type="table" target="#tab_0">Table 1</ref>. Our method outperforms other state-of-the-art alternatives by a noticeable margin, on both of the refer-1 https://sites.google.com/site/vonikakis/datasets ence and no-reference metrics, demonstrating the efficacy of our proposed Bread framework. Note that the absolute brightness is inaccessible during evaluation, which may be unfair to those non-data-driven methods and may interfere the assessment of the fidelity in details, thus we also provide a version of metrics besides the original, the predicted luminance of which is aligned to its ground truth by simple Gamma correction. Visual comparisons on several samples from the LOL evaluation and testing datasets are depicted in <ref type="figure">Figs. 5 and 7</ref>, respectively. The results by our method achieve remarkably higher quality with noise wellsuppressed and less artifacts, while irregular illumination, noise residual, and texture/color distortion exist in the results of other methods. Moreover, we can see from <ref type="figure">Fig.  5</ref> that multi-exposure-data-based color adapter rectifies the greenish hue, producing more realistic tone. Also, we pro- Input Bread Bread-ME <ref type="figure">Figure 9</ref>. Visual comparison between color adaption without and with multi-exposure data used. It is obvious that the introduction of such data contributes to more vibrant color.</p><p>vide non-reference results in <ref type="table" target="#tab_1">Table 2</ref>. For fair comparison, the methods list in the table are all data-driven and trained on the LOL dataset. Our framework occupies the leading position in terms of both two non-reference metrics on three benchmark datasets. Please note that, NIQE does not monotonously change following the quantity of visual noises, which means that a more visual-pleasing noise-free texture can also cause a high NIQE value. We will continue this discussion in the supplementary materials. Additionally, we provide more visual results to further illustrate the effectiveness of the noise suppression and the fidelity of the color adaption in <ref type="figure">Fig. 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>As shown in <ref type="table" target="#tab_2">Table 3</ref>, to evaluate the effectiveness of different settings in our framework, we conduct ablation studies, including removing the denoising process, the noise fusion module and the IA-NS separation setting, respectively. We further evaluate different noise synthesis models, including fixed Gaussian noise (noise level of 25, denoted as FGN), Poisson noise (PN) and our spatially-correlated Gaussian noise (Bread). To validate the necessity of the usage of multi-exposure data, please compare the results of Bread and Bread-ME. The visual comparison is also shown in <ref type="figure">Fig. 9</ref>, which illustrates that the color fidelity meets significant improvement from the data. Several key questions are highlighted as follows:</p><p>The necessity of NFM: We can see that without the NFM, the framework encounters an obvious degradation for the non-blind metrics, because the errors arising in IAN and ANSN are left untreated.</p><p>The necessity of IA-NS separation: For directly learning the restoration without IA-NS separation, we use Y IA and Y high pairs and keep the same architecture of the noise suppression network. The gap between this setting and the noise synthesis approaches emphasizes the importance of the separation strategy.</p><p>The choice of noise synthesis settings: We synthesize noisy images polluted by Poisson noise through applying the estimated low illumination map to the normal-light images and then contaminate them through the Poisson process. We then remove its illumination from the corrupted lowlight images to simulate the adjusted images with amplified noise. For the fixed Gaussian results, we simply apply the AWGN on normal-light images. Unfortunately, the strength of Poisson and fixed-Gaussian denoiser is not adjustable, which means further fusion is futile, thus we adopt the spatial-correlated Gaussian as our first option. More details will be discussed in supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This work discussed the mixture of multi-degradation in low-light images, which increases the training difficulty and limits the enhancement quality of previous methods. To disentangle the complex degradation, the colorspace of an image is first converted from the RGB into a luminancechrominance one, i.e., YCbCr, from a texture-color perspective. By doing so, the main pressure of image brightening and denoising goes to the luminance component Y , while the chrominance components C b and C r respond to color correction, having the enhanced Y as guidance. Regarding different specific illnesses, the sub-networks including IAN, ANSN, and CAN are customized and trained individually, all of which follow the simple U-shaped net. Our designs make the training environment for each sub-network specific to one simple degradation, the effect of which has been validated by the experiments. It is positive that such a divide-and-rule principle with texture-color decomposition can be applied to other enhancement and restoration tasks like dehazing and underwater image enhancement.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>An overview of our proposed Bread architecture and the basic structures of the involved sub-networks. The estimated relative illumination map are shown by heat map for a better view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table><row><cell>Metrics</cell><cell>LIME [9]</cell><cell>SIRE [7]</cell><cell>NPE [23]</cell><cell>RRM [15]</cell><cell>EG [10]</cell><cell cols="2">Zero-DCE [8] MSRNet [20]</cell></row><row><cell>PSNR?</cell><cell>16.76</cell><cell>11.86</cell><cell>16.97</cell><cell>13.88</cell><cell>17.48</cell><cell>14.86</cell><cell>13.17</cell></row><row><cell>SSIM?</cell><cell>0.444</cell><cell>0.494</cell><cell>0.482</cell><cell>0.670</cell><cell>0.654</cell><cell>0.562</cell><cell>0.460</cell></row><row><cell>NIQE?</cell><cell>9.779</cell><cell>8.073</cell><cell>9.788</cell><cell>4.234</cell><cell>5.238</cell><cell>8.811</cell><cell>9.261</cell></row><row><cell>DeltaE?</cell><cell>21.43</cell><cell>32.62</cell><cell>21.77</cell><cell>26.18</cell><cell>19.31</cell><cell>24.56</cell><cell>30.17</cell></row><row><cell>PSNR C ?</cell><cell>19.14</cell><cell>20.97</cell><cell>20.59</cell><cell>20.25</cell><cell>22.48</cell><cell>21.88</cell><cell>16.71</cell></row><row><cell>SSIM C ?</cell><cell>0.471</cell><cell>0.656</cell><cell>0.513</cell><cell>0.774</cell><cell>0.710</cell><cell>0.640</cell><cell>0.461</cell></row><row><cell>NIQE C ?</cell><cell>8.954</cell><cell>7.321</cell><cell>8.890</cell><cell>3.944</cell><cell>4.837</cell><cell>7.889</cell><cell>9.074</cell></row><row><cell>DeltaE C ?</cell><cell>19.06</cell><cell>14.32</cell><cell>17.60</cell><cell>14.03</cell><cell>13.28</cell><cell>14.01</cell><cell>22.72</cell></row><row><cell>Metrics</cell><cell cols="5">DUPE [22] GLAD [25] DRD [26] DRBL [27] KinD++ [29]</cell><cell>Bread</cell><cell>Bread-ME</cell></row><row><cell>PSNR?</cell><cell>14.77</cell><cell>19.72</cell><cell>16.77</cell><cell>18.80</cell><cell>21.80</cell><cell>22.92</cell><cell>22.96</cell></row><row><cell>SSIM?</cell><cell>0.470</cell><cell>0.685</cell><cell>0.428</cell><cell>0.831</cell><cell>0.836</cell><cell>0.836</cell><cell>0.838</cell></row><row><cell>NIQE?</cell><cell>9.079</cell><cell>7.283</cell><cell>10.424</cell><cell>4.103</cell><cell>4.290</cell><cell>3.950</cell><cell>3.946</cell></row><row><cell>DeltaE?</cell><cell>26.19</cell><cell>16.54</cell><cell>23.65</cell><cell>15.59</cell><cell>11.52</cell><cell>11.54</cell><cell>11.19</cell></row><row><cell>PSNR C ?</cell><cell>22.36</cell><cell>23.72</cell><cell>18.73</cell><cell>22.48</cell><cell>23.91</cell><cell>25.98</cell><cell>26.06</cell></row><row><cell>SSIM C ?</cell><cell>0.594</cell><cell>0.724</cell><cell>0.448</cell><cell>0.851</cell><cell>0.847</cell><cell>0.851</cell><cell>0.857</cell></row><row><cell>NIQE C ?</cell><cell>8.110</cell><cell>6.754</cell><cell>9.644</cell><cell>3.753</cell><cell>3.901</cell><cell>3.649</cell><cell>3.614</cell></row><row><cell>DeltaE C ?</cell><cell>14.77</cell><cell>12.54</cell><cell>21.49</cell><cell>11.75</cell><cell>10.08</cell><cell>9.41</cell><cell>9.06</cell></row></table><note>Quantitative results on LOL evaluation dataset of different methods in terms of PSNR, SSIM, NIQE and DeltaE. The subscript C indicates gamma correction on the luminance towards references is conducted before evaluation. The best results are indicated in red, the second-best results in blue and the third in green.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative comparison on the DICM, NPE and VV datasets. All the competitors are trained on the LOL dataset.</figDesc><table><row><cell>Datasets</cell><cell>Metrics</cell><cell>GLAD</cell><cell>DRD</cell><cell cols="4">DRBL KinD++ Bread Bread-ME</cell></row><row><cell>DICM (44)</cell><cell>NIQE? LOE?</cell><cell cols="5">3.0875 4.7120 3.2784 2.8584 3.0893 240.71 608.70 660.10 535.34 400.07</cell><cell>3.0869 388.85</cell></row><row><cell>NPE (8)</cell><cell>NIQE? LOE?</cell><cell cols="5">3.6143 4.0676 3.5843 3.9101 3.5103 211.27 741.78 814.82 494.38 398.57</cell><cell>3.4596 392.72</cell></row><row><cell>VV (24)</cell><cell>NIQE? LOE?</cell><cell cols="5">4.4112 4.1508 3.3770 3.8084 3.6770 199.57 522.04 648.67 300.57 339.56</cell><cell>3.6710 302.26</cell></row><row><cell>-</cell><cell cols="2">Size(MB)? 10.90</cell><cell>9.05</cell><cell>20.10</cell><cell>34.9</cell><cell>8.21</cell><cell>8.21</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation study on different configurations.PSNR? SSIM? PSNR C ? SSIM C ?</figDesc><table><row><cell>w/o DN</cell><cell>16.91</cell><cell>0.586</cell><cell>23.33</cell><cell>0.623</cell></row><row><cell>w/o NFM</cell><cell>17.08</cell><cell>0.721</cell><cell>23.72</cell><cell>0.816</cell></row><row><cell>w/o IA-NS Sep.</cell><cell>18.69</cell><cell>0.785</cell><cell>24.58</cell><cell>0.829</cell></row><row><cell>w/ FGN</cell><cell>21.40</cell><cell>0.799</cell><cell>25.26</cell><cell>0.821</cell></row><row><cell>w/ PN</cell><cell>22.44</cell><cell>0.831</cell><cell>25.84</cell><cell>0.849</cell></row><row><cell>Bread</cell><cell>22.92</cell><cell>0.836</cell><cell>25.98</cell><cell>0.851</cell></row><row><cell>Bread-ME</cell><cell>22.96</cell><cell>0.838</cell><cell>26.06</cell><cell>0.857</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A dynamic histogram equalization for image contrast enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Abdullah-Al-Wadud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Md</forename><surname>Hasanul Kabir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksam</forename><surname>Ali Akber Dewan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Consumer Electronics</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="593" to="600" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning a deep single image contrast enhancer from multi-exposure images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianrui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Contextual and variational contrast enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Turgay</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tardi</forename><surname>Tjahjadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3431" to="3441" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A simple and effective histogram equalization approach to image enhancement. Digital signal processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">J</forename><surname>Heng-Da Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="158" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3-d transform-domain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostadin</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><forename type="middle">O</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2080" to="2095" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Perceptual evaluation for multi-exposure image fusion of dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuming</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kede</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shutao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1127" to="1138" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A weighted variational model for simultaneous reflectance and illumination estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueyang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Delu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Ping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Zero-reference deep curve estimation for low-light image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunle</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jichang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Kwong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runmin</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lime: Low-light image enhancement via illumination map estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Enlightengan: Deep light enhancement without paired supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A multiscale retinex for bridging the gap between color images and the human observation of scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zia-Ur</forename><surname>Daniel J Jobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glenn</forename><forename type="middle">A</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woodell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="965" to="976" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Properties and performance of a center/surround retinex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zia-Ur</forename><surname>Daniel J Jobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glenn</forename><forename type="middle">A</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woodell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="451" to="462" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The retinex theory of color vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Edwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Land</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific american</title>
		<imprint>
			<biblScope unit="volume">237</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Contrast enhancement based on layered difference representation of 2d histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chulwoo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chul</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Structure-revealing low-light image enhancement via robust retinex model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mading</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongming</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Llnet: A deep autoencoder approach to natural low-light image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adedotun</forename><surname>Kin Gwn Lore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumik</forename><surname>Akintayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Making a &quot;completely blind&quot; image quality analyzer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajiv</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">Conrad</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="209" to="212" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Contrast limited adaptive histogram equalization image processing to improve the detection of simulated spiculations in dense mammograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuquan</forename><surname>Etta D Pisano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marla</forename><surname>Hemminger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Deluca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">M</forename><surname>Braeuning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pizer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Digital imaging</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">193</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The ciede2000 color-difference formula: Implementation notes, supplementary test data, and mathematical observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wencheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edul</forename><forename type="middle">N</forename><surname>Dalal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Color Research and Application</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="21" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Msr-net: Low-light image enhancement using deep convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.02488</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Lightening network for low-light image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Wen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Song</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Chi</forename><surname>Siu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel Pk</forename><surname>Lun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Underexposed photo enhancement using deep illumination estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="6849" to="6857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Naturalness preserved enhancement algorithm for non-uniform illumination images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Miao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Naturalness preserved enhancement algorithm for non-uniform illumination images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Miao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3538" to="3548" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Gladnet: Low-light enhancement network with global awareness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face &amp; Gesture Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep retinex decomposition for low-light enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">From fidelity to perceptual quality: A semisupervised approach for low-light image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuming</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3142" to="3155" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Beyond brightening low-light images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1013" to="1037" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Kindling the darkness: A practical low-light image enhancer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1632" to="1640" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
