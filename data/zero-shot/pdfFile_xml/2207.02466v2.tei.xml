<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GLENet: Boosting 3D Object Detectors with Generative Label Uncertainty Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q?ian</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyu</forename><surname>Zhu</surname></persName>
							<email>zhiyuzhu2-c@my.cityu.edu.hk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Hou</surname></persName>
							<email>jh.hou@cityu.edu.hk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Yuan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q?ian</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyu</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Hou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Yuan</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">City University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">City University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GLENet: Boosting 3D Object Detectors with Generative Label Uncertainty Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Received: date / Accepted: date</note>
					<note>This project was partly supported by the Hong Kong Research Grants Council under Grants 11202320 and 11218121, and partly by the Natural Science Foundation of China under Grant 61871342.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The inherent ambiguity in ground-truth annotations of 3D bounding boxes caused by occlusions, signal missing, or manual annotation errors can confuse deep 3D object detectors during training, thus deteriorating the detection accuracy. However, existing methods overlook such issues to some extent and treat the labels as deterministic. In this paper, we formulate the label uncertainty problem as the diversity of potentially plausible bounding boxes of objects, then propose GLENet, a generative framework adapted from conditional variational autoencoders, to model the one-tomany relationship between a typical 3D object and its potential ground-truth bounding boxes with latent variables. The label uncertainty generated by GLENet is a plug-andplay module and can be conveniently integrated into existing deep 3D detectors to build probabilistic detectors and supervise the learning of the localization uncertainty. Besides, we propose an uncertainty-aware quality estimator architecture in probabilistic detectors to guide the training of IoU-branch with predicted localization uncertainty. We incorporate the proposed methods into various popular base 3D detectors and demonstrate significant and consistent performance gains on both KITTI and Waymo benchmark datasets. Especially, the proposed GLENet-VR outperforms all published LiDARbased approaches by a large margin and ranks 1 among single-modal methods on the challenging KITTI test set. We</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As one of the most practical application scenarios of computer vision, 3D object detection has been attracting much academic and industrial attention in the current deep learning era with the rise of autonomous driving and the emergence of large-scale annotated datasets (e.g., KITTI <ref type="bibr" target="#b10">(Geiger et al., 2012)</ref>, and Waymo ).</p><p>In the current community, despite the proliferation of various deep learning-based 3D detection pipelines, it is observed that mainstream 3D object detectors are typically designed as deterministic models, without considering the critical issue of the ambiguity of annotated ground-truth labels. However, different aspects of ambiguity/inaccuracy inevitably exist in the ground-truth annotations of objectlevel bounding boxes, which may significantly influence the overall learning process of such deterministic detectors. For example, in the data collection phase, raw point clouds can be highly incomplete due to the intrinsic properties of Li-DAR sensors as well as uncontrollable environmental occlusion. Moreover, in the data labeling phase, ambiguity naturally occurs when different human annotators subjectively estimate object shapes and locations from 2D images and partial 3D points. To facilitate intuitive understandings, we provide typical examples in <ref type="figure">Fig. 1</ref>, from which we can observe that an incomplete LiDAR observation can correspond to multiple potentially plausible labels and objects with sim- <ref type="figure">Fig. 1: (a)</ref> Given an object with an incomplete LiDAR observation, there may exist multiple potentially plausible ground-truth bounding boxes with varying sizes and shapes. (b) Ambiguity and inaccuracy can be inevitable in the labeling process when annotations are derived from 2D images and partial points. In the given cases, similar point clouds of the car category with only the rear part can be annotated with different ground-truth boxes of varying lengths. ilar LiDAR observation can be annotated with significantly varying bounding boxes.</p><p>Motivated by the afore-mentioned phenomena, there also exists another family of probabilistic detectors that explicitly consider the potential influence of label ambiguity. Conclusively, these methods can be categorized into two paradigms, as illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>. The first paradigm of learning frameworks <ref type="bibr" target="#b14">(He et al., 2019;</ref><ref type="bibr" target="#b28">Meyer et al., 2019;</ref><ref type="bibr" target="#b7">Feng et al., 2018</ref><ref type="bibr" target="#b8">Feng et al., , 2019</ref> tend to output the probabilistic distribution of bounding boxes, instead of directly regressing definite box coordinates in a deterministic fashion. For example, under the pre-assumption of Gaussian distribution, the detection head accordingly predicts mean and variance of the distribution. To supervise such probabilistic models, these works simply treat ground-truth bounding boxes as the Dirac delta distribution, after which KL divergence is applied between the estimated distributions and ground-truths. Obviously, the major limitation of these methods lies in that they fail to essentially address the problem of label ambiguity, since the groundtruth bounding boxes are still considered as deterministic with zero uncertainty (i.e., modeled as a Dirac delta function). To this end, the second paradigm of learning frameworks attempts to quantify label uncertainty derived from some simple heuristics <ref type="bibr" target="#b27">(Meyer and Thakurdesai (2020)</ref>) or Bayes ), such that the detectors can be supervised under more reliable bounding box distributions. However, it is not surprising that these approaches still cannot produce satisfactory label uncertainty estimation results due to insufficient modeling capacity. In general, this line of works is still at its initial stage with very limited number of studies, despite its greater potential in generating higherquality label uncertainty estimation in a data-driven manner.</p><p>Architecturally, this work follows the second type of design philosophy, where we particularly customize a powerful deep learning-based label uncertainty quantification framework to enhance the reliability of the estimated ground-truth bounding box distributions. Technically, we formulate the label uncertainty problem as the diversity of potentially plausible bounding boxes and explicitly model the one-to-many relationship between a typical 3D object and its potentially plausible ground-truth boxes in a learning-based framework. Technically, we propose GLENet, a novel deep generative network adapted from conditional variational auto-encoders (CVAE), which introduces a latent variable to capture the distribution over potentially plausible bounding boxes of point cloud objects. During inference, we sample latent variables multiple times to generate diverse bounding boxes (see <ref type="figure" target="#fig_1">Fig. 3</ref>), the variance of which is taken as label uncertainty to guide the learning of localization uncertainty estimation The point cloud, annotated ground-truth boxes and predictions of GLENet are colored in black, red and green, respectively. GLENet produces diverse predictions for objects represented with sparse point clouds and incomplete outlines, and consistent bounding boxes for objects with high-quality point clouds. The variance of the multiple predictions by GLENet is used to estimate the uncertainty of the annotated ground-truth bounding boxes. in the downstream detection task. Besides, based on the observation that detection results with low localization uncertainty in probabilistic detectors tend to have accurate actual localization quality (see Section 4.2), we further propose uncertainty-aware quality estimator (UAQE), which facilitates the training of the IoU-branch with the localization uncertainty estimation.</p><p>To demonstrate our effectiveness and universality, we integrate GLENet into several popular 3D object detection frameworks to build powerful probabilistic detectors. Experiments on KITTI <ref type="bibr" target="#b10">(Geiger et al., 2012)</ref> and Waymo  datasets demonstrate that our method can bring consistent performance gains and achieve the current state-ofthe-art. Particularly, the proposed GLENet-VR surpasses all published single-modal detection methods by a large margin and ranks 1 among all published LiDAR-based approaches on the highly competitive KITTI 3D detection benchmark on March 29 ? , 2022 .</p><p>We summarize the main contributions of this paper as follows.</p><p>-We are the first to formulate the 3D label uncertainty problem as the diversity of potentially plausible bounding boxes of objects. To capture the one-to-many relationship between a typical 3D object and the potentially plausible ground-truth bounding boxes, we present a deep generative model named GLENet. Besides, we introduce a general and unified deep learning-based paradigm, including the network structure, loss function, evaluation metric, etc.</p><p>www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d</p><p>-Inspired by the strong correlation between the localization quality and the predicted uncertainty in probabilistic detectors, we propose UAQE to facilitate the training of the IoU-branch.</p><p>The remainder of the paper is organized as follows. Section 2 reviews existing works on LiDAR-based detectors and label uncertainty estimation methods. In Section 3, we explicitly formulate the label uncertainty estimation problem from the probabilistic distribution perspective, followed by the technical implementation of GLENet. In Section 4, we introduce a unified way of integrating the label uncertainty statistics predicted by GLENet into the existing 3D object detection frameworks to build more powerful probabilistic detectors, as well as some theoretical analysis. In Section 5, we conduct the experiments on the KITTI dataset and the Waymo Open dataset to demonstrate the effectiveness of our method in enhancing existing 3D detectors and the ablation study to analyze the effect of different components. Finally, Section 6 concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">LiDAR-based 3D Object Detection</head><p>Existing 3D object detectors can be classified into two categories: single-stage and two-stage. For single-stage detectors, <ref type="bibr" target="#b68">Zhou and Tuzel (2018)</ref> proposed to convert raw point clouds to regular volumetric representations and adopted voxel-based feature encoding. <ref type="bibr" target="#b56">Yan et al. (2018b)</ref> presented a more efficient sparse convolution. <ref type="bibr" target="#b18">Lang et al. (2019)</ref> converted point clouds to sparse fake images using pillars. Shi and Rajkumar (2020a) aggregated point information via a graph structure. <ref type="bibr" target="#b13">He et al. (2020)</ref> introduced point segmentation and center estimation as auxiliary tasks in the training phase to enhance model capacity. <ref type="bibr" target="#b66">Zheng et al. (2021a)</ref> constructed an SSFA module for robust feature extraction and a multi-task head for confidence rectification, and proposed DI-NMS for post-processing. For two-stage detectors,  exploited a voxel-based network to learn the additional spatial relationship between intra-object parts under the supervision of 3D box annotations. <ref type="bibr" target="#b39">Shi et al. (2019)</ref> proposed to directly generate 3D proposals from raw point clouds in a bottom-up manner, using semantic segmentation to valid point to regress detection boxes. The followup work  further proposed PointsPool to convert sparse proposal features to compact representations and used spherical anchors to generate accurate proposals.  utilized both point-based and voxel-based methods to fuse multi-scale voxel and point features. <ref type="bibr" target="#b5">Deng et al. (2021)</ref> proposed voxel RoI pooling to extract RoI features from coarse voxels.</p><p>Compared with 2D object detection, there are more serious boundary ambiguity problems in 3D object detection due to occlusion and signal miss. Studies, such as SPG <ref type="bibr" target="#b52">(Xu et al., 2021)</ref>, try to use point cloud completion methods to restore full shape of objects and improve the detection performance <ref type="bibr" target="#b55">(Yan et al., 2021;</ref><ref type="bibr" target="#b30">Najibi et al., 2020)</ref>. However, it's non-trivial to generate complete and precise shapes with incomplete point clouds only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Probabilistic 3D Object Detector</head><p>There are two types of uncertainty in deep learning predictions. A type of uncertainty, called aleatoric uncertainty, is caused by the inherent noise in observational data, which cannot be eliminated. The other type is called epistemic Uncertainty or model uncertainty, which is caused by incomplete training and can be alleviated with more training data. Most existing state-of-the-art 2D <ref type="bibr" target="#b23">(Liu et al., 2016;</ref><ref type="bibr" target="#b47">Tan et al., 2020;</ref><ref type="bibr" target="#b1">Carion et al., 2020)</ref> and 3D  object detectors produce a deterministic box with a confidence score for each detection. While the probability score represents the existence and semantic confidence, it cannot reflect the uncertainty about predicted localization well. By contrast, probabilistic object detectors <ref type="bibr" target="#b14">(He et al., 2019;</ref><ref type="bibr" target="#b28">Meyer et al., 2019;</ref><ref type="bibr" target="#b21">Li et al., 2020;</ref><ref type="bibr" target="#b48">Varamesh and Tuytelaars, 2020)</ref> estimate the probabilistic distribution of predicted bounding boxes rather than take them as deterministic results. For example, <ref type="bibr" target="#b14">He et al. (2019)</ref> and <ref type="bibr" target="#b3">Choi et al. (2019)</ref> modeled the predicted boxes as Gaussian distributions, the variance of which can indicate the localization uncertainty and is predicted with additional layer in the detection head. It introduces the KL Loss between the predicted Gaussian distribution and the ground-truth bounding boxes modeled as a Dirac delta function, so the regression branch is expected to output a larger variance and get a smaller loss for inaccurate localization estimation for the cases with ambiguous boundaries. Unlike common practice to model the box as a Gaussian distribution, <ref type="bibr" target="#b12">Harakeh et al. (2020)</ref> learned the offdiagonal elements of the covariance matrix of a multivariate Gaussian distribution as uncertainty estimation. <ref type="bibr" target="#b28">Meyer et al. (2019)</ref> proposed a probabilistic 3D object detector modeling the distribution of bounding box corners as a Laplacian distribution.</p><p>However, most probabilistic detectors take the groundtruth bounding box as a deterministic Dirac delta distribution and ignore the ambiguity in ground truth. Therefore, the localization variance is actually learned in an unsupervised manner, which may result in sub-optimal localization precision and erratic training (see our theoretical analysis in Section 4.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Label Uncertainty Estimation</head><p>Label noise (or uncertainty) is a common problem in realworld datasets and could seriously affect the performance of supervised learning algorithms. As the neural network is prone to overfit to even complete random noise ), it is important to prevent the network from overfitting noisy labels. An obvious solution is to consider the label of a misclassified sample to be uncertain and remove the samples <ref type="bibr" target="#b4">(Delany et al., 2012)</ref>. <ref type="bibr" target="#b9">Garcia et al. (2015)</ref> used a soft voting approach to approximate a noise level for each sample based on the aggregation of the noise degree prediction calculated for a set of binary classifiers. <ref type="bibr" target="#b24">Luengo et al. (2018)</ref> extended this work by correcting the label when most classifiers predict the same label for noisy samples. Confident Learning <ref type="bibr" target="#b32">Northcutt et al. (2021)</ref> estimated uncertainty in dataset labels by estimating the joint distribution of noisy labels and true labels. However, the above studies mainly focus on the image classification task.</p><p>There only exists a limited number of previous works focusing on quantifying uncertainty statistics of annotated ground-truth bounding boxes. <ref type="bibr" target="#b27">Meyer and Thakurdesai (2020)</ref> proposed to model label uncertainty by the IoU between the label bounding box and the corresponding convex hull of the aggregated LiDAR observations. However, it is nonlearning-based and thus has limited modeling capacity. Besides, it only produces uncertainty of the ground-truth box as a whole instead of each dimension. <ref type="bibr" target="#b51">Wang et al. (2020)</ref> proposed a Bayes method to estimate label noises by quantifying the matching degree of point clouds for the given boundary box with the Gaussian Mixture Model. However, its assumption of conditional probabilistic independence between point clouds is often untenable in practice. Differently, we formulate label uncertainty as diversity of potentially plausible bounding boxes. There may be some objects with few points that exactly match the learned surface points of corresponding labeled Bbox, so the label is considered by ) to be deterministic. But for an object with sparse point clouds, our GLENet will output different and plausible Bboxes and further estimate high label uncertainty based on them, regardless of whether points match the given label. In general, <ref type="bibr" target="#b51">Wang et al. (2020)</ref> used the Bayesian paradigm to estimate the correctness of the annotated box as the label uncertainty, while our method formulates it as the diversity of potentially plausible bounding boxes and predicts it by GLENet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Conditional Variational Auto-Encoder</head><p>The variational auto-encoder (VAE) <ref type="bibr" target="#b17">(Kingma and Welling, 2014)</ref> has been widely used in image and shape generation tasks <ref type="bibr" target="#b53">(Yan et al., 2016;</ref><ref type="bibr" target="#b31">Nash and Williams, 2017)</ref>. It transforms natural samples into a distribution where latent variables can be drawn and passed to a decoder network to generate diverse samples. <ref type="bibr" target="#b45">Sohn et al. (2015)</ref> proposed conditional variational auto-encoder (CVAE) extending the VAE with an extra condition to supervise the generative process. In the NLP field, VAE has been widely applied to many text generation tasks, such as dialogue response <ref type="bibr" target="#b65">(Zhao et al., 2017)</ref>, machine translation <ref type="bibr" target="#b62">(Zhang et al., 2016)</ref>, story generation <ref type="bibr" target="#b50">(Wang and Wan, 2019)</ref>, and poem composing <ref type="bibr" target="#b20">(Li et al., 2018)</ref>. VAE and CVAE have also been applied in computer vision tasks, like image generation <ref type="bibr" target="#b53">(Yan et al., 2016)</ref>, human pose estimation <ref type="bibr" target="#b37">(Sharma et al., 2019)</ref>, medical image segmentation <ref type="bibr" target="#b33">(Painchaud et al., 2020)</ref>, salient object detection <ref type="bibr" target="#b64">Zhang et al., 2020)</ref>, and modeling human motion dynamics <ref type="bibr" target="#b54">(Yan et al., 2018a)</ref>. Recently, VAE and CVAE algorithms have also been applied extensively to applications of 3D point clouds, such as generating grasp poses <ref type="bibr" target="#b29">(Mousavian et al., 2019)</ref> and instance segmentation <ref type="bibr" target="#b60">(Yi et al., 2019)</ref>.</p><p>Inspired by CVAE for generating diverse reasonable responses in dialogue systems, we propose GLENet adapted from CVAE to capture the one-to-many relationship between objects with incomplete point cloud and the potentially plausible ground-truth bounding boxes. To the best of our knowledge, we are the first to employ CVAE in 3D object detection to model label uncertainty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Label Uncertainty Estimation</head><p>As aforementioned, the ambiguity of annotated ground-truth labels widely exists in 3D object detection scenarios and has adverse effects on the deep model learning process, which is not well addressed or even completely ignored by previous works. To this end, we propose GLENet, a generic and unified deep learning framework that generates label uncertainty by modeling the one-to-many relationship between point cloud objects and potentially plausible bounding box labels. Then the variance of the multiple outputs of GLENet for a single object is computed as the label uncertainty, which is extended as an auxiliary regression objective to enhance the performance of the downstream 3D object detection task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>Let = { } =1 be a set of observed LiDAR points belonging to an object, where ? R 3 is a 3D point represented with spatial coordinates. Let be the annotated groundtruth bounding box of parameterized by the center location ( , , ), the size ( length , width , and height ?), and the orientation , i.e., = [ , , , , , ?, ] ? R 7 .</p><p>We formulate the uncertainty of the annotated groundtruth label of an object as the diversity of potentially plausible bounding boxes of the object, which could be quantitatively measured with the variance of the distribution of the potential bounding boxes. First, we model the distribution of these potential boxes conditioned on point cloud , denoted as ( | ). Specifically, based on the Bayes theorem, we introduce an intermediate variable to write the conditional distribution as</p><formula xml:id="formula_0">( | ) = ? ( | , ) ( | ) .<label>(1)</label></formula><p>Then, with ( | , ) and ( | ) known, we can adopt a Monte Carol method to get multiple bounding box predictions by sampling multiple times and approximate the variance of ( | ) with that of the sampled predictions.</p><p>In the following, we will introduce our learning-based framework named GLENet to realize the estimation process. <ref type="figure">Fig. 4 (a)</ref> shows the flowchart of GLENet parameterized by neural parameters , which aims to predict ( | ) and ( | , ). Specifically, under the assumption that the prior distribution ( | ) subjects to a multivariate Gaussian distribution parameterized by( , ), denoted as N ( , 2 ), we design a prior network, which is composed of Point-Net <ref type="bibr" target="#b35">(Qi et al., 2017)</ref> and additional MLP layers, from the input point cloud to predict the values of ( , ). Then, we employ a context encoder to embed the input point cloud into a high dimensional feature space, leading to the geometric feature representation , which is concatenated with sampled from N ( , 2 ) and fed into a prediction network composed of MLPs to regress the bounding box distribution ( | , ), i.e., the localization, dimension and orientation of the bounding box. <ref type="figure">Fig. 4</ref>: The overall workflow of GLENet. In the training phase, we learn parameters and (resp. and ) of latent variable (resp. ) through the prior network (resp. recognition network), after which a sample of and the corresponding geometrical embedding produced by the context encoder are jointly exploited to estimate the bounding box distribution. In the inference phase, we sample from the distribution of multiple times to generate different bounding boxes, whose variance we use as label uncertainty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Inference Process of GLENet</head><p>As empirically observed in various related domains <ref type="bibr" target="#b11">(Goyal et al., 2017)</ref>, it could be difficult to make use of latent variables when the prediction network can generate a plausible output only using the sufficiently expressive features of condition . Therefore, we utilize a simplified PointNet architecture as the backbone of the context encoder to avoid posterior collapse. We refer the readers to Section 5.1.3 for the implementation details of these modules. In the following sections, we also use ( | ), ( | , ), and ( | ) to denote the predictions of ( | ), ( | , ), and ( | ) by GLENet, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training Process of GLENet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Recognition Network</head><p>Given and its annotated bounding box , we assume there is a true posterior distribution ( | , ). Thus, during training, we construct a recognition network parameterized by network parameters (see <ref type="figure">Fig. 4 (b)</ref>) to learn an auxiliary posterior distribution ( | , ) subjecting to a Gaussian distribution, denoted as N ( , 2 ), to regularize ( | ), i.e., ( | ) should be close to ( | , ).</p><p>Specifically, for the recognition network, we adopt the same learning architecture as the prior network to generate point cloud embeddings, which are concatenated with ground-truth bounding box information and fed into the subsequent MLP layers to learn ( | , ). Moreover, to facilitate the learning process, we encode the information into offsets relative to predefined anchors, and then perform normalization as:</p><formula xml:id="formula_1">= , = , = ? , = log , = log , ? = log ? ? , = sin( ),<label>(2)</label></formula><p>where ( , , ? ) is the size of the predefined anchor located in the center of the point cloud, and = ?? ( ) 2 + ( ) 2 is the diagonal of the anchor box. We also take cos( ) as the additional input of the recognition network to handle the issue of angle periodicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Objective Function</head><p>Following CAVE, we optimize GLENet via maximizing the variational lower bound of the conditional log likelihood ( | ):</p><formula xml:id="formula_2">log ( | ) ? ( | , ) [log ( | , )]? ( ( | , )|| ( | )),<label>(3)</label></formula><p>where [ ] returns the expectation of on the distribution of , and (?) denotes KL-divergence. Specifically, the first term <ref type="bibr">( | , )</ref> [log ( | , )] enforces the prediction network to be able to restore groundtruth bounding box from latent variables. Following <ref type="bibr" target="#b56">(Yan et al. (2018b)</ref>) and ), we explicitly define the bounding box reconstruction loss as</p><formula xml:id="formula_3">= + ,<label>(4)</label></formula><p>where denotes the Huber loss imposed on the prediction and encoded regression targets as described in Eq. (2), and denotes the binary cross-entropy loss used for direction classification.</p><p>The second term ( ( | , ) ( | )) is aimed at regularizing the distribution of by minimizing the KLdivergence between ( | ) and ( | , ). Since ( | ) and</p><p>( | , ) are re-parameterized as N ( , 2 ) and N ( , 2 ) through the prior network and the recognition network, respectively, we can explicitly define the regularization loss as:</p><formula xml:id="formula_4">( ( | , ) ( | )) = log + 2 2 2 + ( ? ) 2 2 2 ,<label>(5)</label></formula><p>Thus, the overall objective function is written as</p><formula xml:id="formula_5">= + ,<label>(6)</label></formula><p>where we empirically set the hyperparameter set to 1 in all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Probabilistic 3D Detectors with Label Uncertainty</head><p>To reform a typical detector to be a probabilistic object detector, we can enforce the detection head to estimate a probability distribution over bounding boxes, denoted as ? ( ), instead of a deterministic bounding box location:</p><formula xml:id="formula_6">? ( ) = 1 ? 2?2 ? ( ??) 2 2?2 ,<label>(7)</label></formula><p>where ? indicates learnable network weights of a typical detector,?is the predicted bounding box location, and?is the predicted localization variance. Accordingly, we also assume the ground-truth bounding box as a Gaussian distribution ( ) with variance 2 , whose value is estimated by GLENet:</p><formula xml:id="formula_7">( ) = 1 ? 2 2 ? ( ? ) 2 2 2 ,<label>(8)</label></formula><p>where represents the ground-truth bounding box. Therefore, we can incorporate the generated label uncertainty in the KL loss between the distribution of prediction and groundtruth in the detection head:</p><formula xml:id="formula_8">= ( ( )|| ? ( )) = log?+ 2 2?2 + ( ??) 2 2?2 .<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">More Analysis of KL-Loss</head><p>When ignoring label ambiguity and formulating the groundtruth bounding box as a Dirac delta function, as done in <ref type="bibr" target="#b14">(He et al. (2019)</ref>), the loss in Eq. (9) degenerates into</p><formula xml:id="formula_9">? log(?2) 2 + ( ??) 2 2?2 ,<label>(10)</label></formula><p>and the partial derivative of Eq. (10) with respect to the predicted variance?is:</p><formula xml:id="formula_10">= 1 ? ( ??) 2 3 .<label>(11)</label></formula><p>When minimizing Eq. (10), a potential issue is that with | ??| ? 0,</p><formula xml:id="formula_11">? 1 ,<label>(12)</label></formula><p>resulting in that the derivative for?will explode when?? 0. Based on the property of KL-loss, the prediction is optimal only when the estimated?= 0 and the localization error | ??| = 0. Therefore, the gradient explosion may result in erratic training and sub-optimal localization precision. By contrast, after modeling the ground-truth bounding box as a Gaussian distribution, the partial derivative of Eq. (9) with respect to prediction is:</p><formula xml:id="formula_12">= 1 ? 2 3 ? ( ??) 2 3 ,<label>(13)</label></formula><p>and?=?? 2 .</p><p>As | ??| ? 0 and?&gt; 0,</p><formula xml:id="formula_14">? 1 (1 ? 2 2 ),<label>(15)</label></formula><p>and?? 0.</p><p>Thus, when the predicted distribution reaches the optimal solution that is the distribution of ground-truth, i.e., | ? | ? 0 and?? , the derivatives for both?and?become zero, which is an ideal property for the loss function and avoids the aforementioned gradient explosion issue. <ref type="figure">Fig. 5</ref> shows the landscape of the KL-divergence loss function under different label uncertainty , which are markedly different in shape and property. The approaches infinitesimal and the gradient explodes as | ??| ? 0 and?? 0. However, when we introduce the estimated label uncertainty and the predicted distribution is equal to the ground-truth distribution, the KL Loss has a determined minimum value of 0.5 and the gradient is smoother. <ref type="figure">Fig. 5</ref>: Illustration of the KL-divergence between distributions as a function of localization error | ??| and estimated localization variance?given different label uncertainty . With label uncertainty estimated by GLENet instead of zero, the gradient is smoother when the loss converges to the minimum. Besides, the is smaller when is larger, which prevents the model from overfitting to uncertain annotations. <ref type="figure">Fig. 6</ref>: (a) Illustration of the relationship between the actual localization precision (i.e., IoU between predicted and groundtruth bounding box) and the variance predicted by a probabilistic detector. Here, we reduce the dimension of the variance with PCA to facilitate visualization. (b) Two examples: for the sparse sample, the prediction has high uncertainty and low localization quality, while for the dense sample, the prediction has high localization quality and low uncertainty estimation.</p><formula xml:id="formula_16">(a) ( = 0) (b) ( = 0.2) (c) ( = 0.5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Uncertainty-aware Quality Estimator</head><p>Most state-of-the-art two-stage 3D object detectors predict an IoU-related confidence score indicating the localization quality, rather than the classification score as the sorting criterion in NMS (non-maximum suppression). As illustrated in <ref type="figure">Fig. 6</ref>, it can be observed that there is a strong correlation between the uncertainty and actual localization quality for each bounding box, which encourages us to use uncertainty as a criterion for judging the quality of boxes. However, the estimated uncertainty is 7-dimensional, making it infeasible to directly replace the IoU confidence score with the uncertainty. To this end, we propose uncertainty-aware quality estimator (UAQE), which introduces the uncertainty information to facilitate the training of the IoU-branch and improve the IoU estimation accuracy. Specifically, as shown in <ref type="figure" target="#fig_2">Fig. 7</ref>, given the predicted uncertainty as input, we construct a lightweight sub-module consisting of two fully-connected (FC) layers followed by the Sigmoid activation to generate a coefficient. Then we multiply the original output of the IoU-branch with the coefficient as the final estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: 3D var voting</head><p>Data: is an ? 7 matrix of predicted bounding boxes with parameter ( , , , , , ?, ). is the corresponding variance. is a set of N corresponding confidence values. is a tunable hyperparameter. Result: The final voting results of selected candidate boxes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">3D Variance Voting</head><p>Considering that in probabilistic object detectors, the learned localization variance by the KL loss can reflect the uncertainty of the predicted bounding boxes, following <ref type="bibr" target="#b14">(He et al., 2019)</ref>, we also propose 3D variance voting to combine neighboring bounding boxes to seek a more precise box representation. Specifically, at a single iteration in the loop, box with maximum score is selected and its new location is calculated according to itself and the neighboring boxes. During the merging process, the neighboring boxes that are closer and have a low variance are assigned with higher weights. Note that neighboring boxes with a large angle difference from do not participate in the ensembling of angles. We refer the readers to Algorithm 1 for the details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>To reveal the effectiveness and universality of our method, we integrated GLENet into several popular types of 3D object detection frameworks to form probabilistic detectors, which were evaluated on two commonly used benchmark datasets, i.e., the Waymo Open dataset (WOD)  and the KITTI dataset <ref type="bibr" target="#b10">(Geiger et al., 2012)</ref>. Specifically, we start by introducing specific experiment settings and implementation details in Section 5.1. After that, we report detection performance of the resulting probabilistic detectors and make comparisons with previous state-of-theart approaches in Sections 5.2 and 5.3. Finally, we conduct a series of ablation studies to verify the necessity of different key components and configurations in Section 5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Benchmark Datasets</head><p>The KITTI dataset contains 7481 training samples with annotations in the camera field of vision and 7518 testing samples. According to the occlusion level, visibility and bounding box size, the samples are further divided into three difficulty levels: simple, moderate and hard. Following common practice, when performing experiments on the val set, we further split all training samples into a subset with 3712 samples for training and the rest 3769 samples for validation. We report the performance on both the val set and online test leaderboard for comparison. And we use all training data for the test server submission.</p><p>The Waymo Open dataset is a large-scale autonomous driving dataset with more diverse scenes and object annotations in full 360 ? , which contains 798 sequences (158361 Li-DAR frames) for training and 202 sequences (40077 LiDAR frames) for validation. These frames are further divided into two difficulty levels: LEVEL1 for boxes with more than five points and LEVEL2 for boxes with at least one point. We report performance on both LEVEL 1 and LEVEL 2 difficulty objects using the recommended metrics, mean Average Precision (mAP) and mean Average Precision weighted by heading accuracy (mAPH).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Evaluation Metric for GLENet</head><p>Due to the unavailability of the true distribution of a groundtruth bounding box, we propose to evaluate GLENet in a non-reference manner, in which the negative log-likelihood between the estimated distribution of ground-truth ( | ) subjecting to a Gaussian distribution N (?, 2 ) and ( | ) is computed:</p><formula xml:id="formula_17">( ) = ? ? ( | ) log ( | )d (17) ? ? 1 ?? =1 log ( | ) = ? 1 ?? =1 ?? ?{ , , , , ,?, } ( ??) 2 2 2 + log( 2 ) 2 + log(2 ) 2 ,</formula><p>where denotes the number of inference times, is the result of the -th inference, and?and represent the regression targets and the predicted offsets, respectively. We estimate the integral by randomly sampling multiple prediction results via the Monte Carlo method. Generally, the value of is small when GLENet outputs reasonable bounding boxes, i.e., predicting diverse plausible boxes with high variance for incomplete point cloud and consistent precise boxes with low variance for high-quality point cloud, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Implementation Details</head><p>To prevent data leakage, we kept the dataset division of GLENet consistent with that of the downstream detectors.</p><p>As the initial input of GLENet, the point cloud of each object was uniformly pre-processed into 512 points via random subsampling/upsampling. Then we decentralized the point cloud by subtracting the coordinates of the center point to eliminate the local impact of translation. Architecturally, we realized the prior network and recognition network with an identical PointNet structure consisting of three FC layers of output dimensions (64, 128, 512), followed by another FC layer to generate an 8-dim latent variable. To avoid posterior collapse, we particularly chose a lightweight PointNet structure with channel dimensions <ref type="bibr">(8,</ref><ref type="bibr">8,</ref><ref type="bibr">8)</ref> in the context encoder. The prediction network concatenates the generated latent variable and context features and feeds them into subsequent FC layers of channels (64, 64) before predicting offsets and directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Training and Inference Strategies</head><p>We adopted Adam <ref type="bibr" target="#b16">(Kingma and Ba, 2015)</ref> ( 1 =0.9, 2 =0.99) to optimize GLENet, which was trained for totally 400 epochs on KITTI and 40 epochs on Waymo while maintaining a batch size of 64 on 2 GPUs. We initialized the learning rate as 0.003 and updated it with the one cycle policy <ref type="bibr" target="#b44">(Smith, 2017)</ref>.</p><p>In the training process, we applied common data augmentation strategies, including random flipping, scaling, and rotation, in which the scaling factor and rotation angle were uniformly drawn from [0.95, 1.05] and [? /4, /4], respectively. It is important to include multiple plausible groundtruth boxes in training especially for incomplete point clouds, so we further propose an occlusion-driven augmentation approach, as illustrated in <ref type="figure" target="#fig_4">Fig. 8</ref>, after which a complete point cloud may look similar to another incomplete point cloud, while the ground-truth boxes of them are completely different. To overcome posterior collapse, we also adopted KL annealing <ref type="bibr" target="#b0">(Bowman et al., 2016)</ref> to gradually increase the weight of the KL loss from 0 to 1. We followed k-fold crosssampling to divide all training objects into 10 mutually exclusive subsets. To overcome overfitting, each time we trained GLENet on 9 subsets and then made predictions on the remaining subset to generate label uncertainty estimations on the whole training set. During inference, we sampled the latent variable from the predicted prior distribution ( | ) 30 times to form multiple predictions, the variance of which was used as the label uncertainty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.5">Base Detectors</head><p>We integrated GLENet into three popular deep 3D object detection frameworks, i.e., SECOND <ref type="bibr" target="#b56">(Yan et al., 2018b)</ref>, CIA-SSD <ref type="bibr" target="#b66">(Zheng et al., 2021a)</ref>, and Voxel R-CNN , to construct probabilistic detectors, which are dubbed as GLENet-S, GLENet-C, and GLENet-VR, respectively. Specifically, we introduced an extra FC layer on the top of the detection head to estimate standard deviations along with the box locations. Meanwhile, we applied the proposed UAQE to GLENet-VR to facilitate the training of the IoUbranch. Note that we kept all the other network configurations of these base detectors unchanged for fair comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation on the KITTI Dataset</head><p>We compared GLENet-VR with state-of-the-art detectors on the KITTI test set, and <ref type="table" target="#tab_0">Table 1</ref> reports the AP and mAP that averages over the APs of easy, moderate and hard objects. As of March 29 ? , 2022, our GLENet-VR surpasses all published single-modal detection methods by a large margin and ranks 1 among all published LiDAR-based approaches. Besides, <ref type="figure">Fig. 9</ref> also provides the detailed Prevision-Recall (PR) curves of GLENet-VR on KITTI test split. <ref type="table" target="#tab_1">Table 2</ref> lists the validation results of different detection frameworks on the KITTI dataset, from which we can observe that GLENet-S, GLENet-C, and GLENet-VR consistently outperform their corresponding baseline methods, i.e., SECOND, CIA-SSD, and Voxel R-CNN, by 4.79%, 4.78%, and 1.84% in terms of 3D R11 AP on the category of moderate car. Particularly, GLENet-VR achieves 86.36% AP on the moderate car class, which surpasses all other state-of-theart methods. Besides, as a single-stage method, GLENet-C achieves 84.59% AP for the moderate vehicle class, which is comparable to the exiting two-stage approaches while achieving relatively lower inference costs. It is worth noting that our method is compatible with mainstream detectors and can be expected to achieve better performance when combined with stronger base detectors. <ref type="table" target="#tab_3">Table 3</ref> lists the evaluation results of different approaches on both LEVEL_1 and LEVEL_2 of the Waymo Open dataset, where it can be seen that our method contributes 2.44% and 1.24% improvements in terms of LEVEL_1 mAP for SECOND and Voxel R-CNN, respectively. Besides, the performance boost brought by our method becomes much more obvious in the range of 30-50m and 50m-Inf. Intuitively, this is because distant point cloud objects tend to be sparser and thus have more serious issues of bounding box ambiguity. GLENet-VR achieves better performance than the existing methods with 77.32% mAP and 69.68% mAP for the LEVEL 1 and LEVEL 2 difficulty, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation on the Waymo Open Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Study</head><p>We conducted ablative analyses to verify the effectiveness and characteristics of our processing pipeline. In this section, all the involved model variants are built upon the Voxel R-CNN baseline and evaluated on the KITTI dataset, under the evaluation metric of average precision calculated with 40 recall positions.   TPAMI'20 89.47 79.47 78.54 ---3DSSD  CVPR'20 89.71 79.45 78.67 ---SA-SSD <ref type="bibr" target="#b13">(He et al., 2020)</ref> CVPR  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Comparison with Other Label Uncertainty Estimation</head><p>We compared with other two ways of label uncertainty estimation: 1) treating the label distribution as the deterministic Dirac delta distribution with zero uncertainty; 2) estimating the label uncertainty with simple heuristics, i.e., the number of points in the ground-truth bounding box or the IoU between the label bounding box and its convex hull of the aggregated LiDAR observations <ref type="bibr" target="#b27">(Meyer and Thakurdesai, 2020)</ref>. As shown in <ref type="table" target="#tab_4">Table 4</ref>, our method consistently outperforms existing label uncertainty estimation paradigms. Compared with heuristic strategies, our deep generative learning paradigm can adaptively estimate label uncertainty statistics in 7 dimensions, instead of the uncertainty of bounding boxes as a whole, considering the variance in each dimension could be very different.</p><p>Besides, to compare with , whose code is not publicly available, we evaluated our method under its experiment settings and compared results with its reported performance. As shown in <ref type="table" target="#tab_5">Table 5</ref>, our method outperforms  significantly in terms of AP on both moderate and hard levels. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Key Components of Probabilistic Detectors</head><p>We analyzed the contributions of different key components in our constructed probabilistic detectors and reported results  in <ref type="table" target="#tab_6">Table 6</ref>. According to the second row, we can conclude that only training with the KL loss brings little performance gain. Introducing the label uncertainty generated by GLENet into the KL Loss contributes 0.75%, 0.51%, and 0.3% improvements on the APs of easy, moderate, and hard classes, respectively, which demonstrates its regularization effect on KLD-loss (Eq. 9) and its ability to estimate more reliable uncertainty statistics of bounding box labels. The proposed UAQE module in the probabilistic detection head boosts the easy, moderate, and hard APs by 0.25%, 0.19% and 0.15%, respectively, validating its effectiveness in estimating the localization quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3">Influence of Data Augmentation</head><p>To generate similar point cloud shapes with diverse groundtruth bounding boxes during training of GLENet, we proposed an occlusion data augmentation strategy and generated more incomplete point clouds while keeping the bounding boxes unchanged (see <ref type="figure" target="#fig_4">Fig. 8</ref>). As listed in <ref type="table" target="#tab_7">Table 7</ref>, it can be seen that the occlusion data augmentation effectively enhances the performance of GLENet and the downstream detection task. Besides, the effectiveness of the metric is also validated, which is proposed to evaluate GLENet and select optimal configurations to generate reliable label uncertainty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.4">Conditional Analysis</head><p>To figure out in what cases our method improves the base detector most, we evaluated GLENet-VR on different occlusion levels and distance ranges. As shown in <ref type="table" target="#tab_8">Table 8</ref>, compared with the baseline, our method mainly improves on the heavily  a The results include separate APs for objects belonging to different occlusion levels and APs for moderate vehicle class in different distance ranges. b Definition of occlusion levels: levels 0, 1 and 2 correspond to fully visible samples, partly occluded samples, and samples difficult to see respectively. Method FPS (Hz) SECOND <ref type="bibr" target="#b56">Yan et al. (2018b)</ref> 23.36 GLENet-S (Ours) 22.80 CIA-SSD <ref type="bibr" target="#b66">Zheng et al. (2021a)</ref> 27.18 GLENet-C (Ours)</p><p>28.76 Voxel R- <ref type="bibr">CNN Deng et al. (2021)</ref> 21.08 GLENet-VR (Ours) 20.82 occluded and distant samples, which suffer from more serious boundary ambiguities of ground-truth bounding boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.5">Inference Efficiency</head><p>We evaluated the inference speed of different baselines with a batch size of 1 on a desktop with Intel CPU E5-2560 @ 2.10 GHz and NVIDIA GeForce RTX 2080Ti GPU. As shown in <ref type="table" target="#tab_9">Table 9</ref>, our approach doesn't significantly increase the computational overhead. Particularly, GLENet-VR only takes 0.6 more ms than the base Voxel R-CNN, since the number of candidates for the input of var voting is relatively small in two-stage detectors. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Comparison of Visual Results</head><p>Fig. 10 visualizes the detection results of our GLENet-VR and the baseline Voxel R-CNN on the KITTI val set, where it can be seen that our GLENet-VR obtains better detection results with fewer false-positive bounding boxes and fewer missed heavily occluded and distant objects than Voxel R-CNN. We also compared detection results of SECOND and GLENet-S on the Waymo validation set in <ref type="figure">Fig. 11</ref>, where it can be seen that compared with SECOND <ref type="bibr" target="#b56">(Yan et al., 2018b)</ref>, our GLENet-S has fewer false predictions and achieves more accurate localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We presented a general and unified deep learning-based paradigm for modeling 3D object-level label uncertainty. Technically, we proposed GLENet, adapted from the learning framework of CVAE, to capture one-to-many relationships between incomplete point cloud objects and potentially plausible bounding boxes. As a plug-and-play component, GLENet can generate reliable label uncertainty statistics that can be conveniently integrated into various 3D detection pipelines to build powerful probabilistic detectors. We verified the effectiveness and universality of our method by incorporating the proposed GLENet into several existing deep 3D object detectors, which demonstrated consistent improvement and produced state-of-the-art performance on both KITTI and Waymo datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Illustration of two different learning paradigms of probabilistic object detectors. (a) Methods that adopt probabilistic modeling in the detection head but essentially still ignore the issue of ambiguity in ground-truth bounding boxes. (b) Methods that explicitly estimate ground-truth bounding box distributions to be used as more reliable supervision signals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Illustration of multiple potentially plausible bounding boxes from GLENet by sampling latext variables multiple times.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 7 :</head><label>7</label><figDesc>Illustration of the proposed UAQE module in the detection head using the learned localization variance to assist the training of localization quality (IoU) estimation branch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1= 2 =</head><label>2</label><figDesc>{ 1 , 2 , ..., }; and = { 1 , 2 , ..., }; { 1 , 2 , ..., }; and = {1, 2, ...,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 :</head><label>8</label><figDesc>Illustration of the occlusion data augmentation. (a) The point cloud of the original object associated with the annotated ground-truth bounding box. (b) A sampled dense object (red) is placed between the LiDAR sensor and original object (blue). (c) The projected range image from the point cloud in (b), where the convex hull (the red polygon) of the sampled object is calculated and further jittered to increase the diversity of occluded samples. Based on the convex hull (the blue polygon) of the original point cloud, the occluded area can be obtained. The point cloud of the original object corresponding to the occluded area is removed. (d) Final augmented object with the annotated ground-truth bounding boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 10 :</head><label>10</label><figDesc>Visual comparison of the results by GLENet-VR and Voxel R-CNN on the KITTI dataset. The ground-truth, true positive and false positive bounding boxes are visualized in red, green and yellow, respectively, on both the point cloud and image. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Quantitative comparison with state-of-the-art methods on the KITTI test set for vehicle detection, under the evaluation metric of 3D Average Precision (AP) of 40 sampling recall points. The best and second best results are highlighted in bold and underlined, respectively.</figDesc><table><row><cell>Method</cell><cell>Reference</cell><cell>Modality</cell><cell>Easy</cell><cell cols="2">3D AP 40 Mod. Hard</cell><cell>mAP</cell></row><row><cell>MV3D (Chen et al., 2017)</cell><cell>CVPR'17</cell><cell>RGB+LiDAR</cell><cell>74.97</cell><cell>63.63</cell><cell>54.00</cell><cell>64.20</cell></row><row><cell>F-PointNet (Qi et al., 2018)</cell><cell>CVPR'18</cell><cell>RGB+LiDAR</cell><cell>82.19</cell><cell>69.79</cell><cell>60.59</cell><cell>70.86</cell></row><row><cell>MMF (Liang et al., 2019)</cell><cell>CVPR'19</cell><cell>RGB+LiDAR</cell><cell>88.40</cell><cell>77.43</cell><cell>70.22</cell><cell>78.68</cell></row><row><cell>PointPainting (Vora et al., 2020)</cell><cell>CVPR'20</cell><cell>RGB+LiDAR</cell><cell>82.11</cell><cell>71.70</cell><cell>67.08</cell><cell>73.63</cell></row><row><cell>CLOCs (Pang et al., 2020)</cell><cell>IROS'20</cell><cell>RGB+LiDAR</cell><cell>88.94</cell><cell>80.67</cell><cell>77.15</cell><cell>82.25</cell></row><row><cell>EPNet (Huang et al., 2020)</cell><cell>ECCV'20</cell><cell>RGB+LiDAR</cell><cell>89.81</cell><cell>79.28</cell><cell>74.59</cell><cell>81.23</cell></row><row><cell>3D-CVF (Yoo et al., 2020)</cell><cell>ECCV'20</cell><cell>RGB+LiDAR</cell><cell>89.20</cell><cell>80.05</cell><cell>73.11</cell><cell>80.79</cell></row><row><cell>STD (Yang et al., 2019)</cell><cell>ICCV'19</cell><cell>LiDAR</cell><cell>87.95</cell><cell>79.71</cell><cell>75.09</cell><cell>80.92</cell></row><row><cell>Part-A2 (Shi et al., 2020b)</cell><cell>TPAMI'20</cell><cell>LiDAR</cell><cell>87.81</cell><cell>78.49</cell><cell>73.51</cell><cell>79.94</cell></row><row><cell>3DSSD (Yang et al., 2020)</cell><cell>CVPR'20</cell><cell>LiDAR</cell><cell>88.36</cell><cell>79.57</cell><cell>74.55</cell><cell>80.83</cell></row><row><cell>SA-SSD (He et al., 2020)</cell><cell>CVPR'20</cell><cell>LiDAR</cell><cell>88.80</cell><cell>79.52</cell><cell>72.30</cell><cell>80.21</cell></row><row><cell>PV-RCNN (Shi et al., 2020a)</cell><cell>CVPR'20</cell><cell>LiDAR</cell><cell>90.25</cell><cell>81.43</cell><cell>76.82</cell><cell>82.83</cell></row><row><cell>PointGNN (Shi and Rajkumar, 2020b)</cell><cell>CVPR' 20</cell><cell>LiDAR</cell><cell>88.33</cell><cell>79.47</cell><cell>72.29</cell><cell>80.03</cell></row><row><cell>Voxel-RCNN (Deng et al., 2021)</cell><cell>AAAI'21</cell><cell>LiDAR</cell><cell>90.90</cell><cell>81.62</cell><cell>77.06</cell><cell>83.19</cell></row><row><cell>SE-SSD (Zheng et al., 2021b)</cell><cell>CVPR'21</cell><cell>LiDAR</cell><cell>91.49</cell><cell>82.54</cell><cell>77.15</cell><cell>83.73</cell></row><row><cell>VoTR (Mao et al., 2021b)</cell><cell>ICCV'21</cell><cell>LiDAR</cell><cell>89.90</cell><cell>82.09</cell><cell>79.14</cell><cell>83.71</cell></row><row><cell>Pyramid-PV (Mao et al., 2021a)</cell><cell>ICCV'21</cell><cell>LiDAR</cell><cell>88.39</cell><cell>82.08</cell><cell>77.49</cell><cell>82.65</cell></row><row><cell>CT3D (Sheng et al., 2021)</cell><cell>ICCV'21</cell><cell>LiDAR</cell><cell>87.83</cell><cell>81.77</cell><cell>77.16</cell><cell>82.25</cell></row><row><cell>GLENet-VR (Ours)</cell><cell>-</cell><cell>LiDAR</cell><cell>91.67</cell><cell>83.23</cell><cell>78.43</cell><cell>84.44</cell></row><row><cell cols="2">Fig. 9: PR curves of GLENet-VR on the car class of the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>KITTI test set.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Quantitative comparison of different methods on the KITTI validation set for vehicle detection, under the evaluation metric of 3D Average Precision (AP) calculated with 11 sampling recall positions. The 3D APs under 40 recall sampling recall points are also reported for the moderate car class. The best and second best results are highlighted in bold and underlined, respectively.</figDesc><table><row><cell>Methods</cell><cell>Reference</cell><cell>Easy</cell><cell>3D AP 11 Moderate</cell><cell>Hard</cell><cell>Easy</cell><cell>3D AP 40 Moderate</cell><cell>Hard</cell></row><row><cell>Part-2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Quantitative comparison of different methods on the Waymo validation set for vehicle detection. ?: experiment results re-produced with the official code. The best and second best results are highlighted in bold and underlined, respectively.</figDesc><table><row><cell>Methods</cell><cell cols="10">LEVEL_1 3D mAP Overall 0-30m 30-50m 50m-inf Overall Overall 0-30m 30-50m 50m-inf Overall mAPH LEVEL_2 3D mAP mAPH</cell></row><row><cell>PointPillar (Lang et al., 2019)</cell><cell>56.62</cell><cell>81.01</cell><cell>51.75</cell><cell>27.94</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MVF (Zhou et al., 2020)</cell><cell>62.93</cell><cell>86.30</cell><cell>60.02</cell><cell>36.02</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PV-RCNN (Shi et al., 2020a)</cell><cell>70.30</cell><cell>91.92</cell><cell>69.21</cell><cell>42.17</cell><cell>69.69</cell><cell>65.36</cell><cell>91.58</cell><cell>65.13</cell><cell>36.46</cell><cell>64.79</cell></row><row><cell>VoTr-TSD (Mao et al., 2021b)</cell><cell>74.95</cell><cell>92.28</cell><cell>73.36</cell><cell>51.09</cell><cell>74.25</cell><cell>65.91</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>65.29</cell></row><row><cell>Pyramid-PV (Mao et al., 2021a)</cell><cell>76.30</cell><cell>92.67</cell><cell>74.91</cell><cell>54.54</cell><cell>75.68</cell><cell>67.23</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>66.68</cell></row><row><cell>CT3D (Sheng et al., 2021)</cell><cell>76.30</cell><cell>92.51</cell><cell>75.07</cell><cell>55.36</cell><cell>-</cell><cell>69.04</cell><cell>91.76</cell><cell>68.93</cell><cell>42.60</cell><cell>-</cell></row><row><cell>SECOND ? (Yan et al., 2018b)</cell><cell>69.85</cell><cell>90.71</cell><cell>68.93</cell><cell>41.17</cell><cell>69.40</cell><cell>62.76</cell><cell>86.92</cell><cell>62.57</cell><cell>35.89</cell><cell>62.30</cell></row><row><cell>GLENet-S (Ours)</cell><cell>72.29</cell><cell>91.02</cell><cell>71.86</cell><cell>45.43</cell><cell>71.85</cell><cell>64.78</cell><cell>87.56</cell><cell>65.11</cell><cell>38.60</cell><cell>64.25</cell></row><row><cell>Voxel R-CNN ? (Deng et al., 2021)</cell><cell>76.08</cell><cell>92.44</cell><cell>74.67</cell><cell>54.69</cell><cell>75.67</cell><cell>68.06</cell><cell>91.56</cell><cell>69.62</cell><cell>42.80</cell><cell>67.64</cell></row><row><cell>GLENet-VR (Ours)</cell><cell>77.32</cell><cell>92.97</cell><cell>76.28</cell><cell>55.98</cell><cell>76.85</cell><cell>69.68</cell><cell>92.09</cell><cell>71.21</cell><cell>44.36</cell><cell>68.97</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison of different label uncertainty estimation approaches. "Convex hull" refers to the method in<ref type="bibr" target="#b27">(Meyer and Thakurdesai, 2020)</ref>.</figDesc><table><row><cell>Methods</cell><cell></cell><cell cols="3">3D AP 40 Easy Moderate Hard</cell></row><row><cell>Voxel R-CNN</cell><cell></cell><cell>92.38</cell><cell>85.29</cell><cell>82.86</cell></row><row><cell>GLENet-VR w/</cell><cell>( 2 =0)</cell><cell>92.48</cell><cell>85.37</cell><cell>83.05</cell></row><row><cell>GLENet-VR w/</cell><cell cols="2">(points num) 92.46</cell><cell>85.58</cell><cell>83.16</cell></row><row><cell>GLENet-VR w/</cell><cell cols="2">(convex hull) 92.33</cell><cell>85.45</cell><cell>82.81</cell></row><row><cell>GLENet-VR w/</cell><cell>(Ours)</cell><cell>93.49</cell><cell>86.10</cell><cell>83.56</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparison of our method with (Wang et al., 2020) on the KITTI val set.</figDesc><table><row><cell>Method</cell><cell></cell><cell>for IoU@0.7 Easy Mod. Hard</cell></row><row><cell cols="2">PIXOR (Yang et al., 2018)</cell><cell>86.79 80.75 76.60</cell></row><row><cell>ProbPIXOR + L</cell><cell>( = 0)</cell><cell>88.60 80.44 78.74</cell></row><row><cell>ProbPIXOR + L</cell><cell cols="2">(Wang et al., 2020) 92.22 82.03 79.16</cell></row><row><cell>ProbPIXOR + L</cell><cell>(Ours)</cell><cell>91.50 84.23 81.85</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Contribution of each component in our constructedGLENet-VR pipeline. "LU" denotes the label uncertainty.</figDesc><table><row><cell cols="3">KL loss LU var voting UAQE Easy Moderate Hard</cell></row><row><cell>92.38</cell><cell>85.29</cell><cell>82.86</cell></row><row><cell>92.45</cell><cell>85.25</cell><cell>82.99</cell></row><row><cell>92.48</cell><cell>85.37</cell><cell>83.05</cell></row><row><cell>93.20</cell><cell>85.76</cell><cell>83.29</cell></row><row><cell>93.24</cell><cell>85.91</cell><cell>83.41</cell></row><row><cell>93.49</cell><cell>86.10</cell><cell>83.56</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Ablation study on occlusion augmentation techniques in GLENet, in which we report the for evaluation of GLENet and the 3D average precisions of 40 sampling recall points for evaluation of downstream detectors.</figDesc><table><row><cell>Occlusion</cell><cell>?</cell><cell>Easy</cell><cell>Mod.</cell><cell>Hard</cell></row><row><cell>?</cell><cell>230.1</cell><cell>93.21</cell><cell>85.86</cell><cell>83.35</cell></row><row><cell></cell><cell>91.5</cell><cell>93.49</cell><cell>86.10</cell><cell>83.56</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Comparison on different occlusion levels and distance ranges a , evaluated by the 3D Average Precision (AP) calculated with 40 sampling recall positions on the KITTI val set.</figDesc><table><row><cell cols="2">Methods</cell><cell>Voxel R-CNN (Deng et al., 2021)</cell><cell>GLENet-VR (Ours)</cell><cell>Improvement</cell></row><row><cell></cell><cell>0</cell><cell>92.35</cell><cell>93.51</cell><cell>+ .</cell></row><row><cell>Occlusion b</cell><cell>1</cell><cell>76.91</cell><cell>78.64</cell><cell>+ .</cell></row><row><cell></cell><cell>2</cell><cell>54.32</cell><cell>56.93</cell><cell>+ .</cell></row><row><cell></cell><cell>0-20m</cell><cell>96.42</cell><cell>96.69</cell><cell>+ .</cell></row><row><cell>Distance</cell><cell>20-40m</cell><cell>83.82</cell><cell>86.87</cell><cell>+ .</cell></row><row><cell></cell><cell>40m-Inf</cell><cell>38.86</cell><cell>39.82</cell><cell>+ .</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Inference time comparison for different baselines on the KITTI dataset.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="10" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European conference on computer vision</title>
		<imprint>
			<biblScope unit="page" from="213" to="229" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1907" to="1915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Gaussian yolov3: An accurate and fast object detector using localization uncertainty for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="page" from="502" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Profiling instances in noise reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Delany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Segata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mac</forename><surname>Namee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="28" to="40" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<title level="m">Voxel r-cnn: Towards high performance voxel-based 3d object (a) SECOND (b) GLENet-S (Ours)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The ground-truth, true positive and false positive bounding boxes are visualized in red, green and yellow, respectively. Best viewed in color and zoom in for more details. Additional NMS is conducted for better visualization. detection</title>
	</analytic>
	<monogr>
		<title level="m">Visual comparison of the results by SECOND and GLENet-S on the Waymo val set</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1201" to="1209" />
		</imprint>
	</monogr>
	<note>Proceedings of the AAAI Conference on Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards safe autonomous driving: Capture uncertainty in the deep neural network for lidar 3d vehicle detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dietmayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Conference on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="page" from="3266" to="3273" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Leveraging heteroscedastic aleatoric uncertainties for robust real-time lidar 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Timm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dietmayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1280" to="1287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Using the one-vs-one decomposition to improve the performance of class noise filters via an aggregation strategy in multi-class classification problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>S?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luengo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Lorena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>De Carvalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="153" to="164" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Zforcing: Training stochastic recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>C?t?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6714" to="6724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bayesod: A bayesian approach for uncertainty estimation in deep object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Smart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<biblScope unit="page" from="87" to="93" />
			<date type="published" when="2020" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Structure aware single-stage 3d object detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11870" to="11879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bounding box regression with uncertainty for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2888" to="2897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Epnet: Enhancing point features with image semantics for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="35" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Be?bom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12689" to="12697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Supervae: Superpixelwise variational autoencoder for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8569" to="8576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generating classical chinese poems via conditional variational autoencoder and adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 conference on empirical methods in natural language processing</title>
		<meeting>the 2018 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3890" to="3900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="21002" to="21012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multitask multi-sensor fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7345" to="7353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cnc-nos: Class noise cleaning by ensemble filtering and noise scoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luengo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">O</forename><surname>Shim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alshomrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Altalhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page" from="27" to="49" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pyramid r-cnn: Towards better performance and adaptability for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2723" to="2732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Voxel transformer for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3164" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning an uncertaintyaware object detector for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thakurdesai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10521" to="10527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Lasernet: An efficient probabilistic 3d object detector for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Laddha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vallespi-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Wellington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12677" to="12686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">6-dof graspnet: Variational grasp generation for object manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Eppner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2901" to="2910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dops: Learning to detect 3d objects and predict their 3d shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11910" to="11919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The shape variational autoencoder: A deep generative model of part-segmented 3d objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Confident learning: Estimating uncertainty in dataset labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Northcutt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1373" to="1411" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cardiac segmentation with strong anatomical guarantees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Painchaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Skandarani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Judge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lalande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Jodoin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3703" to="3713" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Clocs: Camera-lidar object candidates fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Radha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10386" to="10393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="918" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation by generation and ordinal ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Varigonda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bindal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2325" to="2334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Improving 3d object detection with channel-wise transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2743" to="2752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="770" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pv-rcnn: Point-voxel feature set abstraction for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10529" to="10538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">From points to parts: 3d object detection from point cloud with partaware and part-aggregation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2647" to="2664" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Point-gnn: Graph neural network for 3d object detection in a point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rajkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1708" to="1716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Point-gnn: Graph neural network for 3d object detection in a point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rajkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1711" to="1719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Cyclical learning rates for training neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="464" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3483" to="3491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Anguelov D (2020) Scalability in perception for autonomous driving: Waymo open dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kretzschmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dotiwalla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chouard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Patnaik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Timofeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ettinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krivokon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<biblScope unit="page" from="2443" to="2451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Efficientdet: Scalable and efficient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10781" to="10790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Mixture dense regression for object detection and human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varamesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13086" to="13095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pointpainting: Sequential fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Helou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Be?bom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4604" to="4612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">T-cvae: Transformer-based conditioned variational autoencoder for story completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5233" to="5239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Inferring spatial uncertainty in object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Timm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dietmayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5792" to="5799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Spg: Unsupervised domain adaptation for 3d object detection via semantic point generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15446" to="15456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Attribute2image: Conditional image generation from visual attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="776" to="791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Mt-vae: Learning motion transformations to generate multimodal human dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hadap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="265" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Sparse single sweep lidar point cloud segmentation via learning contextual shape priors from scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="3101" to="3109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3337</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Pixor: Real-time 3d object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7652" to="7660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Std: Sparse-todense 3d object detector for point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1951" to="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Point-based 3d single stage object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="11040" to="11048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Gspn: Generative shape proposal network for 3d instance segmentation in point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3942" to="3951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Generating joint camera and lidar features using cross-view spatial feature fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="720" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Variational neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 conference on empirical methods in natural language processing</title>
		<meeting>the 2016 conference on empirical methods in natural language processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="521" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Understanding deep learning (still) requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="107" to="115" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Uc-net: Uncertainty inspired rgb-d saliency detection via conditional variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8582" to="8591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning discourselevel diversity for neural dialog models using conditional variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eskenazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2017 -55th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="654" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Ciassd: Confident iou-aware single-stage object detector from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3555" to="3562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Se-ssd: Selfensembling single-stage object detector from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14494" to="14503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">End-to-end multi-view fusion for 3d object detection in lidar point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning, PMLR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="923" to="932" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
