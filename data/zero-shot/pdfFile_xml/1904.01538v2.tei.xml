<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spatial Attentive Single-Image Deraining with a High Quality Real Rain Dataset</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">City University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">City University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">City University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozhe</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rynson</forename><forename type="middle">W H</forename><surname>Lau</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">City University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Spatial Attentive Single-Image Deraining with a High Quality Real Rain Dataset</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Removing rain streaks from a single image has been drawing considerable attention as rain streaks can severely degrade the image quality and affect the performance of existing outdoor vision tasks. While recent CNN-based derainers have reported promising performances, deraining remains an open problem for two reasons. First, existing synthesized rain datasets have only limited realism, in terms of modeling real rain characteristics such as rain shape, direction and intensity. Second, there are no public benchmarks for quantitative comparisons on real rain images, which makes the current evaluation less objective. The core challenge is that real world rain/clean image pairs cannot be captured at the same time. In this paper, we address the single image rain removal problem in two ways. First, we propose a semi-automatic method that incorporates temporal priors and human supervision to generate a high-quality clean image from each input sequence of real rain images. Using this method, we construct a large-scale dataset of ?29.5K rain/rain-free image pairs that covers a wide range of natural rain scenes. Second, to better cover the stochastic distribution of real rain streaks, we propose a novel SPatial Attentive Network (SPANet) to remove rain streaks in a local-to-global manner. Extensive experiments demonstrate that our network performs favorably against the state-of-the-art deraining methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Images taken under various rain conditions often show low visibility, which can significantly affect the performance of some outdoor vision tasks, e.g., pedestrian detection <ref type="bibr" target="#b29">[30]</ref>, visual tracking <ref type="bibr" target="#b35">[37]</ref>, or road sign recognition <ref type="bibr" target="#b46">[48]</ref>. Hence, removing rain streaks from input rain images is an important research problem. In this paper, we focus on the single-image rain removal problem.</p><p>In the last decade, we have witnessed a continuous progress on rain removal research with many methods pro- * Joint first authors. ? Rynson Lau is the corresponding author, and he led this project.</p><p>(a) Rain image (b) Clean image (c) SPANet (d) DDN <ref type="bibr" target="#b10">[11]</ref> (e) DID-MDN <ref type="bibr" target="#b40">[42]</ref> (f) RESCAN <ref type="bibr" target="#b24">[25]</ref>  <ref type="figure">Figure 1</ref>. We address the single-image rain removal problem in two ways. First, we generate a high-quality rain/clean image pair ((a) and (b)) from each sequence of real rain images, to form a dataset. Second, we propose a novel SPANet to take full advantage of the proposed dataset. (c) to (f) compare the visual results from SPANet and from state-of-the-art derainers.</p><p>posed <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b45">47,</ref><ref type="bibr" target="#b8">9]</ref>, through carefully modeling the physical characteristics of rain streaks. Benefited from large-scale training data, recent deep-learning-based derainers <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b40">42,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b43">45,</ref><ref type="bibr" target="#b14">15]</ref> achieve further promising performances. Nonetheless, the single-image rain removal problem remains open in two ways, as discussed below. Lack of real training data. As real rain/clean image pairs are unavailable, existing derainers typically rely on synthesized datasets to train their models. They usually start with a clean image and add synthetic rain on it to form a rain/clean image pair. Although some works have been done to study the physical characteristics of rain, e.g., rain direction <ref type="bibr" target="#b38">[40]</ref> and rain density <ref type="bibr" target="#b40">[42]</ref>, their datasets still lack the ability to model a large range of real world rain streaks. For example, it is often very difficult to classify the rain density into one of the three levels (i.e., light, medium and heavy) as in <ref type="bibr" target="#b40">[42]</ref>, and any misclassification would certainly affect the deraining performance. To simulate global rain effects, some methods adopt the nonlinear "screen blend mode" from Adobe Photoshop, or additionally superimpose haze on the synthesized rain images. However, these global settings can only be used in certain types of rain, or the background may be darkened, with the details lost.</p><p>Lack of a real benchmark. Currently, researchers mainly rely on qualitatively evaluating the deraining performance on real rain images through visual comparisons. Fan et al. <ref type="bibr" target="#b43">[45]</ref> also use an object detection task to help evaluate the deraining performance. Nevertheless, a highquality real deraining benchmark is still much needed for quantitative evaluation of deraining methods.</p><p>In this paper, we address the single-image rain removal problem in two ways, as summarized in <ref type="figure">Figure 1</ref>. First, we address the lack of real training/evaluation datasets based on two observations: (1) as random rain drops fall in high velocities, they unlikely cover the same pixel all the time <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b42">44]</ref>, and (2) the intensity of a pixel covered by rain fluctuates above the true background radiance across a sequence of images. These two observations imply that we can generate one clean image from a sequence of rain images, where individual pixels of the clean image may be coming from different images of the sequence. Hence, we propose a semi-automatic method that incorporates rain temporal properties as well as human supervision to construct a large-scale real rain dataset. We show that it can significantly improve the performance of state-of-the-art derainers on real world rain images.</p><p>Second, we observe that real rain streaks can exhibit highly diverse appearance properties (e.g., rain shape and direction) within a single image, which challenges existing derainers as they lack the ability to identify real rain streaks accurately. To address this limitation, we exploit a spatial attentive network (SPANet), which first leverages horizontal/vertical neighborhood information to model the physical properties of rain streaks, and then remove them by further considering the non-local contextual information. In this way, the discriminative features for rain streak removal can be learned in a two-stage local-to-global manner. Extensive evaluations show that the proposed network performs favorably against the state-of-the-art derainers.</p><p>To summarize, this work has the following contributions:</p><p>1. We present a semi-automatic method that incorporates temporal properties of rain streaks and human supervision to generate a high quality clean image from a sequence of real rain images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>We construct a large-scale dataset of ?29.5K highresolution rain/clean image pairs, which covers a wide range of natural rain scenes. We show that it can significantly improve the performance of state-of-the-art derainers on real rain images.</p><p>3. We design a novel SPANet to effectively learn discriminative deraining features in a local-to-global attentive manner. SPANet achieves superior performance over state-of-the-art derainers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>Single-image rain removal. This problem is extremely challenging due to the ill-posed deraining formulation as:</p><formula xml:id="formula_0">B = O ? R,<label>(1)</label></formula><p>where O, R and B are the input rain image, the rain streak image, and the output derained image, respectively. <ref type="bibr">Kang et al. [20]</ref> propose to first decompose the rain image into high-/low-frequency layers and remove rain streaks in the high frequency layer via dictionary learning. <ref type="bibr">Kim et al. [21]</ref> propose to use non-local mean filters to filter out rain streaks. Luo et al. <ref type="bibr" target="#b28">[29]</ref> propose a sparse coding based method to separate rain streaks from the background. Li et al. <ref type="bibr" target="#b25">[26]</ref> propose to use Gaussian mixture models to model rain streaks and background separately for rain removal. Chang et al. <ref type="bibr" target="#b4">[5]</ref> propose to first affine transform the rain image into a space where rain streaks have vertical appearances and then utilize the low-rank property to remove rain streaks. Zhu et al. <ref type="bibr" target="#b45">[47]</ref> exploit rain streak directions to first determine the rain-dominant regions, which are used to guide the process of separating rain streaks from background details based on rain-dominant patch statistics.</p><p>In <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b9">10]</ref>, deep learning is applied to single image deraining and achieves a significant performance boost. They model rain streaks as "residuals" between the input/output of the networks in an end-to-end manner. Yang et al. <ref type="bibr" target="#b38">[40]</ref> propose to decompose the rain layer into a series of sublayers representing rain streaks of different directions and shapes, and jointly detect and remove rain streaks using a recurrent network. In <ref type="bibr" target="#b41">[43]</ref>, Zhang et al. propose to remove rain streaks and recover the background via the Conditional GAN. Recently, Zhang and Patel <ref type="bibr" target="#b40">[42]</ref> propose to classify rain density to guide the rain removal step. Li et al. <ref type="bibr" target="#b24">[25]</ref> propose a recurrent network with a squeeze-and-excitation block <ref type="bibr" target="#b16">[17]</ref> to remove rain streaks in multiple stages. However, the performances of CNN-based derainers on real rain images are largely limited by being trained only on synthetic datasets. These derainers also lack the ability to attend to rain spatial distributions. In this paper, we propose to leverage real training data as well as a spatial attentive mechanism to address the single image deraining problem.</p><p>Multi-image rain removal. Unlike single-image deraining, rich temporal information can be derived from a sequence of images to provide additional constraints for rain removal. Pioneering works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> propose to apply photometric properties to detect rain streaks and estimate the corresponding background intensities by averaging the irradiance of temporal or spatial neighboring pixels. Subsequently, more intrinsic properties of rain streaks, such as chromatic property, are explored by <ref type="bibr" target="#b42">[44,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">36]</ref>. Recent works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27]</ref> focus on removing the rain streaks from the background with moving objects.</p><p>Chen et al. <ref type="bibr" target="#b6">[7]</ref> further propose a spatial-temporal content alignment algorithm to handle fast camera motion and dynamic scene contents, and a CNN to reconstruct high frequency background details.</p><p>However, these methods cannot be applied for our purpose of generating high-quality rain-free images. This is because if their assumptions (e.g., low-rank <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b23">24]</ref>) are violated, over-/under-deraining can happen to the entire sequence and further bury the true background radiance, i.e., the clean background pixels may not exist in this sequence. Hence, in this paper, we propose to use the original sequence of rain images to generate a clean image, and rely on human judgements on the qualities of generated rain-free images.</p><p>Generating the ground truth from real noisy images. One typical strategy <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b31">33]</ref> to obtain a noise/noise-free image pair is to photograph the scene with a high ISO value and a short exposure time for the noise image, and a low ISO value and a long exposure time for the noise-free image. However, this strategy cannot be used here to capture rain-free images. As rain drops fall at a high speed, increasing the exposure time will enlarge the rain streaks, not removing them. Another approach to obtain a ground truth noise-free image is multi-frame fusion <ref type="bibr" target="#b44">[46,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b0">1]</ref>, which performs weighted averaging of a pre-aligned sequence of images taken from a static scene with a fixed camera setting. However, as rain streaks have brighter appearances and larger shapes than random noise, this approach is not able to accurately remove rain from the rain pixels. In contrast, we propose to refine the rain pixels based on the observation that the intensity values of the pixels covered by rain fluctuate above their true background intensities. <ref type="figure">Figure 2</ref>. We trace the intensity of one pixel across an image sequence in (a). We ask a user to identify if this pixel in each frame is covered by rain (in red) or not (in blue). The intensity distribution of this pixel over all frames is show in (b). It shows that the intensity of the pixel tends to fluctuate in a smaller range if it is not covered by rain, as compared with that covered by rain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Real Rain Image Dataset</head><p>We first conduct an experiment on how to select a suitable background value o b from a collection of pixel values O l = {o 1l , ..., o N l } at spatial position l from a sequence of N rain images. We capture a video of a rain scene over a static background, as shown in <ref type="figure">Figure 2</ref>, and then ask a person to indicate (or predict) when a particular pixel is covered by rain and when it is not, across the N frames. We have observed two phenomena. First, rain streaks do not always cover the same pixel (the temporal property of video deraining <ref type="bibr" target="#b42">[44]</ref>). Second, humans typically predict if a pixel is covered by rain or not based on the pixel intensity. If the intensity of the pixel is lower at a certain frame compared with the other frames, humans would predict that it is not covered by rain. This is because rain streaks tend to brighten the background. These two observations imply that, given a sequence of N consecutive rain images, we can approximate the true background radiance B l at pixel l based on these human predicted rain-free pixel values (i.e., the blue region of the histogram in <ref type="figure">Figure 2</ref>(b)). If we assume that the ambient light is constant during this time span, we can then use the value that appears most frequently (i.e., mode in statistics) to approximate the background radiance.</p><p>Background approximation. Referring to <ref type="figure">Figure 3</ref>, given a set of pixel values O l at position l from a sequence of N rain images, we first compute the mode of O l as:</p><formula xml:id="formula_1">? l = ?(O l ),<label>(2)</label></formula><p>where ? is the mode operation. However, since Eq. 2 does not consider the neighborhood information when computing ? l , the resulting images tend to be noisy in dense rain streaks. So, we identify the percentile range (R min l , R max l ) of the computed ? l in O l based on their intensity values as:</p><formula xml:id="formula_2">R min l = 100% N N i=1 {1|o il &lt; ? l }, R max l = 100% N N i=1 {1|o il ? ? l }.<label>(3)</label></formula><p>Figure 3(c) shows an example. Instead of using polygonal lines to connect the mode values ? l at all spatial positions, we can determine a suitable percentilep so that it crosses the highest number of percentile ranges (the red dash line in <ref type="figure">Figure 3</ref>(c)). In this way, the estimated background image is globally smoothed by computingp as:</p><formula xml:id="formula_3">p = arg max p ({ M ?1 l=0 {1|R min l &lt; p &lt; R max l }} 100 p=0 ),<label>(4)</label></formula><p>where M is the number of pixels in a frame. <ref type="figure">Figure 4</ref>(e) shows an example that using the mode leads to noisy result, while our method in <ref type="figure">Figure 4</ref>(f) produces a cleaner image. Selection of N for different rain scenes. Recall that we aim to generate one clean image from a sequence of N rain images. Our method assumes that for each pixel of the output clean image, we are able to find some input frames where the pixel is not covered by rain. To satisfy this assumption, we need to adjust N according  <ref type="figure">Figure 3</ref>. Overview of our clean image generation pipeline (a). Given a sequence of rain images, we compute the mode for each pixel based on its intensity changes over time, and the percentile range of its mode. We then consider the global spatial smoothness by finding a percentile rank that can cross most of the percentile ranges (b).</p><p>(a) Input (b) Jiang <ref type="bibr" target="#b18">[19]</ref> (c) Wei <ref type="bibr" target="#b37">[39]</ref> (d) Li <ref type="bibr" target="#b23">[24]</ref> (e) Mode filter (f) Ours (g) Ground Truth <ref type="figure">Figure 4</ref>. A deraining example using a synthetic rain video of 100 frames. We show the best result of each method here. Refer to the supplementary for more results.</p><p>to the amount of rain as follows. First, we empirically set N to be {20, 100, 200} depending on whether the rain is {sparse, normal, dense}, respectively, and generate an output image using our method. Second, we ask users to evaluate the image as humans are sensitive to rain streaks as well as other artifacts such as noise. If the image fails in the user evaluation, we adjust N by adding {10, 20, 50} frames for {sparse, normal, dense} rain streaks and then ask the users to evaluate the new output image again. We find that while 20 and 100 frames are usually large enough to obtain a clean image for sparse and normal rain streaks, N may go from 200 to 300 frames for dense rain streaks. We deliberately start with smaller numbers of frames because we find that the more frames that we use, the higher chance that the video may contain noise, blur and shaking.</p><p>Discussion. An intuitive alternative to obtaining a rain-free image is to use a state-of-the-art video deraining method to first generate a sequence of derained results from the input rain sequence, and then average them or select the best result from them to produce a single final rain-free image. Unfortunately, there is no guarantee that rain streaks can be completely removed by the video deraining method, as shown in <ref type="figure">Figure 4</ref>(b)-(d). On the contrary, we rely on human judgements to generate high-quality rain-free images. We show a comparison between our method and three stateof-the-art video deraining methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b23">24]</ref> in <ref type="table">Table 1</ref> on 10 synthesized rain videos (10 black-background rain videos bought from [31] are imposed on 10 different background images), which clearly demonstrates the effectiveness of our method.</p><p>Dataset description. We construct a large-scale dataset using 170 real rain videos, of which 84 scenes are captured by us using an iPhone X or iPhone 6SP and 86 scenes are collected from StoryBlocks or YouTube. These videos cover common urban scenes (e.g., buildings, avenues), suburb scenes (e.g., streets, parks), and some outdoor fields (e.g., forests). When capturing rain scenes, we also control the exposure durations as well as the ISO parameter to cover different lengths of rain streaks and illumination conditions. Using the aforementioned method, we generate 29, 500 high-quality rain/clean image pairs, which are split into 28, 500 for training and 1, 000 for testing. Our experiments show that this dataset helps improve the performance of state-of-the-art derainers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proposed Model</head><p>As real rain streaks may have highly diverse appearances across the image, we propose the SPANet to detect and remove rain streaks in a local-to-global manner, as shown in <ref type="figure">Figure 5</ref>(a). It is a fully convolutional network that takes one rain image as input and outputs a derained image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Spatial Attentive Block</head><p>Review on IRNN architecture. Recurrent neural networks with ReLU and identity matrix initialization (IRNN) for natural language processing <ref type="bibr" target="#b22">[23]</ref> have been shown to be easy to train, good at modeling long-range dependencies as well as efficient. When applied to computer vision problems, their key advantage is that information can be efficiently propagated across the entire image to accumulate (a) Spatial Attentive Network (SPANet)   <ref type="figure">Figure 5</ref>. The architecture of the proposed SPANet (a). It adopts three standard residual blocks (RBs) <ref type="bibr" target="#b15">[16]</ref> to extract features, four spatial attentive blocks (SABs) to identify rain streaks progressively in four stages, and two residual blocks to reconstruct a clean background. A SAB (b) contains three spatial attentive residual blocks (SARBs) (c) and one spatial attentive module (SAM) (d). Dilation convolutions <ref type="bibr" target="#b39">[41]</ref> are used in RBs and SARBs. long range varying contextual information, by stacking at least two RNN layers. In <ref type="bibr" target="#b2">[3]</ref>, a two-round four-directional IRNN architecture is used to exploit contextual information to improve small object detection. While the first round IRNN aims to produce the feature maps that summarize the neighboring contexts for each position of the input image, the second round IRNN further gathers non-local contextual information for producing global aware feature maps. Recently, Hu et al. <ref type="bibr" target="#b17">[18]</ref> also exploit this two-round four-directional IRNN architecture to detect shadow regions based on the observation that directions play an important role in finding strong cues between shadow/non-shadow regions. They design a direction-aware attention mechanism to generate more discriminative contextual features.</p><formula xml:id="formula_4">(b) Spatial Attentive Block (SAB) (c) Spatial Attentive Residual Block (SARB) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z D a X i M d 0 5 h V S 7 k i k C B S l / U e r L t w = " &gt; A A A C 0 H i c j V H L S s N A F D 2 N r 1 p f V Z d u g q 3 g o p S k C L o s u H F Z x T 6 g l p K k 0 x q c J n E y E U s p 4 t Y f c K t f J f 6 B / o V 3 x h T U I j o h y Z l z 7 z k z 9 1 4 3 4 n 4 s L e s 1 Y 8 z N L y w u Z Z d z K 6 t r 6 x v 5 z a 1 G H C b C Y 3 U v 5 K F o u U 7 M u B + w u v Q l Z 6 1 I M G f o c t Z 0 r 4 5 V v H n D R O y H w b k c R a w z d A a B 3 / c 9 R x L V K T a 7 4 6 T E S 7 2 S m B S 7 + Y J V t v Q y Z 4 G d g g L S V Q v z L 7 h A D y E 8 J B i C I Y A k z O E g p q c N G x Y i 4 j o Y E y c I + T r O M E G O t A l l M c p w i L 2 i 7 4 B 2 7 Z Q N a K 8 8 Y 6 3 2 6 B R O r y C l i T 3 S h J Q n C K v T T B 1 P t L N i f / M e a 0 9 1 t x H 9 3 d R r S K z E J b F / 6 a a Z / 9 W p W i T 6 O N I 1 + F R T p B l V n Z e 6 J L o r 6 u b m l 6 o k O U T E K d y j u C D s a e W 0 z 6 b W x L p 2 1 V t H x 9 9 0 p m L V 3 k t z E 7 y r W 9 K A 7 Z / j n A W N S t m 2 y v Z p p V A 9 S E e d x Q 5 2 s U / z P E Q V J 6 i h T t 7 X e M Q T n o 0 z 4 9 a 4 M + 4 / U 4 1 M q t n G t 2 U 8 f A A c + p O e &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z D a X i M d 0 5 h V S 7 k i k C B S l / U e r L t w = " &gt; A A A C 0 H i c j V H L S s N A F D 2 N r 1 p f V Z d u g q 3 g o p S k C L o s u H F Z x T 6 g l p K k 0 x q c J n E y E U s p 4 t Y f c K t f J f 6 B / o V 3 x h T U I j o h y Z l z 7 z k z 9 1 4 3 4 n 4 s L e s 1 Y 8 z N L y w u Z Z d z K 6 t r 6 x v 5 z a 1 G H C b C Y 3 U v 5 K F o u U 7 M u B + w u v Q l Z 6 1 I M G f o c t Z 0 r 4 5 V v H n D R O y H w b k c R a w z d A a B 3 / c 9 R x L V K T a 7 4 6 T E S 7 2 S m B S 7 + Y J V t v Q y Z 4 G d g g L S V Q v z L 7 h A D y E 8 J B i C I Y A k z O E g p q c N G x Y i 4 j o Y E y c I + T r O M E G O t A l l M c p w i L 2 i 7 4 B 2 7 Z Q N a K 8 8 Y 6 3 2 6 B R O r y C l i T 3 S h J Q n C K v T T B 1 P t L N i f / M e a 0 9 1 t x H 9 3 d R r S K z E J b F / 6 a a Z / 9 W p W i T 6 O N I 1 + F R T p B l V n Z e 6 J L o r 6 u b m l 6 o k O U T E K d y j u C D s a e W 0 z 6 b W x L p 2 1 V t H x 9 9 0 p m L V 3 k t z E 7 y r W 9 K A 7 Z / j n A W N S t m 2 y v Z p p V A 9 S E e d x Q 5 2 s U / z P E Q V J 6 i h T t 7 X e M Q T</formula><p>We summarize the four-directional IRNN operation for computing feature h i,j at location (i, j) as:</p><formula xml:id="formula_5">h i,j ? max (? dir h i,j?1 + h i,j , 0) ,<label>(5)</label></formula><p>where ? dir denotes the weight parameter in the recurrent convolution layer for each direction. <ref type="figure" target="#fig_2">Figure 6</ref> illustrates how a two-round four-directional IRNN architecture accumulates global contextual information. Here, we extend the two-round four-directional IRNN model to the single-image rain removal problem, for the purpose of handling the significant appearance variations of real rain streaks. Spatial attentive module (SAM). We build SAM based on the aforementioned two-round four-directional IRNN architecture. We use the IRNN model to project the rain streaks to the four main directions. Another branch is added to capture the spatial contextual information in order to selectively highlight the projected rain features, as shown in <ref type="figure">Figure 5(d)</ref>. Unlike <ref type="bibr" target="#b17">[18]</ref> that implicitly learns direction-aware features in the embedding space, we further use additional convolutions and sigmoid activations to explicitly generate the attention map through explicit supervision. The attention map indicates rain spatial distributions and is used to guide the following deraining process. <ref type="figure" target="#fig_3">Figure 7</ref> shows the input rain images in (a) and our SPANet derained results in (c). We also visualize the attention maps produced by SAM in (b). We can see that SAM can effectively identify the regions affected by rain streaks, even though the rain streaks exhibit significant appearance variations (i.e., smooth and blurry in the first scene and sharp in the second scene).</p><p>Removal-via-detection. As shown in <ref type="figure">Figure 5</ref>(a), given an input rain image, three standard residual blocks (RBs) <ref type="bibr" target="#b15">[16]</ref> are first used to extract features. We feed these features into a spatial attentive block (SAB) ( <ref type="figure">Figure 5(b)</ref>), which uses a SAM to generate an attention map to guide three subsequent spatial attentive residual blocks (SARBs) <ref type="figure">(Figure 5(c)</ref>) to remove rain streaks via the learned negative residuals. The SAB is repeated four times. (Note that the weights of the SAM in the four SABs are shared.) Finally, the resulting feature maps are fed to two standard residual blocks to reconstruct the final clean background image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training Details</head><p>Loss function. We adopt the following loss function to train SPANet:</p><formula xml:id="formula_6">L total = L 1 + L SSIM + L Att .<label>(6)</label></formula><p>We use the standard L 1 loss to measure the per-pixel reconstruction accuracy. L ssim <ref type="bibr" target="#b36">[38]</ref> is used to constrain the structural similarities, and is defined as: 1 ? SSIM (P, C), where P is the predicted result and C is the clean image. We further apply the attention loss L att as:</p><formula xml:id="formula_7">L att = A ? M 2 2 ,<label>(7)</label></formula><p>where A is the attention map from the first SAM in the network and M is the binary map of the rain streaks, which is computed by thresholding the difference between the rain image and clean image. In this binary map, a 1 indicates that the pixel is covered by rain and 0 otherwise. Implementation details. SPANet is implemented using the PyTorch <ref type="bibr" target="#b32">[34]</ref> framework on a PC with a E5-2640 v4 2.4GHz CPU and 4 NVIDIA Titan V GPUs. For loss optimization, we adopt the Adam optimizer <ref type="bibr" target="#b21">[22]</ref> with a batch size of 16. We adopt scaling and cropping to augment the diversity of rain streaks. The learning rate is initialized at 0.005 and divided by 10 after 30K iterations. We train the network for 40K iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, We first evaluate the effectiveness of the proposed dataset on existing CNN-based single-image derainers, and then compare the proposed SPANet to the stateof-the-art single-image deraining methods. Finally, we provide internal analysis to study the contributions of individual components of SPANet. Refer to the supplementary for more results.</p><p>Evaluation on the proposed dataset. The performances of existing CNN-based derainers <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b40">42,</ref><ref type="bibr" target="#b24">25]</ref> trained on our dataset are shown in <ref type="table" target="#tab_3">Table 2</ref>. It demonstrates that our real dataset can significantly improve the performance of CNN-based methods on real images. This is mainly due to the fact that existing synthesized datasets lack the ability to represent highly varying rain streaks. One visual example is given in <ref type="figure">Figure 9</ref>, from which we can see that the retrained derainers can produce cleaner images with more details compared to those trained on synthetic datasets. Note that we use their original codes for evaluation and retraining.</p><p>We also show the performance of non-CNN-based stateof-the-art methods in <ref type="table" target="#tab_3">Table 2</ref>. We have an interesting observation here that the input rain images have similar or even higher average PSNR and SSIM scores compared with those of the derained results by the state-of-the-art derainers. As demonstrated in <ref type="figure">Figure 8</ref>, it is mainly caused by over deraining. Even though <ref type="bibr" target="#b28">[29]</ref> is less dependent on training data (but still depends on a learned dictionary) as the deep learning methods ( <ref type="bibr" target="#b38">[40,</ref><ref type="bibr" target="#b40">42,</ref><ref type="bibr" target="#b24">25]</ref>), it fails when the rain exhibits unseen appearances and mistakenly removes the structures that are similar to rain streaks. <ref type="bibr">Rain</ref>   <ref type="figure">Figure 8</ref>. The difference maps (red boxes shown at the top-right) between the input rain image and results by deraining methods that suffer a PSNR drop. (Brighter indicates a higher difference.) We can see that <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b40">42,</ref><ref type="bibr" target="#b24">25]</ref> tend to over-derain the image.</p><p>Evaluation on the proposed SPANet.  <ref type="table" target="#tab_3">Table 2</ref>. Quantitative results for benchmarking the proposed SPANet and the state-of-the-art derainers on the proposed test set. The original codes of all these derainers are used for evaluation. We have also trained CNN-based state-of-the-art methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b40">42,</ref><ref type="bibr" target="#b24">25]</ref> on our dataset, and results are marked in red. The best performance is marked in bold. Note that due to the lack of density labels for the rain images in our dataset, we only fine-tune the pre-trained model of DID-MDN <ref type="bibr" target="#b40">[42]</ref> without the re-training label classification network.  pared to the state-of-the-art derainers. This is because SPANet can identify the rain streak regions and remove them accurately. <ref type="figure" target="#fig_4">Figure 10</ref> shows a visual example from our test set. We can see that while methods (b)?(e) tend to leave rain streaks unremoved and methods (g)?(j) tend to corrupt the background, the proposed SPANet (f) can produce much cleaner result. We also show some deraining examples on rain images collected from previous derain papers and the Internet in <ref type="figure">Figure 11</ref>. While existing derainers fail to remove the rain streaks and some of them tend to darken or blur the background, our SPANet can handle different kinds of rain streaks and preserve more details. <ref type="table">Table 3</ref> compares the performances of SPANet with the state-of-the-art derainers on the synthetic test set from <ref type="bibr" target="#b40">[42]</ref>, demonstrating the effectiveness of SPANet.</p><p>Internal analysis. We verify the importance of the spatial attentive module (SAM) and different ways of using it in <ref type="table">Table 4</ref>. B a is a basic Resnet-like network that does not use SAM. B b , B c , and B f represent three variants of using only one SAM for four times (recall that we have four SAB blocks), four SAMs, and four SAMs that share the same weights for all operations, respectively. While we can see that all variants of incorporating the SAM improve the performance, B f performs the best, as sharing the weights makes the deraining process inter-dependent on the four SAB blocks, which allows more attention to be put to the challenging real rain streak distributions. B d is the SPANet but without the above attention branch in SAM. The comparison between B d and B f shows that attention branch is effective in leveraging the local contextual information aggregated from different directions. B e is a variant that removes the attention loss supervision. It demonstrates the importance of providing explicit supervision on the attention map generation process.  <ref type="bibr" target="#b10">[11]</ref> (c) JORDER <ref type="bibr" target="#b38">[40]</ref> (d) DID-MDN <ref type="bibr" target="#b40">[42]</ref> (e) RESCAN <ref type="bibr" target="#b24">[25]</ref> (f) Our SPANet <ref type="figure">Figure 11</ref>. Visual comparison of SPANet with the state-of-the-art CNN-based derainers on some real rain images collected from previous derain papers and from the Internet.  <ref type="table">Table 3</ref>. Comparison on the test set from <ref type="bibr" target="#b40">[42]</ref>. SPANet is trained on the synthetic dataset from <ref type="bibr" target="#b40">[42]</ref>.  <ref type="table">Table 4</ref>. Internal analysis of the proposed SPANet. The best performance is marked in bold.  <ref type="figure">Figure 12</ref>. Failure case. Our method fails to remove extremely dense rain streaks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future Work</head><p>In this paper, we have presented a method to produce a high-quality clean image from a sequence of real rain images, by considering temporal priors together with human supervision. Based on this method, we have constructed a large-scale dataset of ?29.5K rain/clean image pairs that cover a wide range of natural rain scenes. Experiments show that the performances of state-of-the-art CNN-based derainers can be significantly improved by training on the proposed dataset. We have also benchmarked state-of-the-art derainers on the proposed test set. We find that the stochastic distributions of real rain streaks, especially the varying appearances of rain streaks, often fail these methods. To this end, we present a novel spatial attentive network (SPANet) that can learn to identify and remove rain streaks in a local-to-global spatial attentive manner. Extensive evaluations demonstrate the superiority of the proposed method over the state-of-the-art derainers.</p><p>Our method does have limitations. One example is given in <ref type="figure">Figure 12</ref>, which shows that our method fails when processing haze-like heavy rain. It is because the proposed dataset generation method fails to select clean pixels from the misty video frames. As a result, the proposed network produces a haze-like result.</p><p>Currently, our dataset generation method relies on human judgements. This is partly due to the fact that there are no existing metrics that can assess the generated rain-free images, without clean images for reference. It would be interesting to develop an unsupervised mechanism for this purpose in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Pipeline of Background Approximations (b) Computing Percentile Range for Mode Value</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(d) Spatial Attentive Module (SAM)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 .</head><label>6</label><figDesc>Illustration of how the two-round four-directional IRNN architecture accumulates global contextual information in two stages. In the first stage, for each position at the input feature map, four-directional (up, left, down, right) recurrent convolutional operations are performed to collect horizontal and vertical neighborhood information. In the second stage, by repeating the previous operations, the contextual information from the entire input feature map are obtained.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>Visualization of the attention map. (a) shows one real rain image. (b) shows the corresponding attention map produced by SAM. Red color indicates pixels that are highly likely covered by rain. (c) shows the corresponding derained result by the proposed SPANet. This demonstrates the effectiveness of SAM in handling significant appearance variations of rain streaks. Refer to the supplementary for more results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 10 .</head><label>10</label><figDesc>Visual comparison of SPANet with the state-of-the-art derainers. PSNR/SSIM results are included for reference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>reports</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Visual comparison of the state-of-the-art CNN-based derainers trained on the original/proposed datasets. Methods in red mean that they are retrained on the proposed dataset. PSNR/SSIM results are included for reference.</figDesc><table><row><cell>(a) Rain / Clean Image</cell><cell>(b) DDN [11]</cell><cell>(c) JORDER [40]</cell><cell>(d) DID-MDN [42]</cell><cell>(e) RESCAN [25]</cell></row><row><cell>33.53 / 0.9372</cell><cell>37.27 / 0.9631</cell><cell>36.67 / 0.9657</cell><cell>22.86 / 0.8721</cell><cell>35.80 / 0.9538</cell></row><row><cell>(f) Our SPANet</cell><cell>(g) DDN [11]</cell><cell>(h) JORDER [40]</cell><cell>(i) DID-MDN [42]</cell><cell>(j) RESCAN [25]</cell></row><row><cell>43.49 / 0.9938</cell><cell>38.36 / 0.9668</cell><cell>40.49 / 0.9834</cell><cell>26.54 / 0.9625</cell><cell>39.29 / 0.9771</cell></row><row><cell>Figure 9. (a) Rain / Clean Image</cell><cell>(b) DSC [29]</cell><cell>(c) LP [26]</cell><cell>(d) SILS [14]</cell><cell>(e) Clearing [10]</cell></row><row><cell>31.06 / 0.9108</cell><cell>34.49 / 0.9316</cell><cell>34.42 / 0.9488</cell><cell>33.20 / 0.9463</cell><cell>31.82 / 0.9353</cell></row><row><cell>(f) Our SPANet</cell><cell>(g) DDN [11]</cell><cell>(h) JORDER [40]</cell><cell>(i) DID-MDN [42]</cell><cell>(j) RESCAN [25]</cell></row><row><cell>38.22 / 0.9764</cell><cell>33.94 / 0.9460</cell><cell>35.09 / 0.9495</cell><cell>21.69 / 0.8018</cell><cell>34.35 / 0.9265</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Our SPANet DID-MDN Test Set 0.7781/21.15 0.7896/21.44 0.8352/22.75 0.8422/22.07 0.8622/24.32 0.8978/ 27.33 0.8522/23.05 0.9087/ 27.95 0.9342/30.05</figDesc><table><row><cell>Methods</cell><cell>Input</cell><cell>DSC [29]</cell><cell>LP [26]</cell><cell>Clear[10]</cell><cell>JORDER [40]</cell><cell>DDN [11]</cell><cell>JBO[47]</cell><cell>DID-MDN[42]</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A high-quality denoising dataset for smartphone cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Abdelhamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Renoir -a benchmark dataset for real noise reduction evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josue</forename><surname>Anaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Barbu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.8230</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavita</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rain or snow detection in image sequences through use of a histogram of orientation of streaks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?r?mie</forename><surname>Bossu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Hauti?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Philippe</forename><surname>Tarel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Transformed lowrank model for line pattern noise removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luxin</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A rain pixel recovery algorithm for videos with highly dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lap-Pui</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Robust video content alignment and compensation for rain removal in a cnn framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheen-Hau</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lap-Pui</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A generalized low-rank appearance model for spatio-temporally correlated rain streaks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi Lei</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiou Ting</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Single image deraining via decorrelating the rain streaks and background scene in gradient domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangli</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiguang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PR</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Clearing the skies: A deep network architecture for single-image rain streaks removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueyang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiabin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghao</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Paisley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Removing rain from single images via a deep detail network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueyang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiabin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Delu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Detection and removal of rain from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kshitiz</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shree</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Vision and rain. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kshitiz</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nayar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Joint convolutional analysis and synthesis sparse representation for single image layer separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Xiang He. Non-locally enhanced encoder-decoder network for single image de-raining</title>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<editor>Wei Zhang Huiyou Chang Le Dong Liang Lin Guanbin Li</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Direction-aware spatial context features for shadow detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A novel tensor-based video rain streaks removal approach via utilizing discriminatively intrinsic priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai-Xiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Zhu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi-Le</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Jian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic single-image-based rain streaks removal via image decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Wei</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Wen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Hsiang</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Video deraining and desnowing using temporal correlation and low-rank matrix completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae-Young</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A simple way to initialize recurrent networks of rectified linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00941</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Video rain streak removal by multiscale convolutional sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recurrent squeeze-and-excitation context aggregation net for single image deraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rain streak removal using layer priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robby</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangbo</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Erase or fill? deep joint recurrent rain removal and reconstruction in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongming</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pixel Based Temporal Analysis Using Chromatic Property for Removing Rain from Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianglong</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CIS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Removing rain from a single image via discriminative sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">What can help pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhimin</forename><surname>Cao</surname></persName>
		</author>
		<ptr target="https://www.motionvfx.com/mplugs-48.html" />
	</analytic>
	<monogr>
		<title level="m">CVPR, 2017. 1 [31] motionvfx</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A holistic approach to cross-channel image noise modeling and its application to image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonghyeon</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngbae</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuyuki</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Benchmarking denoising algorithms with real photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Plotz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<ptr target="http://pytorch.org.6" />
		<title level="m">PyTorch</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Video desnowing and deraining based on matrix decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Utilizing local phase information to remove rain from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Santhaseelan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vijayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Asari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Vital: Visual tracking via adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Rynson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Image quality assessment -from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero P</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Should we encode rain streaks in video as deterministic or stochastic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixuan</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongben</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep joint rain detection and removal from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robby</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongming</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Density-aware single image de-raining using a multi-stream dense network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Image deraining using a conditional generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishwanath</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishal</forename><surname>Patel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.05957</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Wee Kheng Leow, and Teck Khim Ng. Rain removal in video by combining temporal and chromatic properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyi</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Residual-guide feature fusion network for single image deraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueyang</forename><surname>Fu Yue Huang Xinghao Ding Zhiwen Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huafeng</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">From noise modeling to blind image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengyuan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Joint bi-layer optimization for single-image rain streak removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Traffic-sign detection and classification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoli</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
