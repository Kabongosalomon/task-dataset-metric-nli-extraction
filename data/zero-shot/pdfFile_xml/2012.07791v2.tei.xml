<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">img2pose: Face Alignment and Detection via 6DoF, Face Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V?tor</forename><surname>Albiero</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Notre Dame</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan</forename><surname>Pang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">img2pose: Face Alignment and Detection via 6DoF, Face Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>* Joint first authorship. All experiments reported in this paper were performed at the Univer-sity of Notre Dame.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: We estimate the 6DoF rigid transformation of a 3D face (rendered in silver), aligning it with even the tiniest faces, without face detection or facial landmark localization. Our estimated 3D face locations are rendered by descending distances from the camera, for coherent visualization. For more qualitative results, see appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We propose real-time, six degrees of freedom (6DoF), 3D face pose estimation without face detection or landmark localization. We observe that estimating the 6DoF rigid transformation of a face is a simpler problem than facial landmark detection, often used for 3D face alignment. In addition, 6DoF offers more information than face bounding box labels. We leverage these observations to make multiple contributions: (a) We describe an easily trained, efficient, Faster R-CNN-based model which regresses 6DoF pose for all faces in the photo, without preliminary face detection. (b) We explain how pose is converted and kept consistent between the input photo and arbitrary crops created while training and evaluating our model. (c) Finally, we show how face poses can replace detection bounding box training labels. Tests on AFLW2000-3D and BIWI show that our method runs at real-time and outperforms state of the art (SotA) face pose estimators. Remarkably, our method also surpasses SotA models of comparable complexity on the WIDER FACE detection benchmark, despite not been optimized on bounding box labels.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: We estimate the 6DoF rigid transformation of a 3D face (rendered in silver), aligning it with even the tiniest faces, without face detection or facial landmark localization. Our estimated 3D face locations are rendered by descending distances from the camera, for coherent visualization. For more qualitative results, see appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We propose real-time, six degrees of freedom (6DoF), 3D face pose estimation without face detection or landmark localization. We observe that estimating the 6DoF rigid transformation of a face is a simpler problem than facial landmark detection, often used for 3D face alignment. In addition, 6DoF offers more information than face bounding box labels. We leverage these observations to make multiple contributions: (a) We describe an easily trained, efficient, Faster R-CNN-based model which regresses 6DoF pose for all faces in the photo, without preliminary face detection. (b) We explain how pose is converted and kept consistent between the input photo and arbitrary crops created while training and evaluating our model. (c) Finally, we show how face poses can replace detection bounding box training labels. Tests on AFLW2000-3D and BIWI show that our method runs at real-time and outperforms state of the art (SotA) face pose estimators. Remarkably, our method also surpasses SotA models of comparable complexity on the WIDER FACE detection benchmark, despite not been optimized on bounding box labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Face detection is the problem of positioning a box to bound each face in a photo. Facial landmark detection seeks to localize specific facial features: e.g., eye centers, tip of the nose. Together, these two steps are the cornerstones of many face-based reasoning tasks, most notably recognition <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b75">75,</ref><ref type="bibr" target="#b77">77]</ref> and 3D reconstruction <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b72">72,</ref><ref type="bibr" target="#b73">73]</ref>. Processing typically begins with face detection followed by landmark detection in each detected face box. Detected landmarks are matched with corresponding ideal locations on a reference 2D image or a 3D model, and then an alignment transformation is resolved using standard means <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b40">40]</ref>. The terms face alignment and landmark detection are thus sometimes used interchangeably <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b39">39]</ref>.</p><p>Although this approach was historically successful, it has drawbacks. Landmark detectors are often optimized to the particular nature of the bounding boxes produced by specific face detectors. Updating the face detector therefore requires re-optimizing the landmark detector <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b80">80]</ref>. More generally, having two successive components implies separately optimizing two steps of the pipeline for accuracy and -crucially for faces -fairness <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b36">36]</ref>. In addition, SotA detection and pose estimation models can be computationally expensive (e.g., ResNet-152 used by the full Reti-naFace <ref type="bibr" target="#b18">[18]</ref> detector). This computation accumulates when these steps are applied serially. Finally, localizing the standard 68 face landmarks can be difficult for tiny faces such as those in <ref type="figure">Fig. 1</ref>, making it hard to estimate their poses and align them. To address these concerns, we make the following key observations:</p><p>Observation 1: 6DoF pose is easier to estimate than detecting landmarks. Estimating 6DoF pose is a 6D regression problem, obviously smaller than even 5-point landmark detection (5?2D landmarks = 10D), let alone standard 68 landmark detection (=136D). Importantly, pose captures the rigid transformation of the face. By comparison, landmarks entangle this rigid transformation with non-rigid facial deformations and subject-specific face shapes.</p><p>This observation inspired many to recently propose skipping landmark detection in favor of direct pose estimation <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b65">65,</ref><ref type="bibr" target="#b82">82]</ref>. These methods, however, estimate poses for detected faces. By comparison, we aim to estimate poses without assuming that faces were already detected.</p><p>Observation 2: 6DoF pose labels capture more than just bounding box locations. Unlike angular, 3DoF pose estimated by some <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b65">65,</ref><ref type="bibr" target="#b82">82]</ref>, 6DoF pose can be converted to a 3D-to-2D projection matrix. Assuming a known intrinsic camera parameters, pose can therefore align a 3D face with its location in the photo <ref type="bibr" target="#b28">[28]</ref>. Hence, pose already captures the location of the face in the photo. Yet, for the price of two additional scalars (6D pose vs. four values per box), 6DoF pose also provides information on the 3D position and orientation of the face. This observation was recently used by some, most notably, RetinaFace <ref type="bibr" target="#b18">[18]</ref>, to improve detection accuracy by proposing multi-task learning of bounding box and facial landmarks. We, instead, combine the two in the single goal of directly regressing 6DoF face pose.</p><p>We offer a novel, easy to train, real-time solution to 6DoF, 3D face pose estimation, without requiring face detection ( <ref type="figure">Fig. 1</ref>). We further show that predicted 3D face poses can be converted to obtain accurate 2D face bounding boxes with only negligible overhead, thereby providing face detection as a byproduct. Our method regresses 6DoF pose in a Faster R-CNN-based framework <ref type="bibr" target="#b64">[64]</ref>. We explain how poses are estimated for ad-hoc proposals. To this end, we offer an efficient means of converting poses across different image crops (proposals) and the input photo, keeping ground truth and estimated poses consistent. In summary, we offer the following contributions.</p><p>? We propose a novel approach which estimates 6DoF, 3D face pose for all faces in an image directly, and without a preceding face detection step. ? We introduce an efficient pose conversion method to maintain consistency of estimates and ground-truth poses, between an image and its ad-hoc proposals. ? We show how generated 3D pose estimates can be converted to accurate 2D bounding boxes as a byproduct with minimal computational overhead.</p><p>Importantly, all the contributions above are agnostic to the underlying Faster R-CNN-based architecture. The same techniques can be applied with other detection architectures to directly extract 6DoF, 3D face pose estimation, without requiring face detection. Our model uses a small, fast, ResNet-18 <ref type="bibr" target="#b29">[29]</ref> backbone and is trained on the WIDER FACE <ref type="bibr" target="#b81">[81]</ref> training set with a mixture of weakly supervised and human annotated ground-truth pose labels. We report SotA accuracy with real-time inference on both AFLW2000-3D <ref type="bibr" target="#b90">[90]</ref> and BIWI <ref type="bibr" target="#b20">[20]</ref>. We further report face detection accuracy on WIDER FACE <ref type="bibr" target="#b81">[81]</ref>, which outperforms models of comparable complexity by a wide margin. Our implementation and data are publicly available from: http://github. com/vitoralbiero/img2pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Face detection Early face detectors used hand-crafted features <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b74">74]</ref>. Nowadays, deep learning is used for its improved accuracy in detecting general objects <ref type="bibr" target="#b64">[64]</ref> and faces <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b83">83]</ref>. Depending on whether region proposal networks are used, these methods can be classified into singlestage methods <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b62">62,</ref><ref type="bibr" target="#b63">63]</ref> and two-stage methods <ref type="bibr" target="#b64">[64]</ref>.</p><p>Most single-stage methods <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b69">69,</ref><ref type="bibr" target="#b87">87]</ref> were based on the Single Shot MultiBox Detector (SSD) <ref type="bibr" target="#b44">[44]</ref>, and focused on detecting small faces. For example, S 3 FD <ref type="bibr" target="#b87">[87]</ref> proposed a scale-equitable framework with a scale compensation anchor matching strategy. PyramidBox <ref type="bibr" target="#b69">[69]</ref> introduced an anchor-based context association method that utilized contextual information.</p><p>Two-stage methods <ref type="bibr" target="#b76">[76,</ref><ref type="bibr" target="#b84">84]</ref> are typically based on Faster R-CNN <ref type="bibr" target="#b64">[64]</ref> and R-FCN <ref type="bibr" target="#b13">[13]</ref>. FDNet <ref type="bibr" target="#b84">[84]</ref>, for example, proposed multi-scale and voting ensemble techniques to improve face detection. Face R-FCN <ref type="bibr" target="#b76">[76]</ref> utilized a novel position-sensitive average pooling on top of R-FCN. Face alignment and pose estimation. Face pose is typically obtained by detecting facial landmarks and then solving Perspective-n-Point (PnP) algorithms <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b40">40]</ref>. Many landmark detectors were proposed, both conventional <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b45">45]</ref> and deep learning-based <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b67">67,</ref><ref type="bibr" target="#b78">78,</ref><ref type="bibr" target="#b91">91]</ref> and we refer to a recent survey <ref type="bibr" target="#b79">[79]</ref> on this topic for more information. Landmark detection methods are known to be brittle <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b10">10]</ref>, typically requiring a prior face detection step and relatively large faces to position all landmarks accurately.</p><p>A growing number of recent methods recognize that deep learning offers a way of directly regressing the face pose, in a landmark-free approach. Some directly estimated the 6DoF face pose from a face bounding box [8, 9, 10, 37, 52, 65, 82]. The impact of these landmark free alignment methods on downstream face recognition accuracy was evaluated and shown to improve results compared with landmark detection methods <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b10">10]</ref>. HopeNet <ref type="bibr" target="#b65">[65]</ref> extended these methods by training a network with multiple losses, showing significant performance improvement. FSA-Net <ref type="bibr" target="#b82">[82]</ref> introduced a feature aggregation method to improve pose estimation. Finally, QuatNet <ref type="bibr" target="#b32">[32]</ref> proposed a Quaternion-based face pose regression framework which claims to be more effective than Euler angle-based methods. All these methods rely on a face detection step, prior to pose estimation whereas our approach collapses these two to a single step. Some of the methods listed above only regress 3DoF angular pose: the face yaw, pitch, and roll <ref type="bibr" target="#b65">[65,</ref><ref type="bibr" target="#b82">82]</ref> or rotational information <ref type="bibr" target="#b32">[32]</ref>. For some use cases, this information suffices. Many other applications, however, including face alignment for recognition <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b75">75,</ref><ref type="bibr" target="#b77">77]</ref>, 3D reconstruction <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b72">72,</ref><ref type="bibr" target="#b73">73]</ref>, face manipulation <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b57">57]</ref>, also require the translational components of a full 6DoF pose. Our img2pose model, by comparison, provides full 6DoF face pose for every face in the photo <ref type="figure" target="#fig_0">(Fig. 2)</ref>.</p><p>Finally, some noted that face alignment is often performed along with other tasks, such as face detection, land-mark detection, and 3D reconstruction. They consequently proposed solving these problems together in a multi-task manner. Some early examples of this approach predate the recent rise of deep learning <ref type="bibr" target="#b58">[58,</ref><ref type="bibr" target="#b59">59]</ref>. More recent methods add face pose estimation or landmark detection heads to a face detection network <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b60">60,</ref><ref type="bibr" target="#b61">61,</ref><ref type="bibr" target="#b91">91]</ref>. It is unclear, however, if adding these tasks together improves or hurts the accuracy of the individual tasks. Indeed, evidence suggesting the latter is growing <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b70">70,</ref><ref type="bibr" target="#b89">89]</ref>. We leverage the observation that pose estimation already encapsulates face detection, thereby requiring only 6DoF pose as a single supervisory signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed method</head><p>Given an image I, we estimate 6DoF pose for each face, i appearing in I. We use h i ? R 6 to denote each face pose:</p><formula xml:id="formula_0">h i = (r x , r y , r z , t x , t y , t z ),<label>(1)</label></formula><p>where (r x , r y , r z ) represent a rotation vector <ref type="bibr" target="#b71">[71]</ref> and (t x , t y , t z ) is the 3D face translation. It is well known that a 6DoF face pose, h, can be converted to an extrinsic camera matrix for projecting a 3D face to the 2D image plane <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b68">68]</ref>. Assuming known intrinsic camera parameters, the 3D face can then be aligned with a face in the photo <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b28">28]</ref>. To our knowledge, however, previous work never leveraged this observation to propose replacing training for face bounding box detection with 6DoF pose estimation.</p><p>Specifically, assume a 3D face shape represented as a triangulated mesh. Points on the 3D face surface can be projected down to the photo using the standard pinhole model <ref type="bibr" target="#b26">[26]</ref>:</p><formula xml:id="formula_1">[Q, 1] T ? K[R, t][P, 1] T ,<label>(2)</label></formula><p>where K is the intrinsic matrix (Sec. 3.2), R and t are the 3D rotation matrix and translation vector, respectively, obtained from h by standard means <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b68">68]</ref>, and P ? R 3?n is a matrix representing n 3D points on the surface of the 3D face shape. Finally, Q ? R 2?n is the matrix representation of 2D points projected from 3D onto the image. We use Eq. (2) to generate our qualitative figures, aligning the 3D face shape with each face in the photo (e.g., <ref type="figure">Fig. 1</ref>). Importantly, given the projected 2D points, Q, a face detection bounding box can simply be obtained by taking the bounding box containing these 2D pixel coordinates.</p><p>It is worth noting that this approach provides better control over bounding box looseness and shapes, as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. Specifically, because the pose aligns a 3D shape with known geometry to a face region in the image, we can choose to modify face bounding boxes sizes and shapes to match our needs, e.g., including more of the forehead by expanding the box in the correct direction, invariant of pose. White bounding boxes generated with a loose setting, green with very tight setting, and blue with a less tight setting and forehead expansion (which is located through the pose).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Our img2pose network</head><p>We regress 6DoF face pose directly, based on the observation above that face bounding box information is already folded into the 6DoF face pose. Our network structure is illustrated in <ref type="figure" target="#fig_2">Fig. 4</ref>. Our network follows a two-stage approach based on Faster R-CNN <ref type="bibr" target="#b64">[64]</ref>. The first stage is a region proposal network (RPN) with a feature pyramid <ref type="bibr" target="#b43">[43]</ref>, which proposes potential face locations in the image.</p><p>Unlike the standard RPN loss, L rpn , which uses groundtruth bounding box labels, we use projected bounding boxes, B * , obtained from the 6DoF ground-truth pose labels using Eq. (2) (see <ref type="figure" target="#fig_2">Fig. 4</ref>, L prop ). As explained above, by doing so, we gain better consistency in the facial regions covered by our bounding boxes, B * . Other aspects of this stage are similar to those of the standard Faster R-CNN <ref type="bibr" target="#b64">[64]</ref>, and we refer to their paper for technical details.</p><p>The second stage of our img2pose extracts features from each proposal with region of interest (ROI) pooling, and then passes them to two different heads: a standard face/non-face (faceness) classifier and a novel 6DoF face pose regressor (Sec. 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pose label conversion</head><p>Two stage detectors rely on proposals -ad hoc image crops -as they train and while being evaluated. The pose regression head is provided with features extracted from proposals, not the entire image, and so does not have information required to determine where the face is located in the entire photo. This information is necessary because the 6DoF pose values are directly affected by image crop coordinates. For instance, a crop tightly matching the face would imply that the face is very close to the camera (small t z in Eq. (1)) but if the face appears much smaller in the original photo, this value would change to reflect the face being much farther away from the camera.</p><p>We therefore propose adjusting poses for different image crops, maintaining consistency between proposals and the entire photo. Specifically, for a given image crop we define a crop camera intrinsic matrix, K, simply as:</p><formula xml:id="formula_2">K = ? ? f 0 c x 0 f c y 0 0 1 ? ? (3)</formula><p>Here, f equals the face crop height plus width, and c x and c y are the x, y coordinates of the crop center. Pose values are then converted between local (crop) and global (entire photo) coordinate frames, as follows. Let matrix K img be the projection matrix for the entire image, where w and h are the image width and height respectively, and K box be the projection matrix for an arbitrary face crop (e.g., proposal), defined by B = (x, y, w bb , h bb ), where w bb and h bb are the face crop width and height respectively, and c x and c y are the x, y coordinates of the face crop's center. We define these matrices as:</p><formula xml:id="formula_3">K box = ? ? w + h 0 c x + x 0 w + h c y + y 0 0 1 ? ? (4) K img = ? ? w + h 0 w/2 0 w + h h/2 0 0 1 ? ? (5)</formula><p>Converting pose from local to global frames. Given a pose, h prop , in a face crop coordinate frame, B, intrinsic matrix, K img , for the entire image, intrinsic matrix, K box , for a face crop, we apply the method described in Algorithm 1 to convert h prop to h img (see <ref type="figure" target="#fig_2">Fig. 4</ref>).</p><p>Algorithm 1 Local to global pose conversion</p><formula xml:id="formula_4">1: procedure POSE CONVERT(h prop , B, K box , K img ) 2: f ? w + h 3: t z = t z * f /(w bb + h bb ) 4: V = K box [t x , t y , t z ] T 5: [t x , t y , t z ] T = (K img ) ?1 V 6: R = rot vec to rot mat([r x , r y , r z ]) 7: R = (K img ) ?1 K box R 8:</formula><p>(r x , r y , r z ) = rot mat to rot vec(R ) 9:</p><formula xml:id="formula_5">return h img = (r x , r y , r z , t x , t y , t z )</formula><p>Briefly, Algorithm 1 has two steps. First, in lines 2-3, we rescale the pose. Intuitively this step adjusts the camera to view the entire image, not just a crop. Then, in steps 4-8, we translate the focal point, adjusting the pose based on the difference of focal point locations, between the crop and the image. Finally, we return a 6DoF pose relative to the image intrinsic, K img . The functions rot vec to rot mat(?) and rot mat to rot vec(?) are standard conversion functions between rotation matrices and rotation vectors <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b71">71]</ref>. Please see Appendix A for more details on this conversion.  Converting pose from global to local frames. To convert pose labels, h img , given in the image coordinate frame, to local crop frames, h prop , we apply a process similar to Algorithm 1. Here, K img and K box change roles, and scaling is applied last. We provide details of this process in Appendix A (see <ref type="figure" target="#fig_2">Fig. 4</ref>, h img * i ). This conversion is an important step, since, as previously mentioned, proposal crop coordinates vary constantly as the method is trained and so ground-truth pose labels given in the image coordinate frame must be converted to match these changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training losses</head><p>We simultaneously train both the face/non-face classifier head and the face pose regressor. For each proposal, the model employs the following multi-task loss L.</p><formula xml:id="formula_6">L = L cls (p i , p * i ) + p * i ? L pose (h prop i , h prop * i ) + p * i ? L calib (Q c i , Q c * i ),<label>(6)</label></formula><p>which includes these three components:</p><p>(1) Face classification loss. We use standard binary crossentropy loss, L cls , to classify each proposal, where p i is the probability of proposal i containing a face and p * i is the ground-truth binary label (1 for face and 0 for background). These labels are determined by calculating the intersection over union (IoU) between each proposal and the ground-truth projected bounding boxes. For negative proposals which do not contain faces, (p * i = 0), L cls is the only loss that we apply. For positive proposals, (p * i = 1), we also evaluate the two novel loss functions described below.</p><p>(2) Face pose loss. This loss directly compares a 6DoF face pose estimate with its ground truth. Specifically, we define</p><formula xml:id="formula_7">L pose (h prop i , h prop * i ) = h prop i ? h prop * i 2 2 ,<label>(7)</label></formula><p>where h prop i is the predicted face pose for proposal i in the proposal coordinate frame, h prop * i is the ground-truth face pose in the same proposal ( <ref type="figure" target="#fig_2">Fig. 4</ref>, L pose ). We follow the procedure mentioned in Sec. (3) Calibration point loss. As an additional means of capturing the accuracy of estimated poses, we consider the 2D locations of projected 3D face shape points in the image <ref type="figure" target="#fig_2">(Fig. 4</ref>, L calib ). We compare points projected using the ground-truth pose vs. a predicted pose: An accurate pose estimate will project 3D points to the same 2D locations as the ground-truth pose (see <ref type="figure" target="#fig_4">Fig. 5</ref> for a visualization). To this end, we select a fixed set of five calibration points, P c ? R 5?3 , on the surface of the 3D face. P c is selected arbitrarily; we only require that they are not all co-planar.</p><p>Given a face pose, h ? R 6 , either ground-truth or predicted, we can project P c from 3D to 2D using Eq. (2). The calibration point loss is then defined as,</p><formula xml:id="formula_8">L calib = Q c i ? Q c * i 1 ,<label>(8)</label></formula><p>where Q c i are the calibration points projected from 3D using predicted pose h prop i , and Q c * i is the calibration points projected using the ground-truth pose h prop * i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implementation details 4.1. Pose labeling for training and validation</head><p>We train our method on the WIDER FACE training set <ref type="bibr" target="#b81">[81]</ref> (see also Sec. 5.4). WIDER FACE offers manually annotated bounding box labels, but no labels for pose. The RetinaFace project <ref type="bibr" target="#b18">[18]</ref>, however, provides manually annotated, five point facial landmarks for 76k of the WIDER FACE training faces. We increase the number of training pose labels as well as provide pose annotations for the validation set, using the following weakly supervised manner. We run the RetinaFace face bounding box and five point landmark detector on all images containing face box annotations but missing landmarks. We take RetinaFace predicted bounding boxes which have the highest IoU ratio with the ground-truth face box label, unless their IoU is smaller than 0.5. We then use the box predicted by Reti-naFace along with its five landmarks to obtain 6DoF pose labels for these faces, using standard means <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b40">40]</ref>. Importantly, neither box or landmarks are then stored or used in our training; only the 6DoF estimates are kept. Finally, poses are converted to their global, image frames using the process described in Sec. 3.2.</p><p>This process provided us with 12, 874 images containing 138, 722 annotated training faces of which 62, 827 were assigned with weakly supervised poses. Our validation set included 3, 205 images with 34, 294 pose annotated faces, all of which were weakly supervised. During training, we ignore faces which do not have pose labels.</p><p>Data augmentation. Similar to others <ref type="bibr" target="#b84">[84]</ref>, we process our training data, augmenting it to improve the robustness of our method. Specifically, we apply random crop, mirroring and scale transformations to the training images. Multiple scales were produced for each training image, where we define the minimum size of an image as either 640, 672, 704, 736, 768, 800, and the maximum size is set as 1400.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training details</head><p>We implemented our img2pose approach in PyTorch using ResNet-18 <ref type="bibr" target="#b29">[29]</ref> as backbone. We use stochastic gradient descent (SGD) with a mini batch of two images. During training, 256 proposals per image are sampled for the RPN loss computation and 512 samples per image for the pose head losses. Learning rate starts at 0.001 and is reduced by a factor of 10 if the validation loss does not improve over three epochs. Early stop is triggered if the model does not improve for five consecutive epochs on the validation set. Finally, the main training took 35 epochs. On a single NVIDIA Quadro RTX 6000 machine, training time was roughly 4 days.</p><p>For face pose evaluation, Euler angles are the standard metric in the benchmarks used. Euler angles suffer from several drawbacks <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b32">32]</ref>, when dealing with large yaw angles. Specifically, when yaw angle exceeds ?90 ? , any small change in yaw will cause significant differences in pitch and roll (See <ref type="bibr" target="#b7">[7]</ref> Sec. 4.5 for an example of this issue). Given that the WIDER FACE dataset contains many faces whose yaw angles are larger than ?90 ? , to overcome this issue, for face pose evaluation, we fine-tuned our model on 300W-LP <ref type="bibr" target="#b90">[90]</ref>, which only contains face poses with yaw angles in the range of (?90, +90).</p><p>300W-LP is a dataset with synthesized head poses from 300W <ref type="bibr" target="#b66">[66]</ref> containing 122, 450 images. Training pose rotation labels are obtained by converting the 300W-LP groundtruth Euler angles to rotation vectors, and pose translation labels are created using the ground-truth landmarks, using standard means <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b40">40]</ref>. During fine-tuning, 2 proposals per image are sampled for the RPN loss and 4 samples per image for the pose head losses. Finally, learning rate is kept fixed at 0.001 and the model is fine-tuned for 2 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Face pose tests on AFLW2000-3D</head><p>AFLW2000-3D <ref type="bibr" target="#b90">[90]</ref> contains the first 2k faces of the AFLW dataset <ref type="bibr" target="#b35">[35]</ref> along with ground-truth 3D faces and corresponding 68 landmarks. The images in this set have a large variation of pose, illumination, and facial expression. To create ground-truth translation pose labels for AFLW2000-3D, we follow the process described in Sec. 4.1. We convert the manually annotated 68-point, ground-truth landmarks, available as part of AFLW2000-3D, to 6DoF pose labels, keeping only the translation part. For the rotation part, we use the provided ground-truth in Euler angles (pitch, yaw, roll) format, where the predicted rotation vectors are converted to Euler angles for comparison. We follow others <ref type="bibr" target="#b65">[65,</ref><ref type="bibr" target="#b82">82]</ref> by removing images with head poses that are not in the range of [?99, +99], discarding only 31 out of the 2, 000 images.</p><p>We test our method and its baselines on each image, scaled to 400 ? 400 pixels. Because some AFLW2000-3D images show multiple faces, we select the face that has the highest IoU between bounding boxes projected from predicted face poses and ground-truth bounding boxes, which were obtained by expanding the ground-truth landmarks. We verified the set of faces selected in this manner and it is identical to the faces marked by the ground-truth labels.  AFLW2000-3D face pose results. <ref type="table" target="#tab_1">Table 1</ref> compares our pose estimation accuracy with SotA methods on AFLW2000-3D. Importantly, aside from RetinaFace <ref type="bibr" target="#b18">[18]</ref>, all other methods are applied to manually cropped face boxes and not directly to the entire photo. Ground truth boxes provide these methods with 2D face translation and, importantly, scale for either pose of landmarks. This information is unavailable to our img2pose which takes the entire photo as input. Remarkably, despite having less information than its baselines, our img2pose reports a SotA MAE r of 3.913, while running at 41 frames per second (fps) with a single Titan Xp GPU. Other than our img2pose, the only method that processes input photos directly is RetinaFace <ref type="bibr" target="#b18">[18]</ref>. Our method outperforms it, despite the much larger, ResNet-50 backbone used by RetinaFace, its greater supervision in using not only bounding boxes and five point landmarks, but also persubject 3D face shapes, and its more computationally demanding training. This result is even more significant, considering that this RetinaFace model was used to generate some of our training labels (Sec. 4.1). We believe our superior results are due to img2pose being trained to solve a simpler, 6DoF pose estimation problem, compared with the RetinaFace goal of bounding box and landmark regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Face pose tests on BIWI</head><p>BIWI <ref type="bibr" target="#b20">[20]</ref> contains 15, 678 frames of 20 subjects in an indoor environment, with a wide range of face poses. This benchmark provides ground-truth labels for rotation (rotation matrix), but not for the translational elements required for full 6DoF. Similar to AFLW2000-3D, we convert the ground-truth rotation matrix and prediction rotation vectors to Euler angles for comparison. We test our method and its baselines on each image using 933 ? 700 pixels resolution. Because many images in BIWI contain more than a single face, to compare our predictions, we selected the face that is closer to the center of the image with a face score p i &gt; 0.9.</p><p>Here, again, we verified that our direct method detected and processed all the faces supplied with test labels.</p><p>BIWI face pose results. <ref type="table" target="#tab_2">Table 2</ref> reports BIWI results following protocol 1 <ref type="bibr" target="#b65">[65,</ref><ref type="bibr" target="#b82">82]</ref> where models are trained with external data and tested in the entire BIWI dataset. Similarly to the results on AFLW2000, Sec. 5.1, our pose estimation results again outperform the existing SotA, despite being applied to the entire image, without pre-cropped and scaled faces, reporting MAE r of 3.786. Finally, img2pose runtimeon the original 933 ? 700 BIWI images is 30 fps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation study</head><p>We examine the effect of our loss functions, defined in Sec. 3.3, where we compare our results following training with only the face pose loss, L pose , only the calibration points loss, L calib , and both loss functions combined. <ref type="table" target="#tab_5">Table 4</ref> provides our ablation results. The table compares the three loss variations using M AE r and M AE t on the AFLW2000-3D set and M AE r on BIWI. Evidently, combining both loss functions leads to improved accuracy in estimating head rotations, with the gap on BIWI being particularly wide, in favor of the combined loss. Curiously, translation errors on AFLW2000-3D are somewhat higher with the joint loss compared to the use of either loss function, individually. Still, these differences are small and could be   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Face detection on WIDER FACE</head><p>Our method outperforms SotA methods for face pose estimation on two leading benchmarks. Because it is applied to the input images directly, it is important to verify how accurate is it in detecting faces. To this end, we evaluate our img2pose on the WIDER FACE benchmark <ref type="bibr" target="#b81">[81]</ref>. WIDER FACE offers 32, 203 images with 393, 703 faces annotated with bounding box labels. These images are partitioned into 12, 880 training, 3, 993 validation, and 16, 097 testing images, respectively. Results are reported in terms of detection mean average precision (mAP), on the WIDER FACE easy, medium, and hard subsets, for both validation and test sets.</p><p>We train our img2pose on the WIDER FACE training set and evaluate on the validation and test sets using standard protocols <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b88">88]</ref>, including application of flipping, and multi-scaling testing, with the shorter sides of the image scaled to [500, 800, 1100, 1400, 1700] pixels. We use the process described in Sec. 3 to project points from a 3D face shape onto the image and take a bounding box containing the projected points as a detected bounding box (See also <ref type="figure" target="#fig_2">Fig. 4</ref>). Finally, box voting <ref type="bibr" target="#b24">[24]</ref> is applied on the projected boxes, generated at different scales. WIDER FACE detection results. <ref type="table" target="#tab_4">Table 3</ref> compares our results to existing methods. Importantly, the design of our img2pose is motivated by run-time. Hence, with a ResNet-18 backbone, it cannot directly compete with far heavier, SotA face detectors. Although we provide a few SotA results for completeness, we compare our results with methods that, similarly to us, use light and efficient backbones.</p><p>Evidently, our img2pose outperforms models of comparable complexity in the validation and test, Medium and Hard partitions. This results is remarkable, considering that our method is the only one that provides 6DoF pose and direct face alignment, and not only detects faces. Moreover, our method is trained with 20k less faces than prior work. We note that RetinaFace <ref type="bibr" target="#b18">[18]</ref> returns five face landmarks which can, with additional processing, be converted to 6DoF pose. Our img2pose, however, reports better face detection accuracy than their light model and substantially better pose estimation as evident from Sec. 5.1 and Sec. 5.2. <ref type="figure">Fig. 6</ref> visualizes the 3D translational components of our estimated 6DoF poses, for WIDER FACE validation images. Each (t x , t y , t z ) point is color coded by: Easy (blue), Medium (green), and Hard (red). This figure clearly shows how faces in the easy set congregate close to the camera and in the center of the scene, whereas faces from the Medium and Hard sets vary more in their scene locations, with Hard especially scattered, which explains the challenge of that set and testifies to the correctness of our pose estimates. <ref type="figure">Fig. 7</ref> provides qualitative samples of our img2pose on WIDER FACE validation images. We observe that our method can generate accurate pose estimation for faces with various pitch, yaw, roll angles, and for images under various scale, illumination, occlusion variations. These results demonstrate the effectiveness of img2pose for direct pose estimation and face detection. <ref type="figure">Figure 7</ref>: Qualitative img2pose results on WIDER FACE validation images <ref type="bibr" target="#b81">[81]</ref>. In all cases, we only estimate 6DoF face poses, directly from the photo, and without a preliminary face detection step. For more samples, please see Appendix B for more results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We propose a novel approach to 6DoF face pose estimation and alignment, which does not rely on first running a face detector or localizing facial landmarks. To our knowledge, we are the first to propose such a multi-face, direct approach. We formulate a novel pose conversion algorithm to maintain consistency of poses estimated for the same face across different image crops. We show that face bounding box can be generated via the estimated 3D face poseachieving face detection as a byproduct of pose estimation. Extensive experiments have demonstrated the effectiveness of our img2pose for face pose estimation and face detection.</p><p>As a class, faces offer excellent opportunities to this marriage of pose and detection: faces have well-defined appearance statistics which can be relied upon for accurate pose estimation. Faces, however, are not the only category where such an approach may be applied; the same improved accuracy may be obtained in other domains, e.g., retail <ref type="bibr" target="#b25">[25]</ref>, by applying a similar direct pose estimation step as a substitute for object and key-point detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Pose conversion methods</head><p>We elaborate on our pose conversion algorithms, mentioned in Sec. 3.2. Algorithm 1 starts with an initial pose h prop estimated relative to an image crop, B (see, <ref type="figure">Fig. 8b)</ref>, and produces the final converted pose, h img , relative to the whole image, I (in <ref type="figure">Fig. 8d)</ref>.</p><p>At a high level, our pose conversion algorithm, Algorithm 1, consists of the following two steps:</p><p>The first step is a rescaling step (from <ref type="figure">Fig. 8b</ref> to <ref type="figure">Fig. 8c</ref>), where we adjust the camera to view the entire image, I, not just the crop, B. After the first step, we obtain an intermediate pose representation, h intermediate , relative to the camera location, assumed in <ref type="figure">Fig. 8c</ref>.</p><p>The second step is a translation step (from <ref type="figure">Fig.. 8c</ref> to <ref type="figure">Fig. 8d</ref>), where we translate the principal / focal point of the camera from the center of the crop region to image center. After this step, each converted global pose, h img , from different crop, B i , is estimated based on a consistent camera location, as shown in <ref type="figure">Fig. 8d</ref>.</p><p>Each pose, h prop , h intermediate , and h image , is associated with a specific assumed camera location and thus a specific intrinsic camera matrix, K, K box , and K img respectively, where we define again here.</p><p>Here, we assume f equals the image crop height, h bb , plus width, w bb , c x and c y are the x, y coordinates of the image crop center, respectively, and w, h are the full image width and height respectively. The input to Algorithm 1 h prop is estimated based on camera matrix, K, whose principal point is at the center of the image crop B, (c x , c y ), and focal length f is w bb + h bb , which is visualized in <ref type="figure">Fig. 8b.</ref> Step 1 of the algorithm, lines 2-3, first rescales the image. This zoom-out operation pushes the object further away from the camera by multiplying the translation on the z axis, t z , with the factor (w + h)/(w bb + h bb ). This extra factor in z will adjust the projected coordinates, p, on the image plane to reflect the relative ratio of the image crop to the whole image (since the original pose estimate h prop is estimated assuming each image crop is of constant size). Then we also adjust, accordingly, the camera matrices from K to K box . This transformation in intrinsic camera matrices will adjust the principal point, and thus the origin of the image coordinates system from the top left corner of the image crop to the top left corner of the whole image.</p><formula xml:id="formula_9">K = ? ? f 0 c x 0 f c y 0 0 1 ? ? ,<label>(9)</label></formula><formula xml:id="formula_10">K box = ? ? w + h 0 c x + x 0 w + h c y + y 0 0 1 ? ? ,<label>(10)</label></formula><formula xml:id="formula_11">K img = ? ? w + h 0 w/2 0 w + h h/2 0 0 1 ? ? .<label>(11)</label></formula><p>Step 2 of the algorithm, lines 4-8, translates the camera, so that every pose estimate, h img , is based on the camera settings shown in <ref type="figure">Fig. 8d</ref> with principal point at image center and focal length w + h.</p><p>The methodology here is to first adjust the camera matrix from K box to K img , in order to compensate the translation of our desired principal points, and then solve for the asso-ciated pose, h img . Since the image coordinate system does not change in Step 2, the following equality must hold, p = K box [R|t]P, p = K img [R |t ]P.</p><p>In other words,</p><formula xml:id="formula_12">K box [R|t] = K img [R |t ].</formula><p>So we can obtain the rotation matrices R and translation vectors t by the following equations,</p><formula xml:id="formula_13">R = (K img ) ?1 K box R, t = (K img ) ?1 K box t.</formula><p>The new pose, h img , can then be extracted from R and t using standard approaches <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b26">26]</ref> The conversion from global pose, h img , to local pose, h prop , follows the exact same methodology. For completeness, we provide pseudo-code for this step in Algorithm 2.</p><p>Algorithm 2 Global to local pose conversion <ref type="bibr">1:</ref> procedure POSE CONVERT(h img , B, K box , K img ) 2:</p><formula xml:id="formula_14">V = K img [t x , t y , t z ] T</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>[t x , t y , t z ] T = (K box ) ?1 V 4: R = rot vec to rot mat([r x , r y , r z ]) 5:</p><formula xml:id="formula_15">R = (K box ) ?1 K img R 6:</formula><p>(r x , r y , r z ) = rot mat to rot vec(R ) <ref type="bibr">7:</ref> f ? w + h <ref type="bibr">8:</ref> t z = t z /f * (w bb + h bb ) <ref type="bibr">9:</ref> return h prop = (r x , r y , r z , t x , t y , t z )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Qualitative results</head><p>We provide an abundance of qualitative results in <ref type="figure">Fig. 9</ref>, 10 and 11. <ref type="figure">Fig. 9</ref> visually compares the pose estimated by our img2pose with the ground-truth pose labels on the AFLW2000-3D set images <ref type="bibr" target="#b90">[90]</ref>. Our method is clearly robust to a wide range of face poses, as also evident from its state of the art (SotA) numbers reported in <ref type="table" target="#tab_1">Table 1</ref>. The last row in <ref type="figure">Fig. 9</ref> offers samples where our method did not accurately predict the correct pose. <ref type="figure">Fig. 10</ref> offers qualitative results on BIWI images <ref type="bibr" target="#b20">[20]</ref>, comparing our estimated poses with ground truth labels. BIWI provides ground truth angular and translational pose labels. Because we do not have information on the world (3D) coordinates used by BIWI to define their translations, we could only use their rotational ground truth values. The visual comparison should therefore only focus on the angular components of the pose.</p><p>Our img2pose evidently predicts accurate poses, consistent with the quantitative results reported in <ref type="table" target="#tab_2">Table 2</ref>. It is worth noting that BIWI faces are often smaller in size, relative to the entire photos, compared to the face to image sizes in AFLW2000-3D. Nevertheless, our direct method successfully predicts accurate poses. The last row of <ref type="figure">Fig. 10</ref> provides sample pose errors.</p><p>Finally, <ref type="figure">Fig. 11</ref> provides qualitative results on the WIDER FACE validation set images <ref type="bibr" target="#b81">[81]</ref>. The images displayed show the robustness of our method across a wide range of scenarios, with varying illumination, scale, large face poses and occlusion. <ref type="table" target="#tab_2">Truth   img2pose</ref> img2pose Ground-Truth img2pose Ground-Truth img2pose Ground-Truth <ref type="figure">Figure 9</ref>: Qualitative comparing the results of our img2pose method on images from the AFLW2000-3D set to the ground truth poses. Poses visualized using a 3D face shape rendered using the pose on input photos. We provide results reflecting a wide range of face poses and viewing settings. The bottom row provides sample qualitative errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground-</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>img2pose</head><p>img2pose Ground-Truth* <ref type="figure">Figure 10</ref>: Qualitative pose estimation results on BIWI images, comparing the poses estimated by our img2pose with the ground truth. These results demonstrate how well our method correctly estimates poses for even small faces. The bottom row provides samples of the limitations of our model. Note that in all these images, the translation component of the pose, (t x , t y , t z ), was estimated by our img2pose both for our results and the ground truth, as ground truth labels do not provide this information. <ref type="figure">Figure 11</ref>: Qualitative face detection results of our img2pose method on photos from the WIDER FACE validation set. Note that despite not being directly trained to detect faces, our method captures even the smallest faces appearing in the background as well as estimates their poses (zoom in for better views).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The 6DoF face poses estimated by our img2pose capture the positions of faces in the photo (top) and their 3D scene locations (bottom). See also Fig. 6 for a visualization of the 3D positions of all faces in WIDER FACE (val.).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Bounding boxes generated using predicted poses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Overview of our proposed method. Components that only appear in training time are colored in green and red, and components that only appear in testing time are colored in yellow. Gray color denotes default components from Faster R-CNN with FPN<ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b64">64]</ref>. Please see Sec. 3 for more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>3.2 to convert ground-truth poses, h img * i , relative to the entire image, to ground-truth pose, h prop * i , in a proposal frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Visualizing our calibration points. (a) When the estimated pose is wrong, points projected from a 3D face to the photo (in green) fall far from the location of these same 3D point, projected using the ground truth (in blue); (b) With a better pose estimate, calibration points projected using the estimated pose fall closer to their locations following projection using the ground-truth pose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>( a )Figure 8 :</head><label>a8</label><figDesc>An example photo (b) Initial pose estimation h prop (c) Intermediate pose estimation h intermediate (Line 2 -3) (d) Final pose estimation h image (Line 4 -8) Illustrating the pose conversion method. See Sec. A for more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Pose estimation accuracy on AFLW2000-3D<ref type="bibr" target="#b90">[90]</ref>. ? denotes results reported by others. Direct methods, like ours, were not tested on the ground-truth face crops, which capture scale information. Some methods do not produce or did not report translational accuracy. Finally, MAE r and MAE t are the Euler angles and translational MAE, respectively. On a 400 ? 400 pixel image from AFLW2000, our method runs at 41 fps.</figDesc><table><row><cell cols="2">Method</cell><cell></cell><cell>Direct?</cell><cell cols="2">Yaw</cell><cell>Pitch</cell><cell cols="2">Roll MAE r</cell><cell>X</cell><cell>Y</cell><cell>Z</cell><cell>MAE t</cell></row><row><cell cols="2">Dlib (68 points) [34]</cell><cell></cell><cell></cell><cell cols="6">18.273 12.604 8.998 13.292 0.122 0.088 1.130</cell><cell>0.446</cell></row><row><cell cols="2">3DDFA [90]  ?</cell><cell></cell><cell></cell><cell></cell><cell>5.400</cell><cell cols="2">8.530 8.250</cell><cell>7.393</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">FAN (12 points) [4]  ?</cell><cell></cell><cell></cell><cell></cell><cell cols="3">6.358 12.277 8.714</cell><cell>9.116</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Hopenet (? = 2) [65]  ?</cell><cell></cell><cell></cell><cell></cell><cell>6.470</cell><cell cols="2">6.560 5.440</cell><cell>6.160</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">QuatNet [32]  ?</cell><cell></cell><cell></cell><cell></cell><cell>3.973</cell><cell cols="2">5.615 3.920</cell><cell>4.503</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">FSA-Caps-Fusion [82]</cell><cell></cell><cell></cell><cell></cell><cell>4.501</cell><cell cols="2">6.078 4.644</cell><cell>5.074</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>HPE [33]  ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4.870</cell><cell cols="2">6.180 4.800</cell><cell>5.280</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TriNet [7]  ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4.198</cell><cell cols="2">5.767 4.042</cell><cell>4.669</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">RetinaFace R-50 (5 points) [18]</cell><cell></cell><cell></cell><cell>5.101</cell><cell cols="2">9.642 3.924</cell><cell cols="2">6.222 0.038 0.049 0.255</cell><cell>0.114</cell></row><row><cell cols="2">img2pose (ours)</cell><cell></cell><cell></cell><cell></cell><cell>3.426</cell><cell cols="2">5.034 3.278</cell><cell cols="2">3.913 0.028 0.038 0.238</cell><cell>0.099</cell></row><row><cell>Method</cell><cell>Direct?</cell><cell>Yaw</cell><cell>Pitch</cell><cell cols="2">Roll MAE r</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dlib (68 points) [34]  ?</cell><cell cols="5">16.756 13.802 6.190 12.249</cell><cell></cell><cell></cell><cell></cell></row><row><cell>3DDFA [90]  ?</cell><cell cols="5">36.175 12.252 8.776 19.068</cell><cell></cell><cell></cell><cell></cell></row><row><cell>FAN (12 points) [4]  ?</cell><cell></cell><cell>8.532</cell><cell cols="2">7.483 7.631</cell><cell>7.882</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Hopenet (? = 1) [65]  ?</cell><cell></cell><cell>4.810</cell><cell cols="2">6.606 3.269</cell><cell>4.895</cell><cell></cell><cell></cell><cell></cell></row><row><cell>QuatNet [32]  ?</cell><cell></cell><cell>4.010</cell><cell cols="2">5.492 2.936</cell><cell>4.146</cell><cell></cell><cell></cell><cell></cell></row><row><cell>FSA-NET [82]  ?</cell><cell></cell><cell>4.560</cell><cell cols="2">5.210 3.070</cell><cell>4.280</cell><cell></cell><cell></cell><cell></cell></row><row><cell>HPE [33]  ?</cell><cell></cell><cell>4.570</cell><cell cols="2">5.180 3.120</cell><cell>4.290</cell><cell></cell><cell></cell><cell></cell></row><row><cell>TriNet [7]  ?</cell><cell></cell><cell>3.046</cell><cell cols="2">4.758 4.112</cell><cell>3.972</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">RetinaFace R-50 (5 pnt.) [18]</cell><cell>4.070</cell><cell cols="2">6.424 2.974</cell><cell>4.490</cell><cell></cell><cell></cell><cell></cell></row><row><cell>img2pose (ours)</cell><cell></cell><cell>4.567</cell><cell cols="2">3.546 3.244</cell><cell>3.786</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of the state-of-the-art methods on the BIWI dataset. Methods marked with ? are reported by others. Direct methods, like ours, were not tested on ground truth face crops, which capture scale information. On 933 ? 700 BIWI images, our method runs at 30 fps.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>WIDER FACE results. '*' Requires PnP to get pose from landmarks. Our img2pose surpasses other light backbone detectors on Med. and Hard sets, despite not being trained to detect faces.</figDesc><table><row><cell>Figure 6: Visualizing our estimated pose</cell></row><row><cell>translations on WIDER FACE val. images.</cell></row><row><cell>Colors encode Easy (blue), Med. (green),</cell></row><row><cell>and Hard (red). Easy faces seem centered</cell></row><row><cell>close to the camera whereas Hard faces are</cell></row><row><cell>far more distributed in the scene.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparison of the effects of different loss functions on the pose estimation results obtained on the AFLW2000-3D and BIWI benchmarks. MAE r and MAE t are the rotational and translational MAE, respectively.</figDesc><table><row><cell>attributed to stochasticity in the model training due to ran-</cell></row><row><cell>dom initialization and random augmentations applied dur-</cell></row><row><cell>ing training (see Sec. 4.1 and 4.2).</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Is face recognition sexist? no, gendered hairstyles and biology are</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V?tor</forename><surname>Albiero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Mach. Vision Conf., 2020</title>
		<meeting>British Mach. Vision Conf., 2020</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">How does gender balance in training data affect face recognition accuracy?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>V?tor Albiero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Winter Conf. on App. of Comput. Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">3FabRec: Fast few-shot face alignment by reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Browatzki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wallraven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf</title>
		<meeting>Conf</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Vision Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Comput</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="6110" to="6120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2d &amp; 3d face alignment problem? (and a dataset of 230,000 3d facial landmarks)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision</title>
		<meeting>Int. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1021" to="1030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust face landmark estimation under occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xavier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Burgos-Artizzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision</title>
		<meeting>Int. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1513" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Face alignment by explicit shape regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="190" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A vector-based representation to enhance head pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwen</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongcheng</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongfang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingjie</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.07184</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ExpNet: Landmark-free, deep, 3D facial expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng-Ju</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep, landmark-free FAME: Face alignment, modeling, and expression estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng-Ju</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?rard</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">6-7</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">FacePoseNet: Making a case for landmark-free face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng-Ju</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision Workshops</title>
		<meeting>Int. Conf. Comput. Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Selective refinement network for high performance face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Stan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. of Assoc for the Advanc. of Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8231" to="8238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Timothy F Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. Comput. Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="484" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Euler-rodrigues formula variations, quaternion conjugation and intrinsic connections. Mechanism and Machine Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="144" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">De-CaFA: deep convolutional cascade for face alignment in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Dapogny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Bailly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision</title>
		<meeting>Int. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6893" to="6901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Model-based object pose in 25 lines of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Dementhon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="123" to="141" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Retinaface: Single-shot multi-level face localisation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Ververas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="5203" to="5212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niannan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4690" to="4699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Random forests for real time 3d face analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Fanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Dantone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Fossati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Joint 3d face reconstruction and dense alignment with position map regression network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. Comput. Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Face detection, bounding box aggregation and pose estimation for robust facial landmark localisation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Zhen-Hua Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Jun</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition Workshops</title>
		<meeting>Conf. Comput. Vision Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="160" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Computer vision: a modern approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Prentice Hall Professional Technical Reference</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Object detection via a multi-region and semantic segmentation-aware CNN model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision</title>
		<meeting>Int. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1134" to="1142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Precise detection in densely packed scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roei</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviv</forename><surname>Eisenschtat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5227" to="5236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge university press</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Viewing real-world faces in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision</title>
		<meeting>Int. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3607" to="3614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Effective face frontalization in unconstrained images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roee</forename><surname>Enbar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Lffd: A light and fast face detector for edge devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dezhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Jian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10633</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Shiming Xiang, and Chunhong Pan</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Accurate 3d face reconstruction via prior constrained structure from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="14" to="22" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Quatnet: Quaternion-based head pose estimation with multiregression loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng-Wei</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tung-Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung</forename><surname>Wing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yi</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1035" to="1046" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Improving head pose estimation using two-stage ensembles with top-k regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinbang</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">One millisecond face alignment with an ensemble of regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vahid</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1867" to="1874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Annotated facial landmarks in the wild: A largescale, real-world database for facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision Workshops</title>
		<meeting>Int. Conf. Comput. Vision Workshops</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2144" to="2151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Issues related to face recognition accuracy varying based on race and skin tone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Krishnapriya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V?tor</forename><surname>Albiero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kushal</forename><surname>Vangara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">C</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Technology and Society</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep head pose estimation using synthetic images and partial adversarial domain adaption for continuous label spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Kuhnke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorn</forename><surname>Ostermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision</title>
		<meeting>Int. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Kepler: Keypoint and pose estimation of unconstrained faces by learning efficient h-cnn regressors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azadeh</forename><surname>Alavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Automatic Face and Gesture Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="258" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">LUVLi face alignment: Estimating landmarks&apos; location, uncertainty, and visibility likelihood</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiaki</forename><surname>Koike-Akino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8236" to="8246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Epnp: An accurate o (n) solution to the pnp problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">155</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning object detection from a small number of examples: the importance of good features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kobi</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">DSFD: dual shot face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianjun</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="5060" to="5069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. Comput. Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Generic face alignment using boosted appearance model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fully-adaptive feature sharing in multi-task networks with applications in person attribute classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangfei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><surname>Javidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5334" to="5343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A lightweight face detector by integrating the convolutional neural network with the image pyramid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiapeng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Rapid synthesis of massive face sets for improved face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tu?n</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?rard</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Automatic Face and Gesture Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Do we really need to collect millions of faces for effective face recognition?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jatuporn</forename><forename type="middle">Toy</forename><surname>Leksut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?rard</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. Comput. Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Face-specific data augmentation for unconstrained face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gozde</forename><surname>Sahin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?rard</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">6-7</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Robust facial landmark detection via a fully-convolutional localglobal context network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Merget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Rock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Rigoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="781" to="790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Self-supervised viewpoint learning from image collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Siva Karthik Mustikovela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sifei</forename><surname>Shalini De Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">SSH: Single stage headless face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pouya</forename><surname>Samangouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision</title>
		<meeting>Int. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4875" to="4884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Ssh: Single stage headless face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pouya</forename><surname>Samangouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision</title>
		<meeting>Int. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4875" to="4884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Fsgan: Subject agnostic face swapping and reenactment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Nirkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosi</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision</title>
		<meeting>Int. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7184" to="7193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">On face segmentation, face swapping, and face perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Nirkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tran</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Automatic Face and Gesture Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="98" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Deepfake detection based on the discrepancy between the face and its context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Nirkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosi</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.12262,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Synergistic face detection and pose estimation with energybased models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margarita</forename><surname>Osadchy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Le Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew L</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1197" to="1215" />
			<date type="published" when="2007-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Synergistic face detection and pose estimation with energy-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margarita</forename><surname>Osadchy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Cun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1017" to="1024" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Hyperface: A deep multi-task learning framework for face detection, landmark localization, pose estimation, and gender recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="121" to="135" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">An all-in-one convolutional neural network for face analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swami</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Carlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Automatic Face and Gesture Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08242</idno>
		<title level="m">Yolo9000: Better, faster, stronger</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Finegrained head pose estimation without keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nataniel</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunji</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition Workshops</title>
		<meeting>Conf. Comput. Vision Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2074" to="2083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">300 faces in-the-wild challenge: The first facial landmark localization challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision Workshops</title>
		<meeting>Int. Conf. Comput. Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="397" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Deep convolutional network cascade for facial point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3476" to="3483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Computer vision: algorithms and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Pyramidbox: A context-assisted single shot face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeqiang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingtuo</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. Comput. Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="797" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Transferability and hardness of supervised classification tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anh T Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cuong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision</title>
		<meeting>Int. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1395" to="1405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Introductory techniques for 3-D computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Trucco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Verri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Prentice Hall Englewood Cliffs</publisher>
			<biblScope unit="volume">201</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Regressing robust and discriminative 3D morphable models with a very deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Tuan Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?rard</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Extreme 3D face reconstruction: Seeing through occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Tuan Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Nirkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?rard</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Cosface: Large margin cosine loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Detecting faces using region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05256</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Face recognition in unconstrained videos with matched background similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="529" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Facial landmark detection with tweaked convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanggeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Medioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prem</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3067" to="3074" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Facial landmark detection: A literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="115" to="142" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Learn to combine multiple hypotheses for accurate face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision Workshops</title>
		<meeting>Int. Conf. Comput. Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="392" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Wider face: A face detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Fsa-net: Learning fine-grained structure aggregation for head pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsun-Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1087" to="1096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yili</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjiang</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.11228</idno>
		<title level="m">Asfd: Automatic and scalable face detector</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Face detection using improved faster rcnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandan</forename><surname>Tu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.02142</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Fast face detection on mobile devices by leveraging global and local facial characteristics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C-C Jay</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Faceboxes: A cpu real-time and accurate unconstrained face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">364</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="297" to="309" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">S3fd: Single shot scale-invariant face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision</title>
		<meeting>Int. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="192" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">S3fd: Single shot scale-invariant face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision</title>
		<meeting>Int. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="192" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">A modulation module for multi-task learning with applications in image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. Comput. Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="401" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Face alignment across large poses: A 3d solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
