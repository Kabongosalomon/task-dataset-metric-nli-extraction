<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Two-Stage Anomaly Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfei</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">State Key Laboratory of Virtual Reality Technology and Systems</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoqun</forename><surname>Zhuang</surname></persName>
							<email>zhuangchaoqun@buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">State Key Laboratory of Virtual Reality Technology and Systems</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Lu</surname></persName>
							<email>lufeng@buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">State Key Laboratory of Virtual Reality Technology and Systems</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Two-Stage Anomaly Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Anomaly detection from a single image is challenging since anomaly data is always rare and can be with highly unpredictable types. With only anomaly-free data available, most existing methods train an AutoEncoder to reconstruct the input image and find the difference between the input and output to identify the anomalous region. However, such methods face a potential problem -a coarse reconstruction generates extra image differences while a high-fidelity one may draw in the anomaly <ref type="figure">(Fig.1</ref>). In this paper, we solve this contradiction by proposing a two-stage approach, which generates high-fidelity yet anomaly-free reconstructions. Our Unsupervised Two-stage Anomaly Detection (UTAD) relies on two technical components, namely the Impression Extractor (IE-Net) and the Expert-Net. The IE-Net and Expert-Net accomplish the two-stage anomaly-free image reconstruction task while they also generate intuitive intermediate results, making the whole UTAD interpretable. Extensive experiments show that our method outperforms state-of-the-arts on four anomaly detection datasets with different types of real-world objects and textures. * Corresponding author. sume the novel things that occurred on the category or imagelevel are anomalies. They detect if a new input is out-ofdistribution when compared with the training data. These methods can be referred to as one-class-classification or outlier detection <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b45">46]</ref>. However, this setting is not suitable for anomaly detection, which pays more attention on the images within the same category. In this case, anomalies often exist in small areas in the object or image (e.g., crack on the surface, missing small parts, etc.).</p><p>Recently, many approaches tackle this challenging problem with AutoEncoders <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">20]</ref> and GAN based methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b43">44]</ref>, which assume the network cannot reconstruct the unseen regions, then find the input and its difference with the output to identify the anomaly. Although such methods are able to distinguish novel classes from old ones (e.g., find the novel class in MNIST [27]  or CIFAR-10 [26], etc.), these methods face a potential contradiction. As shown in <ref type="figure">Fig.1 (a)</ref>, coarse reconstructions generate extra image difference, which interrupts the anomaly detection. To reconstruct more high-fidelity details, many methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b39">40]</ref> equip with more complex architectures or more trainable parameters. However, the output may draw the anomaly in and loses the anomaly-related difference.</p><p>In this paper, we tackle this contradiction with two steps to ensure anomaly-free and high-fidelity, respectively. More specifically, we introduce a mediate state, namely impression m, which is the anomaly-free reconstruction of the input. Then we design a two stage framework to generate both anomaly-free and high-fidelity reconstruction for anomaly detection. As illustrated in <ref type="figure">Fig. 1 (c)</ref>, our idea is to generate the anomaly-free and high-fidelityx through two stages. Consequently, we introduce the Unsupervised Two-stage Anomaly Detection (UTAD) framework, which contains two key components: the Impression Extractor (IE-Net) and the Expert-Net. The IE-Net is used for the first-stage reconstruction, which generates the anomaly-free m for the given input x (Sec. 3.1). The Expert-Net then restores details on m for the high-fidelityx (Sec. 3.2). Finally, we use Perceptual Measurement (PM) to better detect anomaly-related differences among the input x, impression m and reconstructedx 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Most objects or textures in nature have certain shapes and properties, especially human-made ones. Anomaly detection identifies rare items, events, or observations that raise suspicions by differing significantly from the majority <ref type="bibr" target="#b5">[6]</ref>. It will translate to many computer vision tasks, such as detect defective product parts <ref type="bibr" target="#b9">[10]</ref>, segment lesions in retinopathy images <ref type="bibr" target="#b33">[34]</ref>, and locate intruders in surveillance <ref type="bibr" target="#b32">[33]</ref>, etc. Anomaly detection is challenging since most deep learningbased methods require balanced positive data and negative data for training. However, abnormal data is always limited, and hard or even cannot be obtained in terms of their amount and types.</p><p>To tackle the lack of abnormal data, many methods as-(a) Coarse reconstruction <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12]</ref> (b) Fine reconstruction <ref type="bibr" target="#b2">[3]</ref> (c) Ours: anomaly-free &amp; high-fidelity reconstruction Anomaly-free ? High-fidelity <ref type="figure">Figure 1</ref>. Comparisons of different anomaly detection methods. Many one-stage methods reconstruct the structure of the input but without details (A) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12]</ref> or high-fidelity reconstruction yet draw in anomaly regions (B) <ref type="bibr" target="#b2">[3]</ref>. (c) We tackle the contradiction by extracting the anomaly-free structure (m) and adding high-fidelity details in two stages.</p><p>(Sec. 3.3).</p><p>In summary, our main contributions are:</p><p>? We propose a novel Unsupervised Two-stage Anomaly Detection (UTAD) framework that enables both anomaly-free and high-fidelity reconstruction for anomaly detection from a single image. The method requires no anomaly sample for training.</p><p>? We propose the new concept of anomaly-free reconstruction (namely impression) for anomaly detection. We design an IE-Net and Expert-Net to extract and utilize impression for anomaly-free and high-fidelity reconstructions, respectively.</p><p>? The proposed UTAD framework outperforms state-ofthe-arts on four anomaly detection datasets with different types of real-world objects and textures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Unsupervised anomaly detection</head><p>Many AutoEncoder-based methods are proposed for unsupervised anomaly detection. Carrera et al. <ref type="bibr" target="#b12">[13]</ref> train an Au-toEncoder and make it over-fit on the anomalous-free images, then use the magnitude of reconstruction loss (i.e., MSE loss) on test images to determine anomalous regions. Based on this, Bergmann et al. <ref type="bibr" target="#b11">[12]</ref> propose to replace per-pixel MSE loss with structure similarity loss <ref type="bibr" target="#b48">[49]</ref>. Baur et al. <ref type="bibr" target="#b7">[8]</ref> propose to use a variational auto-encoder (VAE) instead of the auto-encoder. There are also many GAN <ref type="bibr" target="#b38">[39]</ref> based methods for anomaly detection. AnoGAN <ref type="bibr" target="#b43">[44]</ref> is the first to use a generator for unsupervised anomaly detection on retinopathy images. Inspired by AnoGAN, GANormaly <ref type="bibr" target="#b1">[2]</ref> adds an encoder for mapping images to latent space, and greatly decreases the inference time. Fast-AnoGAN <ref type="bibr" target="#b42">[43]</ref> trains a WGAN <ref type="bibr" target="#b4">[5]</ref> and an additional encoder with two stages to boost the performance. Berg et al. <ref type="bibr" target="#b8">[9]</ref> propose to combine progressive growing GAN <ref type="bibr" target="#b23">[24]</ref> and ClusterGAN <ref type="bibr" target="#b30">[31]</ref> together to reconstruct high-resolution images for anomaly detection. Skip-GANomaly <ref type="bibr" target="#b2">[3]</ref> replaces the auto-encoder with U-net <ref type="bibr" target="#b39">[40]</ref> and further gets better reconstructions. However, such methods face a potential problem of whether making a coarse reconstruction or a high-fidelity one -the former generates extra image differences while the latter may draw the anomaly in and lose the anomaly-related difference.</p><p>Based on the uncertainty learning, Bergmann et al. <ref type="bibr" target="#b10">[11]</ref> propose Uniformed Students, which use the teacher-students for unsupervised anomaly detection. However, the learning process is kind of verbose and with limited interpretation. Venkataramanan et al. <ref type="bibr" target="#b47">[48]</ref> propose to use an attention mechanism, and use the activated feature for anomaly detection. There are also other methods that use additional supervision <ref type="bibr" target="#b51">[52]</ref> or memory bank <ref type="bibr" target="#b19">[20]</ref> for anomaly detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">One class classification</head><p>One class classification (i.e., outlier detection) is concerned with distinguishing out-of-detection samples relative to the training set. SVDD <ref type="bibr" target="#b45">[46]</ref> tries to map all the normal training data into predefined kernel space and takes the sample that outside the learned distribution as anomaly. Andrews et al. <ref type="bibr" target="#b3">[4]</ref> use the different layer features from the pre-trained VGG network and model the anomaly-free images with a v-SVM. There are many methods that equip this idea with patched distribution clustering <ref type="bibr" target="#b31">[32]</ref> or extend it to a semi-supervised scenario <ref type="bibr" target="#b40">[41]</ref>. There is a major difference between the one-class classification and anomaly detection since one class classification detects images from a different category, while anomaly detection aims to detect and locate the difference between images that are from the same class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Unsupervised Representation learning</head><p>Learning a good representation of an image is a longstanding problem of computer vision. One branch of research suggests training the encoder by learning with a pretext task (e.g., predicting relative patch location <ref type="bibr" target="#b16">[17]</ref>, solving a jigsaw puzzle <ref type="bibr" target="#b34">[35]</ref>, colorizing images <ref type="bibr" target="#b50">[51]</ref>, counting objects <ref type="bibr" target="#b35">[36]</ref>, and predicting rotations <ref type="bibr" target="#b18">[19]</ref>.). Another branch of research suggests extracting the unique information of the sample so that the sample can be distinguished easily for the downstream tasks. Based on this assumption, many mutual information-based methods are proposed, like f-GAN <ref type="bibr" target="#b36">[37]</ref>, info-GAN <ref type="bibr" target="#b13">[14]</ref>, info-NCE <ref type="bibr" target="#b46">[47]</ref>, and AMDIM <ref type="bibr" target="#b6">[7]</ref>, etc. In this paper, we are inspired by the mutual information and propose to learn the distinctive and anomaly-free features for the anomaly detection task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Unsupervised Two-stage Anomaly Detection</head><p>Here we introduce the proposed Unsupervised Two-stage Anomaly Detection (UTAD) for unsupervised anomaly detection. Given a training set D = {x 1 , x 2 , . . . , x N } of anomaly-free images, our goal is to learn two different networks that serve two stages for anomaly detection without any anomalous samples. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, UTAD is constructed by ? Impression Extractor Network (IE-Net) extracts the anomaly-free impression m on the input x.</p><p>? Expert-Net is an invertible mapping, which generates details on m to produce the high-fidelity reconstruction x. Meanwhile, for a better performance and interpretation, it also produces an intermediate na?ve impression m by mimicking the appearance of the output of IE-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">IE-Net for anomaly-free reconstruction</head><p>IE-Net aims to reconstruct the anomaly-free impression m for anomaly detection. The output of IE-Net is used as the input of Expert Net, which will be introduced in Sec. 3.2. Training phase. The IE-Net is trained on the anomaly-free dataset. The architecture of IE-Net is illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>. IE-Net is constructed by an encoder E M , a discriminator T and a decoder D M . These modules are jointly optimized by minimizing objective Eq. (6). Inference phase. When IE-Net is trained on the anomalyfree dataset, its encoder E M and D M are used for generating anomaly-free impression m for the input image, which may contain anomalous regions.</p><p>Since it is not trivial to control the reconstruction ability for anomaly detection, as illustrated in <ref type="figure">Fig. 1</ref>, we propose to extract the input image with anomaly-free impression m on the basis of mutual information theory. Specifically, we equip IE-Net with mutual information mainly because 1) mutual information is used to describe the distinctive features of the input image <ref type="bibr" target="#b46">[47]</ref> and 2) only anomaly-free samples are used for training. Therefore, the anomaly-free and distinctive features are suitable for reconstructing the impression.</p><p>As illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>, the encoder E M aims to extract the distinctive feature z of the image x by maximizing the mutual information I(D, Z), where z ? Z, Z is the set of latent codes. I(D, Z) is defined as follow</p><formula xml:id="formula_0">I(D, Z) = p(z|x)p(x) log p(z|x) p(z) dxdz,<label>(1)</label></formula><p>where p(x) is the distribution of D. Following the assumptions in VAE <ref type="bibr" target="#b24">[25]</ref>, the distribution p(z) is required to follow the Gaussian distribution q(z), which is implemented by Kullback-Leibler (KL) divergence</p><formula xml:id="formula_1">KL(p(z) q(z)) = p(z) log p(z) q(z) dz.<label>(2)</label></formula><p>As with maximizing the mutual information, the loss function of the encoder E M (x) becomes</p><formula xml:id="formula_2">L e M = ?I(D, Z) + ?KL(p(z) q(z))},<label>(3)</label></formula><p>where ? is the hyper-parameter for balancing these two terms in the objective function. To optimize Eq. <ref type="formula" target="#formula_0">(1)</ref>, we follow f-GAN <ref type="bibr" target="#b36">[37]</ref> and convert the maximizing process of mutual information to discriminate (with the discriminator T (x, z)) the difference between positive samples (x, z) and negative samples (x,z), wherex ? {D \ x} andz is a sample from p(z). For the convenience of implementation,x is a batchshuffled version of x andz ? N (?, ? 2 ), where ? and ? are the mean and variance of p(z). The ? and ? are estimated by a MLP. Thus, the loss function of the discriminator T (x, z) is</p><formula xml:id="formula_3">L t M = ? log(T (x, E M (x))) ? log(1 ? T (x,z)). (4)</formula><p>Thus, the Eq. (3) became L e M = L t M + ?KL(p(z) q(z)). In the next, the decoder D M (z) tries to project latent code z into RGB-space (i.e., impression m) and makes m similar to the input image x. The objective function of D M (z)</p><formula xml:id="formula_4">L d M = |D M (E M (x)) ? x|.<label>(5)</label></formula><p>Therefore, the total loss of IE-Net is</p><formula xml:id="formula_5">L IE = L e M + ? 1 L d M ,<label>(6)</label></formula><p>here we add the term ? 1 to make the objective more flexible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Expert-Net for high-fidelity reconstruction</head><p>Through IE-Net, we can generate the anomaly-free impression m for the input image x. We find the m is often with blurry texture or color-shift. It is reasonable because the Eq. (6) contains more than reconstruction loss. Directly computing the anomaly region by comparing x and m may introduce extra differences in the normal region. To handle this, we further introduce an Expert-Net as the second stage for high-fidelity reconstruction. The training and inference phases are as follow.  Training phase. We first generate the impressions (we noted this set as M below) for the training images, which is based on the training set for IE-Net. The anomaly-free images D and their corresponding impressions M construct the dataset, which is used for training the Expert-Net. As illustrated in <ref type="figure" target="#fig_4">Fig. 4</ref>, the Expert-Net is constructed by a detail extractor</p><formula xml:id="formula_6">E S , two encoders {E A X , E B X }, two decoders {D A X , D B X }.</formula><p>Then Expert-Net aims to learn the invertible mapping between M and D through supervised learning. Inference phase. When the Expert-Net training is complete, we generate the high-fidelity reconstructionx through</p><formula xml:id="formula_7">D A X (E A X (m), E S (x)</formula><p>). We can also get the na?ve impression through D B X (E B X (x)). More specifically, E S encodes the detail vector s of the input image x. Then s is used for rectifying the generated details onx. To make E S focus on local details, we use a small CNN with fewer layers than E X in implementation. Inspired by the unsupervised image-to-image translation methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> that use Adaptive Instance Normalization (AdaIN) layers to fuse different features for image generation, we also use AdaIN in Expert-Net. The AdaIN is defined as below</p><formula xml:id="formula_8">AdaIN(k, ?, ?) = ?( k ? f (k) g(k) ) + ?,<label>(7)</label></formula><p>where k is the activation of the previous convolutional layer, f ( ) and g( ) are channel-wise mean and standard deviation. Take detail vector s as input, ? and ? are parameters generated by MLP. The details of generated imagex are also supervised by making its details vector? identical to the s. Motivated by the asynchronous learning methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b20">21]</ref>, we also enable the Expert-Net to mimic the procedure of impression extraction, but learn to map the D to M directly. We call the mimic version of impressionm as nave impression. The appearance ofm may close to m (e.g., blurry), however, it is not ensured thatm is anomaly-free. Therefore, the difference between m andm can also be a reference for anomaly detection. During training, the generated image x,m and intermediate? are constrained to x, m, s through L1 loss, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Perceptual measurement for anomaly detection</head><p>Traditional methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b42">43]</ref> identify the anomaly by using the pixel-wise difference (e.g., L1, MSE). To effectively detect the anomaly regions, we find that using perceptual measurement (PM) among images on anomaly detection can achieve better performance. Perceptual distance has also been successfully applied to other tasks such as image synthesis and style transfer <ref type="bibr" target="#b22">[23]</ref>. When inference, given an unknown image x, the corresponding {m,x,m} are estimated through UTAD. The anomaly map e of the unknown image is calculated through perceptual distance:</p><formula xml:id="formula_9">e = ?(x,x, m,m) = l ? e l (? l (x,x) + ? l (m,m) + ? l (x, m)),<label>(8)</label></formula><p>where ?( ) is the feature distance (i.e., perceptual distance, which measures the L1 distance of features in a pre-trained VGG-19 network). Based on the anomaly error map e, the anomaly region segmentation y is calculated below</p><formula xml:id="formula_10">y(i, j) = 1, if e(i, j) &gt; ?; 0, otherwise,<label>(9)</label></formula><p>where i, j are pixel index of error map e and ? is threshold for anomaly segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To demonstrate the effectiveness of the proposed UTAD, extensive evaluation and rigorous analysis of ablation studies on a number of datasets are performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental setup</head><p>Benchmark datasets. We evaluate our UTAD on the MVTec AD <ref type="bibr" target="#b9">[10]</ref> with 15 different objects and textures for anomaly detection. Furthermore, MNIST <ref type="bibr" target="#b26">[27]</ref>, Fashion MNIST <ref type="bibr" target="#b49">[50]</ref>, and CIFAR-10 <ref type="bibr" target="#b25">[26]</ref> are also used for anomaly detection. Baseline methods. We compare UTAD with OCGAN <ref type="bibr" target="#b37">[38]</ref>, AnoGAN <ref type="bibr" target="#b43">[44]</ref>, AVID <ref type="bibr" target="#b41">[42]</ref>, AE L2 <ref type="bibr" target="#b11">[12]</ref>, AE SSIM <ref type="bibr" target="#b11">[12]</ref>, LSA <ref type="bibr" target="#b0">[1]</ref>, and AD-VAE <ref type="bibr" target="#b28">[29]</ref> with their publicly official code. Since the official project of UnSt <ref type="bibr" target="#b10">[11]</ref> and ?-VAE <ref type="bibr" target="#b15">[16]</ref> are not publicly available, we use their third-party implementations <ref type="bibr" target="#b11">12</ref> , respectively. We have also compared our method with the unsupervised version of CAVGA <ref type="bibr" target="#b47">[48]</ref>. Implementation details. The detailed implementation of the proposed method is organized as follow: 1 https://github.com/denguir/student-teacher-anomalydetection 2 https://github.com/dbbbbm/energy-projection-anomaly</p><p>? IE-Net. The encoder E M is constructed by 4 inception blocks <ref type="bibr" target="#b44">[45]</ref>. Each of them is followed by a max-pooling layer for down-sampling the feature map. The decoder D M contains 4 inception blocks with a 2? nearest upsample layer, then followed by a 1 ? 1 convolutional layer with sigmoid activation for the reconstruction of the impression. The MLP is constructed by a stack of three fully connected layers. The discriminator T is constructed by a 4-layer MLP with a sigmoid activation layer at the endpoint.</p><p>? Expert-Net. Based on MUNIT <ref type="bibr" target="#b22">[23]</ref>, the encoders (E A X , E B X ) of the Expert-Net contains 4 convolutional blocks and 2 residual blocks. The decoder is constructed by 2 residual blocks with AdaIN as normalization layer, then followed 4 convolutional blocks with upsamples to reconstruct the image. The detail extractor E S contains 3 convolutional layers and follows an adaptive average pooling layer.</p><p>? PM. To effectively involve different features for anomaly detection, we select the layers 'conv1 2', 'conv2 2', and 'conv3 4' in the VGG-19 network and set ? e 1 = ? e 2 = ? e 3 = 1 in our experiments. We empirically set ? = 0.5 for anomaly segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparisons to SOTAs</head><p>We evaluate our method on 15 category-specific dataset with high-resolution images (i.e., MVTec AD <ref type="bibr" target="#b9">[10]</ref>) and three low-resolution datasets (MNIST <ref type="bibr" target="#b26">[27]</ref>, Fashion MNIST <ref type="bibr" target="#b49">[50]</ref>, and CIFAR-10 <ref type="bibr" target="#b25">[26]</ref>).</p><p>MVTec AD dataset. For all experiments on MVTec AD, images are re-scaled to w = h = 256 for training and inference. We train the IE-Net on anomaly-free images for 200 epochs with batch size 4. We use SGD with initial learning rate 10 ?3 and momentum 0.9. For Expert-Net, we use Adam with learning rate 10 ?3 . <ref type="figure" target="#fig_5">Fig. 5</ref> illustrates the visual comparisons among different methods. Our method localizes more accurate and fine-grained anomalous regions than the compared methods. Since the CAVGA is not publicly available, we directly use the results from their supplement materials. As illustrated in <ref type="figure" target="#fig_6">Fig. 6</ref>, our method makes anomaly-free and high-fidelity reconstructions when compared to the two typical one-state methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b2">3]</ref>. <ref type="table" target="#tab_0">Table 1</ref> shows the quantitative comparison over Intersection over Union (IoU) and Area under ROC curve (AuROC). Our method outperforms the state-of-the-art method (CAVGA) in mean IoU by 6%.</p><p>MNIST, Fashion MNIST and CIFAR-10. On the MNIST, Fashion-MNIST, and CIFAR-10 datasets, we follow the same settings as in <ref type="bibr" target="#b14">[15]</ref> (i.e., training/testing uses a single class as normal and the rest of the classes as anomalous. Each image is zoomed to w = h = 64 for training and inference. Because the resolution and input size of these Input image AE SSIM <ref type="bibr" target="#b11">[12]</ref> AE L2 <ref type="bibr" target="#b11">[12]</ref> LSA <ref type="bibr" target="#b0">[1]</ref> -VAE <ref type="bibr" target="#b14">[15]</ref> UnSt <ref type="bibr" target="#b9">[10]</ref> CAVGA <ref type="bibr" target="#b43">[44]</ref> Ours Ground truth   three datasets are lower than those in MVTec AD dataset, we adjust the architecture of IE-Net and Expert-Net to fit these cases. Specifically, we reduce the number of inception blocks to 3 in IE-Net. Meanwhile, we set the number of residual blocks to 1 in Expert-Net. <ref type="table" target="#tab_1">Table 2</ref> shows the quantitative comparison results. We find the ablation studies verify different components of our framework. Our proposed outperforms the other methods for many settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Interpretation and analysis</head><p>Interpretation of UTAD.</p><p>Interpretation can be performed with two folds: illustration of intermediate results of UTAD and visualization of feature distributions. As illustrated in <ref type="figure" target="#fig_7">Fig. 7</ref>, it is clear that   <ref type="bibr" target="#b43">[44]</ref> 0.937 0.824 0.612 SkipGA <ref type="bibr" target="#b2">[3]</ref> 0.941 0.807 0.731 LSA <ref type="bibr" target="#b0">[1]</ref> 0.975 0.641 0.876 AEL2 <ref type="bibr" target="#b11">[12]</ref> 0.983 0.747 0.790 CapsNet <ref type="bibr" target="#b27">[28]</ref> 0.871 0.679 0.531 UnSt <ref type="bibr" target="#b10">[11]</ref> 0.993 -0.803 CAVGA <ref type="bibr" target="#b47">[48]</ref> 0 the impression m automatically fixes the anomaly region. Based on the impression, the Expert-Net reconstructs the exact details of the anomaly-free imagex. Meanwhile, the na?ve impression versionm often contains the anomaly area with the total image become blurry. Next, the anomaly region e is calculated by a pixel-wise scoring as Eq. <ref type="formula" target="#formula_9">(8)</ref>, in which pixels in the anomaly region have higher scores than that in the anomaly-free region. <ref type="figure" target="#fig_9">Fig. 8</ref> shows the learned distributions of anomaly-free images and anomalous images. It can be found that the anomaly image and anomalous image are with different mean and variance. Furthermore, based  on the mutual information, one can be found the k-Nearest anomaly-free images when the anomalous image is given. For instance, the cracked hazelnut share a similar pose with the matched anomaly-free one at the top of <ref type="figure" target="#fig_9">Fig. 8</ref>. Here two types of anomalous images are given (i.e., crack and print). Ablation study.</p><p>To evaluate how different parts of UTAD contribute to the final performance on the two tasks, we conduct rigorous ablation studies by removing or replacing a subset of models. Details of different baselines are described as follows:</p><p>? Without maximizing mutual information (Without L inf o ). To illustrate the contribution of the mutual information learning for impression extraction, we remove this part and re-train the remaining model.  the effectiveness of Expert-Net in anomaly detection, we remove Expert-Net totally and the anomaly error map e is calculated by e = l (? l (x, m)). ? Detail Guidance module (Without E S ). To examine the contribution of the guidance from detailed information, we remove the E S and feed the input of AdaIN in the decoder of Expert-Net with random weights of Gaussian distributions (i.e., N (0, 1)).</p><p>The ablation studies are mainly performed on 10 categories on the MNIST, Fashion MNIST, and CIFAR-10 datasets. The mean AuROC of each dataset is shown in <ref type="table" target="#tab_1">Table 2</ref>. Since for UnSt <ref type="bibr" target="#b10">[11]</ref>, one teacher net and 5 student nets are trained, it performs exceptionally well on these small datasets. However, on average, our method still outperforms all evaluated approaches. Furthermore, the visual comparison is provided in the 6th -8th column in <ref type="figure">Fig 9.</ref> Without L inf o , the anomaly region cannot be corrected detected. Without EN, misaligned edges occur in anomaly detection. Anomaly-free regions on the capsule are mistakenly detected as anomaly when the E S is removed (the detail code s is generated with random numbers). More visual results are provided in <ref type="figure">Fig. 10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Different measurements for anomaly detection.</head><p>A variety of measurements for anomaly detection has been tested on MVTec AD dataset. Numerical comparisons of different measurements on 4 objects are listed in <ref type="table" target="#tab_4">Table 3</ref>. The combination of 'conv1 2', 'conv2 2' and 'conv3 4' (i.e., PM) gets the best performance on different objects. We find the difference of features from 'conv2 2' gets secondary performance. The performance is deceased in 'conv3 4' since the resolution is too small. As illustrated in <ref type="figure">Fig. 9</ref>, It can be clearly found that the pixel-level or low-level differences like |x?x| and 'conv1 2' are sensible on the local difference, but often misled by misalignment. Meanwhile, the high-level difference (i.e., 'conv2 2') gets relative correct regions. The result of PM gets the best visual quality. Effect of the model capacity.</p><p>Finally, we investigate the relationship between the model capacity and anomaly detection performance. Numerical results on MVTecAD Hazelnut are reported in <ref type="table" target="#tab_5">Table 4</ref>. We find that more learnable parameters, which result in better reconstruction ability of an AutoEncoder (AE) <ref type="bibr" target="#b11">[12]</ref>, do not help the anomaly detection. This confirms our observation in <ref type="figure">Fig. 1</ref>. On the other hand, benefiting from the proposed two-stage reconstruction scheme, our method outperforms UnSt <ref type="bibr" target="#b10">[11]</ref> with a similar number of parameters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a novel method (i.e., the Unsupervised Two-stage Anomaly Detection) for unsupervised anomaly detection in natural images. In particular, we propose to utilize a two-stage framework (i.e., IE-Net, Expert-Net) for anomaly detection. The IE-Net and Expert-Net are used to generate high-fidelity and anomaly-free reconstructions of the input. UTAD generates rich and intuitive intermediate results, make the framework interpretable. Extensive experiments demonstrate state-of-the-art performance on different datasets, which contain different types of real-world objects and textures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Overview of the proposed method. Our framework mainly contains two components: IE-Net and Expert-Net. (a) IE-Net tries to generate the anomaly-free reconstruction (i.e., impression m). (b) The Expert-Net aims to learn an invertible mapping to manipulate the high-fidelity details between impression m and input x. Finally, the anomaly map e is calculated through Perceptual Measurement ? by leveraging the difference among x,x, m, andm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Architecture of the IE-Net, which is constructed by an encoder EM , a discriminator T , and a decoder DM . When training is done, the components in the dashed box are used for impression extraction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Architecture of the Expert-Net, which contains a detail extractor ES, two encoders, and two decoders. The invertible mapping between impressions and images is learned.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative comparisons among different methods on MVTec AD dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Our method generates anomaly-free impression m and its high-fidelity reconstruction. The anomaly areas are highlighted in the input images with red contours.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Interpretation of the intermediate results in UTAD. As illustrated in dashed boxes, the impression m is robust to anomalous regions, while the na?ve impressionm is affected by the anomalous regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>Analysis of the learned features in IE-Net. Feature distributions of anomaly-free images (blue) and anomalous images (green) are illustrated via t-SNE<ref type="bibr" target="#b29">[30]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 .Figure 10 .</head><label>910</label><figDesc>Qualitative comparison of different combinations of our method on anomaly detection. Input image Impression w/o E S w/o E S with E S Illustration of the effectiveness of detail guidance module ES. Two results are provided without ES (Column 3-4). With ES, some inconsistencies in details e.g., color-shift (top row) and misplaced text (i.e., '500', dashed box in bottom row) are tackled.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparison of anomaly detection in category-specific IoU, mean IoU (IoU) and mean AuROC (AuROC) on the MVTec AD dataset. The best results are highlighted with bold font, and the second bests are underlined.</figDesc><table><row><cell cols="11">Category AnoGAN [44] AVID [42] AEL2 [12] AESSIM [12] LSA [1] AD-VAE [29] ?-VAE [16] UnSt [11] CAVGA [48] Ours</cell></row><row><cell>Bottle</cell><cell>0.05</cell><cell>0.28</cell><cell>0.22</cell><cell>0.15</cell><cell>0.27</cell><cell>0.27</cell><cell>0.27</cell><cell>0.28</cell><cell>0.34</cell><cell>0.37</cell></row><row><cell>Cable</cell><cell>0.01</cell><cell>0.27</cell><cell>0.05</cell><cell>0.01</cell><cell>0.36</cell><cell>0.18</cell><cell>0.26</cell><cell>0.11</cell><cell>0.38</cell><cell>0.38</cell></row><row><cell>Capsule</cell><cell>0.04</cell><cell>0.21</cell><cell>0.11</cell><cell>0.09</cell><cell>0.22</cell><cell>0.11</cell><cell>0.24</cell><cell>0.24</cell><cell>0.31</cell><cell>0.41</cell></row><row><cell>Carpet</cell><cell>0.34</cell><cell>0.25</cell><cell>0.38</cell><cell>0.69</cell><cell>0.76</cell><cell>0.10</cell><cell>0.79</cell><cell>0.50</cell><cell>0.73</cell><cell>0.79</cell></row><row><cell>Grid</cell><cell>0.04</cell><cell>0.51</cell><cell>0.83</cell><cell>0.88</cell><cell>0.20</cell><cell>0.02</cell><cell>0.36</cell><cell>0.19</cell><cell>0.38</cell><cell>0.89</cell></row><row><cell>Hazelnut</cell><cell>0.02</cell><cell>0.54</cell><cell>0.41</cell><cell>0.00</cell><cell>0.41</cell><cell>0.44</cell><cell>0.63</cell><cell>0.36</cell><cell>0.51</cell><cell>0.65</cell></row><row><cell>Leather</cell><cell>0.34</cell><cell>0.32</cell><cell>0.67</cell><cell>0.34</cell><cell>0.77</cell><cell>0.24</cell><cell>0.41</cell><cell>0.44</cell><cell>0.79</cell><cell>0.79</cell></row><row><cell>Metal Nut</cell><cell>0.00</cell><cell>0.05</cell><cell>0.26</cell><cell>0.01</cell><cell>0.38</cell><cell>0.49</cell><cell>0.22</cell><cell>0.31</cell><cell>0.45</cell><cell>0.47</cell></row><row><cell>Pill</cell><cell>0.17</cell><cell>0.11</cell><cell>0.25</cell><cell>0.07</cell><cell>0.18</cell><cell>0.18</cell><cell>0.48</cell><cell>0.23</cell><cell>0.40</cell><cell>0.49</cell></row><row><cell>Screw</cell><cell>0.01</cell><cell>0.22</cell><cell>0.34</cell><cell>0.03</cell><cell>0.38</cell><cell>0.17</cell><cell>0.38</cell><cell>0.17</cell><cell>0.48</cell><cell>0.44</cell></row><row><cell>Tile</cell><cell>0.08</cell><cell>0.09</cell><cell>0.23</cell><cell>0.04</cell><cell>0.32</cell><cell>0.23</cell><cell>0.38</cell><cell>0.22</cell><cell>0.38</cell><cell>0.40</cell></row><row><cell>Toothbrush</cell><cell>0.07</cell><cell>0.43</cell><cell>0.51</cell><cell>0.08</cell><cell>0.48</cell><cell>0.14</cell><cell>0.37</cell><cell>0.21</cell><cell>0.57</cell><cell>0.53</cell></row><row><cell>Transistor</cell><cell>0.08</cell><cell>0.22</cell><cell>0.22</cell><cell>0.01</cell><cell>0.21</cell><cell>0.30</cell><cell>0.44</cell><cell>0.15</cell><cell>0.35</cell><cell>0.47</cell></row><row><cell>Wood</cell><cell>0.14</cell><cell>0.14</cell><cell>0.29</cell><cell>0.36</cell><cell>0.41</cell><cell>0.14</cell><cell>0.45</cell><cell>0.16</cell><cell>0.59</cell><cell>0.59</cell></row><row><cell>Zipper</cell><cell>0.01</cell><cell>0.25</cell><cell>0.13</cell><cell>0.10</cell><cell>0.14</cell><cell>0.06</cell><cell>0.17</cell><cell>0.08</cell><cell>0.16</cell><cell>0.30</cell></row><row><cell>IoU</cell><cell>0.09</cell><cell>0.26</cell><cell>0.33</cell><cell>0.19</cell><cell>0.37</cell><cell>0.20</cell><cell>0.39</cell><cell>0.24</cell><cell>0.47</cell><cell>0.53</cell></row><row><cell>AuROC</cell><cell>0.74</cell><cell>0.78</cell><cell>0.82</cell><cell>0.87</cell><cell>0.79</cell><cell>0.86</cell><cell>0.86</cell><cell>0.87</cell><cell>0.89</cell><cell>0.90</cell></row><row><cell cols="3">Input image Recons. [12] Recons. [3]</cell><cell>Ours</cell><cell>Ours recons.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative comparison of different methods and ablation study on MNIST (DM ), Fashion MNIST (DF ) and CIFAR-10 (DC ) datasets. Here we use mean AuROC as the evaluation metric.</figDesc><table><row><cell>Existing method</cell><cell>DM DF</cell><cell>DC</cell></row><row><cell>OCGAN [38]</cell><cell cols="2">0.975 0.895 0.657</cell></row><row><cell>AnoGAN</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>?</head><label></label><figDesc>Effectiveness of Expert-Net (Without EN). To examine</figDesc><table><row><cell>Input image</cell><cell>Ground truth</cell><cell>| ? ?|</cell><cell>'conv1_2'</cell><cell>'conv2_2'</cell><cell>w/o</cell><cell>w/o EN</cell><cell>w/o E S</cell><cell>Final</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Ablation study. Performance comparison among differences between input and intermediate results and different choices of layers in VGG-19 on part of MVTec AD dataset.</figDesc><table><row><cell>Measurement</cell><cell>Bottle</cell><cell>Cable</cell><cell>Capsule</cell><cell>Hazelnut</cell></row><row><cell>|x ?x|</cell><cell>0.23</cell><cell>0.17</cell><cell>0.33</cell><cell>0.39</cell></row><row><cell>|x ? m|</cell><cell>0.29</cell><cell>0.20</cell><cell>0.21</cell><cell>0.47</cell></row><row><cell>|m ?m|</cell><cell>0.15</cell><cell>0.06</cell><cell>0.15</cell><cell>0.41</cell></row><row><cell>'conv1 2'</cell><cell>0.30</cell><cell>0.11</cell><cell>0.33</cell><cell>0.39</cell></row><row><cell>'conv2 2'</cell><cell>0.33</cell><cell>0.31</cell><cell>0.35</cell><cell>0.54</cell></row><row><cell>'conv3 4'</cell><cell>0.24</cell><cell>0.20</cell><cell>0.31</cell><cell>0.49</cell></row><row><cell>PM</cell><cell>0.37</cell><cell>0.38</cell><cell>0.41</cell><cell>0.65</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Effect of parameters' number in training.</figDesc><table><row><cell>Methods</cell><cell cols="5">AE-64 AE-128 AE-256 UnSt [11] Ours</cell></row><row><cell cols="2"># Param (M) 2.98</cell><cell>11.89</cell><cell>47.47</cell><cell>14.28</cell><cell>16.77</cell></row><row><cell>IoU (?)</cell><cell>0.301</cell><cell>0.188</cell><cell>0.197</cell><cell>0.363</cell><cell>0.649</cell></row><row><cell>AuROC (?)</cell><cell>0.896</cell><cell>0.954</cell><cell>0.945</cell><cell>0.937</cell><cell>0.976</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Latent space autoregression for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Abati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelo</forename><surname>Porrello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ganomaly: Semi-supervised anomaly detection via adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samet</forename><surname>Akcay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Skip-ganomaly: Skip connected and adversarially trained encoder-decoder anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samet</forename><surname>Ak?ay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
		<editor>IJCNN. IEEE</editor>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Transfer representation-learning for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerone</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Tanay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewis D</forename><surname>Morton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Griffin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. JMLR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<title level="m">Soumith Chintala, and L?on Bottou. Wasserstein gan</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Outlier detection. Encyclopedia of Database Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimek</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Schubert</forename><surname>Erich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep autoencoding models for unsupervised anomaly segmentation in brain mr images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Baur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benedikt</forename><surname>Wiestler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shadi</forename><surname>Albarqouni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International MICCAI Brainlesion Workshop</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Unsupervised learning of anomaly detection from contaminated image data using simultaneous encoder training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Ahlberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11034</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mvtec ad-a comprehensive real-world dataset for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Uninformed students: Student-teacher anomaly detection with discriminative latent embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Improving unsupervised defect segmentation by applying structural similarity to autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sindy</forename><surname>L?we</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.02011</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Defect detection in sem images of nanofibrous materials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Carrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Manganini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giacomo</forename><surname>Boracchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ettore</forename><surname>Lanzarone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image anomaly detection with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Mandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Iterative energy-based projection on a normal data manifold for anomaly localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dehaene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriel</forename><surname>Frigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?bastien</forename><surname>Combrexelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Eline</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Mutual meanteaching: Pseudo label refinery for unsupervised domain adaptation on person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno>ICLR, 2020. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Moussa Reda Mansour, Svetha Venkatesh, and Anton van den Hengel. Memorizing normality to detect anomaly: Memoryaugmented deep autoencoder for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vuong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Budhaditya</forename><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno>CVPR, 2020. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multimodal unsupervised image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Exploring deep anomaly detection methods based on capsule net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iluju</forename><surname>Kiringa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tet</forename><surname>Yeap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifeng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CCAI</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Towards visually explaining variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runze</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikrishna</forename><surname>Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bir</forename><surname>Bhanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Octavia</forename><surname>Radke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Camps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Clustergan: Latent space clustering in generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudipto</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Asnani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sreeram</forename><surname>Kannan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Anomaly detection in nanofibrous materials by cnn-based self-similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Napoletano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flavio</forename><surname>Piccoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raimondo</forename><surname>Schettini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Weakly supervised action localization by sparse temporal pooling network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phuc</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Xunzhang Dai, and Tingting Cheng. Pathological evidence exploration in deep retinal image diagnosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifan</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongji</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imari</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Representation learning by learning to count</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">fgan: Training generative neural samplers using variational divergence minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botond</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Tomioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ocgan: One-class novelty detection using gans with constrained latent representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pramuditha</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCI</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep semi-supervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>G?rnitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Avid: Adversarial visual irregularity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masoud</forename><surname>Pourreza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahim</forename><surname>Entezari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmood</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">f-anogan: Fast unsupervised anomaly detection with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Seeb?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Sebastian M Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ursula</forename><surname>Langs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidt-Erfurth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection with generative adversarial networks to guide marker discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Seeb?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ursula</forename><surname>Sebastian M Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Langs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on information processing in medical imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Support vector data description. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">On mutual information maximization for representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josip</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Paul K Rubenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lucic</surname></persName>
		</author>
		<idno>ICLR, 2020. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Attention guided anomaly detection and localization in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashanka</forename><surname>Venkataramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuan-Chuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Rajat Vikram Singh, and Abhijit Mahalanobis</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>ECCV</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero P</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<editor>ECCV. Springer</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Encoding structure-texture relation with p-net for anomaly detection in retinal images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiwang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
