<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">In Search of Lost Domain Generalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">In Search of Lost Domain Generalization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The goal of domain generalization algorithms is to predict well on distributions different from those seen during training. While a myriad of domain generalization algorithms exist, inconsistencies in experimental conditions-datasets, architectures, and model selection criteria-render fair and realistic comparisons difficult. In this paper, we are interested in understanding how useful domain generalization algorithms are in realistic settings. As a first step, we realize that model selection is non-trivial for domain generalization tasks. Contrary to prior work, we argue that domain generalization algorithms without a model selection strategy should be regarded as incomplete. Next, we implement DOMAINBED, a testbed for domain generalization including seven multi-domain datasets, nine baseline algorithms, and three model selection criteria. We conduct extensive experiments using DO-MAINBED and find that, when carefully implemented, empirical risk minimization shows state-of-the-art performance across all datasets. Looking forward, we hope that the release of DOMAINBED, along with contributions from fellow researchers, will streamline reproducible and rigorous research in domain generalization. * Alphabetical order, equal contribution.</p><p>Preprint. Under review.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:2007.01434v1 [cs.</head><p>LG] 2 Jul 2020 Strike (with) a pose: Neural networks are easily fooled by strange poses of familiar objects. CVPR, 2019.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Machine learning systems often fail to generalize out-of-distribution, crashing in spectacular ways when tested outside the domain of training examples <ref type="bibr" target="#b32">[Torralba and Efros, 2011]</ref>. The overreliance of learning systems on the training distribution manifests widely. For instance, self-driving car systems struggle to perform under conditions different to those of training, including variations in light <ref type="bibr">[Dai and Van Gool, 2018]</ref>, weather <ref type="bibr" target="#b35">[Volk et al., 2019]</ref>, and object poses <ref type="bibr">[Alcorn et al., 2019]</ref>. As another example, systems trained on medical data collected in one hospital do not generalize to other health centers <ref type="bibr">[Castro et al., 2019</ref><ref type="bibr" target="#b2">, AlBadawy et al., 2018</ref><ref type="bibr">, Perone et al., 2019</ref><ref type="bibr" target="#b11">, Heaven, 2020</ref>. <ref type="bibr" target="#b90">Arjovsky et al. [2019]</ref> suggest that failing to generalize out-of-distribution is failing to capture the causal factors of variation in data, clinging instead to easier-to-fit spurious correlations, which are prone to change from training to testing domains. Examples of spurious correlations commonly absorbed by learning machines include racial biases <ref type="bibr" target="#b28">[Stock and Cisse, 2018]</ref>, texture statistics <ref type="bibr" target="#b5">[Geirhos et al., 2018]</ref>, and object backgrounds <ref type="bibr" target="#b95">[Beery et al., 2018]</ref>. Alas, the capricious behaviour of machine learning systems out-of-distribution is a roadblock to their deployment in critical applications.</p><p>Aware of this problem, the research community has spent significant effort during the last decade to develop algorithms able to generalize out-of-distribution. In particular, the literature in domain generalization assumes access to multiple datasets during training, each of them containing examples about the same task, but collected under a different domain or environment <ref type="bibr">[Blanchard et al., 2011</ref><ref type="bibr">, Muandet et al., 2013</ref>. The goal of domain generalization algorithms is to incorporate the invariances across these training datasets into a classifier, in hopes that such invariances also hold in novel test domains. Different domain generalization solutions assume different types of invariances and propose algorithms to estimate them from data. Office-Home A C P R Average  59.2 52.3 74.6 76.0 65.5 Our ERM 62.7 53.4 76.5 77.3 67.5</p><p>Despite the enormous importance of domain generalization, the literature is scattered: a plethora of different algorithms appear yearly, and these are evaluated under different datasets and model selection criteria. Borrowing from the success of standard computer vision benchmarks such as ImageNet <ref type="bibr">[Russakovsky et al., 2015]</ref>, the purpose of this work is to perform a standardized, rigorous comparison of domain generalization algorithms. In particular, we ask: how useful are domain generalization algorithms in realistic settings? Towards answering this question, we first study model selection criteria for domain generalization methods, resulting in the recommendation:</p><p>A domain generalization algorithm should be responsible for specifying a model selection method.</p><p>We then carefully implement nine domain generalization algorithms on seven multi-domain datasets and three model selection criteria, leading us to the conclusion reflected in <ref type="table" target="#tab_0">Tables 1 and 4</ref>:</p><p>When equipped with modern neural network architectures and data augmentation techniques, empirical risk minimization achieves state-of-the-art performance in domain generalization.</p><p>As a result of our research, we release DOMAINBED, a framework to streamline rigorous and reproducible experimentation in domain generalization. Using DOMAINBED, adding a new algorithm or dataset is a matter of a few lines of code; a single command runs all the experiments, performs all the model selections, and auto-generates all the tables included in this work. Moreover, our motivation is to keep DOMAINBED alive, welcoming pull requests from our fellow colleagues to update the available algorithms, datasets, model selection criteria, and result tables.</p><p>Section 2 kicks off our exposition with a review of the domain generalization setup. Section 3 discusses the difficulties of model selection in domain generalization and makes recommendations for a path forward. Section 4 introduces DOMAINBED, describing the algorithms and datasets contained in the initial release. Section 5 discusses the experimental results of running the entire DOMAINBED suite; these illustrate the strength of ERM and the importance of model selection criteria. Finally, Section 6 offers our view on future research directions in domain generalization. Our Appendices review one hundred articles spanning a decade of research in this topic, collecting the experimental performance of over thirty published algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The problem of domain generalization</head><p>The goal of supervised learning is to predict values y ? Y of a target random variable Y , given values x ? X of an input random variable X. Predictions? about x originate from a predictor f : X ? Y, such that? = f (x). We often decompose predictors as f = w ? ?, where we call ? : X ? H the featurizer, and w : H ? Y the classifier. Our main tool to solve the prediction task is the training dataset D = {(x i , y i )} n i=1 , which contains identically and independently distributed (iid) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training inputs Test inputs</head><p>Generative learning</p><formula xml:id="formula_0">U 1 ? Unsupervised learning U 1 U 1 Supervised learning L 1 U 1 Semi-supervised learning L 1 , U 1 U 1 Multitask learning L 1 , . . . , L dtr U 1 , . . . , U dtr Continual (or lifelong) learning L 1 , . . . , L ? U 1 , . . . , U ? Domain adaptation L 1 , . . . , L dtr , U dtr+1 U dtr+1 Transfer learning U 1 , . . . , U dtr , L dtr+1 U dtr+1 Domain generalization L 1 , . . . , L dtr U dtr+1</formula><p>examples from the joint probability distribution P (X, Y ). Given a loss function : Y ? Y ? [0, ?) measuring the prediction error at one example, we often cast supervised learning as finding a predictor minimizing the population risk E (x,y)?P [ (f (x), y)]. Since we only have access to the data distribution P (X, Y ) via the dataset D, we instead choose a predictor minimizing the empirical risk <ref type="bibr" target="#b33">[Vapnik, 1998]</ref>. The rest of this paper studies the problem of domain generalization, an extension of supervised learning where training datasets from multiple domains (or environments) are available to train our predictor <ref type="bibr">[Blanchard et al., 2011]</ref>. More specifically, we characterize each domain d by a dataset</p><formula xml:id="formula_1">1 n n i=1 (f (x i ), y i )</formula><formula xml:id="formula_2">D d = {(x d i , y d i )} n d d=1 containing iid examples from some probability distribution P (X d , Y d ), for all training domains d ? {1, . . . , d tr }.</formula><p>The goal of domain generalization is out-of-distribution generalization: learning a predictor able to perform well at some unseen test domain d te = d tr + 1.</p><p>Since no data about the test domain is available during training, we must assume the existence of some statistical invariances across training and testing domains in order to incorporate such invariances (but nothing else) into our predictor. The type of invariance assumed, as well as how to estimate it from the training datasets, varies between domain generalization algorithms.</p><p>Domain generalization differs from unsupervised domain adaptation. In the latter, it is assumed that unlabeled data from the test domain is available during training <ref type="bibr">[Pan and Yang, 2009</ref><ref type="bibr">, Patel et al., 2015</ref><ref type="bibr" target="#b39">, Wilson and Cook, 2018</ref>. <ref type="table" target="#tab_1">Table 2</ref> compares different machine learning setups to highlight the nature of domain generalization problems. The causality literature refers to domain generalization as learning from multiple environments <ref type="bibr">[Peters et al., 2016</ref><ref type="bibr" target="#b90">, Arjovsky et al., 2019</ref>. Although challenging, domain generalization is the best approximation to real prediction problems, where unforeseen distributional discrepancies between training and testing data are surely expected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model selection as part of the learning problem</head><p>Here we discuss issues surrounding model selection (choosing hyperparameters, training checkpoints, architecture variants) in domain generalization and make specific recommendations for a path forward. Because we lack access to a validation set identically distributed to the test data, model selection in domain generalization is not as straightforward as in supervised learning. Some works adopt heuristic strategies whose behavior is not well-studied, while others simply omit a description of how to choose hyperparameters. This leaves open the possibility that hyperparameters were chosen using the test data, which is not methodologically sound. Differences in results arising from inconsistent tuning practices may be misattributed to the algorithms under study, complicating fair assessments.</p><p>We believe that much of the confusion surrounding model selection in domain generalization arises from treating it as a question of experimental design. In reality, selecting hyperparameters is a learning problem at least as hard as fitting the model (inasmuch as we may interpret any model parameter as a hyperparameter). Like all learning problems, model selection requires assumptions about how the test data relates to the training data. Different domain generalization algorithms make different assumptions, and it is not clear a priori what assumptions are correct, or how these assumptions influence the model selection criterion. Indeed, choosing reasonable assumptions is at the heart of domain generalization research. Therefore, a domain generalization algorithm without a strategy to choose its hyperparameters remains incomplete.</p><p>Recommendation 1 A domain generalization algorithm should be responsible for specifying a model selection method.</p><p>While algorithms without well-justified model selection methods are incomplete, they may be useful as stepping-stones in a research agenda. In this case, instead of using an ad-hoc model selection method, we can evaluate incomplete algorithms by considering an oracle model selection method, where we select hyperparameters on the test domain. Of course, it is important that we avoid invalid comparisons between oracle results and baselines tuned without an oracle method. Also, unless we restrict access to the test domain data somehow, we risk obtaining meaningless results. For instance, we could just train on such test domain data using supervised learning.</p><p>Recommendation 2 Researchers should disclaim any oracle-selection results as such and specify policies to limit access to the test domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Three model selection methods</head><p>Having made broad recommendations, we review and justify three methods for model selection in domain generalization, often used but rarely discerned.</p><p>Training-domain validation set We split each training domain into training and validation subsets. Then, we pool the validation subsets of each training domain to create an overall validation set. Finally, we choose the model maximizing the accuracy on the overall validation set.</p><p>This strategy assumes that the training and test examples follow similar distributions. For example, <ref type="bibr">Ben-David et al. [2010]</ref> bound the test domain error of a classifier by the training domain error, plus a divergence measure between the training and test domains.</p><p>Leave-one-domain-out cross-validation Given d tr training domains, we train d tr models with equal hyperparameters, each holding one of the training domains out. We evaluate each model on its held-out domain, and average the accuracies of these models over their held-out domains. Finally, we choose the model maximizing this average accuracy, re-trained on all d tr domains.</p><p>This strategy assumes that training and test domains are drawn from a meta-distribution over domains, and that our goal is to maximize the expected performance under this meta-distribution.</p><p>Test-domain validation set (oracle) We choose the model maximizing the accuracy on a validation set that follows the distribution of the test domain. Following our earlier recommendation to limit test domain access, we allow 20 queries per algorithm (one query per choice of hyperparameters in our random search). This means that we do not allow early stopping based on the validation set. Instead, we train all models for the same fixed number of steps and consider only the final checkpoint. Recall that we do not consider this a valid benchmarking methodology, since it requires access to the test domain. Oracle-selection results can be either optimistic, because we access the test distribution, or pessimistic, because the query limit reduces the number of considered hyperparameter combinations.</p><p>As an alternative to limiting the number of queries, we could borrow tools from differential privacy, previously applied to enable multiple re-uses of validation sets in standard supervised learning <ref type="bibr">[Dwork et al., 2015]</ref>. In a nutshell, differential privacy tools add Laplace noise to the accuracy statistic of the algorithm before reporting it to the practitioner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Considerations from the literature</head><p>Some references in prior work discuss additional strategies to choose hyperparemeters in domain generalization problems. For instance, <ref type="bibr">Krueger et al. [2020, Appendix B.1]</ref> suggest choosing hyperparameters to maximize the performance across all domains of an external dataset. The validity of this strategy depends on the relatedness between datasets. Albuquerque et al. <ref type="bibr">[2019, Section 5.3.2]</ref> suggest performing model selection based on the loss function (which often incorporates an algorithm-specific regularizer), and DInnocente and Caputo [2018, Section 3] derive an strategy specific to their algorithm. The initial release comprises nine algorithms, seven datasets, and three model selection methods (described in Section 3), as well as the infrastructure to run all the experiments and generate all the L A T E X tables below with a single command. DOMAINBED is a living project: we expect to update the above repository with new results, algorithms, and datasets. Contributions via pull requests from fellow researchers are welcome. Adding a new algorithm or dataset to DOMAINBED is a matter of a few lines of code (see Appendix E for an example).  <ref type="table" target="#tab_2">Table 3</ref>, and provide their full details in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DOMAINBED includes downloaders and loaders</head><p>The datasets differ in many ways but two are particularly important. The first difference is between synthetic and real datasets. In Rotated MNIST and Colored MNIST, domains are synthetically constructed such that we know what features will generalize a priori, so using too much prior knowledge (e.g. by augmenting with rotations) is off-limits, whereas the other datasets contain domains arising from natural processes, making it sensible to use prior knowledge. The second difference is about what changes across domains. On one hand, in datasets other than Colored MNIST, the domain changes the distribution of images, but likely bears no information about the true image-to-label mapping. On the other hand, in Colored MNIST, the domain influences the true image-to-label mapping, biasing algorithms that try to estimate this function directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Algorithms</head><p>The initial release of DOMAINBED includes implementations of nine baseline algorithms:</p><p>? Empirical Risk Minimization (ERM, <ref type="bibr" target="#b33">Vapnik [1998]</ref>) minimizes the sum of errors across domains and examples. such that the optimal linear classifier on top of that representation matches across domains.</p><p>Appendix D describes the network architectures and hyperparameter search spaces for all algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation choices for realistic evaluation</head><p>Our goal is a realistic evaluation of domain generalization algorithms. To that end, we make several implementation choices which depart from prior work, explained below.</p><p>Large models Most prior work on VLCS and PACS borrows features from or finetune ResNet-18 models <ref type="bibr" target="#b10">[He et al., 2016]</ref>. Since larger ResNets are known to generalize better, we opt to finetune ResNet-50 models for all datasets except Rotated MNIST and Colored MNIST, where we use a smaller CNN architecture (see Appendix D).</p><p>Data augmentation Data augmentation is a standard ingredient to train image classification models. In domain generalization, data augmentation can play an especially important role when augmentations can approximate some of the variations between domains. Therefore, for all non-MNIST datasets, we train using the following data augmentations: crops of random size and aspect ratio, resizing to 224 ? 224 pixels, random horizontal flips, random color jitter, grayscaling the image with 10% probability, and normalization using the ImageNet channel means and standard deviations. For MNIST datasets, we use no data augmentation.</p><p>Using all available data In Rotated MNIST, whereas the usual version of the dataset constructs all domains from the same set of 1000 digits, we divide all the MNIST digits evenly among domains. We deviate from standard practice for two reasons: we believe that using the same digits across training and test domains amounts to leaking test data, and we believe that artificially restricting the available training domain data complicates the task in an unrealistic way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We run experiments for all algorithms (Section 4.2), datasets (Section 4.1), and model selection criteria (Section 3) shipped in DOMAINBED. We consider all configurations of a dataset where we hide one domain for testing and train on the remaining ones.</p><p>Hyperparameter search For each algorithm and test environment, we conduct a random search <ref type="bibr">[Bergstra and Bengio, 2012]</ref> of 20 trials over the hyperparameter distribution (see Appendix D). We use each model selection method from Section 3 to select amongst the 20 models from the random search. We split the data from each domain into 80% and 20% splits. We use the larger splits for training and final evaluation, and the smaller splits to select hyperparameters.</p><p>Standard error bars While some domain generalization literature reports error bars across seeds, randomness arising from model selection is often ignored. While this is acceptable if the goal is a best-versus-best comparison, it prohibits nuanced analyses. For instance, does method A outperform method B only because random search for A got lucky? We therefore repeat our entire study three times making every random choice anew: hyperparameters, weight initializations, and dataset splits. Every number we report is a mean over these repetitions, together with their estimated standard error.</p><p>This experimental protocol amounts to training a total of 45,900 neural networks. <ref type="table" target="#tab_4">Table 4</ref> summarizes the results of our experiments. For each dataset and model, we average the best results (according to each model selection criterion) across test domains. We then report the average of this number across three independent runs of the entire sweep, and its corresponding standard error. For results per dataset and domain, we refer the reader to Appendix B. We draw three main conclusions from our results:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results</head><p>Our ERM baseline outperforms all previously published results <ref type="table" target="#tab_0">Table 1</ref> summarizes this result when model selection is performed using a training domain validation set. What is responsible for this strong performance? We suspect four factors: a bigger network architecture (ResNet-50), strong data augmentations, careful hyperparameter tuning and, in Rotated MNIST, using the full training data to construct our domains (instead of using a 1000-image subset). While we are not first to use any these techniques alone, we may be first to combine all of them. Interestingly, these results suggest standard techniques to improve in-distribution generalization are very effective at improving out-of-distribution generalization. Our result does not refute prior work: it is possible that with similar techniques, some competing methods may improve upon ERM. Rather, our results highlight the importance of comparing domain generalization algorithms to strong and realistic baselines. Incorporating novel algorithms into DOMAINBED is an easy way to do so. For an extensive review of results published in the literature about more than thirty algorithms, we refer the reader to Appendix A.5.</p><p>When all conditions are equal, no algorithm outperforms ERM by a significant margin We observe this result in <ref type="table" target="#tab_4">Table 4</ref>, obtained from running from scratch every combination of dataset, algorithm, and model selection criteria included in DOMAINBED. Given any model selection criterion, no method improves upon the average performance of ERM in more than one point. We do not claim that any of these algorithms cannot possibly improve upon ERM, but getting substantial domain generalization improvements over ERM on these datasets proved challenging.</p><p>Model selection methods matter We observe that model selection with a training domain validation set outperforms leave-one-domain-out cross-validation across multiple datasets and algorithms. This does not mean that using a training domain validation set is the right way to tune hyperparameters. After all, it did not enable any algorithm to significantly outperform the ERM baseline. Moreover, the stronger performance of oracle-selection (+2%) suggests possible headroom for improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Outlook</head><p>We have conducted an extensive empirical evaluation of domain generalization algorithms. Our results led to two major conclusions. First, empirical risk minimization achieves state-of-the-art performance when compared to eight popular domain generalization alternatives, also improving upon all the numbers previously reported in the literature. Second, model selection has a significant effect on domain generalization, and it should be regarded as an integral part of any proposed method. We conclude with a series of mini-discussions that answer some questions, but raise even more.</p><p>How can we push data augmentation further? While conducting our experiments, we became aware of the power of data augmentation.  show that strong data augmentation can improve out-of-distribution generalization while not impacting in-distribution generalization.</p><p>We think of data augmentation as feature removal: the more we augment a training example, the more invariant we make our predictor with respect to the applied transformations. If the practitioner is lucky and performs the data augmentations that cancel the spurious correlations varying from domain to domain, then out-of-distribution performance should improve. Are these the right datasets? Some of the datasets considered in the domain-generalization literature do not reflect realistic situations. In reality, if one wanted to classify cartoons, the easiest option would be to collect a small labeled dataset of cartoons. Should we consider more realistic, impactful tasks for better research in domain generalization? Attractive alternatives include medical imaging in different hospitals and self-driving cars in different cities.</p><p>It is all about (untestable) assumptions Every time we use ERM, we assume that training and testing examples are drawn from the same distribution. Also every time, this is an untestable assumption. The same applies for domain generalization: each algorithm assumes a different (untestable) type of invariance across domains. Therefore, the performance of a domain generalization algorithm depends on the problem at hand, and only time can tell if we have made a good choice. This is akin to the generalization of a scientific theory such as Newton's gravitation, which cannot be proved but has so far resisted falsification. We believe there is promise in algorithms with self-adaptation capabilities during test time.</p><p>Benchmarking and the rules of the game While limiting the use of modern techniques cheapens experiments, it also distorts them from more realistic scenarios, which is the focus of our study. Our view is that benchmark designers should balance these factors to promote a set of rules of the game that are not only well-defined, but realistic and well-motivated. Synthetic datasets are helpful tools, but we must not lose sight of the goal, which is artificial intelligence able to generalize in the real world. In words of Marcel Proust:</p><p>Perhaps the immobility of the things that surround us is forced upon them by our conviction that they are themselves, and not anything else, and by the immobility of our conceptions of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader impact</head><p>Current machine learning systems fail capriciously when facing novel distributions of examples. This unreliability hinders the application of machine learning systems in critical applications such as transportation, security, and healthcare. Here we strive to find robust machine learning models that discard spurious correlations, as we expect invariant patterns to generalize out-of-distribution. This should lead to fairer, safer, and more reliable machine learning systems. But with great powers comes great responsibility: researchers in domain generalization must adhere to the strictest standards of model selection and evaluation. We hope that our results and the release of DOMAINBED are some small steps in this direction, and we look forward to collaborate with fellow researchers to streamline reproducible and rigorous research towards true generalization power. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Augmenting data</head><p>Data augmentation is an effective strategy to address domain generalization . Unfortunately, how to design efficient data augmentation routines depends on the type of data at hand, and demands a significant amount of work from human experts. , <ref type="bibr" target="#b41">Yan et al. [2020]</ref>, <ref type="bibr" target="#b38">Wang et al. [2020]</ref> use mixup <ref type="bibr" target="#b42">[Zhang et al., 2018]</ref> to blend examples from the different training distributions. Carlucci et al.</p><p>[2019a] constructs an auxiliary classification task aimed at solving jigsaw puzzles of image patches. The authors show that this self-supervised learning task learns features that improve domain generalization. Albuquerque et al. <ref type="bibr">[2020]</ref> introduce the self-supervised task of predicting responses to Gabor filter banks, in order to learn more transferrable features.  remove textural information from images to improve domain generalization. <ref type="bibr" target="#b36">Volpi et al. [2018]</ref> show that training with adversarial data augmentation on a single domain is sufficient to improve domain generalization. Nam et al. <ref type="bibr">[2019]</ref>, Asadi et al. <ref type="bibr">[2019]</ref> promote representations of data that ignore texture and focus on shape. , , Carlucci et al.</p><p>[2019a] are three alternatives that use GANs to augment the data available during training time.</p><p>A.5 Previous state-of-the-art numbers <ref type="table" target="#tab_7">Table 5</ref> compiles the best out-of-distribution test accuracies reported across a decade of domain generalization research.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Architectures</head><p>We list the neural network architecture used for each dataset in <ref type="table" target="#tab_9">Table 6</ref> and specify the details of our MNIST network in 7.  GroupNorm <ref type="formula">(groups=8)  4</ref> Conv2D (in=64, out=128, stride=2) 5</p><p>ReLU <ref type="formula">6</ref> GroupNorm <ref type="formula">(8 groups)  7</ref> Conv2D (in=128, out=128) 8</p><p>ReLU 9</p><p>GroupNorm (8 groups) 10 Conv2D (in=128, out=128) 11 ReLU 12 GroupNorm (8 groups) 13 Global average-pooling For the architecture "Resnet-50", we replace the final (softmax) layer of a ResNet50 pretrained on ImageNet and fine-tune. Observing that batch normalization interferes with domain generalization algorithms (as different minibatches follow different distributions), we freeze all batch normalization layers before fine-tuning. We insert a dropout layer before the final linear layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Hyperparameters</head><p>We list all hyperparameters, their default values, and the search distribution for each hyperparameter in our random hyperparameter sweeps, in <ref type="table" target="#tab_11">Table 8</ref>. By inheriting from ERM, this new class has access to a default classifier .network, optimizer .optimizer, and prediction method .predict(x). Finally, we should tell DOMAINBED about the hyperparameters of this new algorithm. To do so, add the following line to the function hparams from hparams registry.py: hparams [ ' dro_eta '] = (1 e -2 , 10** random_state . uniform ( -3 , -1)) class MyDataset ( M u l t i p l e E n v i r o n m e n t I m a g e F o l d e r ): N_STEPS = 2500 CHECKPOINT_FREQ = 300 def __init__ ( self , root , test_envs = None ): self . dir = os . path . join ( root , " MyDataset / " ) super ( MyDataset , self ). __init__ ( self . dir )</p><p>In the previous, N STEPS determines the number of gradient updates an algorithm should perform to learn this dataset. The variable CHECKPOINT FREQ determines the number of gradient steps an algorithm should wait before reporting its performance in all domains.</p><p>We are now ready to launch an experiment with our new algorithm and dataset:</p><p>python train . py --model DRO --dataset MyDataset --data_dir / path --test \ _envs 1 \ --output_dir / path / to / logs_files --hparams ' {" dro_eta ": 0.2} ' Finally, we can run a fully automated sweep on all datasets, algorithms, test domains, and model selection criteria by simply invoking python sweep.py. After adapting the file sweep.py to the computing infrastructure at hand, this single command automatically generates all the result tables that we report in this manuscript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Extension to UDA</head><p>By extending the method .update(minibatches, unlabeled) to accept a minibatch of unlabeled examples from the test domain, we can immediately use DOMAINBED as a framework to perform experimentation on unsupervised domain adaptation algorithms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>?</head><label></label><figDesc>Group Distributionally Robust Optimization (DRO, Sagawa et al. [2019]) performs ERM while increasing the importance of domains with larger errors. ? Inter-domain Mixup (Mixup, Xu et al. [2019], Yan et al. [2020], Wang et al. [2020]) performs ERM on linear interpolations of examples from random pairs of domains and their labels. ? Meta-Learning for Domain Generalization (MLDG, Li et al. [2018a]) leverages MAML [Finn et al., 2017] to meta-learn how to generalize across domains. ? Different variants of the popular algorithm of Ganin et al. [2016] to learn features ?(X d ) with distributions matching across domains: -Domain-Adversarial Neural Networks (DANN, Ganin et al. [2016]) employ an adversarial network to match feature distributions. -Class-conditional DANN (C-DANN, Li et al. [2018d]) is a variant of DANN matching the conditional distributions P (?(X d )|Y d = y) across domains, for all labels y. -CORAL [Sun and Saenko, 2016] matches the mean and covariance of feature distributions. -MMD [Li et al., 2018b] matches the MMD [Gretton et al., 2012] of feature distributions. ? Invariant Risk Minimization (IRM [Arjovsky et al., 2019]) learns a feature representation ?(X d )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>ERM95.6 ? 0.1 99.0 ? 0.1 98.9 ? 0.0 99.1 ? 0.1 99.0 ? 0.0 96.7 ? 0.2 IRM 95.9 ? 0.2 98.9 ? 0.0 99.0 ? 0.0 98.8 ? 0.1 98.9 ? 0.1 95.5 ? 0.3 DRO 95.9 ? 0.1 98.9 ? 0.0 99.0 ? 0.1 99.0 ? 0.0 99.0 ? 0.0 96.9 ? 0.1 Mixup 96.1 ? 0.2 99.1 ? 0.0 98.9 ? 0.0 99.0 ? 0.0 99.0 ? 0.1 96.6 ? 0.1 MLDG 95.9 ? 0.2 98.9 ? 0.1 99.0 ? 0.0 99.1 ? 0.0 99.0 ? 0.0 96.0 ? 0.2 CORAL 95.7 ? 0.2 99.0 ? 0.0 99.1 ? 0.1 99.1 ? 0.0 99.0 ? 0.0 96.7 ? 0.2 MMD 96.6 ? 0.1 98.9 ? 0.0 98.9 ? 0.1 99.1 ? 0.1 99.0 ? 0.0 96.2 ? 0.1 DANN 95.6 ? 0.3 98.9 ? 0.0 98.9 ? 0.0 99.0 ? 0.1 98.9 ? 0.0 95.9 ? 0.5 C-DANN 96.0 ? 0.5 98.8 ? 0.0 99.0 ? 0.1 99.1 ? 0.0 98.9 ? 0.1 96.5 ? 0.3 Model selection method: leave-one-domain-out cross-? 0.2 99.0 ? 0.1 99.0 ? 0.0 99.0 ? 0.1 99.0 ? 0.0 96.3 ? 0.1 IRM 95.5 ? 0.4 98.7 ? 0.2 98.7 ? 0.1 98.5 ? 0.3 98.7 ? 0.1 96.1 ? 0.1 DRO 95.5 ? 0.5 98.4 ? 0.5 99.0 ? 0.1 99.0 ? 0.0 98.8 ? 0.2 96.6 ? 0.1 Mixup 95.9 ? 0.3 98.8 ? 0.1 99.0 ? 0.0 99.0 ? 0.0 99.0 ? 0.0 96.5 ? 0.0 MLDG 95.8 ? 0.4 98.9 ? 0.1 99.0 ? 0.1 99.0 ? 0.0 98.9 ? 0.0 96.2 ? 0.1 CORAL 96.2 ? 0.1 98.9 ? 0.1 99.1 ? 0.0 99.0 ? 0.1 98.7 ? 0.2 96.5 ? 0.2 MMD 96.5 ? 0.2 98.9 ? 0.0 98.8 ? 0.2 99.0 ? 0.1 98.7 ? 0.1 96.4 ? 0.1 DANN 85.5 ? 4.7 78.1 ? 16.5 98.1 ? 0.6 98.7 ? 0.0 93.8 ? 1.8 95.9 ? 0.7 C-DANN 73.7 ? 0.0 98.7 ? 0.0 98.7 ? 0.1 97.0 ? 0.0 98.3 ? 0.4 94.6 ? 1.2 Model selection method: test-domain validation set (oracle) 0.2 98.8 ? 0.1 98.8 ? 0.1 99.0 ? 0.0 99.0 ? 0.0 96.8 ? 0.1 IRM 96.0 ? 0.2 98.9 ? 0.0 99.0 ? 0.0 98.8 ? 0.1 98.9 ? 0.1 95.7 ? 0.3 DRO 96.2 ? 0.1 98.9 ? 0.0 99.0 ? 0.1 98.7 ? 0.1 99.1 ? 0.0 96.8 ? 0.1 Mixup 95.8 ? 0.3 98.9 ? 0.1 99.0 ? 0.1 99.0 ? 0.1 98.9 ? 0.1 96.5 ? 0.1 MLDG 96.2 ? 0.1 99.0 ? 0.0 99.0 ? 0.1 98.9 ? 0.1 99.0 ? 0.1 96.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>State-of-the-art domain generalization for typical datasets and their domains. Our implementation of Empirical Risk Minimization (ERM) outperforms previous literature.</figDesc><table><row><cell>Dataset / algorithm</cell><cell></cell><cell cols="6">Out-of-distribution accuracy (by domain)</cell></row><row><cell>Rotated MNIST</cell><cell>0 ?</cell><cell>15 ?</cell><cell>30 ?</cell><cell>45 ?</cell><cell>60 ?</cell><cell cols="2">75 ? Average</cell></row><row><cell>Ilse et al. [2019]</cell><cell cols="6">93.5 99.3 99.1 99.2 99.3 93.0</cell><cell>97.2</cell></row><row><cell>Our ERM</cell><cell cols="6">95.6 99.0 98.9 99.1 99.0 96.7</cell><cell>98.0</cell></row><row><cell>PACS</cell><cell>A</cell><cell>C</cell><cell>P</cell><cell>S</cell><cell></cell><cell></cell><cell>Average</cell></row><row><cell>Asadi et al. [2019]</cell><cell cols="4">83.0 79.4 96.8 78.6</cell><cell></cell><cell></cell><cell>84.5</cell></row><row><cell>Our ERM</cell><cell cols="4">88.1 78.0 97.8 79.1</cell><cell></cell><cell></cell><cell>85.7</cell></row><row><cell>VLCS</cell><cell>C</cell><cell>L</cell><cell>S</cell><cell>V</cell><cell></cell><cell></cell><cell>Average</cell></row><row><cell cols="5">Albuquerque et al. [2019] 95.5 67.6 69.4 71.1</cell><cell></cell><cell></cell><cell>75.9</cell></row><row><cell>Our ERM</cell><cell cols="4">97.6 63.3 72.2 76.4</cell><cell></cell><cell></cell><cell>77.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Learning setups. L d and U d denote the labeled and unlabeled distributions from domain d.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Datasets included in DOMAINBED. For each dataset, we pick a single class and show illustrative images from each domain.</figDesc><table><row><cell>Dataset</cell><cell>Domains</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>+90%</cell><cell>+80%</cell><cell>-90%</cell><cell></cell><cell></cell></row><row><cell>Colored MNIST</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">(degree of correlation between color and label)</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0 ?</cell><cell>15 ?</cell><cell>30 ?</cell><cell>45 ?</cell><cell>60 ?</cell><cell>75 ?</cell></row><row><cell>Rotated MNIST</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Caltech101</cell><cell>LabelMe</cell><cell>SUN09</cell><cell>VOC2007</cell><cell></cell></row><row><cell>VLCS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Art</cell><cell>Cartoon</cell><cell>Photo</cell><cell>Sketch</cell><cell></cell></row><row><cell>PACS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Art</cell><cell>Clipart</cell><cell>Product</cell><cell>Photo</cell><cell></cell></row><row><cell>Office-Home</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>L100</cell><cell>L38</cell><cell>L43</cell><cell>L46</cell><cell></cell></row><row><cell>Terra Incognita</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">(camera trap location)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Clipart</cell><cell>Infographic</cell><cell>Painting</cell><cell>QuickDraw</cell><cell>Photo</cell><cell>Sketch</cell></row><row><cell>DomainNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">4 DOMAINBED: A PyTorch testbed for domain generalization</cell></row><row><cell cols="7">At the heart of our large scale experimentation is DOMAINBED, a PyTorch [Paszke et al., 2019]</cell></row><row><cell cols="7">testbed to streamline reproducible and rigorous research in domain generalization:</cell></row><row><cell cols="7">https://github.com/facebookresearch/DomainBed (coming soon)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Average out-of-distribution test accuracies for all algorithms, datasets and model selection criteria included in the initial release of DOMAINBED. These experiments compare nine popular domain generalization algorithms in the exact same conditions, showing the state-of-the-art performance of ERM. For a comparison against the numbers reported for thirty other algorithms in the previous literature, we refer the reader to Appendix A.5.</figDesc><table><row><cell></cell><cell cols="4">Model selection method: training domain validation set</cell><cell></cell></row><row><cell cols="2">Algorithm CMNIST RMNIST</cell><cell>VLCS</cell><cell>PACS</cell><cell cols="3">Office-Home TerraInc DomainNet Avg</cell></row><row><cell>ERM</cell><cell cols="3">52.0 ? 0.1 98.0 ? 0.0 77.4 ? 0.3 85.7 ? 0.5</cell><cell>67.5 ? 0.5</cell><cell>47.2 ? 0.4</cell><cell>41.2 ? 0.2</cell><cell>67.0</cell></row><row><cell>IRM</cell><cell cols="3">51.8 ? 0.1 97.9 ? 0.0 78.1 ? 0.0 84.4 ? 1.1</cell><cell>66.6 ? 1.0</cell><cell>47.9 ? 0.7</cell><cell>35.7 ? 1.9</cell><cell>66.0</cell></row><row><cell>DRO</cell><cell cols="3">52.0 ? 0.1 98.1 ? 0.0 77.2 ? 0.6 84.1 ? 0.4</cell><cell>66.9 ? 0.3</cell><cell>47.0 ? 0.3</cell><cell>33.7 ? 0.2</cell><cell>65.5</cell></row><row><cell>Mixup</cell><cell cols="3">51.9 ? 0.1 98.1 ? 0.0 77.7 ? 0.4 84.3 ? 0.5</cell><cell>69.0 ? 0.1</cell><cell>48.9 ? 0.8</cell><cell>39.6 ? 0.1</cell><cell>67.1</cell></row><row><cell>MLDG</cell><cell cols="3">51.6 ? 0.1 98.0 ? 0.0 77.1 ? 0.4 84.8 ? 0.6</cell><cell>68.2 ? 0.1</cell><cell>46.1 ? 0.8</cell><cell>41.8 ? 0.4</cell><cell>66.8</cell></row><row><cell>CORAL</cell><cell cols="3">51.7 ? 0.1 98.1 ? 0.1 77.7 ? 0.5 86.0 ? 0.2</cell><cell>68.6 ? 0.4</cell><cell>46.4 ? 0.8</cell><cell>41.8 ? 0.2</cell><cell>67.2</cell></row><row><cell>MMD</cell><cell cols="3">51.8 ? 0.1 98.1 ? 0.0 76.7 ? 0.9 85.0 ? 0.2</cell><cell>67.7 ? 0.1</cell><cell>49.3 ? 1.4</cell><cell>39.4 ? 0.8</cell><cell>66.8</cell></row><row><cell>DANN</cell><cell cols="3">51.5 ? 0.3 97.9 ? 0.1 78.7 ? 0.3 84.6 ? 1.1</cell><cell>65.4 ? 0.6</cell><cell>48.4 ? 0.5</cell><cell>38.4 ? 0.0</cell><cell>66.4</cell></row><row><cell>C-DANN</cell><cell cols="3">51.9 ? 0.1 98.0 ? 0.0 78.2 ? 0.4 82.8 ? 1.5</cell><cell>65.6 ? 0.5</cell><cell>47.6 ? 0.8</cell><cell>38.9 ? 0.1</cell><cell>66.1</cell></row><row><cell></cell><cell cols="5">Model selection method: Leave-one-domain-out cross-validation</cell></row><row><cell cols="2">Algorithm CMNIST RMNIST</cell><cell>VLCS</cell><cell>PACS</cell><cell cols="3">Office-Home TerraInc DomainNet Avg</cell></row><row><cell>ERM</cell><cell cols="3">34.2 ? 1.2 98.0 ? 0.0 76.8 ? 1.0 83.3 ? 0.6</cell><cell>67.3 ? 0.3</cell><cell>46.2 ? 0.2</cell><cell>40.8 ? 0.2</cell><cell>63.8</cell></row><row><cell>IRM</cell><cell cols="3">36.3 ? 0.4 97.7 ? 0.1 77.2 ? 0.3 82.9 ? 0.6</cell><cell>66.7 ? 0.7</cell><cell>44.0 ? 0.7</cell><cell>35.3 ? 1.5</cell><cell>62.9</cell></row><row><cell>DRO</cell><cell cols="3">32.2 ? 3.7 97.9 ? 0.1 77.5 ? 0.1 83.1 ? 0.6</cell><cell>67.1 ? 0.3</cell><cell>42.5 ? 0.2</cell><cell>32.8 ? 0.2</cell><cell>61.8</cell></row><row><cell>Mixup</cell><cell cols="3">31.2 ? 2.1 98.1 ? 0.1 78.6 ? 0.2 83.7 ? 0.9</cell><cell>68.2 ? 0.3</cell><cell>46.1 ? 1.6</cell><cell>39.4 ? 0.3</cell><cell>63.6</cell></row><row><cell>MLDG</cell><cell cols="3">36.9 ? 0.2 98.0 ? 0.1 77.1 ? 0.6 82.4 ? 0.7</cell><cell>67.6 ? 0.3</cell><cell>45.8 ? 1.2</cell><cell>42.1 ? 0.1</cell><cell>64.2</cell></row><row><cell>CORAL</cell><cell cols="3">29.9 ? 2.5 98.1 ? 0.1 77.0 ? 0.5 83.6 ? 0.6</cell><cell>68.6 ? 0.2</cell><cell>48.1 ? 1.3</cell><cell>41.9 ? 0.2</cell><cell>63.9</cell></row><row><cell>MMD</cell><cell cols="3">42.6 ? 3.0 98.1 ? 0.1 76.7 ? 0.9 82.8 ? 0.3</cell><cell>67.1 ? 0.5</cell><cell>46.3 ? 0.5</cell><cell>39.3 ? 0.9</cell><cell>64.7</cell></row><row><cell>DANN</cell><cell cols="3">29.0 ? 7.7 89.1 ? 5.5 77.7 ? 0.3 84.0 ? 0.5</cell><cell>65.5 ? 0.1</cell><cell>45.7 ? 0.8</cell><cell>37.5 ? 0.2</cell><cell>61.2</cell></row><row><cell>C-DANN</cell><cell cols="3">31.1 ? 8.5 96.3 ? 1.0 74.0 ? 1.0 81.7 ? 1.4</cell><cell>64.7 ? 0.4</cell><cell>40.6 ? 1.8</cell><cell>38.7 ? 0.2</cell><cell>61.1</cell></row><row><cell></cell><cell cols="5">Model selection method: Test-domain validation set (oracle)</cell></row><row><cell cols="2">Algorithm CMNIST RMNIST</cell><cell>VLCS</cell><cell>PACS</cell><cell cols="3">Office-Home TerraInc DomainNet Avg</cell></row><row><cell>ERM</cell><cell cols="3">58.5 ? 0.3 98.1 ? 0.1 77.8 ? 0.3 87.1 ? 0.3</cell><cell>67.1 ? 0.5</cell><cell>52.7 ? 0.2</cell><cell>41.6 ? 0.1</cell><cell>68.9</cell></row><row><cell>IRM</cell><cell cols="3">70.2 ? 0.2 97.9 ? 0.0 77.1 ? 0.2 84.6 ? 0.5</cell><cell>67.2 ? 0.8</cell><cell>50.9 ? 0.4</cell><cell>36.0 ? 1.6</cell><cell>69.2</cell></row><row><cell>DRO</cell><cell cols="3">61.2 ? 0.6 98.1 ? 0.0 77.4 ? 0.6 87.2 ? 0.4</cell><cell>67.7 ? 0.4</cell><cell>53.1 ? 0.5</cell><cell>34.0 ? 0.1</cell><cell>68.4</cell></row><row><cell>Mixup</cell><cell cols="3">58.4 ? 0.2 98.0 ? 0.0 78.7 ? 0.4 86.4 ? 0.2</cell><cell>68.5 ? 0.5</cell><cell>52.9 ? 0.3</cell><cell>40.3 ? 0.3</cell><cell>69.0</cell></row><row><cell>MLDG</cell><cell cols="3">58.4 ? 0.2 98.0 ? 0.1 77.8 ? 0.4 86.8 ? 0.2</cell><cell>67.4 ? 0.2</cell><cell>52.4 ? 0.3</cell><cell>42.5 ? 0.1</cell><cell>69.1</cell></row><row><cell>CORAL</cell><cell cols="3">57.6 ? 0.5 98.2 ? 0.0 77.8 ? 0.1 86.9 ? 0.2</cell><cell>68.6 ? 0.4</cell><cell>52.6 ? 0.6</cell><cell>42.1 ? 0.1</cell><cell>69.1</cell></row><row><cell>MMD</cell><cell cols="3">63.4 ? 0.7 97.9 ? 0.1 78.0 ? 0.4 87.1 ? 0.5</cell><cell>67.0 ? 0.2</cell><cell>52.7 ? 0.2</cell><cell>39.8 ? 0.7</cell><cell>69.4</cell></row><row><cell>DANN</cell><cell cols="3">58.3 ? 0.2 97.9 ? 0.0 80.1 ? 0.6 85.4 ? 0.7</cell><cell>65.6 ? 0.3</cell><cell>51.6 ? 0.6</cell><cell>38.3 ? 0.1</cell><cell>68.2</cell></row><row><cell>C-DANN</cell><cell cols="3">62.0 ? 1.1 97.8 ? 0.1 80.2 ? 0.1 85.7 ? 0.3</cell><cell>65.6 ? 0.3</cell><cell>51.0 ? 1.0</cell><cell>38.9 ? 0.1</cell><cell>68.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Are those features expressible by a neural network? Even in the presence of correct model selection, is the out-of-distribution performance of modern ERM implementations as good as it gets? Or is it simply as bad as every other alternative? How can we establish upper-bounds on what performance is achievable out-of-distribution via domain generalization techniques?</figDesc><table /><note>Given a particular domain generalization problem, what sort of data augmentation pipelines should we implement? Is this as good as it gets? We question whether domain generalization is expected in the considered datasets. Why do we assume a neural network should be able to classify cartoons, given only photore- alistic training data? In the case of Rotated MNIST, do truly rotation-invariant features discriminative of the digit class exist?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Michael A Alcorn, QiLi, Zhitao Gong, Chengfei Wang, Long Mai, Wei-Shinn Ku, and Anh Nguyen. Ya Li, Mingming Gong, Xinmei Tian, Tongliang Liu, and Dacheng  Tao. Domain generalization via conditional invariant representations. AAAI, 2018c. Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao. Deep domain generalization via conditional invariant adversarial networks. ECCV, 2018d. A decade of literature on domain generalization In this section, we provide an exhaustive literature review on a decade of domain generalization research. The following classifies domain generalization algorithms according into four strategies to learn invariant predictors: learning invariant features, sharing parameters, meta-learning, or performing data augmentation. Motiian et al. [2017] learn a feature representation such that (i) examples from different domains but the same class are close, (ii) examples from different domains and classes are far, and (iii) training examples can be correctly classified. Ilse et al. [2019] train a variational autoencoder [Kingma and Welling, 2014] where the bottleneck representation factorizes knowledge about domain, class label, and residual variations in the input space. Fang et al. [2013] learn a structural SVM metric such that the neighborhood of each example contains examples from the same category and all training domains. The algorithms of Sun and Saenko [2016], Sun et al. [2016], Rahman et al. [2019a] match the feature covariance (second order statistics) across training domains at some level of representation. The algorithms of Ghifary et al. [2016], Hu et al. [2019] use kernel-based multivariate component analysis to minimize the mismatch between training domains while maximizing class separability. Although popular, learning domain-invariant features has received some criticism [Zhao et al., 2019, Johansson et al., 2019]. Some alternatives exist, as we review next. Peters et al. [2016], Rojas-Carulla et al. [2018] considered that one should search for features that lead to the same optimal classifier across training domains. In their pioneering work, Peters et al. [2016] linked this type of invariance to the causal structure of data, and provided a basic algorithm to learn invariant linear models, based on feature selection. Arjovsky et al. [2019] extend the previous to general gradient-based models, including neural networks, in their Invariant Risk Minimization (IRM) principle. Teney et al. [2020] build on IRM to learn a feature transformation that minimizes the relative variance of classifier weights across training datasets. The authors apply their method to reduce the learning of spurious correlations in Visual Question Answering (VQA) tasks. Ahuja et al. [2020] analyze IRM under a game-theoretic perspective to develop an alternative algorithm. Krueger et al. [2020] propose an approximation to the IRM problem consisting in reducing the variance of error averages across domains. Bouvier et al. [2019] attack the same problem as IRM by re-weighting data samples. Since the distributional identity of test instances is unknown, these embeddings are estimated using single test examples at test time. See Blanchard et al. [2017], Deshmukh et al. [2019] for theoretical results on this family of algorithms. Khosla et al. [2012] learn one max-margin linear classifier w d = w + ? d per domain d, from which they distill their final, invariant predictor w. Ghifary et al. [2015] use a multitask autoencoder to learn invariances across domains. To achieve this, the authors assume that each training dataset contains the same examples; for instance, photographs about the same objects under different views. Mancini et al. [2018b] train a deep neural network with one set of dedicated batch-normalization layers [Ioffe and Szegedy, 2015] per training dataset. Then, a softmax domain classifier predicts how to linearly-combine the batch-normalization layers at test time. Similarly, Mancini et al. [2018a] learn a softmax domain classifier used to linearly-combine domain-specific predictors at test time. DInnocente and Caputo [2018] explore more sophisticated ways of aggregating domain-specific predictors. Li et al. [2017] extends Khosla et al. [2012] to deep neural networks by extending each of their parameter tensors with one additional dimension, indexed by the training domains, and set to a neutral value to predict domain-agnostic test examples. Ding and Fu [2017] implement parameter-tying and low-rank reconstruction losses to learn a predictor that relies on common knowledge across training domains. Hu et al. [2016], Sagawa et al. [2019] weight the importance of the minibatches of the training distributions proportional to their error.</figDesc><table><row><cell>Yiying Li, Yongxin Yang, Wei Zhou, and Timothy M Hospedales. Feature-critic networks for</cell></row><row><cell>heterogeneous domain generalization. arXiv, 2019b.</cell></row><row><cell>Massimiliano Mancini, Samuel Rota Bul?, Barbara Caputo, and Elisa Ricci. Best sources forward: A.1 Learning invariant features</cell></row><row><cell>domain generalization through source-specific nets. ICIP, 2018a. Muandet et al. [2013] use kernel methods to find a feature transformation that (i) minimizes the</cell></row><row><cell>Massimiliano Mancini, Samuel Rota Bulo, Barbara Caputo, and Elisa Ricci. Robust place categoriza-distance between transformed feature distributions across domains, and (ii) does not destroy any</cell></row><row><cell>tion with deep domain generalization. IEEE Robotics and Automation Letters, 2018b. of the information between the original features and the targets. In their pioneering work, Ganin</cell></row><row><cell>et al. [2016] propose Domain Adversarial Neural Networks (DANN), a domain adaptation technique</cell></row><row><cell>Toshihiko Matsuura and Tatsuya Harada. Domain generalization using a mixture of multiple latent which uses generative adversarial networks (GANs, Goodfellow et al. [2014]), to learn a feature</cell></row><row><cell>domains. arXiv, 2019. representation that matches across training domains. Akuzawa et al. [2019] extend DANN by</cell></row><row><cell>considering cases where there exists an statistical dependence between the domain and the class label</cell></row><row><cell>variables. Albuquerque et al. [2019] extend DANN by considering one-versus-all adversaries that try</cell></row><row><cell>to predict to</cell></row></table><note>Awhich training domain does each of the examples belong to. Li et al. [2018b] employ GANs and the maximum mean discrepancy criteria [Gretton et al., 2012] to align feature distributions across domains. Matsuura and Harada [2019] leverages clustering techniques to learn domain- invariant features even when the separation between training domains is not given. Li et al. [2018c,d] learns a feature transformation ? such that the conditional distributions P (?(X d ) | Y d = y) match for all training domains d and label values y. Shankar et al. [2018] use a domain classifier to construct adversarial examples for a label classifier, and use a label classifier to construct adversarial examples for the domain classifier. This results in a label classifier with better domain generalization. Li et al. [2019a] train a robust feature extractor and classifier. The robustness comes from (i) asking the feature extractor to produce features such that a classifier trained on domain d can classify instances for domain d = d, and (ii) asking the classifier to predict labels on domain d using features produced by a feature extractor trained on domain d = d. Li et al. [2020] adopt a lifelong learning strategy to attack the problem of domain generalization.A.2 Sharing parameters Blanchard et al. [2011] build classifiers f (x d , ? d ), where ? d is a kernel mean embedding [Muandet et al., 2017] that summarizes the dataset associated to the example x d .A.3 Meta-learning Li et al. [2018a] employ Model-Agnostic Meta-Learning, or MAML [Finn et al., 2017], to build a predictor that learns how to adapt fast between training domains. Dou et al. [2019] use a similar MAML strategy, together with two regularizers that encourage features from different domains to respect inter-class relationships, and be compactly clustered by class labels. Li et al. [2019b] extend the MAML meta-learning strategy to instances of domain generalization where the categories vary from domain to domain. Balaji et al. [2018] use MAML to meta-learn a regularizer encouraging the model trained on one domain to perform well on another domain.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Previous state-of-the-art in the literature of domain generalization.</figDesc><table><row><cell>Benchmark</cell><cell></cell><cell></cell><cell cols="3">Accuracy (by domain)</cell><cell></cell><cell>Algorithm</cell></row><row><cell></cell><cell>0</cell><cell>15</cell><cell>30</cell><cell>45</cell><cell>60</cell><cell>75</cell><cell>Average</cell></row><row><cell></cell><cell cols="6">82.50 96.30 93.40 78.60 94.20 80.50</cell><cell>87.58</cell><cell>D-MTAE [Ghifary et al., 2015]</cell></row><row><cell></cell><cell cols="6">84.60 95.60 94.60 82.90 94.80 82.10</cell><cell>89.10</cell><cell>CCSA [Motiian et al., 2017]</cell></row><row><cell></cell><cell cols="6">83.70 96.90 95.70 85.20 95.90 81.20</cell><cell>89.80</cell><cell>MMD-AAE [Li et al., 2018b]</cell></row><row><cell>Rotated MNIST</cell><cell cols="6">85.60 95.00 95.60 95.50 95.90 84.30 88.80 97.60 97.50 97.80 97.60 91.90 88.30 98.60 98.00 97.70 97.70 91.40</cell><cell>92.00 95.20 95.28</cell><cell>BestSources [Mancini et al., 2018a] ADAGE [Carlucci et al., 2019b] CrossGrad [Shankar et al., 2018]</cell></row><row><cell></cell><cell cols="6">90.10 98.90 98.90 98.80 98.30 90.00</cell><cell>95.80</cell><cell>HEX [Wang et al., 2019]</cell></row><row><cell></cell><cell cols="6">89.23 99.68 99.20 99.24 99.53 91.44</cell><cell>96.39</cell><cell>FeatureCritic [Li et al., 2019b]</cell></row><row><cell></cell><cell cols="6">93.50 99.30 99.10 99.20 99.30 93.00</cell><cell>97.20</cell><cell>DIVA [Ilse et al., 2019]</cell></row><row><cell></cell><cell>C</cell><cell>L</cell><cell>S</cell><cell>V</cell><cell></cell><cell></cell><cell>Average</cell></row><row><cell></cell><cell cols="4">88.92 59.60 59.20 64.36</cell><cell></cell><cell></cell></row><row><cell>VLCS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>? 0.3 19.2 ? 0.4 46.3 ? 0.5 12.8 ? 0.0 60.6 ? 0.5 49.7 ? 0.D Model architectures, hyperparameter spaces, and other training detailsIn this section we describe the model architectures and hyperparameter search spaces used in our experiments.</figDesc><table><row><cell cols="2">B.3 VLCS B.4 PACS B.5 Office-Home B.6 TerraIncognita B.7 DomainNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">Model selection method: training domain validation set Model selection method: training domain validation set Model selection method: training domain validation set Model selection method: training domain validation set Model selection method: training domain validation set</cell><cell></cell></row><row><cell>Algorithm</cell><cell>Algorithm Algorithm Algorithm Algorithm clipart</cell><cell>C A A L100 infograph</cell><cell>L C C L38 painting</cell><cell>S P P L43 quickdraw</cell><cell>V S R L46 real</cell><cell>sketch</cell></row><row><cell>ERM</cell><cell cols="6">ERM ERM ERM ERM 58.4 8 97.6 ? 1.0 63.3 ? 0.9 72.2 ? 0.5 76.4 ? 1.5 88.1 ? 0.1 77.9 ? 1.3 97.8 ? 0.0 79.1 ? 0.9 62.7 ? 1.1 53.4 ? 0.6 76.5 ? 0.4 77.3 ? 0.3 50.8 ? 1.8 42.5 ? 0.7 57.9 ? 0.6 37.6 ? 1.2</cell></row><row><cell>IRM</cell><cell cols="6">IRM IRM IRM IRM 51.0 ? 3.3 16.8 ? 1.0 38.8 ? 2.1 11.8 ? 0.5 51.5 ? 3.6 44.2 ? 3.1 97.6 ? 0.3 65.0 ? 0.9 72.9 ? 0.5 76.9 ? 1.3 85.0 ? 1.6 77.6 ? 0.9 96.7 ? 0.3 78.5 ? 2.6 61.8 ? 1.0 52.3 ? 1.0 75.2 ? 0.8 77.2 ? 1.1 52.2 ? 3.1 43.4 ? 2.4 57.7 ? 1.5 38.1 ? 0.7</cell></row><row><cell>DRO</cell><cell cols="6">DRO DRO DRO DRO 47.8 ? 0.6 17.1 ? 0.6 36.6 ? 0.7 97.7 ? 0.4 62.5 ? 1.1 70.1 ? 0.7 78.4 ? 0.9 86.4 ? 0.3 79.9 ? 0.8 98.0 ? 0.3 72.1 ? 0.7 61.6 ? 0.7 52.9 ? 0.2 75.5 ? 0.5 77.7 ? 0.2 47.2 ? 1.6 40.1 ? 1.6 57.6 ? 0.9 43.0 ? 0.7 8.8 ? 0.4 51.5 ? 0.6 40.7 ? 0.3</cell></row><row><cell>Mixup</cell><cell cols="6">Mixup Mixup Mixup Mixup 55.3 ? 0.3 18.2 ? 0.3 45.0 ? 1.0 12.5 ? 0.3 57.1 ? 1.2 49.2 ? 0.3 97.9 ? 0.3 64.5 ? 0.6 71.5 ? 0.9 76.9 ? 1.3 86.5 ? 0.4 76.6 ? 1.5 97.7 ? 0.2 76.5 ? 1.2 64.7 ? 0.7 54.7 ? 0.6 77.3 ? 0.3 79.2 ? 0.3 60.6 ? 1.3 41.1 ? 1.8 58.5 ? 0.8 35.2 ? 1.1</cell></row><row><cell>MLDG</cell><cell cols="6">MLDG MLDG MLDG MLDG 59.5 ? 0.0 19.8 ? 0.4 48.3 ? 0.5 13.0 ? 0.4 59.5 ? 1.0 50.4 ? 0.7 98.1 ? 0.3 63.0 ? 0.9 73.5 ? 0.6 73.7 ? 0.3 89.1 ? 0.9 78.8 ? 0.7 97.0 ? 0.9 74.4 ? 2.0 63.7 ? 0.3 54.5 ? 0.6 75.9 ? 0.4 78.6 ? 0.1 48.5 ? 3.3 42.8 ? 0.4 56.8 ? 0.9 36.3 ? 0.5</cell></row><row><cell>CORAL</cell><cell cols="6">CORAL CORAL CORAL CORAL 58.7 ? 0.2 20.9 ? 0.3 47.3 ? 0.3 13.6 ? 0.3 60.2 ? 0.3 50.2 ? 0.6 98.8 ? 0.1 64.6 ? 0.8 71.7 ? 1.4 75.8 ? 0.4 87.7 ? 0.6 79.2 ? 1.1 97.6 ? 0.0 79.4 ? 0.7 64.4 ? 0.3 55.3 ? 0.5 76.7 ? 0.5 77.9 ? 0.5 48.6 ? 0.9 42.2 ? 3.5 55.9 ? 0.6 38.7 ? 0.7</cell></row><row><cell>MMD</cell><cell cols="6">MMD MMD MMD MMD 54.6 ? 1.7 19.3 ? 0.3 44.9 ? 1.1 11.4 ? 0.5 59.5 ? 0.2 47.0 ? 1.6 97.1 ? 0.4 63.4 ? 0.7 71.4 ? 0.8 74.9 ? 2.5 84.5 ? 0.6 79.7 ? 0.7 97.5 ? 0.4 78.1 ? 1.3 63.0 ? 0.1 53.7 ? 0.9 76.1 ? 0.3 78.1 ? 0.5 52.2 ? 5.8 47.0 ? 0.6 57.8 ? 1.3 40.3 ? 0.5</cell></row><row><cell>DANN</cell><cell cols="6">DANN DANN DANN DANN 53.8 ? 0.7 17.8 ? 0.3 43.5 ? 0.3 11.9 ? 0.5 56.4 ? 0.3 46.7 ? 0.5 98.5 ? 0.2 64.9 ? 1.1 73.1 ? 0.7 78.3 ? 0.3 85.9 ? 0.5 79.9 ? 1.4 97.6 ? 0.2 75.2 ? 2.8 59.3 ? 1.1 51.7 ? 0.2 74.1 ? 0.8 76.6 ? 0.6 49.0 ? 3.8 46.3 ? 1.7 57.6 ? 0.8 40.6 ? 1.7</cell></row><row><cell>C-DANN</cell><cell cols="6">C-DANN C-DANN C-DANN C-DANN 53.4 ? 0.4 18.3 ? 0.7 44.8 ? 0.3 12.9 ? 0.2 57.5 ? 0.4 46.7 ? 0.2 97.5 ? 0.1 65.2 ? 0.4 73.4 ? 1.1 76.9 ? 0.2 84.0 ? 0.9 78.5 ? 1.5 97.0 ? 0.4 71.8 ? 3.9 61.0 ? 1.4 51.1 ? 0.7 74.1 ? 0.3 76.0 ? 0.7 49.5 ? 3.8 44.8 ? 1.0 57.3 ? 1.1 38.8 ? 1.7</cell></row><row><cell></cell><cell cols="5">Model selection method: leave-one-domain-out cross-validation Model selection method: leave-one-domain-out cross-validation Model selection method: leave-one-domain-out cross-validation Model selection method: leave-one-domain-out cross-validation Model selection method: leave-one-domain-out cross-validation</cell><cell></cell></row><row><cell>Algorithm</cell><cell>Algorithm Algorithm Algorithm Algorithm clipart</cell><cell>C A A L100 infograph</cell><cell>L C C L38 painting</cell><cell>S P P L43 quickdraw</cell><cell>V S R L46 real</cell><cell>sketch</cell></row><row><cell>ERM</cell><cell cols="6">ERM ERM ERM ERM 56.0 ? 1.1 19.6 ? 0.2 47.3 ? 0.3 12.5 ? 0.3 60.5 ? 0.5 49.1 ? 0.2 97.8 ? 0.0 63.3 ? 1.6 70.3 ? 1.6 75.9 ? 1.4 83.9 ? 1.6 78.6 ? 2.0 97.3 ? 0.1 73.5 ? 1.1 62.3 ? 0.5 54.1 ? 0.5 75.3 ? 0.2 77.4 ? 0.5 47.5 ? 0.2 43.8 ? 0.2 55.4 ? 1.3 38.3 ? 1.3</cell></row><row><cell>IRM</cell><cell cols="6">IRM IRM IRM IRM 49.0 ? 2.4 16.7 ? 0.9 38.8 ? 2.1 10.2 ? 0.6 53.2 ? 2.1 43.7 ? 2.1 98.9 ? 0.0 63.6 ? 0.8 71.1 ? 2.2 75.4 ? 1.5 82.5 ? 2.6 78.0 ? 0.3 96.7 ? 1.1 74.4 ? 1.3 62.1 ? 0.9 51.4 ? 0.6 75.5 ? 0.7 77.6 ? 0.8 44.2 ? 2.7 41.3 ? 0.6 54.3 ? 2.0 36.0 ? 1.7</cell></row><row><cell>DRO</cell><cell cols="4">DRO DRO DRO DRO 47.3 ? 0.7 16.8 ? 0.3 35.2 ? 0.1 99.2 ? 0.2 62.0 ? 1.6 73.4 ? 0.8 87.1 ? 0.3 77.6 ? 1.9 97.2 ? 0.4 62.7 ? 0.7 52.8 ? 1.0 75.4 ? 0.1 31.8 ? 0.3 43.7 ? 1.2 58.0 ? 0.7 8.8 ? 0.4</cell><cell cols="2">75.5 ? 1.0 70.7 ? 3.0 77.7 ? 0.2 36.6 ? 1.3 50.1 ? 2.3 38.9 ? 0.7</cell></row><row><cell>Mixup</cell><cell cols="6">Mixup Mixup Mixup Mixup 54.4 ? 0.6 18.1 ? 0.3 45.2 ? 0.3 12.1 ? 0.4 57.9 ? 1.1 48.6 ? 0.1 97.9 ? 0.7 65.5 ? 0.8 73.3 ? 0.8 77.8 ? 0.5 88.0 ? 0.5 74.3 ? 4.0 97.2 ? 0.2 75.3 ? 0.2 63.8 ? 0.4 52.9 ? 0.4 77.3 ? 0.4 78.7 ? 0.4 49.6 ? 4.8 44.4 ? 0.9 55.0 ? 1.4 35.2 ? 1.9</cell></row><row><cell>MLDG</cell><cell cols="6">MLDG MLDG MLDG MLDG 58.7 ? 0.4 20.3 ? 0.1 48.8 ? 0.1 13.0 ? 0.4 61.2 ? 0.2 50.3 ? 0.2 96.3 ? 1.1 65.1 ? 0.9 71.9 ? 1.5 75.0 ? 0.5 85.8 ? 0.7 77.3 ? 0.5 96.8 ? 0.5 69.9 ? 3.6 62.9 ? 0.5 53.5 ? 0.7 76.0 ? 0.4 77.9 ? 0.6 50.9 ? 5.1 39.9 ? 0.9 58.0 ? 1.8 34.6 ? 1.0</cell></row><row><cell>CORAL</cell><cell cols="6">CORAL CORAL CORAL CORAL 57.9 ? 0.7 20.8 ? 0.3 47.5 ? 0.4 13.5 ? 0.3 61.0 ? 0.3 50.6 ? 0.5 97.5 ? 0.1 64.0 ? 0.2 69.7 ? 2.0 76.7 ? 0.3 86.0 ? 1.1 75.5 ? 2.3 96.2 ? 0.9 76.6 ? 2.1 64.4 ? 0.3 55.4 ? 0.1 76.2 ? 0.2 78.4 ? 0.4 51.8 ? 2.2 42.1 ? 1.1 59.6 ? 0.8 38.7 ? 2.3</cell></row><row><cell>MMD</cell><cell cols="6">MMD MMD MMD MMD 54.0 ? 2.2 19.3 ? 0.3 44.9 ? 1.1 11.4 ? 0.5 59.5 ? 0.2 47.0 ? 1.6 97.7 ? 0.4 63.1 ? 1.9 68.6 ? 1.5 77.5 ? 1.2 85.9 ? 0.5 78.1 ? 2.1 96.2 ? 1.3 71.1 ? 3.0 62.2 ? 0.3 52.7 ? 1.0 75.5 ? 0.4 78.1 ? 0.3 51.5 ? 1.7 37.4 ? 2.0 58.9 ? 0.9 37.4 ? 1.8</cell></row><row><cell>DANN</cell><cell cols="6">DANN DANN DANN DANN 53.1 ? 0.4 17.5 ? 0.6 42.8 ? 0.4 10.2 ? 0.5 56.4 ? 0.3 44.9 ? 0.9 95.3 ? 1.8 61.3 ? 1.8 74.3 ? 1.0 79.7 ? 0.9 86.7 ? 0.3 78.5 ? 0.5 97.4 ? 0.4 73.3 ? 2.3 61.1 ? 0.1 51.6 ? 0.5 73.6 ? 0.5 75.8 ? 0.3 47.2 ? 4.5 40.6 ? 0.0 55.7 ? 2.6 39.4 ? 1.3</cell></row><row><cell>C-DANN</cell><cell cols="6">C-DANN C-DANN C-DANN C-DANN 53.4 ? 0.4 18.3 ? 0.7 44.2 ? 0.5 12.9 ? 0.2 57.1 ? 0.2 46.7 ? 0.2 92.3 ? 4.2 60.3 ? 1.5 68.4 ? 2.1 74.9 ? 1.3 83.6 ? 3.8 75.9 ? 1.8 97.4 ? 0.5 70.0 ? 3.6 60.0 ? 0.4 50.2 ? 0.7 72.1 ? 1.0 76.4 ? 0.5 43.2 ? 3.5 30.9 ? 4.1 50.4 ? 4.4 37.8 ? 1.5</cell></row><row><cell></cell><cell cols="5">Model selection method: test-domain validation set (oracle) Model selection method: test-domain validation set (oracle) Model selection method: test-domain validation set (oracle) Model selection method: test-domain validation set (oracle) Model selection method: test-domain validation set (oracle)</cell><cell></cell></row><row><cell>Algorithm</cell><cell>Algorithm Algorithm Algorithm Algorithm clipart</cell><cell>C A A L100 infograph</cell><cell>L C C L38 painting</cell><cell>S P P L43 quickdraw</cell><cell>V S R L46 real</cell><cell>sketch</cell></row><row><cell>ERM</cell><cell cols="6">ERM ERM ERM ERM 58.4 ? 0.3 19.8 ? 0.2 47.3 ? 0.3 13.4 ? 0.2 60.7 ? 0.5 49.9 ? 0.7 97.7 ? 0.3 65.2 ? 0.4 73.2 ? 0.7 75.2 ? 0.4 87.8 ? 0.4 82.8 ? 0.5 97.6 ? 0.4 80.4 ? 0.6 61.2 ? 1.4 54.0 ? 0.5 75.9 ? 0.7 77.3 ? 0.4 59.9 ? 1.0 48.7 ? 0.4 58.9 ? 0.3 43.3 ? 0.9</cell></row><row><cell>IRM</cell><cell cols="6">IRM IRM IRM IRM 51.0 ? 3.3 16.7 ? 0.9 38.8 ? 2.1 11.8 ? 0.5 53.2 ? 2.1 44.7 ? 2.7 97.6 ? 0.5 64.7 ? 1.1 69.7 ? 0.5 76.6 ? 0.7 85.7 ? 1.0 79.3 ? 1.1 97.6 ? 0.4 75.9 ? 1.0 62.4 ? 0.9 53.4 ? 0.7 75.5 ? 0.8 77.7 ? 0.6 56.8 ? 2.0 46.5 ? 0.3 57.9 ? 0.6 42.4 ? 0.5</cell></row><row><cell>DRO</cell><cell cols="6">DRO DRO DRO DRO 47.8 ? 0.6 17.2 ? 0.6 36.3 ? 0.5 97.8 ? 0.0 66.4 ? 0.5 68.7 ? 1.2 76.8 ? 1.0 88.2 ? 0.7 82.4 ? 0.8 97.7 ? 0.2 80.6 ? 0.9 63.6 ? 0.5 54.4 ? 0.7 75.9 ? 0.1 77.0 ? 0.4 61.2 ? 1.2 47.5 ? 0.6 59.5 ? 0.6 44.1 ? 0.8 9.0 ? 0.2 52.8 ? 0.3 40.7 ? 0.3</cell></row><row><cell>Mixup</cell><cell cols="6">Mixup Mixup Mixup Mixup 55.8 ? 0.6 19.2 ? 0.2 46.2 ? 0.6 12.8 ? 0.2 58.7 ? 0.6 49.2 ? 0.3 98.3 ? 0.3 66.7 ? 0.5 73.3 ? 1.1 76.3 ? 0.8 87.4 ? 1.0 80.7 ? 1.0 97.9 ? 0.2 79.7 ? 1.0 65.1 ? 0.6 54.6 ? 0.7 76.8 ? 0.6 77.7 ? 0.6 65.1 ? 1.8 46.8 ? 0.6 59.5 ? 0.3 40.0 ? 1.1</cell></row><row><cell>MLDG</cell><cell cols="6">1 ? 0.2 59.3 ? 0.2 20.3 ? 0.1 48.8 ? 0.1 14.0 ? 0.3 61.2 ? 0.2 51.2 ? 0.1 MLDG 98.4 ? 0.2 65.9 ? 0.5 70.7 ? 0.8 76.1 ? 0.6 MLDG 87.1 ? 0.9 81.3 ? 1.5 97.6 ? 0.4 81.2 ? 1.0 MLDG 61.0 ? 0.9 54.3 ? 0.3 75.8 ? 0.5 78.6 ? 0.1 MLDG 58.7 ? 0.5 48.9 ? 0.7 59.5 ? 0.4 42.4 ? 0.6</cell></row><row><cell>CORAL CORAL</cell><cell cols="6">96.4 ? 0.1 99.0 ? 0.0 99.0 ? 0.1 99.0 ? 0.0 98.9 ? 0.1 96.8 ? 0.2 CORAL 98.1 ? 0.1 67.1 ? 0.8 70.1 ? 0.6 75.8 ? 0.5 CORAL 87.4 ? 0.6 82.2 ? 0.3 97.6 ? 0.1 80.2 ? 0.4 CORAL 65.0 ? 0.5 54.3 ? 0.8 76.8 ? 0.4 78.2 ? 0.2 CORAL 60.5 ? 1.0 47.6 ? 1.8 59.1 ? 0.3 43.2 ? 0.5 58.8 ? 0.1 20.8 ? 0.3 47.5 ? 0.4 13.6 ? 0.2 61.0 ? 0.3 50.8 ? 0.4</cell></row><row><cell>MMD MMD</cell><cell cols="6">95.7 ? 0.4 98.8 ? 0.0 98.9 ? 0.1 98.8 ? 0.1 99.0 ? 0.0 96.3 ? 0.2 MMD 98.1 ? 0.3 66.2 ? 0.2 70.5 ? 1.0 77.2 ? 0.6 MMD 87.6 ? 1.2 83.0 ? 0.4 97.8 ? 0.1 80.1 ? 1.0 MMD 62.4 ? 0.2 53.6 ? 0.5 75.8 ? 0.4 76.4 ? 0.3 MMD 60.0 ? 1.6 46.7 ? 0.8 60.0 ? 1.0 44.2 ? 0.4 54.6 ? 1.7 19.6 ? 0.1 44.9 ? 1.1 12.6 ? 0.1 59.7 ? 0.2 47.5 ? 1.2</cell></row><row><cell>DANN DANN</cell><cell cols="6">96.0 ? 0.1 98.8 ? 0.1 98.6 ? 0.1 98.7 ? 0.1 98.8 ? 0.1 96.4 ? 0.1 DANN 98.2 ? 0.3 67.8 ? 1.1 74.2 ? 0.7 80.1 ? 0.6 DANN 86.4 ? 1.4 80.6 ? 1.0 97.7 ? 0.2 77.1 ? 1.3 DANN 60.5 ? 0.9 51.9 ? 0.4 73.7 ? 0.4 76.4 ? 0.6 DANN 57.6 ? 1.3 48.1 ? 1.1 58.2 ? 0.5 42.7 ? 1.4 53.8 ? 0.7 17.5 ? 0.6 43.5 ? 0.3 11.8 ? 0.6 56.4 ? 0.3 46.7 ? 0.5</cell></row><row><cell>C-DANN C-DANN</cell><cell cols="6">95.8 ? 0.2 98.8 ? 0.0 98.9 ? 0.0 98.6 ? 0.1 98.8 ? 0.1 96.1 ? 0.2 C-DANN 98.9 ? 0.3 68.8 ? 0.6 73.7 ? 0.6 79.3 ? 0.6 C-DANN 87.0 ? 1.2 80.8 ? 0.9 97.4 ? 0.5 77.6 ? 0.1 C-DANN 60.0 ? 0.5 52.0 ? 0.4 74.2 ? 0.5 76.3 ? 0.4 C-DANN 56.3 ? 2.6 46.9 ? 1.5 57.8 ? 0.8 43.3 ? 0.5 53.4 ? 0.4 18.4 ? 0.6 44.7 ? 0.3 12.9 ? 0.2 57.5 ? 0.4 46.5 ? 0.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Neural network architectures used for each dataset.</figDesc><table><row><cell>Dataset</cell><cell>Architecture</cell></row><row><cell cols="2">Colored MNIST MNIST ConvNet Rotated MNIST</cell></row><row><cell>PACS</cell><cell></cell></row><row><cell>VLCS</cell><cell></cell></row><row><cell>Office-Home</cell><cell>ResNet-50</cell></row><row><cell>TerraIncognita</cell><cell></cell></row><row><cell>DomainNet</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell></cell><cell>: Details of our MNIST ConvNet archi-</cell></row><row><cell cols="2">tecture. All convolutions use 3 ? 3 kernels and</cell></row><row><cell cols="2">"same" padding.</cell></row><row><cell>#</cell><cell>Layer</cell></row><row><cell>1</cell><cell>Conv2D (in=d, out=64)</cell></row><row><cell>2</cell><cell>ReLU</cell></row><row><cell>3</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Hyperparameters, their default values and distributions for random search.We optimize all models using Adam<ref type="bibr" target="#b18">[Kingma and Ba, 2015]</ref>.E Adding new datasets and algorithms to our frameworkIn their basic form, algorithms are classes that implement a method .update(minibatches) and a method .predict(x). The update method receives a list of minibatches, one minibatch per training domain, and each minibatch containing a number of input-output pairs. For example, to implement group DRO [Sagawa et al., 2019, Algorithm 1], we simply write the following in algorithms.py:</figDesc><table><row><cell>Condition</cell><cell>Parameter</cell><cell cols="2">Default value Random distribution</cell></row><row><cell></cell><cell>learning rate</cell><cell>0.00005</cell><cell>10 Uniform(?5,?3.5)</cell></row><row><cell>ResNet</cell><cell>batch size generator learning rate</cell><cell>32 0.00005</cell><cell>2 Uniform(3,5.5) 10 Uniform(?5,?3.5)</cell></row><row><cell></cell><cell>discriminator learning rate</cell><cell>0.00005</cell><cell>10 Uniform(?5,?3.5)</cell></row><row><cell></cell><cell>learning rate</cell><cell>0.001</cell><cell>10 Uniform(?4.5,?3.5)</cell></row><row><cell>not ResNet</cell><cell>batch size generator learning rate</cell><cell>64 0.001</cell><cell>2 Uniform(3,9) 10 Uniform(?4.5,?2.5)</cell></row><row><cell></cell><cell>discriminator learning rate</cell><cell>0.001</cell><cell>10 Uniform(?4.5,?2.5)</cell></row><row><cell>MNIST</cell><cell>weight decay generator weight decay</cell><cell>0 0</cell><cell>0 0</cell></row><row><cell>not MNIST</cell><cell>weight decay generator weight decay</cell><cell>0 0</cell><cell>10 Uniform(?6,?2) 10 Uniform(?6,?2)</cell></row><row><cell></cell><cell>lambda</cell><cell>1.0</cell><cell>10 Uniform(?2,2)</cell></row><row><cell>DANN, C-DANN</cell><cell>discriminator weight decay discriminator steps gradient penalty</cell><cell>0 1 0</cell><cell>10 Uniform(?6,?2) 2 Uniform(0,3) 10 Uniform(?2,1)</cell></row><row><cell></cell><cell>adam ? 1</cell><cell>0.5</cell><cell>RandomChoice([0, 0.5])</cell></row><row><cell>IRM</cell><cell cols="2">lambda iterations of penalty annealing 500 100</cell><cell>10 Uniform(?1,5) 10 Uniform(0,4)</cell></row><row><cell>Mixup</cell><cell>alpha</cell><cell>0.2</cell><cell>10 Uniform(0,4)</cell></row><row><cell>DRO</cell><cell>eta</cell><cell>0.01</cell><cell>10 Uniform(?1,1)</cell></row><row><cell>MMD</cell><cell>gamma</cell><cell>1</cell><cell>10 Uniform(?1,1)</cell></row><row><cell>MLDG</cell><cell>beta</cell><cell>1</cell><cell>10 Uniform(?1,1)</cell></row><row><cell>all</cell><cell>dropout</cell><cell>0</cell><cell>RandomChoice([0, 0.1, 0.5])</cell></row><row><cell cols="2">D.3 Other training details</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthikeyan</forename><surname>Shanmugam</surname></persName>
		</author>
		<title level="m">Kush Varshney, and Amit Dhurandhar. Invariant risk minimization games. arXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Adversarial invariant feature learning with accuracy constraint for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kei</forename><surname>Akuzawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Iwasawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Matsuo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep learning for segmentation of brain tumors: Impact of cross-institutional training and testing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashirbani</forename><surname>Ehab A Albadawy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej A</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mazurowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical physics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Adversarial targetinvariant representation learning for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Isabela Albuquerque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tiago</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Falk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitliagkas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Improving out-of-distribution generalization via multi-task self-supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Isabela Albuquerque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Felix A Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brendel</surname></persName>
		</author>
		<title level="m">ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. arXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Domain generalization for object recognition with multi-task autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengjie</forename><surname>Bastiaan Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Balduzzi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scatter component analysis: A unified framework for domain adaptation and domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengjie</forename><surname>Bastiaan Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Generative adversarial nets. NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A kernel two-sample test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Googles medical AI was super accurate in a lab. real life was a different story</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><forename type="middle">D</forename><surname>Heaven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIT Technology Review</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Domain generalization via multidomain discriminant analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoubo</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laiwan</forename><surname>Chan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>UAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Does distributionally robust supervised learning give robust classifiers? arXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Issei</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Ilse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jakub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Diva</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10427</idno>
		<title level="m">Domain invariant variational autoencoders</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Fredrik D Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajesh</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ranganath</surname></persName>
		</author>
		<title level="m">Support and invertibility in domaininvariant representations. arXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Undoing the damage of dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Out-of-distribution generalization via risk extrapolation (REx)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joern-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Binas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Le Priol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The mnist database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deeper, broader and artier domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning to generalize: Metalearning for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Episodic training for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Sequential learning for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Domain generalization with adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kot</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Generalizing across domains via cross-gradient training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiv</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vihari</forename><surname>Piratla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preethi</forename><surname>Jyothi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<title level="m">Convnets and imagenet beyond accuracy: Understanding mistakes and uncovering biases. ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Deep CORAL: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Return of frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Ehsan Abbasnejad, and Anton van den Hengel. Unshuffling data for improved generalization. arxiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Unbiased look at dataset bias. CVPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Statistical learning theory wiley</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hemanth</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shayok</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sethuraman</forename><surname>Panchanathan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Towards robust cnn-based object detection through augmentation with synthetic rain variations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Volk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">Von</forename><surname>Bernuth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Hospach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Bringmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ITSC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Generalizing to unseen domains via adversarial data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riccardo</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongseok</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zexue</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zachary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
		<title level="m">Learning robust representations by projecting superficial statistics out. arXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Heterogeneous domain generalization via domain mixup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">A survey of unsupervised deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrett</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><forename type="middle">J</forename><surname>Cook</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Adversarial domain adaptation with domain mixup. arXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Improve unsupervised domain adaptation with mixup training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Shen Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanxiang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lincan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<title level="m">mixup: Beyond empirical risk minimization. ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">When unseen domain generalization is unnecessary? rethinking data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaosong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Sanford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Harmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baris</forename><surname>Turkbey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Myronenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daguang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyue</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">On learning invariant representation for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Tachet Des Combes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Deep domain-adversarial image generation for domain generalisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.06054,2020.64.06</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sca [ghifary</surname></persName>
		</author>
		<idno>92.30 62.10 59.10 67.10 65.00</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ccsa [motiian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mtssl [albuquerque</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D-Mtae [</forename><surname>Ghifary</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cidg [li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ciddg [li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mda [hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mda [ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dbadg [li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mmd-Aae [</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Fcr [</forename><surname>Epi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Jigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carlucci</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Rex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krueger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S-Mldg [</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mmld [matsuura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harada</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Masf [dou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ddec [asadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Atir [albuquerque</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dbadg [li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mtssl [albuquerque</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ciddg [li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Combo [</forename><surname>Rahman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mldg [li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hex [wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Bestsoruces</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mancini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Featurecritic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Rex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krueger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caadg [rahman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Fcr [</forename><surname>Epi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Atir [albuquerque</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Masf [dou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Jigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carlucci</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S-Mldg [</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D-Sam-? [</forename><surname>Dinnocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caputo</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ddaig [zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mmld [matsuura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harada</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Sagnets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Metareg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Balaji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ddec [asadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Combo [</forename><surname>Rahman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Jigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carlucci</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D-Sam-? [</forename><surname>Dinnocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caputo</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Sagnets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ddaig [zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Domain d ? {0.1, 0.3, 0.9} contains a disjoint set of digits colored either red or blue. The label is a noisy function of the digit and color</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arjovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">C Dataset details DOMAINBED includes downloaders and loaders for seven multi-domain image classification tasks: ? Colored MNIST</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
	<note>is a variant of the MNIST handwritten digit classification dataset [LeCun. such that color bears correlation d with the label and the digit bears correlation 0.75 with the label. This dataset contains 70, 000 examples of dimension (2, 28, 28) and 2 classes</note>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">] is a variant of MNIST where domain d ? { 0, 15, 30, 45, 60, 75 } contains digits rotated by d degrees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Rotated</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mnist [</forename><surname>Ghifary</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Our dataset contains 70, 000 examples of dimension (1, 28, 28) and 10 classes</note>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">2017] comprises four domains d ? { art, cartoons, photos, sketches }. This dataset contains 9</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Pacs [li</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>991 examples of dimension (3, 224, 224) and 7 classes</note>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Vlcs [fang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>comprises photographic domains d ? { Caltech101, LabelMe, SUN09, VOC2007 }. This dataset contains 10, 729 examples of dimension (3, 224, 224) and 5 classes</note>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>? Office-Home</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Venkateswara</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>includes domains d ? { art, clipart, product, real }. This dataset contains 15, 588 examples of dimension (3, 224, 224) and 65 classes</note>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">contains photographs of wild animals taken by camera traps at locations d ? {L100, L38, L43, L46}. Our version of this dataset contains 24</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>? Terra Incognita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beery</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>788 examples of dimension (3, 224, 224) and 10 classes</note>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">2019] has six domains d ? { clipart, infograph, painting, quickdraw, real, sketch }</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Domainnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Peng</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>This dataset contains 586, 575 examples of size (3, 224, 224) and 345 classes</note>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">For all datasets, we first pool the raw training, validation, and testing images together. For each random seed, we then instantiate random training, validation, and testing splits</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
