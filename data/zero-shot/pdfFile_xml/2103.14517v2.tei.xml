<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On the hidden treasure of dialog in video question answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Engin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria</orgName>
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<region>IRISA</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">InterDigital</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Schnitzler</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">InterDigital</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><forename type="middle">Q K</forename><surname>Duong</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">InterDigital</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Avrithis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria</orgName>
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<region>IRISA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">On the hidden treasure of dialog in video question answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Code is available at https://engindeniz.github. io/dialogsummary-videoqa</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>High-level understanding of stories in video such as movies and TV shows from raw data is extremely challenging. Modern video question answering (VideoQA) systems often use additional human-made sources like plot synopses, scripts, video descriptions or knowledge bases. In this work, we present a new approach to understand the whole story without such external sources. The secret lies in the dialog: unlike any prior work, we treat dialog as a noisy source to be converted into text description via dialog summarization, much like recent methods treat video. The input of each modality is encoded by transformers independently, and a simple fusion method combines all modalities, using soft temporal attention for localization over long inputs. Our model outperforms the state of the art on the KnowIT VQA dataset by a large margin, without using question-specific human annotation or humanmade plot summaries. It even outperforms human evaluators who have never watched any whole episode before. Code is available at https://engindeniz.github. io/dialogsummary-videoqa</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep learning has accelerated progress in vision and language tasks. Visual-semantic embeddings <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b8">9]</ref> have allowed zero-shot learning, cross-modal retrieval and generating new descriptions from embeddings. Image captioning <ref type="bibr" target="#b32">[33]</ref> and visual question answering (VQA) <ref type="bibr" target="#b1">[2]</ref> have demonstrated generation of realistic natural language description of images and a great extent of multimodal semantic understanding. The extension to video captioning <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b31">32]</ref> and video question answering (VideoQA) <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b19">20]</ref> has enabled further progress because video requires a higher level of reasoning to understand complex events <ref type="bibr" target="#b36">[37]</ref>.</p><p>VideoQA systems typically have similar architecture focusing on multimodal embeddings/description, temporal attention and localization, multimodal fusion and reasoning. While it is often hard to isolate progress in individual components, there are some clear trends. For instance, custom self-attention and memory mechanisms for fusion and rea-vised temporal localization <ref type="bibr" target="#b19">[20]</ref> biases system design towards two-stage localization?answering <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b15">16]</ref>; fixed question structure focusing on temporal localization <ref type="bibr" target="#b19">[20]</ref> often results in mere alignment of questions with subtitles and matching answers with the discovered context <ref type="bibr" target="#b13">[14]</ref>, providing little progress on the main objective, which is to study the level of understanding.</p><p>Bias can be removed by removing localization supervision and balancing questions over different aspects of comprehension, for instance visual, textual, or semantic <ref type="bibr" target="#b10">[11]</ref>. However, the requirement of external knowledge, which can be in the form of hints or even ground truth, does not leave much progress in inferring such knowledge from raw data <ref type="bibr" target="#b10">[11]</ref>. Even weakening this requirement to plain text human-generated summaries <ref type="bibr" target="#b9">[10]</ref>, still leaves a system unusable in the absence of such data.</p><p>In many cases, as illustrated in <ref type="figure">Figure 1</ref>, a question on some part of a story may require knowledge that can be recovered from dialog in other parts of the story. However, despite being textual, raw dialog is often informal and repetitive; searching over all available duration of such noisy source is error-prone and impractical. Inspired by the trend of video captioning, we go a step further and apply the same idea to dialog: We summarize raw dialog, converting it into text description for question answering.</p><p>Our finding is astounding: our dialog summary is not only a valid replacement for human-generated summary in handling questions that require knowledge on a whole story, but it outperforms them by a large margin.</p><p>Our contributions can be summarized as follows:</p><p>1. We apply dialog summarization to video question answering for the first time (Subsection 5.1). 2. Building on a modern VideoQA system, we convert all input sources into plain text description. 3. We introduce a weakly-supervised soft temporal attention mechanism for localization (Subsection 6.2). 4. We devise a very simple multimodal fusion mechanism that has no hyperparameters (Section 7). 5. We set a new state of the art on KnowIT VQA dataset <ref type="bibr" target="#b10">[11]</ref> and we beat non-expert humans for the first time, working only with raw data (Section 8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Video Question Answering Progress on video question answering has been facilitated and driven by several datasets and benchmarks. VideoQA by Tapaswi et al. <ref type="bibr" target="#b28">[29]</ref> addresses answering questions created from plot synopses using a variety of input sources, including video, subtitles, scene descriptions, scripts and the plot synopses themselves. Methods experimenting on MovieQA focus on memory networks capturing information from the whole movie by videos and subtitles <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b14">15]</ref>, scene-based memory attention networks to learn joint representations of frames and captions <ref type="bibr" target="#b16">[17]</ref>, and LSTM-based sequence encoders to learn visual-text embeddings <ref type="bibr" target="#b22">[23]</ref>. TVQA <ref type="bibr" target="#b19">[20]</ref> and TVQA+ <ref type="bibr" target="#b20">[21]</ref> address scene-based questions containing temporal localization of the answer in TV shows, using video and subtitles. The questions are structured in two parts: one specifying a temporal location in the scene and the other requesting some information from that location. This encourages working with more than one modalities. Methods experimenting on these datasets focus on temporal localization and attention <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b15">16]</ref>, captioning <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b2">3]</ref> and transformer-based pipelines capturing visual-semantic and language information <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>KnowIT VQA <ref type="bibr" target="#b10">[11]</ref> is a knowledge-based dataset, including questions related to the scene, the episode or the entire story of a TV show, as well as knowledge annotation required to address certain questions, in the form of hints. Transformer-based methods are proposed to address this task by employing knowledge annotation <ref type="bibr" target="#b10">[11]</ref> or external human-generated plot summaries <ref type="bibr" target="#b9">[10]</ref>. Our method differs in substituting human-generated knowledge by summaries automatically generated from raw dialog.</p><p>Dialog Summarization Dial2Desc dataset <ref type="bibr" target="#b24">[25]</ref> addresses generating high-level short descriptions from dialog using a transformer-based text generator. SAMSum corpus <ref type="bibr" target="#b11">[12]</ref> is a human-annotated dialog summarization dataset providing speaker information. Methods experimenting on this dataset include existing document summarization methods <ref type="bibr" target="#b11">[12]</ref>, graph neural networks integrating cross-sentence information flow <ref type="bibr" target="#b38">[39]</ref> and graph construction from utterance and commonsense knowledge <ref type="bibr" target="#b7">[8]</ref>. Since dialog differs from structured text and requires extraction of the conversation structure, recent work focuses on representing the dialog from different views by sequence to sequence models <ref type="bibr" target="#b3">[4]</ref>. We follow this approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Overview</head><p>We address knowledge-based video question answering on TV shows. Each episode is split in scenes. For each scene, we are given the video (frames) and dialog (speaker names followed by subtitle text) and a number of multiplechoice questions. Certain questions require high-level understanding of the whole episode or show. Garcia et al. <ref type="bibr" target="#b9">[10]</ref> rely on human-generated plot summaries (or plot for short), which we use only for comparison. Our objective is to extract the required knowledge from raw data.</p><p>As shown in <ref type="figure">Figure 2</ref>, we first convert inputs into plain text description, including both video (by visual recognition) and dialog (by summarization) (Section 5). A number of separate streams then map text to embeddings, at the level of both scene (video and scene dialog summary) and episode (episode dialog summary and plot). The ques-What did the guys name their robot?</p><formula xml:id="formula_0">A) Killer Robot B) Terminator C) Monte D) Crippler</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inputs</head><p>Kripke is going to name his robot Scrap Metal. Sheldon and Leonard are going to defeat Kripke's robot because theirs is better in design and execution. tion and answers are embedded together with the input text of each stream. A temporal attention mechanism localizes relevant intervals from episode inputs. Finally, question answering is addressed both in a single-stream (Section 6) and a multi-stream (Section 7) scenario. The latter amounts to multi-modal fusion. We begin our discussion with transformer networks (Section 4), which we use both for dialog summarization and text embeddings in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Transformers</head><p>The transformer <ref type="bibr" target="#b30">[31]</ref> is a network architecture that allows for efficient pairwise interaction between input elements. Its main component is an attention function, which acts as a form of associative memory.</p><p>Multi-head attention is a fusion of several attention functions. The architecture is a stack of multi-head attention, element-wise fully-connected and normalization layers with residual connections. Originally developed for machine translation, it includes an encoder and a decoder stack. The decoder additionally attends over the output of the encoder stack and is auto-regressive, consuming previously generated symbols when generating the next.</p><p>BERT <ref type="bibr" target="#b5">[6]</ref> is a transformer bidirectional encoder only, mapping a sequence of tokens to a sequence of ddimensional vectors. It is pre-trained on unsupervised tasks including prediction of masked tokens and next sentence, and can be also fine-tuned on supervised downstream tasks. It can take a number of sentences as in input, where a sentence is an arbitrary span of contiguous text.</p><p>We use BERT as the backbone of our model architecture to represent text, using two sentences at a time. Given strings A and B, the input is given as</p><formula xml:id="formula_1">tok k ([CLS] + A + [SEP] + B + [SEP]),<label>(1)</label></formula><p>where + is string concatenation and tok k is tokenization into k tokens, with zero padding if the input length is less than k and truncation if it is greater. Tokens are represented by WordPiece embeddings <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b34">35]</ref>, concatenated with position embeddings representing their position in the input sequence and segment embeddings, where segments correspond to sentences and are defined according to occurrences of the separator token [SEP]. The output vector in R d corresponding to token [CLS] is an aggregated representation of the entire input sequence and we denote it as</p><formula xml:id="formula_2">f (A, B).<label>(2)</label></formula><p>Sentence-BERT <ref type="bibr" target="#b25">[26]</ref> takes a single sentence as input and is trained by metric learning objectives, e.g. in a siamese or triplet structure, facilitating efficient sentence similarity search. It is learned by fine-tuning a pre-trained BERT model on supervised semantic textual similarity.</p><p>BART <ref type="bibr" target="#b21">[22]</ref> combines a bidirectional encoder and an auto-regressive decoder. It is pre-trained as an unsupervised denoising autoencoder, i.e., corrupting input text and learning to reconstruct the original, and fine-tuned on supervised classification, generation or translation tasks. It is particularly effective on text generation, including abstractive dialog, question answering and summarization tasks.</p><p>Following <ref type="bibr" target="#b3">[4]</ref>, we use sentence-BERT and BART to segment and summarize dialog respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Input description</head><p>All input sources, i.e., video, dialog and plot, are converted into plain text description before being used for question answering. Video is first converted into a scene graph by a visual recognition pipeline and then to text description by a set of rules. Importantly, although already in textual form, dialog is also converted into text description by dialog summarization. The plot, already in text description form, is used as is, but for comparison only: Our main contribution is to replace human-generated plots by automatically generated descriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Dialog</head><p>As the main form of human communication, dialog is an essential input source for video understanding and question answering. We use dialog in three ways: raw dialog per scene, dialog summary per scene and the collection of dialog summary over a whole episode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Raw scene dialog</head><p>As in all prior work, we use the raw dialog associated to the scene of the question, as is. Although in textual form, it is not a text description. It may still contain more information than dialog summary, which is important to investigate.</p><p>Scene dialog summary Given the dialog associated to the scene of the question, we convert this input source into text description by dialog summarization. Despite being of textual form, dialog is very different from text description: conversations are often informal, verbose and repetitive, with few utterances being informative; while a description is a narrative in third-person point of view with clear information flow structured in paragraphs <ref type="bibr" target="#b3">[4]</ref>. Identifying the speaking person is also substantial, especially with multiple people in a conversation. Rather than generic document summarization <ref type="bibr" target="#b11">[12]</ref>, we follow a dedicated dialog summarization method <ref type="bibr" target="#b3">[4]</ref>, which blends character names with events in the generated summaries.</p><p>A dialog is a sequence of utterances, each including a speaker (character) name and a sentence (sequence of tokens). Each utterance is mapped to a vector embedding by Sentence-BERT <ref type="bibr" target="#b25">[26]</ref>. The sequence of embeddings over the entire dialog is segmented according to topic, e.g. greetings, today's plan, etc. by C99 <ref type="bibr" target="#b4">[5]</ref>, as well as stage, e.g. opening, intention, discussion, conclusion by a hidden Markov model (HMM) <ref type="bibr" target="#b0">[1]</ref>. As a result, for each view (topic or stage), the dialog is represented by a sequence of blocks, each containing several utterances.</p><p>Given the above structure, the input is re-embedded and the summary is generated using an extension of BART <ref type="bibr" target="#b21">[22]</ref>. In particular, there is one encoder per view, mapping each block to an embedding. An LSTM <ref type="bibr" target="#b12">[13]</ref> follows, aggregating the entire view into one embedding, obtained as its last hidden state. The decoder attends over the output of each encoder using a multi-view attention layer to weight the contribution of each view. It is auto-regressive, using previous tokens from ground truth at training and previously predicted tokens by the encoder at inference.</p><p>We train the HMM on the dialog sources of our video QA training set; otherwise, we use Sentence-BERT and BART as used/trained by <ref type="bibr" target="#b3">[4]</ref>. Once a scene dialog summary is generated, it is re-embedded by BERT <ref type="bibr" target="#b5">[6]</ref> like all other input sources, as discussed in Section 6.</p><p>Episode dialog summary We collect the scene dialog summaries for all scenes of an episode and we concatenate them into an episode dialog summary. Assuming that the episode of the scene of the question is known, we make available the associated episode dialog summary for question answering. This is a long input source and requires temporal attention, as discussed in Subsection 6.2. Importantly, episode dialog summary is our most important contribution in substituting plot summary by an automatically generated description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Plot summary</head><p>As part of our comparison to <ref type="bibr" target="#b9">[10]</ref>, we use publicly available plot summaries 1 , already in text description form. Assuming that the episode of the scene of the question is known, we make available the associated plot as is, to help answering knowledge-based questions. A plot is shorter and higher-level than our episode dialog summary, but it is still long enough to require temporal attention. It is important to investigate whether we can dispense of such a humangenerated input and how much more information it contains relative to what we can extract automatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Video</head><p>We use a visual recognition pipeline to convert raw input video into text description. Following <ref type="bibr" target="#b9">[10]</ref>, this pipeline comprises four components: character recognition <ref type="bibr" target="#b26">[27]</ref>, place recognition <ref type="bibr" target="#b39">[40]</ref>, object relation detection <ref type="bibr" target="#b37">[38]</ref>, and action recognition <ref type="bibr" target="#b33">[34]</ref>. The outputs of these components are character, place, object, relation and action nodes. A directed video scene graph is generated by collecting all nodes along with edges and then a textual scene description is obtained according to a set of predefined rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Single-stream QA</head><p>As shown in <ref type="figure">Figure 2</ref>, there is one stream per input source, using a transformer to map inputs to embeddings. Following <ref type="bibr" target="#b9">[10]</ref>, we first attempt question answering on each stream alone. In doing so, we learn a linear classifier while fine-tuning the entire transformer representation per stream. Unlike most existing works, this allows adapting to the data at hand, for instance a particular TV show.</p><p>We differentiate scene from episode inputs, as discussed below. In both cases, the given question and candidate answer strings are denoted as q and a c for c = 1, . . . , n c respectively, where n c is the number of candidate answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Scene input sources</head><p>Scene input sources refer to the scene of the question, i.e., raw scene dialog, scene dialog summary or video. The input string is denoted by x. For each c = 1, . . . , n c , we embed x, q and a c jointly to d-dimensional vector</p><formula xml:id="formula_3">y c := f (x + q, a c ),<label>(3)</label></formula><p>where + is string concatenation and f is BERT <ref type="bibr" target="#b1">(2)</ref>. A linear classifier with parameters w ? R d , b ? R yields a score per candidate answer</p><formula xml:id="formula_4">z c := w ? y c + b.<label>(4)</label></formula><p>The score vector z := (z 1 , ..., z nc ) is followed by softmax and cross-entropy loss. At training, we use f as pre-trained and we fine-tune it while optimizing W, b on the correct answers of the QA training set. At inference, we predict arg max c z c .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Episode input sources</head><p>Episode input sources refer to the entire episode of the scene of the question, i.e., episode dialog summary and plot. Because such input is typically longer than the transformer's maximum sequence length k (1), we split it into overlapping parts in a sliding window fashion. Each part contains the question and one answer, so the window length is w = k ? |q| ? |a c |. Given an input of length tokens, the number of parts is n := ?w s + 1, where s is the stride. Because all inputs in a mini-batch must have the same number of parts n p to be stacked in a tensor, certain parts are zero-padded if n &lt; n p and discarded if n &gt; n p .</p><p>Embedding The input strings of the parts are denoted by p j for j = 1, . . . , n p . Each part p j is combined with each candidate answer a c separately, yielding the d-dimensional vectors</p><formula xml:id="formula_5">y c j := f (p j + q, a c )<label>(5)</label></formula><p>for c = 1, . . . , n c and j = 1, . . . , n p . A classifier with parameters w ? R d , b ? R yields a score per candidate answer c and part j:</p><formula xml:id="formula_6">z c j := w ? y c j + b.<label>(6)</label></formula><p>Temporal attention At this point, unlike scene inputs (4), predictions from <ref type="formula" target="#formula_6">(6)</ref> are not meaningful unless a part j is known, which amounts to temporal localization of the part of the input sequence that contains the information needed to answer a question. In TVQA <ref type="bibr" target="#b19">[20]</ref> and related work <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16]</ref>, localization ground truth is available, allowing a two-stage localize-then-answer approach. Without such information, the problem is weakly supervised. Previous work <ref type="bibr" target="#b9">[10]</ref> simply chooses the part j corresponding to the maximum score z c j over all answers c and all parts j in <ref type="formula" target="#formula_6">(6)</ref>, which is called hard temporal attention in the following. Such hard decision may be harmful when the chosen j is incorrect, especially when the predicted answer happens to be correct, because then the model may receive arbitrary gradient signals at training. To alleviate this, we follow a soft temporal attention approach.</p><p>In particular, let S be the n p ? n c matrix with elements z c j over all answers c and all parts j (6). For each part j, we take the maximum score over answers</p><formula xml:id="formula_7">s j := max c z c j ,<label>(7)</label></formula><p>giving rise to a vector s := (s 1 , . . . , s np ), containing a single best score per part. Then, by soft assignment over the rows of S-corresponding to parts-we obtain a score for each answer c, represented by score vector z ? R c :</p><formula xml:id="formula_8">z := softmax(s/T ) ? S,<label>(8)</label></formula><p>where T is a temperature parameter. With this definition of z, we have a single score vector and we proceed as in (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Multi-stream QA</head><p>Once a separate transformer has been fine-tuned separately for each stream, we combine all streams into a single question answering classifier, which amounts to multimodal fusion. Here, we introduce two new simple solutions.</p><p>In both cases, we freeze all transformers and obtain ddimensional embeddings y c for each candidate answer c and for each stream. For scene inputs, y c is obtained directly from (3). Episode input streams produce n p embeddings per answer. Temporal localization is thus required for part selection, similar to single stream training. Again, hard temporal attention amounts to choosing the part with the highest score according to <ref type="bibr" target="#b5">(6)</ref>: y c := y c j * where j * := arg max j (z c j ) and y c j is given by <ref type="bibr" target="#b4">(5)</ref>. Instead, similar to (8), we follow soft temporal attention:</p><formula xml:id="formula_9">y c := softmax(s/T ) ? Y emb c ,<label>(9)</label></formula><p>where Y emb c is a n p ? d matrix collecting the embeddings y c j (5) of all parts j. Finally, for each answer c, the embeddings y c of all streams are stacked into a n s ? d embedding matrix Y c , where n s is the number of streams.</p><p>Multi-stream attention The columns of Y c are embeddings of different streams. We weight them according to weights w c ? R ns obtained from Y c itself, using a multistream attention block, consisting of two fully connected layers followed by softmax:</p><formula xml:id="formula_10">Y att c = diag(w c ) ? Y c .<label>(10)</label></formula><p>For each answer c, a fully connected layer maps the d ? n s matrix Y att c to a scalar score. All n c scores are followed by softmax and cross-entropy loss, whereby the parameters of all layers are jointly optimized.</p><p>Self-attention Alternatively, Y c is mapped to Y att c ? R d?ns by a single multi-head self-attention block, as in transformers <ref type="bibr" target="#b30">[31]</ref>:</p><formula xml:id="formula_11">Y att c = MultiHeadAttention(Y c , Y c , Y c ).<label>(11)</label></formula><p>The remaining pipeline is the same as in the previous case. We fine-tune the BERT BASE <ref type="bibr" target="#b5">[6]</ref> uncased model with N = 12 transformer blocks, h = 12 self-attention heads and embedding dimension d = 768 for single-stream models. The maximum token length k is 512 for scene, 200 for plot and 300 for episode dialog summary inputs. The stride s is 100 for plot and 200 for episode dialog summary. The maximum number of parts n p is 10 for both. The batch size is 8 for all single-stream models and 32 for multistream. We use SGD with momentum 0.9 scheduled with initial learning rate 10 ?4 for multi-stream fusions. We use h = 1 attention head, and N = 2 stacks for self-attention and multi-stream self-attention methods. The number of streams n s varies per experiment. <ref type="table">Table 1</ref> compares of our method with the state of the art. Rookies and Masters are human evaluators: Masters have watched most of the show, whereas Rookies have never watched an episode before <ref type="bibr" target="#b10">[11]</ref>. TVQA <ref type="bibr" target="#b19">[20]</ref> encodes visual features and subtitles without considering knowledge information; its results are as reported in <ref type="bibr" target="#b10">[11]</ref>. ROCK <ref type="bibr" target="#b10">[11]</ref> uses four visual representations (image, concepts, facial, caption); ROCK facial is one of its best results. ROCK GT <ref type="bibr" target="#b10">[11]</ref> and ROLL human <ref type="bibr" target="#b9">[10]</ref> use the human knowledge annotation provided by the dataset <ref type="bibr" target="#b10">[11]</ref>, while ROLL <ref type="bibr" target="#b9">[10]</ref> uses humanwritten plot summaries instead. Our method uses scene video and scene dialog summary as well as the episode dialog summary that it automatically generates, without any human annotation. Ours plot additionally uses the same plot as <ref type="bibr" target="#b9">[10]</ref>. TVQA uses LSTM; all other methods are based on BERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.">Quantitative results</head><p>Our method outperforms the best state of the art method (ROLL <ref type="bibr" target="#b9">[10]</ref>) by 6.6%, without any human annotation. By using additional human-generated plots, the gain decreases to 5.8%. This indicates that our episode dialog summary captures the required knowledge and removes the requirement of human-generated input; in fact, human-generated input is harmful. On temporal and knowledge questions in particular, we gain 13.9% and 7.6%, respectively, without any human annotation. This implies that our automatically generated episode dialog summary increases the understanding of the episode and helps answering all types of questions. Despite ROLL human <ref type="bibr" target="#b9">[10]</ref> and ROCK GT <ref type="bibr" target="#b10">[11]</ref> using ground-truth knowledge, we outperform them by 16.1% and 5.0%, respectively, without any human annotation. We also outperform Rookies, presumably by having access to the dialog of the entire episode. Comparing to Masters, there is still room for improvement.  <ref type="figure" target="#fig_0">Figure 3(a)</ref> shows a knowledge question, answered based on episode dialog summary, which has the highest attention score. As shown in <ref type="figure" target="#fig_0">Figure 3</ref>(b), a textual question can be answered by using scene dialog summary, but also by episode dialog summary, since the latter includes the former. Temporal questions can be answered from scene inputs such as scene dialog summary or video description. According to attention scores, the question in <ref type="figure" target="#fig_0">Figure 3</ref>(c) is answered by episode dialog summary, which includes the correct answer. Finally, <ref type="figure" target="#fig_0">Figure 3(d)</ref> shows a visual question answered by video description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.">Qualitative analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4.">Ablation studies</head><p>Single-stream results <ref type="table" target="#tab_3">Table 2</ref> shows our single-stream QA results. We reproduce <ref type="bibr" target="#b9">[10]</ref> for dialog, video, and plot inputs. We replace the plot stream by one using our new temporal attention (Subsection 6.2) and other improvements (  Howard and Amy. Amy wearing glass. Amy has hand, arm, nose and hair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Video Description</head><p>Amy doesn't like listening to music in the car. Sheldon doesn't want her to be mistaken for a gang member. Amy loves Neil Diamond's music. edge questions, episode dialog summary and plot inputs have higher accuracy than other input sources since they span an entire episode. Our episode dialog summary helps in answering questions better than the plot <ref type="bibr" target="#b9">[10]</ref>, bringing an accuracy improvement of 5.4%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scene Dialog Summary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>QA</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-stream results</head><p>We evaluate our two multi-stream QA methods introduced in Section 7, namely multi-stream attention and self-attention, comparing them with the following combinations/baselines/competitors:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1.</head><p>Multi-stream self-attention: combination of multistream attention and self-attention: the output of the latter is weighted by the former. The remaining pipeline is the same as in multi-stream attention.    tained scores by single-stream classifiers are combined by a multi-stream classifier and another loss function applies. The overall loss all is a linear combination with weight ? ? on the multi-stream loss and 1 ? ? ? uniformly distributed over single-stream losses. <ref type="table" target="#tab_4">Table 3</ref> shows results for fusion of video, scene dialog summary and episode dialog summary. For modality weighting, we set ? ? = 0.7 according to the validation set. Our multi-stream attention outperforms other fusion methods. Besides, it does not require tuning of modality weight hyperparameter ? ? or selecting the number of heads and blocks for self-attention. Unless specified, we use multistream attention for fusion by default.</p><p>Improvements over <ref type="bibr" target="#b9">[10]</ref> We reproduce ROLL <ref type="bibr" target="#b9">[10]</ref> using official code by the authors and default parameters. This is our baseline, shown in the first row of <ref type="table">Table 4</ref>. Then, we evaluate our improvements, adding them one at a time.  <ref type="table">Table 4</ref>: Accuracy improvements over ROLL <ref type="bibr" target="#b9">[10]</ref>. ?: our reproduction. Each row adds a new improvement except the last two, where we replace streams. P: plot; E: episode dialog summary; D: dialog; S: scene dialog summary.</p><p>First, we replace modality weighting with multi-stream attention. Despite its simplicity, its performance is on par, losing only 0.1%, while requiring no hyperparameter tuning. Then, we increase the number of parts of plot summaries from 5 to 10, eliminating information loss by truncation and bringing an accuracy improvement of 1.1%. We change the order of arguments of BERT for episode input sources from f (q, a c +p j ) to f (p j +q, a c ) (5), which is consistent with (3) and improves only slightly by 0.1%. Our new temporal attention mechanism improves accuracy by 0.9%. Replacing plot with episode dialog summary, which is our main contribution, brings an improvement of 5.1%. Finally, the accuracy is improved by 0.6% by using scene dialog summary instead of raw dialog. The overall gain over <ref type="bibr" target="#b9">[10]</ref> is 7.7%. Note that the relative improvement of each new idea depends on the order chosen in <ref type="table">Table 4</ref>. For instance, the order of BERT arguments brings improvements of up to 2.3% in experiments including the episode dialog summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Conclusion</head><p>KnowIT VQA is a challenging dataset where it was previously believed that some form of external knowledge was needed to handle knowledge questions, as if knowledge was yet another modality. Our results indicate that much of this required knowledge was hiding in dialog, waiting to be harnessed. It is also interesting that our soft temporal attention helps a lot more with our episode dialog summary than human plot summary, which may be due to the episode dialog summary being longer. This may also explain the astounding performance of episode dialog summary, despite its low overall quality: plot summaries are of much higher quality but may be missing a lot of information.</p><p>Dialog summarization In the example of <ref type="figure">Figure 4</ref>, Howard says, "I invited her." in scene B. Our dialog summarization interprets this sentence by assigning the correct character name: "Howard invited Bernadette in." Hence, we can answer the question of scene A, "Who did Howard invite to join him and Raj in Raj's lab?" correctly. Thanks to the episode dialog summary spanning all scenes and the use of character names instead of pronouns, our method can answer character-related questions correctly.</p><p>(...) It's a Romulan battle bagel, not a starship.</p><p>Howard invited Bernadette in. The telescope is in Hawaii, but Raj controls it from here. (...)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Episode Dialog Summary</head><p>Who did Howard invite to join him and Raj in Raj's lab?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A) Bernadette B) Leonard C) Penny D) Amy</head><p>Howard: How could that be a miss? C-6 was a hit, C-8 was a hit. Part of your starship has to be on C-7.  <ref type="figure">Figure 4</ref>: Dialog summarization converts pronouns in dialog to character names in episode dialog summary, supporting question answering. In particular, "I" is substituted by "Howard" and "her" by "Bernadette".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dialog (Scene B)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>QA (Scene A)</head><p>Plot vs. episode dialog summary A comparison of plot summary and episode dialog summary is given in <ref type="figure">Figure 5</ref>.</p><p>There are three different topics in the story line, and each is highlighted with the same color in both summaries. The first topic, highlighted in purple, is "Sheldon's forgotten flash drive." The second, highlighted in yellow, is "Sheldon's grandmother." The third, highlighted in blue, is "Asking Summer out." The plot summary is topic-centered, while the episode dialog summary is following the narrative order. Hence, topics may be fragmented in the latter. The episode dialog summary has more detail than the plot. In particular, it contains enough information to answer the question Why does Sheldon's grandmother call him Moon Pie? That is, because he's nummy-nummy. This information is missing from the plot summary, which focuses on the main topics/events of an episode. Even though the episode dialog summary is noisy, it contains details that help in question answering. <ref type="figure">Figure 6</ref> shows examples of failed predictions of our model along with stream attention scores for different question types. The model receives three input  <ref type="figure">Figure 5</ref>: An example of plot summary and episode dialog summary, with each topic highlighted in the same color in both summaries. Phrases relevant to QA in blue. Only the episode dialog summary contains enough information to answer the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Failure cases</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>QA</head><p>sources, question/answers and attention scores over inputs. <ref type="figure">Figure 6</ref>(a) refers to a knowledge question, which requires recurrent knowledge of the whole TV show. In other words, the correct answer cannot be found in episode dialog summary. The question is answered as "a lasagna" found in episode dialog summary, even though it is wrong. <ref type="figure">Figure 6</ref>(b) refers to a textual question, which should have been answered by scene dialog summary. However, scene dialog summary does not contain the correct answer. Our model gives most attention to episode dialog summary. The prediction is made according to the highlighted text, which is the same in both sources. However, this prediction refers to the wrong person. <ref type="figure">Figure 6</ref>(c) refers to another knowledge question, which could be answered by the highlighted text in episode dialog summary. Even though episode dialog summary has the most attention, the prediction is incorrect. <ref type="figure">Figure 6</ref>(d) refers to another textual question, which should have been answered by scene dialog summary. Although both scene dialog summary and episode dialog summary include the correct answer, and episode dialog summary has the most attention, the prediction indicates the wrong person.  <ref type="figure">Figure 6</ref>: Failed predictions of multi-stream attention. We highlight in blue the part of the source text that might be relevant to answering the question. "Pred"/blue: model predictions. "GT"/green: ground truth. <ref type="figure">Figure 6</ref>(e) refers to a temporal question. The scene dialog summary and episode dialog summary imply that Raj and Howard might be changing the tire. The video description is not helpful either. Hence, our model predicts Raj, while the correct answer is Howard. <ref type="figure">Figure 6</ref>(f) is a visual question. However, the video description fails to convey relevant information to answer the question. The other inputs do not contain relevant information either. One of the character names appearing in episode dialog summary is predicted, which is incorrect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional ablation studies</head><p>Hyperparameter validation Modality weighting <ref type="bibr" target="#b9">[10]</ref> fusion method requires selection of hyperparameter ? ? . <ref type="figure" target="#fig_7">Figure 7</ref> shows validation accuracy vs. ? ? for fusion of video, scene dialog summary and episode dialog summary. We choose ? ? = 0.7 for both soft and hard temporal selection to report results in <ref type="table" target="#tab_4">Table 3 and Table 6</ref>. The remaining weight of 1?? ? is evenly distributed over individual stream losses as 0.1 per stream.</p><p>Effect of temporal attention on single-stream QA We investigate the effect of our soft temporal attention (Subsection 6.2) on single-stream QA for episode input sources. We also evaluate the effect of single-stream training with soft or hard temporal attention on multi-stream attention, where we use soft temporal attention. According to Table 5, temporal attention improves the accuracy of plot and episode dialog summary by 1.9% and 3.3%, respectively. Accordingly, the accuracy of multi-stream QA on the same episode sources as well as video and scene dialog sum-   <ref type="table">Table 5</ref>: Effect of temporal attention on single-stream QA on KnowIT VQA. Soft Attn.: soft temporal attention on single-stream training. We use soft temporal attention for multi-stream QA, but this is still affected by the temporal attention used in single-stream training. V: video; S: scene dialog summary; P: plot; E: episode dialog summary. mary increases by 0.6% and 7.0%, respectively. The gain is higher when episode dialog summary is used, since the episode dialog summary is longer than plot.</p><p>Effect of temporal attention on multi-stream QA <ref type="table">Table 6</ref> shows the effect of soft temporal attention on multi-stream QA for fusion of video, scene dialog summary and episode dialog summary input sources. We use soft temporal attention for single-stream QA of episode dialog summary. In all fusion methods, the overall accuracy is improved by using soft temporal attention. <ref type="table">Table 7</ref> shows the accuracy of multi-stream QA for different input combinations, where the number of input streams varies in {2, 3, 4, 5}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Different input combinations</head><p>Scene dialog summaries improves the accuracy compared with single-stream QA results in <ref type="table" target="#tab_3">Table 2</ref>. Moreover, using the episode dialog summary always improves the overall accuracy by a large margin. The best overall accuracy METHOD SOFT VIS. TEXT. TEMP. KNOW. ALL ATTN.  <ref type="table">Table 6</ref>: Effect of temporal attention on multi-stream QA on KnowIT VQA for fusion of video, scene dialog summary, and episode dialog summary input sources. Soft Attn.: soft temporal attention on multi-stream training. We use soft temporal attention for single-stream QA of episode dialog summary.  <ref type="table">Table 7</ref>: Multi-stream QA accuracy on KnowIT VQA: comparison of different input combinations for multi-stream attention. D: dialog; V: video; P: plot; S: scene dialog summary; E: episode dialog summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Product</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ANALYSED INPUTS</head><p>of 0.781 is achieved by video, scene dialog summary, and episode dialog summary.</p><p>Question type ? attention scores We perform significance testing for the dependence between the question type and attention scores. There are 2 independent variables in the scores of 3 streams, whose values we discretize into 10 ? 10 bins. We form a 4 ? 10 ? 10 joint histogram of question type (X) and scores (Y ) and compute the mutual information I(X; Y ). We perform a G-test 2 with G = 2N ? I(X; Y ), where N = 2361 is the number of test questions. Finally, using a chi-square distribution of 3 ? 9 ? 9 DoF, we find a p-value of 1.52 ? 10 ?25 for the null hypothesis. This indicates that attention scores depend on question type.</p><p>Replacing attention scores with oracle scores determined by question type Assuming that we know the question type for the test set, we perform an oracle experiment where attention scores are based on question type rather than our fusion method. We only consider visual, textual, and knowledge types of question. In particular, we assign visual questions to video input, textual questions to scene dialog summary and knowledge questions to episode dialog summary. We exclude temporal questions since they can be answerable by scene dialog summary or video. Only 3.6% of questions are of temporal type in the test set. We find that our multi-stream attention method (0.781%) is 3.6% better than the oracle experiment (0.745%). This indicates that our fusion mechanism is more effective than a na?ve oracle that assumes more knowledge.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3</head><label>3</label><figDesc>visualizes the correct predictions of our method with stream attention scores for different question types. In all examples, the model receives three input sources, question/answers and attention scores over inputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>will be the last ones there, so they need to hurry up. Sheldon thinks it's a marathon not a sprint. (...) Episode Dialog Summary Penny and Sheldon are the last ones to arrive where? A) To Sheldon's apartment B) The to mall C) To the comic book store D) To the Cheescake Factory Leonard, Sheldon and Penny are smiling at The comic book store. Face of Leonard. Shirt and jacket on Leonard. (...)Video DescriptionStuart invited Penny to Raj's murder mystery party. Penny and Leonard will be the last ones there, so they need to hurry up. (...) Multi-stream attention visualization. We highlight in blue the part of the source text that is relevant to answering the question. The most attended stream is episode dialog summary for (a), (b), (c) and video description for (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2 . 3 .</head><label>23</label><figDesc>Product: Hadamard product on embeddings of all streams per answer, followed by a linear classifier per answer. The remaining pipeline is the same. Modality weighting [10]: a linear classifier (4) and loss function is used as in single-stream QA but with transformers frozen for each stream separately. The ob-METHOD INPUT VIS. TEXT. TEMP. KNOW.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>METHOD VIS. TEXT. TEMP. KNOW. ALL Product 0.743 0.659 0.756 0.751 0.739 Modality weighting [10] 0.708 0.786 0.767 0.787 0.769 Self-attention 0.759 0.764 0.767 0.777 0.771 Multi-stream attention 0.755 0.783 0.779 0.789 0.781 Multi-stream self-attn. 0.755 0.768 0.756 0.777 0.770</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>METHOD VIS. TEXT. TEMP. KNOW. ALL ROLL [10] ? 0.722 0.703 0.709 0.697 0.704 + Multi-stream attention 0.724 0.721 0.721 0.691 0.703 + More parts for plot 0.722 0.703 0.651 0.717 0.714 + New order of plot inputs 0.730 0.710 0.686 0.712 0.715 + Temporal attention 0.734 0.725 0.663 0.724 0.724 ? Replacing P ? E 0.753 0.815 0.814 0.773 0.775 ? Replacing D ? S 0.755 0.783 0.779 0.789 0.781</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Accuracy vs. ? ? for fusion of video, scene dialog summary and episode dialog summary by modality weighting [10] on KnowIT VQA validation set. STREAM INPUTS SOFT VIS. TEXT. TEMP. KNOW. ALL ATTN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>0.812 0.779 0.779 0.775 V+E 0.732 0.761 0.767 0.788 0.772 P+E 0.716 0.743 0.721 0.791 0.766 D+S+E 0.743 0.822 0.802 0.771 0.772 V+S+E 0.755 0.783 0.779 0.789 0.781 P+S+E 0.739 0.779 0.733 0.783 0.771 D+V+P+S+E 0.751 0.797 0.744 0.781 0.775</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Sheldon, Dr. Beverly Hofstadter, Leonard and Barry are sitting at a table at The Caltech cafeteria. (...)</figDesc><table><row><cell></cell><cell>Converted Inputs</cell><cell>Streams</cell><cell></cell><cell></cell><cell>Fusion</cell><cell></cell></row><row><cell>Video</cell><cell>Video Description</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Video Stream</cell><cell>Fusion Answer1</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>QA</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Kripke: ...his name is gonna be Dialog Scrap Metal. Leonard: Come on. Is that really</cell><cell>Scene Dialog Summary Episode Dialog Summary</cell><cell>Scene Dialog Summary Stream</cell><cell>Fusion Fusion Answer2 Fusion Answer2 Fusion Answer3 Answer3</cell><cell>Concat</cell><cell>Softmax</cell><cell>A) Killer Robot B) Terminator C) Monte D) Crippler Prediction</cell></row><row><cell>necessary?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sheldon: Leonard, I believe it is.</cell><cell>(...) Leonard and Raj have built a robot called Monte. (...)</cell><cell>Episode Dialog Summary Stream</cell><cell>Fusion Answer4 Fusion Answer4</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>QA</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>QA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>QA Figure 2: Our VideoQA system converts both video and dialog to text descriptions/summaries, the latter at both scene and episode level. Converted inputs are processed independently in streams, along with the question and each answer, producing a score per answer. Finally, stream embeddings are fused separately per answer and a prediction is made.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 )Table 1 :</head><label>41</label><figDesc>and we add two new sources automatically generated from dialog: scene dialog summary and episode dialog summary. Due to the dataset having a majority of knowl-State-of-the-art accuracy on KnowIT VQA. Ours uses the video and scene dialog summary as well as the episode dialog summary that we generate from the dialog of the entire episode. Ours plot also uses human-generated plot summaries, like<ref type="bibr" target="#b9">[10]</ref>. TVQA uses an LSTM based encoder; all other methods use BERT. Rookies and Masters are humans.</figDesc><table><row><cell></cell><cell>METHOD</cell><cell>KNOWLEDGE</cell><cell>VIS.</cell><cell cols="3">TEXT. TEMP. KNOW.</cell><cell>ALL</cell></row><row><cell></cell><cell>Rookies [11]</cell><cell>-</cell><cell>0.936</cell><cell>0.932</cell><cell>0.624</cell><cell>0.655</cell><cell>0.748</cell></row><row><cell></cell><cell>Masters [11]</cell><cell></cell><cell>0.961</cell><cell>0.936</cell><cell>0.857</cell><cell>0.867</cell><cell>0.896</cell></row><row><cell></cell><cell>ROCKGT [11]</cell><cell>question GT</cell><cell>0.747</cell><cell>0.819</cell><cell>0.756</cell><cell>0.708</cell><cell>0.731</cell></row><row><cell></cell><cell cols="2">ROLLhuman [10] question GT</cell><cell>0.708</cell><cell>0.754</cell><cell>0.570</cell><cell>0.567</cell><cell>0.620</cell></row><row><cell></cell><cell>TVQA [20]</cell><cell>-</cell><cell>0.612</cell><cell>0.645</cell><cell>0.547</cell><cell>0.466</cell><cell>0.522</cell></row><row><cell></cell><cell>ROCKfacial [11]</cell><cell>dataset GT</cell><cell>0.654</cell><cell>0.688</cell><cell>0.628</cell><cell>0.646</cell><cell>0.652</cell></row><row><cell></cell><cell>ROLL [10]</cell><cell>plot</cell><cell>0.718</cell><cell>0.739</cell><cell>0.640</cell><cell>0.713</cell><cell>0.715</cell></row><row><cell></cell><cell>Ours</cell><cell>-</cell><cell>0.755</cell><cell>0.783</cell><cell>0.779</cell><cell>0.789</cell><cell>0.781</cell></row><row><cell></cell><cell>Oursplot</cell><cell>plot</cell><cell>0.749</cell><cell>0.783</cell><cell>0.721</cell><cell>0.783</cell><cell>0.773</cell></row><row><cell>Video Description</cell><cell>Scene Dialog Summary</cell><cell cols="2">Episode Dialog Summary</cell><cell cols="2">Video Description</cell><cell cols="2">Scene Dialog Summary</cell><cell>Episode Dialog Summary</cell></row><row><cell>(...)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(...)</cell></row><row><cell>Sheldon at table. Sheldon sitting on chair. Curtain and building behind Sheldon. Sheldon wearing jacket and shoe. Sheldon holding paper.</cell><cell>Sheldon will send him an email when they get back. He needs to read it. (...)</cell><cell cols="2">(...) Sheldon forgot his flash drive, so he has to go back and get it. (...)</cell><cell cols="2">Dr. Beverly Hofstadter, Sheldon and Penny are holding a laptop at The main building. (...)</cell><cell cols="2">Penny wants Sheldon to go to a coffee shop, but he doesn't drink coffee. (...)</cell><cell>Penny wants Sheldon to go to a coffee shop, but he doesn't drink coffee. She wants him to try some cookies, pastries and bear claws.</cell></row><row><cell>(...)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(...)</cell></row><row><cell>QA</cell><cell cols="2">Attention Score</cell><cell></cell><cell></cell><cell>QA</cell><cell></cell><cell>Attention Score</cell></row><row><cell cols="2">What has Sheldon forgotten here?</cell><cell></cell><cell></cell><cell cols="3">What does Sheldon not drink?</cell></row><row><cell></cell><cell cols="2">Video Description</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Video Description</cell></row><row><cell>A) His flash drive B) His thesis</cell><cell cols="2">Scene Dialog Summary</cell><cell></cell><cell>A) milke B) tea</cell><cell></cell><cell></cell><cell>Scene Dialog Summary</cell></row><row><cell>C) His suitcase</cell><cell cols="2">Episode Dialog Summary</cell><cell></cell><cell>C) alcohol</cell><cell></cell><cell></cell><cell>Episode Dialog Summary</cell></row><row><cell>D) His laptop</cell><cell></cell><cell></cell><cell></cell><cell>D) coffee</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Single-stream QA accuracy on KnowIT VQA.</figDesc><table><row><cell>ROLL [10]: as reported; [10] ?: our reproduction. Our</cell></row><row><cell>model incorporates the scene dialog and video streams of</cell></row><row><cell>the latter as well as the plot, scene dialog summary and</cell></row><row><cell>episode dialog summary streams. Plot differs between [10] ?</cell></row><row><cell>and our model by our temporal attention and other improve-</cell></row><row><cell>ments (Table 4). D: dialog; V: video; P: plot; S: scene dialog</cell></row><row><cell>summary; E: episode dialog summary.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Multi-stream QA accuracy on KnowIT VQA, fusing video, scene dialog summary and episode dialog summary input sources. All fusion methods use soft temporal attention for localization of episode input sources. Top: baseline/competitors. Bottom: ours.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Penny got a box with letters from Sheldon's grandmother, but it's the wrong box.</head><label></label><figDesc>He forgot his flash drive. Leonard and Sheldon have to go back and get it from him. Leonard forgot Sheldon's flash drive with his paper on astrophysical probes on M-theory effects in the early universe, which he was going to give to George Smoot at the conference. (...) Sheldon doesn't read the letters. (...)</figDesc><table><row><cell>Plot Summary</cell><cell>Episode Dialog Summary</cell></row><row><cell></cell><cell>(...)</cell></row><row><cell>(...)</cell><cell></cell></row><row><cell>Meanwhile, Sheldon has bigger problems on</cell><cell></cell></row><row><cell>his mind when he realizes he's left his flash</cell><cell></cell></row><row><cell>drive, which contains a document he wants to</cell><cell></cell></row><row><cell>present to Nobel prize winner George Smoot,</cell><cell></cell></row><row><cell>at home.</cell><cell></cell></row><row><cell>(...)</cell><cell>Episode Dialog Summary</cell></row><row><cell>While in his room, Penny finds a bunch of letters from Sheldon's grandmother who refers to him as "Moon Pie".</cell><cell>She calls him Moon Pie. (...) Sheldon tells Penny to put the letter back on. (...)</cell></row><row><cell>(...)</cell><cell>Howard likes Summer and wants to ask her a question about him. Howard wants to ask</cell></row><row><cell>Back on the train, Howard's lost his way with Summer Glau and, after a boring conversation, he flatly asks her whether he has a chance. She lets him down gently, but Howard tries his luck at getting a photo with her and she breaks his phone.</cell><cell>Summer out, but she doesn't want to go on a date with him. He will leave her in peace, but before he goes, he will ask her out. Howard will take a picture of Summer and himself together for his Facebook page. (...) Sheldon tells her to insert the flash drive into</cell></row><row><cell>(...)</cell><cell>the USB port. She calls him Moon Pie because he's nummy-nummy.</cell></row><row><cell></cell><cell>(...)</cell></row><row><cell>Why does MeeMaw call Sheldon MoonPie?</cell><cell></cell></row><row><cell cols="2">A) Because Sheldon dislikes Moon Pie's, and MeeMaw is teasing Sheldon.</cell></row><row><cell cols="2">B) Because he has huge eyes like moons, and she's teasing him.</cell></row><row><cell cols="2">C) Because he is 'nommy nommy' and she could 'eat him up'</cell></row><row><cell cols="2">D) Because Sheldon wanted to actually fly to the moon as a scientist, and Mee Maw would call him</cell></row><row><cell>Moon Pie because of this.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Raj will try to turn it the other way. Scene Dialog Summary QA Attention Score Scene Dialog Summary Video Description Episode Dialog Summary Video Description</head><label></label><figDesc></figDesc><table><row><cell>Video Description</cell><cell></cell><cell>Scene Dialog Summary</cell><cell>Episode Dialog Summary</cell><cell>Video Description</cell><cell></cell><cell>Scene Dialog Summary</cell><cell>Episode Dialog Summary</cell></row><row><cell>Howard and Bernadette are smiling at The Cheesecake Factory. Hand and face of Howard. Shirt, hat and cap on Howard. (...)</cell><cell cols="2">(...) Sheldon and Bernadette are going to order a seven-day course of penicillin, a syrup of ipecac to induce vomiting and a mint.</cell><cell>(...) Howard had lunch with Bernadette today. Mrs Wolowitz had a pastrami sandwich and she had eggplant lasagna. (...)</cell><cell>Amy, Bernadette and Penny are eating something at Penny's apartment. Face of Amy. Shirt, glass and jacket on Amy. (...)</cell><cell cols="2">Bernadette is a successful microbiologist. She should be celebrated for her achievements, not her looks. (...)</cell><cell>(...) Bernadette is a successful microbiologist. She should be celebrated for her achievements, not her looks. (...)</cell></row><row><cell>QA</cell><cell></cell><cell cols="2">Attention Score</cell><cell>QA</cell><cell></cell><cell>Attention Score</cell></row><row><cell cols="2">What does Sheldon always have at the Cheesecake Factory? A) Shepherd's Pie B) A club sandwich C) A burger (GT) D) a lasagna (Pred)</cell><cell cols="2">Scene Dialog Summary Video Description Episode Dialog Summary</cell><cell>What is Penny's new job? A) Pharmaceutical rep (GT) B) Scientist D) Microbiologist (Pred) C) Waitress</cell><cell></cell><cell>Video Description Scene Dialog Summary Episode Dialog Summary</cell></row><row><cell cols="3">(a) Knowledge QA (1)</cell><cell></cell><cell></cell><cell cols="2">(b) Textual QA (1)</cell></row><row><cell>Video Description</cell><cell></cell><cell>Scene Dialog Summary</cell><cell>Episode Dialog Summary</cell><cell>Video Description</cell><cell></cell><cell>Scene Dialog Summary</cell><cell>Episode Dialog Summary</cell></row><row><cell>Sheldon and Amy are smiling at Amy's apartment. Hand and face of Sheldon. Shirt on Sheldon. Cabinet, door and sign behind Sheldon. (...)</cell><cell cols="2">Amy is going to attempt an experiment on Sheldon to get him to transfer his feelings to her in an accelerated time-frame. (...)</cell><cell>(...) Amy and Sheldon are playing doctor, Star Trek style. Sheldon is in hell. Leonard wants them to stop, but Amy doesn't want to stop. (...)</cell><cell>Penny is holding a bag. Hand of Penny. Shirt on Penny. Penny has nose, face, hair and head. Car behind Penny.</cell><cell cols="2">(...) Howard and Bernadette are singing to her. Amy and Penny missed each other. (...)</cell><cell>(...) Penny is sorry. Howard and Bernadette are singing to her. Amy and Penny missed each other. (...)</cell></row><row><cell>QA</cell><cell></cell><cell cols="2">Attention Score</cell><cell>QA</cell><cell></cell><cell>Attention Score</cell></row><row><cell>What game do Sheldon and Amy play? A) Paint ball B) Archery C) Doctors (GT) D) Chess (Pred)</cell><cell></cell><cell cols="2">Scene Dialog Summary Video Description Episode Dialog Summary</cell><cell cols="2">Who tells Penny they missed her? A) Bernadette (Pred) B) Amy (GT) D) Penny C) Emily</cell><cell>Video Description Scene Dialog Summary Episode Dialog Summary</cell></row><row><cell cols="3">(c) Knowledge QA (2)</cell><cell></cell><cell></cell><cell cols="2">(d) Textual QA (2)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Episode Dialog Summary</cell><cell>Video Description</cell><cell></cell><cell>Scene Dialog Summary</cell><cell>Episode Dialog Summary</cell></row><row><cell>Leonard, Sheldon and Raj are laughing. Face and head of Leonard. Jacket and coat on Leonard. Leonard wearing glass. (...)</cell><cell cols="6">(...) Howard took the other four off and when he got to this one, (...) Raj will try to turn it the other way. (...) one, (...) (...) Howard took the other four off and when he got to this Bernadette and Stuart are looking for a new roommate for (...) Sheldon and Bernadette are smiling at Howard and Bernadette and Stuart are looking for a new roommate for Sheldon. Sheldon would like Bernadette's house. Sheldon. (...) Stuart will think (...) (...) Gandalf, but he's a smoker. Sheldon in room. about it, but Sheldon won't accept it.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>QA</cell><cell></cell><cell>Attention Score</cell></row><row><cell>Who is changing the tire?</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Who is Sheldon sitting with on the couch?</cell><cell>Video Description</cell></row><row><cell>A) Leonard B) Raj (Pred) C) Sheldon D) Howard (GT)</cell><cell></cell><cell></cell><cell></cell><cell>A) Raj D) Leonard B) Stuart (Pred) C) Bernadette (GT)</cell><cell></cell><cell>Scene Dialog Summary Episode Dialog Summary</cell></row><row><cell></cell><cell cols="2">(e) Temporal QA</cell><cell></cell><cell></cell><cell></cell><cell>(f) Visual QA</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://the-big-bang-theory.com/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://en.wikipedia.org/wiki/G-test</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work was supported by the European Commission under European Horizon 2020 Programme, grant number 951911 -AI4Media. This work was granted access to the HPC resources of IDRIS under the allocation 2020-AD011012263 made by GENCI.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional qualitative analysis</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Large-scale analysis of counseling conversations: An application of natural language processing to mental health</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Althoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. ACL</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="463" to="476" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">VQA: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">iPerceive: Applying common-sense reasoning to multi-modal dense video captioning and video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aman</forename><surname>Chadha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gurneet</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navpreet</forename><surname>Kaloty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WACV, 2021</title>
		<meeting>WACV, 2021</meeting>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-view sequence-tosequence models with conversational structure for abstractive dialogue summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP, 2020</title>
		<meeting>EMNLP, 2020</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Advances in domain independent linear text segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Freddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Heterogeneous memory enhanced multimodal attention model for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyou</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wensheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Incorporating commonsense knowledge into abstractive dialogue summarization via heterogeneous graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiachong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.10044</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">De-ViSE: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Knowledge-based video question answering with unsupervised scene descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noa</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Nakashima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">KnowIT VQA: Answering knowledge-based questions about videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noa</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayu</forename><surname>Otani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhui</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Nakashima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI, 2020</title>
		<meeting>AAAI, 2020</meeting>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SAMSum corpus: A human-annotated dialogue dataset for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Gliwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iwona</forename><surname>Mochol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Biesek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Wawer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on New Frontiers in Summarization. ACL</title>
		<meeting>the 2nd Workshop on New Frontiers in Summarization. ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Densecaption matching and frame-selection gating for temporal localization in VideoQA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyounghun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zineng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL, 2020. 1</title>
		<meeting>ACL, 2020. 1</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Progressive attention memory network for movie story question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyeong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minuk</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungsu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang D</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Modality shifting attention network for multimodal video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyeong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minuk</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungsu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><forename type="middle">D</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR, 2020. 1</title>
		<meeting>CVPR, 2020. 1</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multimodal dual attention memory for video story question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyung-Min</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Ho</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.2539</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dense-captioning events in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">TVQA: Localized, compositional video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">TVQA+: Spatio-temporal grounding for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ves</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL, 2020. 3</title>
		<meeting>ACL, 2020. 3</meeting>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Focal visual-text attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangliang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A readwrite memory network for movie story understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seil</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jisung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Dial2desc: end-to-end dialogue description generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junpei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00185</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sentence-BERT: Sentence embeddings using Siamese BERT-networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP-IJCNLP</title>
		<meeting>EMNLP-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">FaceNet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Japanese and korean voice search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisuke</forename><surname>Nakajima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">MovieQA: Understanding stories in movies through question-answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">MMFT-BERT: Multimodal fusion transformer with bert encodings for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aisha</forename><surname>Urooj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Mazaheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niels</forename><surname>Da Vitoria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Lobo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sequence to sequence-video to text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Long-term feature banks for detailed video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Yuta Nakashima, and Haruo Takemura. BERT representations for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noa</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhui</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayu</forename><surname>Otani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WACV, 2020</title>
		<meeting>WACV, 2020</meeting>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">From recognition to cognition: Visual commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Manohar Paluri, Ahmed Elgammal, and Mohamed Elhoseiny. Largescale visual relationship understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Improving abstractive dialogue summarization with graph structures and topic words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lulu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. COLING, 2020</title>
		<meeting>COLING, 2020</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1452" to="1464" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
