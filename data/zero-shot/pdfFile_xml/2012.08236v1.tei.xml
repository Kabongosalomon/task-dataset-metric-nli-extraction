<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Point-Level Temporal Action Localization: Bridging Fully-supervised Proposals to Weakly-supervised Losses</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Ju</surname></persName>
							<email>juchen@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peisen</forename><surname>Zhao</surname></persName>
							<email>pszhao@sjtu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
							<email>yazhang@sjtu.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
							<email>wangyanfeng@sjtu.edu.cn</email>
							<affiliation key="aff3">
								<orgName type="institution">Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
							<email>tian.qi1@huawei.com</email>
							<affiliation key="aff4">
								<orgName type="institution">Huawei Cloud &amp; AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Point-Level Temporal Action Localization: Bridging Fully-supervised Proposals to Weakly-supervised Losses</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Point-Level temporal action localization (PTAL) aims to localize actions in untrimmed videos with only one timestamp annotation for each action instance. Existing methods adopt the frame-level prediction paradigm to learn from the sparse single-frame labels. However, such a framework inevitably suffers from a large solution space. This paper attempts to explore the proposal-based prediction paradigm for point-level annotations, which has the advantage of more constrained solution space and consistent predictions among neighboring frames. The point-level annotations are first used as the keypoint supervision to train a keypoint detector. At the location prediction stage, a simple but effective mapper module, which enables backpropagation of training errors, is then introduced to bridge the fully-supervised framework with weak supervision. To our best of knowledge, this is the first work to leverage the fully-supervised paradigm for the point-level setting. Experiments on THUMOS14, BEOID, and GTEA verify the effectiveness of our proposed method both quantitatively and qualitatively, and demonstrate that our method outperforms state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Temporal action localization (TAL), which localizes actions from untrimmed videos, plays an important role in video understanding. Recently, the fully-supervised setting has achieved impressive results <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b37">38]</ref>, but the precise action boundary annotations are timeconsuming and hence expensive. The video-level weaksupervised setting <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b16">17]</ref> only requires cheaper category labels for localization, but the lack of explicit location guidance limits its performance highly infe- rior to the fully-supervised counterpart.</p><p>To bridge the performance gap between fully-supervised setting and video-level weakly-supervised setting while maintaining low annotation overhead, a point-level supervision setting (abbreviated as PTAL) is introduced <ref type="bibr" target="#b17">[18]</ref>, which provides a single frame (point) annotation for each action instance during training. To reduce the label sparsity associated with the single-frame annotation, pseudo label mining is employed to increase the number of labeled action frames and background frames through a self-training like expansion strategy. However, the labels obtained from such expansion strategy are usually incomplete and imprecise due to the trade-offs between the quality and quantity of such pseudo labels. To learn from these pseudo labels, SF-Net <ref type="bibr" target="#b17">[18]</ref> adopts the frame-level prediction paradigm as illustrated in <ref type="figure" target="#fig_0">Figure 1 (a)</ref>, which enables each frame to independently make a prediction and independently be evaluated. Such a framework inevitably suffers from a large solution space, similar to other weakly-supervised frameworks <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22]</ref>. As a result, predicting frame-level probabilities with such pseudo-labeled frames has led to high false-positive rates and discontinuous actions.</p><p>In this paper, we attempt to explore the proposal-based prediction paradigm <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b34">35]</ref> widely adopted in the fully-supervised setting for PTAL. As shown in <ref type="figure" target="#fig_0">Figure 1 (b)</ref>, instead of predicting frame-level probabilities, the proposalbased paradigm generates proposals based on anchor points to represent action instances. The proposal location is restricted near the anchor point, and the action probabilities of frames within the same proposal are naturally constrained to be consistent, thus greatly reducing the solution space. Following the proposal-based paradigm, we regard the pointlevel annotations as the keypoint supervision, then train a keypoint detector to identify action anchor points as the rough location of each action instance. For each anchor point, similar to the fully-supervised setting, an action proposal in terms of the action center and its corresponding length is directly predicted. However, it is non-trivial to supervise the prediction of action proposals with category weak labels due to the lack of boundary locations.</p><p>A straight-forward choice is to turn the fully-supervised proposal, i.e., action center and its length, into the proposallevel classification probability, so that the supervision in terms of category labels can be leveraged. Inspired by the attention mechanism <ref type="bibr" target="#b31">[32]</ref>, we propose to transform the proposal location into a binary temporal mask which is further used as temporal attention to obtain the proposal-level classification probability. While the mapping between the location of the action proposal and the mask seems mathematically simple, it is unfortunate that such a direct transformation function is non-differentiable. To back-propagate training errors for optimization, we further design a simple but novel mapper module, which learns the transformation from simulated data. Through the above design, we bridge fully-supervised proposals to weakly-supervised labels. To our best of knowledge, this is the first work to leverage the fully-supervised paradigm for the point-level setting.</p><p>On three benchmark datasets, BEOID <ref type="bibr" target="#b3">[4]</ref>, GTEA <ref type="bibr" target="#b10">[11]</ref> and THUMOS14 <ref type="bibr" target="#b5">[6]</ref>, our method outperforms previous state-of-the-art methods, both quantitatively and qualitatively. We further perform extensive ablation analyses and comparisons to reveal the effectiveness of each component. In summary, our contributions are as follows. (1) We introduce the proposal-based prediction paradigm to the pointlevel supervision setting for temporal action localization, which greatly reduces the issues of discontinuous action predictions and false positives. <ref type="bibr" target="#b1">(2)</ref> We propose a simple and effective mapper to bridge fully-supervised proposals to weak supervision, hence enabling the paradigm to be compatible with weak and full supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Fully-supervised temporal action localization, which requires precise action boundary annotations, has made great progress recently. The popular solution is to generate proposals representing action instances first, then classify them. There are two main paradigms for generating proposals, namely top-down framework <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b2">3]</ref> and bottomup framework <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b12">13]</ref>. The former generates sufficient anchor points on the video through sliding windows, and produces a proposal for each anchor point based on the preset length. A regressor is utilized to adjust the proposal boundaries. The latter trains a detector to search extreme points (such as boundary points, center points) as action anchor points, then combines extreme points or performs length predictions to generates proposals. Both kinds of proposal-based paradigms employ anchor points to identify the rough locations of action instances, and constrain the consistency within proposals, thus greatly reducing the solution space. However, since all these methods require huge annotation costs, they are not compatible with weak supervision settings and cannot be widely used in reality.</p><p>Weakly-supervised temporal action localization is proposed to reduce high annotation costs. The most widely used is video-level category labels, and the corresponding methods are divided into two branches. The MIL-based framework <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22]</ref> first trains a video-level classifier, then obtains frame-level action probabilities by checking the produced Class Activation Sequence (CAS). The Attention-based framework <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b16">17]</ref> directly predicts frame-level action probabilities from raw data, which is regarded as attention to calculate video-level classification probabilities for model optimization. Besides, the number of action instances <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b33">34]</ref> has also been explored to provide more action information. Nevertheless, all above methods rely on insufficient category or count labels to predict frame-level action probabilities and require the empirically preset threshold to post-process the probabilities for localization results. Therefore, they are all troubled by serious background false positives and incomplete action predictions <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b15">16]</ref>, which lead to a huge performance gap between them and fully-supervised methods.</p><p>Point-level supervision is widely used to balance labeling costs and model performance. In image semantic segmentation task, WTP <ref type="bibr" target="#b0">[1]</ref> introduced the point-level setting by annotating a single pixel for each instance. PDML <ref type="bibr" target="#b26">[27]</ref> followed this setting and took advantage of these point labels for metric learning. In object counting task, CLPS <ref type="bibr" target="#b7">[8]</ref> designed novel split-level loss and false-positive loss based on point-level annotations. In video tasks, marking one spa-  <ref type="figure">Figure 2</ref>. Framework pipeline. The model is trained in two sequential stages. In stage (a), the keypoint detector is trained via point-level annotations to evaluate the key probability of each timestamp. The peaks in the key heatmap are regarded as anchor points to segment the video into several short videos. In stage (b), the location predictor outputs a proposal for each short video. Then the pre-trained mapper is frozen to transform the proposal location into a binary temporal mask. The resulting masked features are fed into the classifier to construct loss functions with weakly-supervised category labels. tial location in the frame for each instance was first proposed by SPOT <ref type="bibr" target="#b18">[19]</ref> to improve spatial-temporal localization. ARST <ref type="bibr" target="#b20">[21]</ref> annotated a single timestamp of each action instance for action recognition. SF-Net <ref type="bibr" target="#b17">[18]</ref> extended it to temporal action localization and obtained certain improvements. However, they use insufficient labels to predict the action probability for each frame, causing serious discontinuous action predictions and background false positives. On the contrary, we introduce the proposal-based paradigm to constrain and reduce the solution space, and design a novel mapper to bridge the paradigm to category weak labels, effectively tackling their issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>We set point-level supervision following <ref type="bibr" target="#b17">[18]</ref>: for each action instance in N untrimmed training videos, we provide it with one timestamp t and action category y, where y ? R C and C is the total number of action categories. For each testing video, we are expected to predict a set of action instances in the form of the start time, the end time and the action category. Notably, each video can contain multiple categories and multiple action instances.</p><p>Due to the great variation in video lengths, we first sample T consecutive snippets from each video, then generate the RGB and flow features using the pre-trained extractor. By concatenating two-stream features, we obtain the video feature map X ? R T ?D , where D is the feature dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>The absence of complete labels in PTAL makes the popular frame-level probability prediction paradigm confused in a large solution space, causing serious discontinuous action predictions and false positives. To solve these issues, we introduce the proposal-based prediction paradigm in full supervision, to add more prediction constraints and reduce the solution space. As illustrated in <ref type="figure">Figure 2</ref>, to identify the rough locations of action instances, we first generate some action anchor points via a keypoint detector supervised by point-level annotations (stage (a)). Each anchor point indicates an action instance, thereby limiting action regions and eliminating false positives. Then for each anchor point, we predict the action center and action length to form a proposal (stage (b)), ensuring the continuity of predictions. To optimize this fully-supervised paradigm with weakly-supervised labels, we further design a novel mapper module to transform the proposal location into a binary temporal mask, and sequentially classify the masked videos to construct supervision with category labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Keypoint Detection</head><p>Keypoint detector. The goal of keypoint detector is to identify the rough action locations through some an-chor points, thereby eliminating background false positives. However, there are no precise location labels, but only point-level annotations available in PTAL. Fortunately, we observe that annotators tend to point out the discriminative moments for action instances. Hence, these annotated points are keypoints of actions, providing explicit guidance for distinguishing discriminative action points from the background, even if they are similar in appearance.</p><p>With such point-level annotations, we learn a keypoint detector (implemented by a fully convolutional network) to evaluate the key probability of each timestamp in the video. The detector is fed with video features X, and output key-</p><formula xml:id="formula_0">point estimate heatmapK ? R T ?C .</formula><p>For the training labels, if a video frame is selected as the annotation point, it is regarded as a positive sample; otherwise, it is treated as a negative sample. And we follow <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b13">14]</ref> to define the keypoint loss with the weighted cross-entropy:</p><formula xml:id="formula_1">L key = 1 T + t?? + H(k t ,k t ) + 1 T ? t?? ? H(k t ,k t ),<label>(1)</label></formula><p>where k t ? R C andk t ? R C are the ground truth and estimated key probability of the t-th frame, H denotes the regular cross-entropy loss, ? + and ? ? mean the positive and negative sample sets, T + and T ? mean the number of positive and negative samples, respectively. Keypoint generation. From the keypoint heatmap, we get anchor points by mining local maximum points. For any timestamp, we regard it as a keypoint if its key probability peaks in the heatmap and exceeds the set threshold ?. The filtered timestamps are then sorted and grouped into a candidate keypoint set P = {p j } Np j=1 , where N p is the number of keypoints. Each keypoint correspondings to a unique action instance, and there is the background between any two keypoints to divide the corresponding instances.</p><p>Due to the absence of precise location labels, it is difficult to process videos with a different number of action instances. Since keypoints naturally divide different instances, we propose to separate the entire video into several short videos based on the keypoint set P, to ensure that each short video only contains a complete instance. Formally, for the j-th keypoint p j in the set, we set the temporal interval of its corresponding short video as [p j?1 + 1, p j+1 ? 1]. To fix the temporal length, we rescale each short video to T s frames by linear interpolation. Next, we predict the action location for each short video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Location Prediction</head><p>In the location prediction stage, we expect to predict an action proposal for each short video, and apply category labels for optimization. Concretely, we generate the proposal with a location predictor, then utilize a novel map-  per to transform the proposal location into a binary temporal mask. Regarding the mask as weights, we calculate the foreground action features and background features, and finally classify them with a classifier. Location predictor. The goal of location predictor is to generate proposals indicating action locations based on the keypoints, to ensure the continuity of predictions. As we are not confident whether the keypoint is at the center of the action, we form a proposal by the action length and the offset between the keypoint and the center point. Formally, we input the raw features X s ? R Ts?D of the short video into the location predictor, and obtain the proposal v = [?p + p, l], where p, ?p, l represent the keypoint location, the center offset and the action length. The temporal boundaries of the proposal are given by:</p><formula xml:id="formula_2">r a = ?p + p ? l 2 r b = ?p + p + l 2 ,<label>(2)</label></formula><p>where r a and r b denote the left and right boundaries.</p><p>Mapper. To optimize the location predictor only with classification supervision, we are urgent to calculate the proposal-level category probability according to the proposal v. Inspired by the attention mechanism <ref type="bibr" target="#b31">[32]</ref>, we propose to transform the proposal location into a binary temporal mask m ? {0, 1} Ts , then produce the proposal-level features through the mask for subsequent classification. Although mathematically simple, such a direct transformation is non-differentiable, which makes it infeasible to backpropagate training errors for model optimization.</p><p>To tackle this, we leverage a simple but effective mapper module (constructed by the Multi-Layer Perceptron) to fit this transformation in advance, then freeze the pre-trained weights of the mapper during the training of location prediction stage, thus ensuring the accurate transformation and maintaining the error back-propagation. To train the mapper, all we need is to generate enough paired data defined in <ref type="figure" target="#fig_2">Figure 3</ref>. We first randomly generate lots of simulated proposals, representing action centers and action lengths re-spectively, as the input data. Then for each proposal, we define a T s -dimensional binary temporal mask as its ground truth label. In specific, we assign the positive foreground for all frames whose temporal locations are inside the proposal interval, and the negative background for all frames whose temporal locations are outside the interval. The binary mask m t of t-th frame is formalized as follows:</p><formula xml:id="formula_3">m t = 1, if t ? [r a , r b ] 0, if t / ? [r a , r b ]<label>(3)</label></formula><p>We also employ the weighted cross-entropy loss to optimize the mapper module.</p><formula xml:id="formula_4">L map = 1 T + s t?? + H(m t ,m t ) + 1 T ? s t?? ? H(m t ,m t )<label>(4)</label></formula><p>wherem t is the mapper output of the t-th frame, H is the regular cross-entropy loss, ? + and ? ? denote the positive and negative sample sets, T + s and T ? s are the number of positive and negative samples.</p><p>Classifier. In this module, we perform foreground classification and background modeling to better distinguish foreground actions and background. As ideal attention, the binary mask m ? {0, 1} Ts excellently identifies the foreground location. Hence, we regard it as weights to filter out all the foreground action features, then perform average pooling over these filtered features to calculate the proposallevel foreground feature x fg ? R D :</p><formula xml:id="formula_5">x fg = 1 T s Ts t=1 m t x t .<label>(5)</label></formula><p>Similarly, the complement weights 1 ? m can be utilized to indicate the background location, and the proposallevel background feature x bg ? R D is obtained by:</p><formula xml:id="formula_6">x bg = 1 T s Ts t=1 (1 ? m t )x t ,<label>(6)</label></formula><p>where x t ? R D is the short video feature of t-th frame. After that, we input these two features into the same classifier (built by the Multi-Layer Perceptron) to predict the foreground classification probability? fg and the background aware probability? bg . Note that we adopt C action categories and one background category for classification, hence? fg ? R C+1 and? bg ? R C+1 . To optimize the classifier, we apply the regular crossentropy loss between the predicted category probabilities and the corresponding ground truth category labels:</p><formula xml:id="formula_7">L cls = L bg + ?L fg = H(y bg ,? bg ) + ? C c=1 H(y c fg ,? c fg ),<label>(7)</label></formula><p>where H means the regular cross-entropy loss, ? is a tradeoff hyperparameter, y fg = [y 1 , ..., y C , 0] T ? R C+1 and y bg = [0, ..., 0, 1] T ? R C+1 are the foreground and background category labels, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Inference</head><p>At testing time, different from previous methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b23">24]</ref>, our method is elegant without the need for postprocessing, e.g., non-maxima suppression (NMS). For a given video, we first utilize the keypoint detector to predict its key heatmap, then extract the peaks in the heatmap as action anchor points. Based on these points, we divide the entire video into several short videos, each containing only one keypoint. Afterward, for each short video, we feed it into the location predictor to calculate the start time and the end time of the action proposal, and use the classifier to predict the proposal category. Each proposal is scored with the corresponding keypoint probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation</head><p>We conduct experiments on the following three datasets. For the sake of fairness, we adopt the single-frame annotations labeled in <ref type="bibr" target="#b17">[18]</ref> to provide point-level location supervision for each action instance during training.</p><p>THUMOS14 <ref type="bibr" target="#b5">[6]</ref> contains 413 untrimmed sports videos, which belong to 20 action categories. Following the convention, we train on 200 validation videos and evaluate on 213 testing videos. There are 3007 point-level annotations available for training and each video contains an average of 15 action instances. Besides, the actions and videos vary widely in length, making this dataset particularly challenging. BEOID <ref type="bibr" target="#b3">[4]</ref> covers 58 videos in 34 action categories. According to <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b20">21]</ref>, we set the proportion of training and testing videos to 80-20%, and obtain a total of 594 pointlevel annotations. GTEA [11] records 7 fine-grained actions in the kitchen. There are 28 videos in total, divided into 21 videos for training and 7 videos for testing. Each training video contains 17.5 point-level labels on average.</p><p>Evaluation Metrics. We follow the standard protocols to evaluate with mean Average Precision (mAP) under different intersection over union (IoU) thresholds. And a proposal is regarded as positive only if both IoU exceeds the set threshold and the category prediction is correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Feature Extraction. Following previous literature <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b16">17]</ref>, we first split each untrimmed video into 16frame non-overlapping snippets, then extract the optical flow from RGB data via the TV-L1 algorithm <ref type="bibr" target="#b32">[33]</ref>. For fair comparison, we adopt the classic two-stream I3D <ref type="bibr" target="#b1">[2]</ref>  network as the feature extractor and fix the parameters pretrained on Kinetics dataset <ref type="bibr" target="#b1">[2]</ref>. After obtaining RGB and flow features, we integrate them in an early-fusion fashion, and get a 2048-dimensional vector for each snippet. The number of snippets T is fixed to 2500, 360 and 128 for THUMOS14, BEOID and GTEA, respectively.</p><p>Parameter settings. For all datasets, our method is trained by the Adam optimizer <ref type="bibr" target="#b6">[7]</ref> with a learning rate of 10 ?4 . To train the mapper, we simulate one million paired data and employ the Adam optimizer with a learning rate of 10 ?5 . For the trade-off hyperparameter ? in Eq. 7, we set it to 1.25 on THUMOS14 and BEOID, and 2 on GTEA. And the threshold ? in keypoint generation is set to 0, 0.01 and 0.15 for GTEA, BEOID and THUMOS14, respectively. To eliminate the high-frequency noise in the keypoint heatmap, we employ the Savitzky-Golay filter <ref type="bibr" target="#b25">[26]</ref> for smoothing. The specific network architectures and more details are reported in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation of Core Modules</head><p>In this section, we evaluate the effectiveness of keypoint detection and proposal-based location prediction. Firstly, the baseline is set to predict the action probability for each frame through point-level annotations without keypoint detection. We adopt the frame classification loss and video classification loss in <ref type="bibr" target="#b17">[18]</ref> for optimization, and use the classic threshold post-processing <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b15">16]</ref> to produce the final predictions. Secondly, we add the keypoint detec- tion to the baseline, and only retain the predictions containing keypoints. For the predictions with multiple keypoints, we divide them by regarding keypoints as boundaries. Finally, we continue to retain the keypoint detection and replace the frame-level probability prediction paradigm with the proposal-based location prediction paradigm, to form our complete method. Note that the proposal-based prediction paradigm cannot stand alone without keypoints, as keypoints act as anchor points for actions.</p><p>The mAP results are summarized in <ref type="table">Table 1</ref>. We can obverse that the baseline performs worst among these three methods. And with the help of keypoint detection, there yields a stable increase in performance, with a gain of 2.1% on the average mAP. This observation indicates that detecting keypoints as action anchor points indeed eliminates many false positives. With anchor points, the detector limits action localization regions, thus greatly reducing the solution space and suppressing background activation. Moreover, even if only supervised by classification, the proposal-based prediction paradigm outperforms the frame-level probability paradigm by a large margin. We attribute the improvement to more complete action predictions. By directly generating the action length, the proposalbased paradigm naturally ensures the continuity of predictions, thereby further constraining the solution space and stably outputting high-quality predictions. Furthermore, to explore the performance upper bound of our method, we replace point-level labels with precise boundary labels to evolve into a fully-supervised setting. There emerge significant boosts in performance, especially at high IoU thresholds, revealing the wide compatibility of our method. <ref type="table">Table 5</ref>. Comparison with the state-of-the-art methods on THUMOS14. AVG(0.1-0.5) and AVG(0.3-0.7) are the average mAP from IoU 0.1 to 0.5 and from IoU 0.3 to 0.7, respectively. The superscript ? means that point-level labels are annotated manually. And the superscript ? indicates that point-level labels are simulated from ground truth boundary annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervision</head><p>Method mAP@IoU AVG (0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Studies and Comparisons</head><p>Ablation studies of the location prediction stage. The foreground classification loss L fg , the background aware loss L bg and the center offset ?p are three important components. To investigate their contributions, we perform experiments on THUMOS14 and report the results in <ref type="table">Table 2</ref>.</p><p>(Without ?p, we treat the keypoint as the action center.)</p><p>Consistent with existing background modeling methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b9">10]</ref>, the background aware loss brings consider-able improvement, with a gain of 6.4% in mAP@0.5. By modeling the auxiliary background category, our method is explicitly guided to distinguish actions from the background, resulting in more precise predictions. Interestingly, predicting the center offset only leads to a slight improvement of 0.7% in mAP@0.5. We conjecture that it is because classification supervision focuses more on the discriminative action regions, while has limited guidance on the action center. Nevertheless, all components are effective and essential to achieve the best performance.</p><p>Comparison of different label distributions. Due to some errors in manual point-level annotations, SF-Net <ref type="bibr" target="#b17">[18]</ref> explores to simulate labels via sampling on THUMOS14. For a comprehensive comparison, we follow the same settings to sample from existing ground-truth boundary labels through uniform distribution and Gaussian distribution, then obtain the simulated point-level annotations.</p><p>The detailed experimental results are shown in <ref type="table" target="#tab_2">Table 3</ref>. No matter which distribution is used, our method achieves gratifying results, revealing the generality of our method. Moreover, it can be found that our method performs best on Gaussian distribution and worst on uniform distribution, which is the opposite of SF-Net. We infer the reasons as follows. The proposal-based prediction paradigm makes our performance highly correlated with the action center. As  discussed before, classification supervision only provides limited guidance to adjust the action center. Hence, keypoint detection plays a decisive role in finding the action center. Among the three distributions, Gaussian distribution can usually produce the sampling labels closest to the middle timestamps of actions, which causes the predicted keypoints nearest to action centers, and causally achieves the best performance in our method. By contrast, uniform distribution provides more temporal boundary information, which is more beneficial for the frame-level probability prediction paradigm, and more suitable for SF-Net. Comparison of statistics. To further confirm our efficacy, we collect some statistics based on the final localization results. As the existing state-of-the-art method in PTAL, SF-Net <ref type="bibr" target="#b17">[18]</ref> is chosen for comparison. The results in <ref type="table">Table 4</ref> suggest that our method greatly reduces the false alarm and improves the precision accordingly, indicating that false positives are significantly suppressed. Moreover, a boost in recall demonstrates that our method effectively reduces omissions and detects more complete actions. <ref type="table">Table 5</ref> compares our method with current state-of-theart approaches on THUMOS14. Using the same point-level annotations, our method outperforms other methods by a large margin regardless of the label distribution. It can be observed that there still exists a large performance gap of more than 10% average mAP between previous pointsupervised methods and fully-supervised methods. Benefits from the great reduction of the solution space, our method gains a substantial improvement and bridges the gap to 6% average mAP. Moreover, at some low IoU thresholds, our method is even comparable to several fully-supervised counterparts <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b37">38]</ref>. However, due to the lack of precise location supervision, our performance drops significantly as the IoU threshold increases. Furthermore, our results under the fully-supervised setting are also demonstrated. In general, our method outperforms existing approaches at IoU thresholds 0.1, 0.2 and 0.3, while performs comparably to these methods at higher IoU thresholds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison with state-of-the-art methods</head><p>We also present the quantitative comparison on GTEA and BEOID in <ref type="table">Table 6</ref>. SF, SFB and SFBA, which are three benchmark models designed in SF-Net <ref type="bibr" target="#b17">[18]</ref>, are included for comparison. On GTEA, our method achieves a new state-of-the-art performance, reaching 33.7% average mAP. On BEOID, our method surpasses the best competitor by more than 4.5% on the average mAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Qualitative Results</head><p>To demonstrate the superiority of our method intuitively, we visualize several qualitative results in <ref type="figure" target="#fig_4">Figure 4</ref>. We also reproduce the results of SF-Net <ref type="bibr" target="#b17">[18]</ref> for better comparison. As is evident, our method detects more precise and complete actions than SF-Net. More specifically, the framelevel probability curve of SF-Net has poor continuity and serious background noise, leading to inferior results. On the contrary, by detecting keypoints as action anchor points, our method effectively eliminates false positives. By directly predicting the center offset and action length, our method constrains continuity and obtains more complete results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we introduced the proposal-based prediction paradigm for PTAL. We trained a keypoint detector to discover action anchor points and rule out false positives. Based on each anchor point, we directly predicted the action length and center offset to form a proposal, ensuring the continuity of predictions. To bridge this paradigm with weak supervision, we further designed a mapper module to transform the proposal location into a binary mask so that the model can be optimized by category labels. Extensive Experiments on three benchmarks have verified the effectiveness and superior performance of our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Comparison of two prediction paradigms. (a): the framelevel paradigm uses partial pseudo labels to supervise the action probability for each frame. (b): the proposal-based paradigm directly predicts the action center and action length, and has a smaller solution space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Paired training data of the mapper. The input is a two-dimensional proposal, indicating the action center and action length. The label is a Ts-dimensional binary mask, indicating the temporal interval of the proposal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative comparison with SF-Net<ref type="bibr" target="#b17">[18]</ref> on THUMOS14. The first two rows are frame-level probabilities and localization results of SF-Net, the third row is the ground truth action intervals, and the last two rows are keypoint probabilities and localization results of our method. Left: Our method detects keypoints as anchor points, effectively eliminating false positives in SF-Net. Right: SF-Net suffers from discontinuous probabilities and gets scattered action fragments. While our method directly predicts action lengths and obtains more complete actions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Evaluation of core modules on THUMOS14. 'Key' indicates whether to detect keypoints. Two location prediction paradigms: frame-level probability and proposal-based prediction are abbreviated as 'frame' and 'proposal'. AVG is the average mAP at IoU thresholds 0.1:0.1:0.7. Ablation studies of the location prediction stage on THU-MOS14. ?p is the center offset, L fg and L bg are the foreground and background losses in Eq. 7. AVG means the average mAP at</figDesc><table><row><cell cols="4">Setting Key Paradigm</cell><cell>0.3</cell><cell>mAP@IoU 0.5</cell><cell>0.7</cell><cell>AVG</cell></row><row><cell></cell><cell>no</cell><cell>frame</cell><cell></cell><cell cols="2">51.7 29.3 9.2</cell><cell>39.6</cell></row><row><cell>Point</cell><cell>yes</cell><cell>frame</cell><cell></cell><cell cols="2">55.2 30.7 9.8</cell><cell>41.7</cell></row><row><cell></cell><cell>yes</cell><cell cols="4">proposal 58.1 34.5 11.9 44.3</cell></row><row><cell>Full</cell><cell>yes</cell><cell cols="4">proposal 60.1 39.2 18.4 48.3</cell></row><row><cell cols="3">IoU thresholds 0.1:0.1:0.7.</cell><cell></cell><cell></cell></row><row><cell cols="3">L fg L bg ?p</cell><cell>0.3</cell><cell cols="2">mAP@IoU 0.5</cell><cell>0.7</cell><cell>AVG</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">51.9 27.2</cell><cell>8.0</cell><cell>38.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">57.1 33.8 11.5</cell><cell>43.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">53.0 28.1</cell><cell>8.7</cell><cell>39.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">58.1 34.5 11.9</cell><cell>44.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison of different label distributions on THU-MOS14. AVG denotes the average mAP at IoU thresholds 0.1:0.1:0.7. The labels are generated by simulation.</figDesc><table><row><cell>Method</cell><cell>Distribution</cell><cell>0.3</cell><cell cols="2">mAP@IoU 0.5</cell><cell>0.7</cell><cell>AVG</cell></row><row><cell></cell><cell>Manual</cell><cell cols="4">53.3 28.8 9.7</cell><cell>40.6</cell></row><row><cell>SF-Net [18]</cell><cell>Uniform</cell><cell cols="5">52.0 30.2 11.8 40.5</cell></row><row><cell></cell><cell>Gaussian</cell><cell cols="4">47.4 26.2 9.1</cell><cell>36.7</cell></row><row><cell></cell><cell>Manual</cell><cell cols="5">58.1 34.5 11.9 44.3</cell></row><row><cell>Ours</cell><cell>Uniform</cell><cell cols="5">55.6 32.3 12.3 42.9</cell></row><row><cell></cell><cell>Gaussian</cell><cell cols="5">58.2 35.9 12.8 44.8</cell></row><row><cell cols="7">Table 4. Statistical comparison of final localization results on</cell></row><row><cell cols="7">THUMOS14. False Alarm, Precision, Recall, and F-measure are</cell></row><row><cell cols="3">calculated under IoU threshold 0.5.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="6">False Alarm Precision Recall F-measure</cell></row><row><cell>SF-Net [18]</cell><cell>73.3</cell><cell cols="2">26.7</cell><cell cols="2">51.8</cell><cell>35.1</cell></row><row><cell>Ours</cell><cell>59.5</cell><cell cols="2">40.5</cell><cell cols="2">57.6</cell><cell>47.4</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">What&apos;s the point: Semantic segmentation with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Bearman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="549" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rethinking the faster r-cnn architecture for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1130" to="1139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">You-do, i-learn: Discovering task relevant objects and their modes of interaction from multi-user egocentric video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teesid</forename><surname>Leelasawassuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osian</forename><surname>Haines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Calway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walterio W Mayol-Cuevas</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Turn tap: Temporal unit regression network for temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3628" to="3636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Thumos challenge: Action recognition with a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roshan</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Where are the blobs: Counting by localization with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Issam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Negar</forename><surname>Laradji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">O</forename><surname>Rostamzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="547" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Background suppression network for weakly-supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pilhyeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjung</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeran</forename><surname>Byun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11320" to="11327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Background modeling via uncertainty estimation for weakly-supervised action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pilhyeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeran</forename><surname>Byun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07006</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Temporal deformable residual networks for action segmentation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Graph attention based proposal 3d convnets for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuofan</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanru</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkuan</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4626" to="4633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast learning of temporal action proposal via dense boundary generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11499" to="11506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bmn: Boundary-matching network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="3889" to="3898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bsn: Boundary sensitive network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haisheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Completeness modeling and context separation for weakly supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daochang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Weakly supervised temporal action localization through contrast based evaluation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanning</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenxing</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="3899" to="3908" />
		</imprint>
	</monogr>
	<note>Nanning Zheng, and Gang Hua</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sf-net: Single-frame supervision for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengxin</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gourab</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="420" to="437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spot on: Action localization from pointly-supervised proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="437" to="453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adversarial background-aware loss for weakly-supervised temporal activity localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="283" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Action recognition from single timestamp supervision in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">3c-net: Category count and center loss for weakly-supervised action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanath</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisham</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Weakly supervised action localization by sparse temporal pooling network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phuc</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Weakly-supervised action localization with background modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Phuc Xuan Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless C</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="5502" to="5511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Wtalc: Weakly-supervised temporal activity localization and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujoy</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourya</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit K Roy-Chowdhury</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="563" to="579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Savitzky-golay smoothing filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Teukolsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in Physics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="669" to="672" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Weakly supervised scene parsing with point-based distance metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8843" to="8850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Weakly-supervised action localization by generative attention modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baifeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cdc: Convolutional-deconvolutional networks for precise temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5734" to="5743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Autoloc: Weakly-supervised temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="154" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An improved algorithm for tv-l 1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Wedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Statistical and geometrical approaches to visual motion analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="23" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Segregated temporal assembly recurrent networks for weakly supervised multiple action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanzhan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Revisiting anchor mechanisms for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Two-stream consensus network for weakly-supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhao</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="37" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bottom-up temporal action localization with mutual regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peisen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2914" to="2923" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
