<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Two Stream 3D Semantic Scene Completion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Garbade</surname></persName>
							<email>garbade@iai.uni-bonn.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Bonn</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueh-Tung</forename><surname>Chen</surname></persName>
							<email>yuehtung.chen@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johann</forename><surname>Sawatzky</surname></persName>
							<email>sawatzky@iai.uni-bonn.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Bonn</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
							<email>gall@iai.uni-bonn.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Bonn</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<addrLine>2 b-it</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Two Stream 3D Semantic Scene Completion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Inferring the 3D geometry and the semantic meaning of surfaces, which are occluded, is a very challenging task. Recently, a first end-to-end learning approach has been proposed that completes a scene from a single depth image. The approach voxelizes the scene and predicts for each voxel if it is occupied and, if it is occupied, the semantic class label. In this work, we propose a two stream approach that leverages depth information and semantic information, which is inferred from the RGB image, for this task. The approach constructs an incomplete 3D semantic tensor, which uses a compact three-channel encoding for the inferred semantic information, and uses a 3D CNN to infer the complete 3D semantic tensor. In our experimental evaluation, we show that the proposed two stream approach substantially outperforms the state-of-the-art for semantic scene completion.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Humans quickly infer the 3D semantics of a scene, i.e., an estimate of the 3D geometry and the semantic meaning of the surfaces. While RGB-D sensors in combination with CNNs provide geometry and semantic information, the resulting representation is very sparse since large parts of the 3D scene are occluded and not visible. The perception, however, is not limited to the visible part of the scene. When looking at a mug on a table, a human can estimate the full geometry of both objects including parts which are invisible since they are occluded by the objects themselves. This information is obtained from semantic understanding of the scene which allows to estimate the spatial extent of the objects from experience. Such an ability is highly desirable for autonomous agents, e.g., to navigate or interact with objects. A robot that has an intuition about the geometry behind the surface it sees, for example, could plan ahead given a single view instead of exhaustively explore the occluded parts of a scene first.</p><p>In this work, we aim to estimate the semantics not only of the visible part, but of the entire scene including the occluded space. To this end, we build on the work of Song et al. <ref type="bibr" target="#b35">[36]</ref>. They show that semantic scene understanding and 3D scene completion benefit from each other. On one hand, recognizing a part of the object helps to estimate its location in the 3D space and the voxels it occupies. On the other hand, knowing the occupancy in the 3D space gives information on form and size of the object and thus facilitates semantic recognition. For estimating for each voxel in the scene the occupancy and semantic label, they proposed an end-to-end trainable 3D convolutional neural network (3D CNN) which incorporates context from a large field of view via dilated convolutions. The approach, however, only uses depth as input and neglects the RGB image. This means that the semantic label has to be inferred from the geometry alone and properties such as color, texture, or reflectance are not taken into account.</p><p>We therefore extend the approach <ref type="bibr" target="#b35">[36]</ref> by keeping its beneficial context incorporation and end-to-end trainability while modifying it to leverage semantic information inferred from the RGB image at the input stage as well as at the loss. Given a single RGB-D image, we first use a 2D CNN to infer the semantic labels from the RGB data and construct an incomplete 3D semantic tensor. To this end, we map the inferred semantic labels to the 3D space and label each visible surface voxel by the inferred class label. The 3D semantic tensor is incomplete since it only contains the labels of the visible voxels but not of the occluded voxels. The 3D projection is performed using the depth image. The tensor is then used as input for a 3D CNN that infers a complete 3D semantic tensor, which includes the occupancy and semantic labels for all voxels.</p><p>Using the RGB images as input leads to a significant performance gain in scene completion and semantic scene completion as our experiments show. We outperform <ref type="bibr" target="#b35">[36]</ref> by a substantial margin of up to 9.4 % on NYU. This implies that RGB images provide a rich discriminative signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Several works address the problem of semantic segmentation of RGB-D images, e.g., <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b33">34]</ref>, but they infer semantic labels only for the visible pixels of the image, which means that occluded voxels are not reconstructed.</p><p>A possible strategy for semantic scene completion is the generation of 3D object proposals and subsequent 3D shape completion of the respective object. The problem of completing the 3D shape has been addressed in several works <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b6">7]</ref>. In the context of inferring a voxel-wise segmentation, holes between objects have to be filled. As long as these missing parts are small, they can be filled using plane fitting <ref type="bibr" target="#b27">[28]</ref> or object symmetry <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b25">26]</ref>. Objects that are not detected, however, heavily disturb the 3D scene completion. Completing the scene geometry without predicting the semantics has been addressed by <ref type="bibr" target="#b8">[9]</ref>. Their model assumes that objects of semantically dissimilar classes can still be represented by similar 3D shapes, i.e., it is possible to predict the unobserved voxels from the frontal geometry. However, this approach fails for complex scenes where these geometric constraints are violated.</p><p>An alternative is to use instances of 3D mesh models and fit them to the scene geometry <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b32">33]</ref>. The mesh models, however, do not model shape variations of objects of the same category and increasing the number of mesh models per category is not practical since the number of available CAD models is limited and the shape retrieval becomes more expensive. The approaches <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b23">24]</ref> even neglect fine-grained details and simply fit 3D primitives to the scene.</p><p>Various contextual cues proved to be helpful for semantic scene completion. While physical reasoning is employed in <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b17">[18]</ref> predict voxel labels with a conditional random field (CRF) whose unary potentials are determined by floor plans. The CRF, however, only models contextual information within a short distance. In <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b14">[15]</ref>, semantic scene completion and multi-view reconstruction are jointly performed. The approaches do not rely on end-to-end learning approaches, but they use predefined features and heuristics to integrate context information.</p><p>To facilitate learning of scene completion in an end-toend manner, <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b1">2]</ref> collected large scale datasets with real world data. Previously, synthetic datasets were employed to provide ground truth data for object completion <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b39">40]</ref> or for entire scenes <ref type="bibr" target="#b13">[14]</ref>. Due to these datasets, training of an end-to-end approach for semantic scene completion became feasible <ref type="bibr" target="#b35">[36]</ref>. Parallel to our work, <ref type="bibr" target="#b41">[42]</ref> have proposed a new network architecture which leverages sparse feature map encodings and allows for much deeper network architectures. While we address the problem of scene completion from a single viewpoint as in <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b41">42]</ref>, semantic scene completion from multiple RGB-D images is addressed in <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Two Stream Semantic Scene Completion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Semantic Scene Completion</head><p>The goal of 3D semantic scene completion is to classify every voxel in the view frustum into one of K + 1 labels c = c 0 , ..., c K where c 0 represents an empty voxel and c 1 , ..., c K represents one of K = 11 class labels like ceiling, floor, wall, window, chair, bed, sofa, table, tv, furniture and object. As illustrated in <ref type="figure">Figure 1</ref>, the camera observes only a part of the scene while other voxels are occluded. The occluded voxels can either be empty (c 0 ) or belong to one of the K classes.</p><p>To address the task of 3D semantic scene completion, we propose an approach that leverages two input streams, namely RGB and depth. An overview of the approach is given in <ref type="figure">Figure 2</ref> a). While the depth data is converted into a volumetric representation, the RGB image is first processed in a separate branch to infer 2D semantic segmentation maps and then transformed into a volumetric representation referred to as color-volume (Section 3.3). The volumetric representation is then fed to a 3D convolutional neural network (3D-CNN). The 3D-CNN infers a 3D semantic tensor where every voxel is classified as either being empty or belonging to one of the K semantic classes. In the following, each step will be discussed in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Depth Input Stream</head><p>To obtain the volumetric input encoding, the depth map is projected into a regular voxel grid using the camera pose, which is provided along with each image. The voxel grid is of size 240 x 144 x 240 voxels and encodes a scene of 4.80m horizontally, 2.88m vertically, and 4.80m in depth with a resolution of 0.02m. For every pixel in the depth map, its corresponding voxel in the 3D input volume is computed using the camera pose. The obtained binary voxel mask encodes the location of surface points that are visible to the camera, see <ref type="figure">Figure 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>b).</head><p>As pre-processing, all 3D scenes are rotated such that the room orientations are aligned. For indoor room scenes, one can assume that most of the observed surface normals are oriented either like the normals of the walls, floor or ceiling, which are usually planar. Therefore a principal component analysis of the surface normals is used to infer the room orientation, which is used to align the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Color Input Stream</head><p>The input RGB image is first processed by a 2D-CNN <ref type="bibr" target="#b3">[4]</ref>, which is an adaptation of the Resnet101 architecture <ref type="bibr" target="#b15">[16]</ref> for semantic segmentation. While all but one pooling layer are omitted, dilated convolutions are used to keep the output resolution high while simultaneously increasing the receptive field. The 2D-CNN predicts the softmax probabilities for every class and pixel at a resolution which is  <ref type="figure">Figure 1</ref>. Using the protocol of <ref type="bibr" target="#b35">[36]</ref>, ground truth labels are provided for all voxels of a 3D volume. Voxels that are outside the intersection of the camera frustum and ground truth volume are outside the room or outside the view and not taken into account. Within the intersection, there are observed surface voxels (green) and observed non-occupied voxels (light gray), but other voxels are not observed by the camera. These voxels are either non-occupied (blue) or belong to an object (black).</p><p>Depth RGB</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2D Sematic</head><p>Seg.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Binary Voxel Mask</head><p>Color Volume Semantic Volume  <ref type="figure">Figure 2</ref>. a) The proposed two stream approach for semantic scene completion transforms first the depth data and RGB image into a volumetric representation, which represents the geometry and semantic of the visible scene and then uses a 3D-CNN to infer a 3D semantic tensor for the entire scene. b) Given 2D depth map and camera pose, a binary voxel mask is created by setting each voxel that belongs to a depth pixel to one and all other voxels to zero (blue). c) Visualization of TSDF vs. flipped TSDF. One can see the long 'shadow' caused by the observed surface which produces high gradients at the occlusion boundary (between -1 and 1). In the flipped TSDF, this effect is suppressed. The gradient is highest at the surface.</p><formula xml:id="formula_0">3D-CNN ... ... ... ... ... ... ... ... ...</formula><p>four times smaller than the input image. The output is then upsampled to the original resolution of the image using bilinear interpolation. A densely connected CRF <ref type="bibr" target="#b19">[20]</ref> is then used in combination with the inferred class probabilities and the RGB image to refine the semantic segmentation map. For training, we use the same setting as in <ref type="bibr" target="#b3">[4]</ref>. As initialization, we use a model that is pre-trained on MSCOCO <ref type="bibr" target="#b24">[25]</ref> and fine-tune it on the dataset for 3D semantic scene completion. Furthermore, we present results using the more recent model Deeplab v3+ <ref type="bibr" target="#b4">[5]</ref> which is pretrained on ADE-20k and finetune it on NYUv2 using an initial learning rate of 0.001. We also apply a CRF <ref type="bibr" target="#b19">[20]</ref> on the resulting outputs.</p><p>As in Section 3.2, we convert the 2D segmentation map into a volumetric representation. Since each pixel in the depth map corresponds to a pixel in the 2D semantic seg-mentation map, every class pixel can be projected into the 3D volume at the location of its corresponding depth value. This yields an incomplete 3D semantic tensor that assigns to every surface voxel its corresponding class label. The class labels can be encoded by one-hot encoding, i.e., a channel for each class, or by a single channel for the class label. In our experiments, however, we show that none of them is optimal. Encoding semantic classes with only one channel implies a semantic proximity of classes by the numerical proximity of their class values, which introduces undesirable artifacts based on the class values. The one-hot encoding has the disadvantage that it is insufficient in terms of memory consumption since it requires to store a K dimensional vector per voxel. We therefore represent the semantic information by a lower dimensional vector. We use a three-dimensional vector and encode the classes linearly from (0, 0, 1) over (0, 1, 1), (0, 1, 0), (1, 1, 0) to (1, 0, 0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">3D-CNN</head><p>For the 3D-CNN, we adapt the architecture of <ref type="bibr" target="#b35">[36]</ref> by increasing the number of input channels of the first convolutional layer such that it fits to our input. The architecture is illustrated in <ref type="figure">Figure 3</ref>. It is inspired by the 2D-CNN for semantic segmentation. The major difference apart from using 3D instead of 2D convolutions is that the network only has a depth of 14 convolutional layers. The network has therefore significantly less parameters than its two dimensional counter part. Moreover, batch-normalization layers are omitted due to the small size of the batches.</p><p>We adapt the training protocol of <ref type="bibr" target="#b35">[36]</ref> as follows. We train for 150,000 steps with a learning rate of 0.01 that is reduced by a factor of 0.1 after 100,000 iterations. As optimizer, stochastic gradient (SGD) with momentum is applied. As initialization, we chose a random initialization with a Gaussian distribution with mean ? = 0 and a standard deviation of ? = 0.01.</p><p>The output of the 3D-CNN is a semantic tensor of size 60 x 36 x 60 x (K + 1), where K is the number of object classes and an additional class is added for empty voxels. We compute a softmax cross entropy loss on the unnormalized network outputs y:</p><formula xml:id="formula_1">L = ? i,c w ic?ic log ? ? e yic c ?C e y ic ? ?<label>(1)</label></formula><p>where? ic are the binary ground truth vectors, i.e.,? ic = 1 if voxel i is labeled by class c, and w ic are the loss weights.</p><p>Since the ratio of empty vs. occupied voxels is 9:1, the empty space is randomly subsampled. Therefore w ic is chosen as binary mask such that only 2N empty voxels are selected for loss calculation where N is the number of occupied voxels in the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation Metric</head><p>For evaluation, we follow the evaluation protocol of <ref type="bibr" target="#b35">[36]</ref>, which evaluates the accuracy on a subset of voxels. The evaluation considers only voxels that are part of the occluded space and within both the room and the field-of-view as shown in <ref type="figure">Figure 1</ref>. While generating the 3D semantic labels from the annotated CAD models, every voxel in the input volume is marked as being on surface, free space, occluded space, outside field of view, outside room or outside ceiling. For semantic scene completion, a binary evaluation mask is computed such that the evaluation metric is only computed for voxels which are either occluded, on surface or close to the surface (within the range of the TSDF function defined by <ref type="bibr" target="#b35">[36]</ref>). For scene completion another mask is computed which comprises all voxels in the occluded space. To assess the quality of 3D scene completion, several metrics are computed. First we compute the Jaccard index, which measures the intersection over union (IoU) between ground truth and predicted voxel for every object category c 1 , ..., c K . As an overall segmentation performance, we compute the average across all classes. For scene completion all voxels are considered to belong to one of the two classes empty vs. non-empty. All object categories c 1 , ..., c K are counted as 'non-empty'. For completion, IoU as well as precision and recall are computed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Datasets</head><p>We evaluate our method on the NYUv2 dataset, which is in the following denoted as NYU. NYU consists of indoor scenes that are captured via a Kinect sensor. For 3D semantic scene completion labels, we use the annotated 3D labels provided by <ref type="bibr" target="#b30">[31]</ref>. They provide 1449 scenes, annotated with 11 classes, 795 of which are used for training and 654 for testing. These annotations consist of CAD models that are fitted into the scene. Since the CAD models do not exactly fit the shape of the annotated objects and neglect small objects such as clutter, there is a significant mismatch between the Kinect input data and the output labels. To address this problem, depth maps generated from the projections of the 3D annotations as in <ref type="bibr" target="#b30">[31]</ref> are used for training. For evaluation, we consider two test sets. The first test set, which is denoted by NYU Kinect, consists of the depth maps from the Kinect sensor and the second test set, which is denoted as NYU CAD, uses the depth maps generated by projection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>We conduct an ablation study on Kinect to analyze the design choices of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Effect of Semantic Input</head><p>As mentioned in Section 3.3, we compare two network architectures for the 2D-CNN, namely Deeplab v2 (DLv2) <ref type="bibr" target="#b3">[4]</ref> and Deeplab v3+ (DLv3+) <ref type="bibr" target="#b4">[5]</ref>. <ref type="table">Table 1</ref> shows that DLv3+ increases the accuracy for semantic scene completion from 31.3 % to 33.8 %. This is expected since DLv3+ provides a better 2D segmentation accuracy compared to DLv2 as shown in <ref type="table" target="#tab_3">Table 2</ref>. For scene completion, IoU is slightly higher for DLv2 than for DLv3+.</p><p>We compare the results to a setting when we use ground truth semantic segmentation masks for the RGB images as input to the 3D-CNN, which is denoted by GT in <ref type="table">Table 1</ref>. This also serves as an upper bound for our method when the used 2D-CNN provides perfect segmentation masks. As expected, using ground truth segmentation masks improves the semantic scene completion compared to DLv3+  <ref type="figure">Figure 3</ref>. Architecture of the 3D-CNN. The parameters of the convolution kernels are denoted as (number of filters, kernel size, stride, dilation). All but the last convolution layer have a ReLU activation function assigned to it. Arrows indicate skip connections <ref type="bibr" target="#b15">[16]</ref> where the output of one convolution layer is added to another output at a later stage. Pool denotes max pooling. The output is a volume that is 4 fold downsampled with respect to the input of the 3D CNN and encodes for every voxel the probability of it being empty (label 0) or to belong to one of K semantic classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scene Completion</head><p>Semantic by +4.7 %. For scene completion, the improvements compared to DLv3+ are +1.0 %. This shows that the quality of the 2D-CNN has a strong impact on the accuracy of semantic scene completion but only a minor impact on scene completion, which is also not the focus of this work.</p><p>As it is illustrated in <ref type="figure">Figure 2 a)</ref>, the RGB images are processed by a 2D-CNN and the inferred pixel-wise labels are used to construct the semantic volume. We also evaluate in <ref type="table">Table 1</ref> what happens if the three-channel encoding is not based on the semantic labels but if the RGB values of the pixels are directly used for the encoding, i.e., without inferring semantic information from the visible part of the scene. This setting is denoted by 'RGB image' and performs as expected poorly. Besides of the class 'floor', all categories are poorly estimated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Input Encoding</head><p>As we discussed in Section 3.3, the encoding of the semantic information in the semantic volume should provide a numerical equidistance between classes, which can be achieved by using one-hot encoding. However, this approach has a high memory footprint. As an alternative, we evaluate a one-channel and a three-channel input encoding. In the one-channel setup, the numeric class values are normalized to the range from 0 to 1. For the proposed 3-channel input encoding, every label is mapped to a 3 dimensional vector as described in Section 3.3. <ref type="table">Table 3</ref> shows that using only one channel performs poorly since it introduces undesirable artifacts based on the class values. While some classes like 'floor' and 'bed' are well recognized, the accuracy for 'window' and 'tv' is very low. Using one-hot encoding (12 channels) performs much better than 1 channel but it is expensive in terms of memory consumption. The proposed three-channel encoding requires less memory while it only slightly decreases the accuracy. Also the training time of the 3 channel setup is by a factor of 1.7 faster which reduces the training time from 4 to 2.5 days. Therefore we adopt the 3 channel setup as it provides an efficient alternative to the one-hot encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Fusion with flipped TSDF</head><p>Furthermore, we have conducted an experiment where we combine our input with the flipped truncated signed distance function (fTSDF) proposed by <ref type="bibr" target="#b35">[36]</ref> and evaluate different fusing schemes.</p><p>The fTSDF is computed as follows: The previously computed binary voxel mask <ref type="figure">(Figure 2 b)</ref> is used to first compute a truncated signed distance function (TSDF) encoding as illustrated in <ref type="figure">Figure 2 c)</ref>. In the TSDF, every voxel contains as value the distance d to the next surface point. The sign of the distance value indicates whether a voxel lies in the empty (1) or occluded space (-1). The TSDF has the disadvantage of having high gradients at the occlusion boundary, i.e., the boundary between observed and unobserved ceil. floor wall win. chair bed sofa  <ref type="table">Table 3</ref>. Impact of the number of channels for the semantic volume. 12 channels refers to one-hot encoding.</p><p>space behind a surface. Therefore in the TSDF encoding every surface yields a shadow into the unobserved space as shown in <ref type="figure">Figure 2</ref> c).</p><p>To provide a more meaningful input signal, the signed distance function is transformed into a flipped TSDF <ref type="bibr" target="#b35">[36]</ref>, where every signed distance value d is converted into a distance d f which is 1 or -1 at a surface and linearly falls to 0 at a distance d max from the surface:</p><formula xml:id="formula_2">d f = sign(d)H(d max ? |d|) d max ? |d| d max<label>(2)</label></formula><p>where d max is the maximum distance of 24 cm and H is the Heaviside function:</p><formula xml:id="formula_3">H(x) = 1 if x ? 0 0 if x &lt; 0.<label>(3)</label></formula><p>We perform different fusion experiments to evaluate whether the proposed fTSDF encoding can give us a meaningful signal for semantic scene completion. As one can see from <ref type="figure">Figure 3</ref> the 3D-CNN consists of several blocks. We concatenate the fTSDF before block 1 (early fusion) and also after block 1, 2 and 5 (fusion 1, 2 and 5). Before concatenation, both the color and the fTSDF input stream are processed separately. In the case of "late fusion" we take the maximum of the softmax probabilities of both streams.</p><p>As can be seen from <ref type="table">Table 4</ref>, all fusion schemes perform slightly worse than our approach. This indicates that fTSDF provides a superfluous signal for our approach. This is interesting since computing the flipped TSDF volume is the most time-consuming part for inference and our approach provides a substantial faster alternative while also increasing the accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison to State-of-the-Art</head><p>We evaluate our approach on the two test sets NYU CAD and NYU Kinect, which are in the following denoted as CAD and Kinect, and we compare our approach to the stateof-the-art. The results for scene completion and semantic scene completion are reported in <ref type="table" target="#tab_5">Table 5</ref>. Both of our approaches with 3-channel input encoding and one-hot encoding perform comparably. Since one-hot encoding yields a slightly higher accuracy, we only discuss the difference between the latter and other approaches from the literature.</p><p>Our approach sets the new state-of-the-art for semantic scene completion. We achieve 46.2 % on CAD and 34.1 % on Kinect and outperform the approach by Song et al. <ref type="bibr" target="#b35">[36]</ref> by +6.2 % on CAD and +3.6 % on Kinect, although they use SUNCG as additional training data. If the same training data, i.e. only NYU, is used, our approach outperforms <ref type="bibr" target="#b35">[36]</ref> by +9.4 % on Kinect. For scene completion, we outperform <ref type="bibr" target="#b35">[36]</ref> by +5.8 % on CAD and +4.9 % on Kinect if NYU is used as training data. However, even if <ref type="bibr" target="#b35">[36]</ref> uses additional training data from SUNCG, our approach still outperforms it by +2.9 % on CAD and +3.4 % on Kinect.</p><p>Compared to the recent ESSCN approach <ref type="bibr" target="#b41">[42]</ref>, we perform better in both scene completion (+3.8 %) and semantic scene completion (+7.4 %). Note also that pretraining on SUNCG <ref type="bibr" target="#b35">[36]</ref> and using a stronger 3D-CNN architecture <ref type="bibr" target="#b41">[42]</ref> are orthogonal to our proposed method. One can assume that our performance would further increase by incorporating both ideas. <ref type="table" target="#tab_5">Table 5</ref> also includes the results of other approaches that do not rely on end-to-end learning <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b9">10]</ref>. Furthermore, the approaches <ref type="bibr" target="#b43">[44]</ref> and <ref type="bibr" target="#b8">[9]</ref> only address scene completion but not semantic scene completion. These methods perform substantially worse than the end-to-end learning approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this work, we have proposed a two stream approach for 3D semantic scene completion. In contrast to previous works, the proposed approach leverages depth and semantic information of the visible part of the scene for this task. In our experiments, we have shown that the proposed three-channel encoding for the semantic volume is not only memory efficient but it also results in higher accuracies compared to a single-channel encoding and is competitive to a memory expensive one-hot encoding. The proposed approach achieves state-of-the-art results for semantic scene completion on the NYUv2 dataset while also providing much faster inference times than approaches based  <ref type="table">Table 4</ref>. Impact of the input for the 3D-CNN. The proposed architecture is shown in <ref type="figure">Figure 2 a)</ref>. The versions 'with fTSDF' refers to a version where not only the semantic volume but also the flipped TSDF volume <ref type="bibr" target="#b35">[36]</ref> are used.   <ref type="bibr" target="#b35">[36]</ref>, and result obtained by our approach. Overall, our completed semantic 3D scenes are less cluttered and show a higher voxel class accuracy compared to <ref type="bibr" target="#b35">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NYU CAD Scene Completion</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative results on NYUv2 Kinect. From left to right: Input RGB-D image, ground truth, result obtained by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>table tvs furn. objs. avg. Deeplab v2 58.1 85.7 76.6 62.9 58.5 65.8 62.8 37.9 56.8 56.5 54.7 61.5 Deeplab v3+ 71.1 89.8 82.8 72.8 65.8 72.4 66.1 50.7 63.0 64.7 62.9 69.3 2D semantic segmentation accuracies on the NYUv2 dataset (%IoU ). In both cases, a CRF is used.</figDesc><table><row><cell></cell><cell>Scene Completion</cell><cell cols="2">Semantic Scene Completion</cell><cell></cell><cell></cell></row><row><cell>channels</cell><cell>IoU</cell><cell>ceil. floor wall win. chair</cell><cell>bed sofa table</cell><cell>tv furn. objs</cell><cell>avg</cell></row><row><cell>1</cell><cell>59.3</cell><cell cols="2">8.3 93.3 25.0 13.4 11.5 43.0 31.8 11.2</cell><cell cols="2">2.4 26.5 16.8 25.8</cell></row><row><cell>3</cell><cell>60.0</cell><cell cols="4">6.7 93.2 26.2 20.6 17.8 56.9 49.2 16.5 29.4 37.6 18.3 33.8</cell></row><row><cell>12 (one-hot)</cell><cell>60.0</cell><cell cols="4">9.7 93.4 25.5 21.0 17.4 55.9 49.2 17.0 27.5 39.4 19.3 34.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>93.2 26.2 20.6 17.8 56.9 49.2 16.5 29.4 37.6 18.3 33.8 with fTSDF, early fusion 60.2 8.1 94.4 25.6 17.1 17.8 53.6 48.0 17.0 28.0 36.0 18.4 33.1 with fTSDF, fusion 1 60.0 4.8 94.1 25.5 21.5 16.6 56.9 47.2 16.7 27.5 37.3 18.1 33.3 with fTSDF, fusion 2 54.4 5.9 93.6 22.0 11.0 16.5 50.4 41.0 12.4 23.1 31.9 12.4 29.1 with fTSDF, fusion 5 59.1 5.1 92.9 23.0 19.4 15.1 53.9 46.7 16.3 28.2 34.6 15.0 31.8 with fTSDF, late fusion 60.4 5.7 93.9 25.7 20.3 15.9 55.7 44.8 17.0 28.1 34.9 16.0 32.5</figDesc><table><row><cell></cell><cell>Scene Completion</cell><cell></cell><cell cols="2">Semantic Scene Completion</cell><cell></cell></row><row><cell>input</cell><cell>IoU</cell><cell cols="2">ceil. floor wall win. chair</cell><cell>bed sofa table</cell><cell>tv furn. objs</cell><cell>avg</cell></row><row><cell cols="3">proposed, no fTSDF 6.7 RGB image 60.0 58.2 4.2 93.4 19.3</cell><cell cols="2">4.4 10.8 34.9 20.2 11.8</cell><cell cols="2">4.9 17.2 10.3 21.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>93.2 26.2 20.6 17.8 56.9 49.2 16.5 29.4 37.6 18.3 33.8 Ours, one-hot NYU 60.0 9.7 93.4 25.5 21.0 17.4 55.9 49.2 17.0 27.5 39.4 19.3 34.1 Comparison to the state-of-the-art. of TSDF input features.</figDesc><table><row><cell>Semantic Scene Completion</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Large-scale semantic 3d reconstruction: An adaptive multi-resolution model for multi-class volumetric labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blaha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Matterport3D: Learning from RGB-D Data in Indoor Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Niessner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">ShapeNet: An Information-Rich 3D Model Repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi-Xing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<idno>abs/1512.03012</idno>
		<editor>CoRR</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno>abs/1802.02611</idno>
	</analytic>
	<monogr>
		<title level="m">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Shape Completion using 3D-Encoder-Predictor CNNs and Shape Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">Ruizhongtai</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ScanComplete: Large-Scale Scene Completion and Semantic Segmentation for 3D Scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rithie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bokeloh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Structured Prediction of Unobserved Voxels From a Single Depth Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Julier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Joint 3D Object and Layout Inference from a single RGB-D Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">9358</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Perceptual organization and recognition of indoor scenes from RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="564" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Aligning 3D models to RGB-D images of cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><forename type="middle">Andr?s</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4731" to="4740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">High-Resolution Shape Completion Using Deep Neural Networks for Global Structure and Local Geometry Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">SceneNet: Understanding Real World Indoor Scenes With Synthetic Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Viorica Patraucean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Joint 3D Scene Reconstruction and Class Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>H?ne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Angst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A linear approach to matchig cuboids in RGBD images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3D scene understanding by Voxel-CRF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byung-Soo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Acquiring 3D Indoor Environments with Variability and Repetition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niloy</forename><forename type="middle">J</forename><surname>Young Min Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Ming</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1" to="138" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning for 3d scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3050" to="3057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Object Recognition in 3D Point Clouds Using Web Data and Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Journal of Robotics Research</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1019" to="1037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Database-Assisted Object Retrieval for Real-Time 3D Reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Niessner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="435" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">holistic scene understanding for 3D object detection with RGBD cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Object detection and classification from large-scale cluttered indoor scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Mattausch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Panozzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Mura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renato</forename><surname>Pajarola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="11" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Object Detection and Classification from Large-Scale Cluttered Indoor Scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Mattausch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Panozzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Mura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renato</forename><surname>Pajarola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">RAPter: Rebuilding Man-made Scenes with Regular Arrangements of Planes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aron</forename><surname>Monszpart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Mellado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niloy</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="103" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A Searchclassify Approach for Cluttered Indoor Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangliang</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Sharf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1" to="137" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">RGB-(D) scene labeling: Features and Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>X Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Completing 3D object shape from one depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Rock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmay</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Thorsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daeyun</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An Interactive Approach to Semantic Modeling of Indoor Scenes with an RGBD Camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjia</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongping</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Data-driven Contextual Modeling for 3D Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinxin</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueshan</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="55" to="67" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Indoor Segmentation and Support Inference from RGBD Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep sliding shapes for amodal 3d object detection in rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Semantic Scene Completion from a Single Depth Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A field model for repairing 3D shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Binh-Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khoi</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quang-Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai-Kit</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Shape completion enabled robotic grasping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Varley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chad</forename><surname>Dechant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joaqu?n</forename><surname>Ruales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Shape Inpainting using 3D Generative Adversarial Network and Recurrent Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suya</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
		<idno>abs/1711.06375</idno>
	</analytic>
	<monogr>
		<title level="m">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">3D ShapeNets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">3D Object Reconstruction from a Single Depth View with Adversarial Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Markham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Trigoni</surname></persName>
		</author>
		<idno>abs/1708.07969</idno>
	</analytic>
	<monogr>
		<title level="m">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Efficient Semantic Scene Completion Network with Spatial Group Convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anbang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongen</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Physically-Based Rendering for Indoor Scene Understanding Using Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Beyond point clouds: Scene understanding by reasoning geometry and physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Ikeuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sx-Cx</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
