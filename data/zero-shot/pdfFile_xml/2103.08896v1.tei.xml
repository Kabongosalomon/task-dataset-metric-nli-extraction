<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Anti-Adversarially Manipulated Attributions for Weakly and Semi-Supervised Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungbeom</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunji</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
							<email>sryoon@snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">ASRI</orgName>
								<orgName type="institution" key="instit2">INMC</orgName>
								<orgName type="institution" key="instit3">ISRC, and Institute of Engineering Research</orgName>
								<orgName type="institution" key="instit4">Seoul National University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Anti-Adversarially Manipulated Attributions for Weakly and Semi-Supervised Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Weakly supervised semantic segmentation produces a pixel-level localization from a classifier, but it is likely to restrict its focus to a small discriminative region of the target object. AdvCAM is an attribution map of an image that is manipulated to increase the classification score. This manipulation is realized in an anti-adversarial manner, which perturbs the images along pixel gradients in the opposite direction from those used in an adversarial attack. It forces regions initially considered not to be discriminative to become involved in subsequent classifications, and produces attribution maps that successively identify more regions of the target object. In addition, we introduce a new regularization procedure that inhibits the incorrect attribution of regions unrelated to the target object and limits the attributions of the regions that already have high scores. On PASCAL VOC 2012 test images, we achieve mIoUs of 68.0 and 76.9 for weakly and semi-supervised semantic segmentation respectively, which represent a new state-of-the-art. The code is available at: https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation involves the allocation of a semantic label to each pixel of an image. It is an essential task in image recognition and scene understanding. Deep neural networks (DNNs) have facilitated tremendous progress in semantic segmentation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22]</ref>; but they require a large number of training images annotated with pixel-level labels. Preparing such a training dataset is very expensive: pixel-level annotation of images containing an average of 2.8 objects takes about 4 minutes <ref type="bibr" target="#b3">[4]</ref> per image, and a single large (2048?1024) image depicting a complicated scene requires more than 90 minutes for pixel-level annotation <ref type="bibr" target="#b8">[9]</ref>.</p><p>The need for pixel-level annotation is addressed by weakly supervised learning, in which a segmentation network is trained on images with less comprehensive anno-* Correspondence to: Sungroh Yoon &lt;sryoon@snu.ac.kr&gt;. tations that are cheaper to obtain than pixel-level labels. Weakly supervised methods can use scribbles <ref type="bibr" target="#b52">[53]</ref>, points <ref type="bibr" target="#b3">[4]</ref>, bounding boxes <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b49">50]</ref>, and class labels <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b46">47]</ref> as annotations. Labeling an image with class labels takes about 20 seconds <ref type="bibr" target="#b3">[4]</ref>, making class labels the cheapest option. In addition, many public datasets are already annotated with class labels <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12]</ref>, and automated web searches can also provide images with class labels <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b45">46]</ref> although the accuracy of such labels may be low. These considerations make class labels the most popular form of weak supervision. Most weakly supervised segmentation methods that use class labels depend on attribution maps obtained from a trained classifier <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b62">63]</ref>. Such a map identifies the image regions on which the classifier concentrated. However, these important, or discriminative, regions are relatively small, and most attribution maps do not represent the whole region occupied by a target object, which makes those attribution maps unsuitable for training a semantic segmentation network. Therefore, many researchers have tried to extend regions to cover more of a target object, by manipulating images <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b55">56]</ref> or feature maps <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b61">62]</ref>.</p><p>One popular method for manipulation is erasure: the classifier is forced to find new regions of the target object from which discriminative regions previously located have been removed. Erasure is effective, but it requires modification of the network, often by adding additional layers <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b61">62]</ref>, or additional training steps <ref type="bibr" target="#b55">[56]</ref>. Another difficulty is the provision of a reliable termination condition for the iterative erasure; the erasure of discriminative region of an image can cause the DNN to misclassify that image. If the image from which the discriminative region has been erased crosses the decision boundary as shown in <ref type="figure" target="#fig_0">Figure 1(a)</ref>, an erroneous attribution map may be generated. An alternative method for manipulation is a stochastic perturbation shown in <ref type="bibr">Figure 1(b)</ref>. FickleNet <ref type="bibr" target="#b28">[29]</ref> diversifies attribution maps from an image by applying random dropout to the feature maps of a DNN and aggregates them into a unified map.</p><p>We propose a new manipulation method for extending the discriminative regions of a target object. Our method is based on adversarial attack <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b27">28]</ref>, but with a benign purpose. Adversarial attack finds a small perturbation of an image that pushes it across the decision boundary to change the classification result. By contrast, our method operates in an anti-adversarial manner , which is the reversal of adversarial attack. It aims to find a perturbation that pushes the manipulated image away from the decision boundary, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>(c). This manipulation is realized by adversarial climbing, in which an image is perturbed along pixel gradients which increase the classification score of the target class. The result is that non-discriminative regions, which are nevertheless relevant to that class, gradually become involved in the classification, so that the CAM of the manipulated image identifies more regions of the object. Ascending the gradient ensures that classification score increases, but the repetitive ascending may cause irrelevant areas, such as parts of the backgrounds or regions of other objects, to be activated together or the attribution scores of some part of the target object to be increased dramatically. We can address these problems by introducing regularization terms that suppress the scores of other classes and limit the attribution scores of the regions that already have high scores.</p><p>The attribution maps obtained from images that have been iteratively manipulated in this way can be used as pseudo ground-truth masks to train a semantic segmentation network in a weakly and semi-supervised manner.</p><p>Our method is a post-hoc analysis of the trained classifier, and can be used to improve the performance of existing methods without modification, resulting in new state-of-theart performance on the PASCAL VOC 2012 benchmark in both weakly and semi-supervised semantic segmentation.</p><p>The main contributions of this paper are three-fold: ? We propose AdvCAM, an attribution map of an image that is manipulated to increase the classification score, allowing it to identify more regions of an object. ? We empirically demonstrate that our method improves the performance of several methods of weakly supervised semantic segmentation without modification or re-training of their networks. ? Our technique produces significantly better performance on the Pascal VOC 2012 benchmark than existing methods, in both weakly and semi-supervised semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Weakly Supervised Learning</head><p>Existing weakly supervised semantic segmentation methods aim to find the whole region occupied by a target object by obtaining an improved initial seed which contains a good approximation of the region occupied by the object, and growing that region so that more of the object is identified.</p><p>Obtaining a High Quality Seed: Several methods have been proposed to improve the quality of the initial seeds obtained from classifiers. Wang et al. <ref type="bibr" target="#b54">[55]</ref> use equivariance regularization during the training of their classifier so that the attribution maps obtained from differently transformed images are equivariant to those transformations. Chang et al. <ref type="bibr" target="#b5">[6]</ref> improve feature learning by using latent semantic classes that are sub-categories of annotated parent classes, which can be pseudo-labeled by clustering image features. Fan et al. <ref type="bibr" target="#b12">[13]</ref> and Sun et al. <ref type="bibr" target="#b51">[52]</ref> capture information shared between several images by considering cross-image semantic similarities and differences. Wei et al. <ref type="bibr" target="#b56">[57]</ref> and Lee et al. <ref type="bibr" target="#b30">[31]</ref> consider the target object in several contexts by combining multiple attribution maps from differently dilated convolutions or from different layers of a DNN.</p><p>Growing the Object Region: Some researchers expand an initial CAM <ref type="bibr" target="#b62">[63]</ref> seed using a method analogous to region growing by examining the neighborhood of each pixel. Semantic labels are propagated from regions which can confidently be associated with the target object to regions which were initially ambiguous. SEC <ref type="bibr" target="#b26">[27]</ref> and DSRG <ref type="bibr" target="#b22">[23]</ref> start with a initial CAM seed containing ambiguous regions, and allocates pseudo labels to those ambiguous region during the training of the segmentation network. PSA <ref type="bibr" target="#b1">[2]</ref> and IRN <ref type="bibr" target="#b0">[1]</ref> extend the object region to semantically similar areas by a random walk. BEM <ref type="bibr" target="#b6">[7]</ref> synthesizes a pseudo boundary from a CAM and then uses a similar propagation with PSA [2].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Semi-Supervised Learning</head><p>In semi-supervised learning, a segmentation network is trained using a small number of images with pixel-level annotations, together with a much larger number of images with weak annotations or none at all. Cross-consistency training (CCT) <ref type="bibr" target="#b40">[41]</ref> involves the training of a segmentation network with unlabeled, or weakly labeled, images by enforcing an invariance of the predictions over different perturbations, such as injecting random noise. Souly et al. <ref type="bibr" target="#b50">[51]</ref> improve feature learning by using images synthesized by generative adversarial network <ref type="bibr" target="#b14">[15]</ref>. Hung et al. <ref type="bibr" target="#b23">[24]</ref> adopt adversarial training scheme that reduces the distribution gap between predicted segmentation maps and ground-truth maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Adversarial Attack</head><p>Methods of adversarial attack attempt to fool a DNN by presenting it with manipulated input with the intent to deceive. Adversarial attack can be applied to classification <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b38">39]</ref>, semantic segmentation <ref type="bibr" target="#b2">[3]</ref>, and object detection <ref type="bibr" target="#b58">[59]</ref>. Deceptive attribution maps can also be produced by adversarial image manipulation <ref type="bibr" target="#b10">[11]</ref> or model parameter manipulation <ref type="bibr" target="#b18">[19]</ref>. The aim of such attacks is to replace an attribution map with a spurious map, which highlights another location in the same image, without significantly changing the output of the DNN. Those methods are interested in manipulating the image to cause the neural network's unintended behavior. By contrast, we are interested in finding the proper manipulation of the input image, so the resulting attribution map can cover the target object better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>We look more closely at adversarial attack methods and class activation map in Section 3.1. In Sections 3.2 and 3.3, we introduce AdvCAM and explain how we generate pseudo ground truth for weakly supervised semantic segmentation. Finally, we show how to train a semantic segmentation network with generated pseudo ground-truth in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries</head><p>Adversarial Attack in more detail: An adversarial attack aims to find a small pixel-level perturbation that can change the output from a DNN. In other words, given an input x, it finds the perturbation n satisfying NN(x) = NN(x + n), where NN(?) is the output of the neural network. A representative method <ref type="bibr" target="#b15">[16]</ref> of constructing n is to consider the normal vector to the decision boundary of NN, which can be realized by finding the gradients of NN with respect to x. A manipulated image x can then be obtained as follows:</p><formula xml:id="formula_0">x = x ? ?? x NN(x),<label>(1)</label></formula><p>where ? determines the extent of the change to the image. This process can be understood as gradient descent. Class Activation Map (CAM): It identifies the region of an image which a classifier has used. A CAM is computed from the class-specific contribution of each channel of the feature map to the classification score. It is based on a convolutional neural network that has global average pooling (GAP) before the last classification layer. A class activation map CAM(x) from an image x can be computed as follows:</p><formula xml:id="formula_1">CAM(x) = w c f (x),<label>(2)</label></formula><p>where w c is the weights of the final classification layer for class c, and f (x) is the feature map of x prior to GAP. A CAM bridges the gap between image-level and pixellevel annotations. However, the regions obtained by a CAM are usually much smaller than the full extent of the target object, since the small discriminative regions provide sufficient information for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">AdvCAM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Adversarial Climbing</head><p>AdvCAM is an attribution map obtained through adversarial climbing, which is an anti-adversarial technique that manipulates the image so as to increase the classification score of that image, with the result that the classifier identifies more regions of objects. This is the reverse of an adversarial attack based on Eq. 1, which manipulates the image to reduce the classification score. Inspired by PGD <ref type="bibr" target="#b27">[28]</ref>, iterative adversarial climbing of the initial image x 0 can be performed using the following relation:</p><formula xml:id="formula_2">x t = x t?1 + ?? x t?1 y t?1 c ,<label>(3)</label></formula><p>where t (1 ? t ? T ) is the adversarial step index, x t is the manipulated image at the t?th step, and y t?1 c is the classification logit of x t?1 for class c.</p><p>This process makes the previously non-discriminative yet relevant features become more involved in the classification. Thus, the CAMs obtained from successive images manipulated by the iteration can be expected to identify an increasing amount of the region of the target object. We produce a localization map A which encapsulates the results of the iteration by aggregating the CAMs obtained from the manipulated images at each iteration t, as follows:</p><formula xml:id="formula_3">A = T t=0 CAM(x t ) max T t=0 CAM(x t )</formula><p>.</p><p>(4)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">How can Adversarial Climbing Improve CAMs?</head><p>The connection between a classification logit y c and a CAM, i.e. y c = GAP(CAM) <ref type="bibr" target="#b61">[62]</ref>, infers that adversarial climbing increases y c , and thus the CAM. In this process, features involved in classification are enhanced. To provide a better understanding how adversarial climbing generates a denser CAM, we consider two questions: 1 Can non-discriminative features be enhanced? 2 Are those enhanced features classrelevant from a human point of view? 1 Can non-discriminative features be enhanced?: One might think that changing a pixel with a large gradient primarily enhances discriminative features. This pixel change affects many features due to the receptive field. However, not all the affected features are necessarily discriminative. We support this analysis empirically. We define the discriminative region grow, but enhances non-discriminative features more than discriminative ones, resulting in a denser CAM.</p><formula xml:id="formula_4">R D = {i|CAM(x 0 ) i ? 0.5} and the non-discriminative region R ND = {i|0.1 &lt; CAM(x 0 ) i &lt; 0.5}, where i is the location index. The pixel amplification ra- tio s i t is CAM(x t ) i /CAM(x 0 ) i at location i and step t.</formula><p>2 Are those enhanced features class-relevant from a human point of view? We now consider whether the highlighted non-discriminative features are class-relevant from a human point of view. Moosavi et al. <ref type="bibr" target="#b39">[40]</ref> argued that a loss landscape that is sharply curved with respect to input makes a NN vulnerable to adversarial attack. Researchers have subsequently shown that a flattened loss landscape, obtained by reducing the curvature of the loss surface <ref type="bibr" target="#b39">[40]</ref> or encouraging the loss to behave linearly <ref type="bibr" target="#b42">[43]</ref>, can improve the robustness of a NN. Systems which are robust in this sense have been shown to produce features that align better with human perception and operate in a easier way to understand <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b53">54]</ref>.</p><p>By the same token, we can expect that images manipulated by adversarial climbing will produce features that align with human perception well because the curvature of loss surface affected by adversarial climbing is small. To support this, we visualize the loss landscape of our trained classifier, following Moosavi et al. <ref type="bibr" target="#b39">[40]</ref>: we obtain a manipulation vector n and a random vector r from the classification loss computed from an image. We determine the surfaces of classification loss values computed from images, manipulated by a vector which is interpolated between n and r using a range of interpolation ratios. The loss landscape obtained by adversarial climbing <ref type="figure" target="#fig_4">(Figure 3</ref>(a)) is much more flatten than that obtained by adversarial attacking <ref type="figure" target="#fig_4">(Figure 3(b)</ref>). Therefore, we can legitimately expect it to increase the attribution of features relevant to the class from a human point of view, resulting in a better CAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Regularization</head><p>Even if the loss surface obtained by adversarial climbing is reasonably flat, too much repetitive adversarial manipula- tion may cause regions corresponding to objects in the wrong class to be activated, or increase the attribution scores of the regions that already have high scores. We address this by (i) suppressing the logit values associated with other classes and (ii) restricting high attributions on discriminative regions of the target object. Suppressing Other Classes: In an image, objects of different classes can mutually increase logit values. For example, since a chair and a dining table mainly occur together in an image, a NN may infer an increased logit value for the chair from the region of the table. We thus add regularization that reduces logit values for all classes except c.</p><p>Restricting High Attributions: As mentioned in Section 3.2.2, adversarial climbing increases the attribution scores for both discriminative and non-discriminative regions in the feature map. However, the growth of attribution scores for discriminative regions is problematic for two reasons: 1) it prevents new regions from being additionally attributed to the classification score, and 2) if the maximum value of the attribution score increases during adversarial climbing, the normalized scores of the remaining area may decrease. Please see the blue boxes in <ref type="figure" target="#fig_5">Figure 4</ref>(b).</p><p>Therefore we limit the attribution scores in regions that already have high scores during adversarial climbing, so the attribution scores of those regions remain similar to that of x 0 . We realize this scheme by introducing a restricting mask M that contains the regions whose attribution scores of CAM(x t?1 ) are higher than the threshold ? . More specifically, M can be represented as follows:</p><formula xml:id="formula_5">M = 1(CAM(x t?1 ) &gt; ? ),<label>(5)</label></formula><p>where 1(?) is an indicator function. An example mask M is shown in <ref type="figure" target="#fig_5">Figure 4(a)</ref>. We add the regularization term so that the values of the CAM corresponding to the regions of M are forced to equal to that of CAM(x 0 ). With this regularization, s i?RD discriminative features (&lt; 2?), and regularization makes this difference even larger (&gt; 2.5?). Thus, new regions of the target object are found more effectively, resulting in a denser CAM <ref type="figure" target="#fig_5">(Figure 4(b)</ref>).</p><p>To apply regularization, we modify Eq. 3 as follows:</p><formula xml:id="formula_6">x t = x t?1 + ?? x t?1 L, where (6) L = y t?1 c ? k?C\c y t?1 k ? ? M |CAM(x t?1 ) ? CAM(x 0 )| 1 .<label>(7)</label></formula><p>C is the set of all classes, ? is a hyper-parameter that controls the influence of masking regularization, and is elementwise multiplication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training Segmentation Networks</head><p>Since CAM is obtained from down-sampled intermediate features produced by the classifier, it localizes the target object coarsely and cannot represent its exact boundary. Many methods of generating an initial seed for weakly supervised semantic segmentation construct a pseudo ground-truth by modifying their initial seeds using existing seed refinement methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b22">23]</ref>. For example, SEAM <ref type="bibr" target="#b54">[55]</ref> and Chang et al. <ref type="bibr" target="#b5">[6]</ref> use PSA <ref type="bibr" target="#b1">[2]</ref>; and MBMNet <ref type="bibr" target="#b35">[36]</ref> and CONTA <ref type="bibr" target="#b59">[60]</ref> use IRN <ref type="bibr" target="#b0">[1]</ref>. We also apply the seed refinement method to the coarse map A. For weakly supervised learning, we use the resulting profiles as pseudo ground-truth for training DeepLab-v2, pre-trained on the ImageNet dataset <ref type="bibr" target="#b9">[10]</ref>. For semi-supervised learning, we employ CCT <ref type="bibr" target="#b40">[41]</ref>, which uses IRN <ref type="bibr" target="#b0">[1]</ref> to generate pseudo-ground truth masks; we replace these with our masks, constructed as just described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Dataset: We conducted experiments on the PASCAL VOC 2012 <ref type="bibr" target="#b11">[12]</ref> dataset. The images in this dataset come with masks for fully supervised semantic segmentation, but we only used them for evaluation. In a weakly supervised setting, we trained our network on 10,582 training images provided by Hariharan et al. <ref type="bibr" target="#b16">[17]</ref>, which have image-level annotations.</p><p>In a semi-supervised setting, we used 1,464 training images with pixel-level annotations and 9,118 training images with class labels, following previous works <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b56">57]</ref>. We evaluated our results by calculating mean intersection-overunion (mIoU) values for 1,449 validation images and 1,456 test images. Since the labels for test images are not publicly available, the results for those images were obtained from the official PASCAL VOC evaluation server.</p><p>Reproducibility: We performed iterative adversarial climbing with T = 27 and ? = 0.008. We set ? to 7 and ? to 0.5. To generate the initial seed, we followed the procedure of Ahn et al. <ref type="bibr" target="#b0">[1]</ref>, including the use of ResNet-50 <ref type="bibr" target="#b17">[18]</ref>. For final segmentation, we used DeepLab-v2-ResNet101 <ref type="bibr" target="#b7">[8]</ref> as the backbone network. We followed the default settings of <ref type="bibr" target="#b7">[8]</ref> for training, which included cropping the images to 321?321 pixels. In a semi-supervised setting we used the same settings as Ouali et al. <ref type="bibr" target="#b40">[41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Results</head><p>Quality of the Mask: <ref type="table" target="#tab_0">Table 1</ref> compares the initial seed and pseudo ground truth masks obtained by our method and by other recent techniques. Both seeds and masks were generated from training images of the PASCAL VOC dataset. For initial seeds, we report the best results by applying a range of thresholds to separate the foreground and background in the map A, as following SEAM <ref type="bibr" target="#b54">[55]</ref>. Our initial seeds are 6.8% better than the original CAMs <ref type="bibr" target="#b62">[63]</ref>, which provide a baseline, and this also outperforms the other methods. Note that Chang et al. <ref type="bibr" target="#b5">[6]</ref> and SEAM <ref type="bibr" target="#b54">[55]</ref> use Wide ResNet-38 <ref type="bibr" target="#b57">[58]</ref>, which provides better representation than ResNet-50 <ref type="bibr" target="#b17">[18]</ref>. SEAM <ref type="bibr" target="#b54">[55]</ref> also uses an auxiliary self-attention module that performs pixel-level refinement of the initial CAM by considering the relationship between pixels. We apply CRF, a widely used post-processing method, to the initial seeds of Chang et al. <ref type="bibr" target="#b5">[6]</ref>, SEAM <ref type="bibr" target="#b54">[55]</ref>, IRN <ref type="bibr" target="#b0">[1]</ref>, and our method. With the exception of SEAM, CRF improves the seed by more than 5% on average, but it improves the seed of SEAM only by 1.4%. We believe this is because the seed of SEAM is already refined by the self-attention module. Our seed after applying CRF is 5.3% better than that of SEAM.</p><p>We also compared pseudo ground truth masks, extracted after seed refinement, with existing methods. Most methods refine their initial seeds with PSA <ref type="bibr" target="#b1">[2]</ref> or IRN <ref type="bibr" target="#b0">[1]</ref>. For a fair comparison, we produced pseudo ground truth masks using both these seed refinement techniques. <ref type="table" target="#tab_0">Table 1</ref> shows that our method outperforms the others by a large margin, whichever seed refinement technique is used.</p><p>Weakly Supervised Semantic Segmentation: <ref type="table" target="#tab_1">Table 2</ref> compares our method with other recently introduced weakly supervised semantic segmentation methods with various levels of supervision: fully supervised pixel-level masks (P), bounding boxes (B) or image class labels (I), with and without salient object masks (S). All the results in <ref type="table" target="#tab_1">Table 2</ref>  were obtained using a ResNet-based backbone <ref type="bibr" target="#b17">[18]</ref>. With image-level annotation alone, our method achieves mIoU values of 68.1 and 68.0 for the PASCAL VOC 2012 validation and test images respectively. This is significantly better than the other methods under the same level of supervision. In particular, the mIoU value for validation images is 4.6% higher than that for IRN <ref type="bibr" target="#b0">[1]</ref>, which is our baseline. CONTA <ref type="bibr" target="#b59">[60]</ref>, the best-performing method among our competitors, achieves an mIoU value of 66.1; but their method depends upon SEAM <ref type="bibr" target="#b54">[55]</ref>, which is known to outperform IRN <ref type="bibr" target="#b0">[1]</ref>. If CONTA is implemented with IRN, the resulting mIoU value is 65.3, which is 2.8% worse than our method. <ref type="figure">Figure 5</ref> presents examples of semantic masks produced by FickleNet <ref type="bibr" target="#b28">[29]</ref>, IRN <ref type="bibr" target="#b0">[1]</ref>, and our method. Our method also outperforms other methods using auxiliary salient object mask supervision <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref> that provides exact boundary information of salient objects in an image, or extra web images or videos <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b51">52]</ref>. The performance of our method is also comparable with that of methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b49">50]</ref> that use bounding box supervision.</p><p>Semi-Supervised Semantic Segmentation: <ref type="table" target="#tab_2">Table 3</ref> compares the mIoU scores of our method on the PASCAL VOC validation and test images with those of other recent semi-supervised segmentation methods, which use 1.5K images with fully supervised masks and 9.1K images with weak annotations. All the methods in <ref type="table" target="#tab_2">Table 3</ref> were implemented on the ResNet-based backbone <ref type="bibr" target="#b17">[18]</ref>, except that daggered ( ?) methods which used the VGG-based backbone <ref type="bibr" target="#b47">[48]</ref>. We achieve mIoU values of 77.8 and 76.9 for the PASCAL VOC 2012 validation and test images respectively, which is better than the other methods under the same level of supervision. Specifically, the performance of our method on the validation images was 4.6% better than that of CCT <ref type="bibr" target="#b40">[41]</ref>, which is our baseline. Our method even outperforms Song et al. <ref type="bibr" target="#b49">[50]</ref> which uses bounding box labels for 9.1K images, instead of class labels. <ref type="figure">Figure 5</ref> presents examples of semantic masks produced by CCT <ref type="bibr" target="#b40">[41]</ref> and our method.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Iterative Adversarial Climbing</head><p>We analyzed the effectiveness of the iterative adversarial climbing and regularization technique introduced in Section 3.3 by evaluating the initial seed in terms of mIoU. <ref type="figure">Figure 6(a)</ref> shows the mIoU of the initial seed for each adversarial iteration. Initially, the mIoU rises steeply, with or without regularization; but without regularization the curves peaks around iteration 8.</p><p>To analyze this, we evaluate the truthfulness of the newly localized region at each adversarial climbing iteration in terms of the proportion of noise, which we define to be the proportion of pixels that are classified as foreground but are  actually background. Without regularization, the proportion of noise rises steeply after some iterations as shown in <ref type="figure">Figure 6(b)</ref>, which means that new regions tend to be in the regions of background. Regularization allows new regions of the target object to be found in as many as 30 adversarial steps, keeping the proportion of noise much lower than that of initial CAM. <ref type="figure" target="#fig_7">Figure 7</ref> shows examples of attribution maps at each adversarial iteration with and without regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Hyper-Parameter Analysis</head><p>In the previous section, we looked at the effect of the number of adversarial iterations <ref type="figure">(Figures 6(a) and (b)</ref>). We also analyzed the sensitivity of the mIoU of the initial seed to the other three hyper-parameters used by AdvCAM.</p><p>Regularization Coefficient ?: It controls the influence of the masking technique that limits the attribution scores of the regions that already have high scores during adversarial climbing, in Eq. 7. <ref type="figure">Figure 6</ref>(c) shows the mIoU of the initial seed for different values of ?. When ? = 0, there is no regularization. Masking technique improves performance by more than 5% (50.43 for ? = 0 vs. 55.55 for ? = 7). The flattening of the curve after ? = 5 suggests that it is not difficult to select a good value of ?.</p><p>Masking Threshold ? : It controls the size of the restricting mask M in Eq. 5, determining how many pixels' attribution values will remain similar to that of the original CAM during adversarial climbing. <ref type="figure">Figure 6(d)</ref> shows the mIoU of the initial seed for different values of ? . This parameter is even less sensitive than ?: varying ? between 0.3 and 0.7 produces less than 1% change in mIoU.</p><p>Step Size ?: It determines the extent of the manipulation to the image in Eq. 6. <ref type="figure">Figure 6</ref>(e) shows the mIoU of the initial seed for different values of ?. In our system, changes in step size ? are not particularly significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Generality of Our Method</head><p>In addition to IRN <ref type="bibr" target="#b0">[1]</ref>, we experimented with two stateof-the-art methods of generating an initial seed for weakly  <ref type="figure">Figure 8</ref>: Feature manifold of images with "bird" (blue) and "cat" (green), and a trajectory of adversarial climbing for an image of each class. The dimensionality of the feature was reduced by t-SNE <ref type="bibr" target="#b37">[38]</ref>.</p><p>supervised semantic segmentation, namely Chang et al. <ref type="bibr" target="#b5">[6]</ref> and SEAM <ref type="bibr" target="#b54">[55]</ref>. We used the authors' pre-trained classifier where possible, but we re-trained the classifier of IRN <ref type="bibr" target="#b0">[1]</ref> since the authors do not provide pre-trained one. We also followed their experimental settings including the backbone networks and mask refinement methods, i.e., we used PSA <ref type="bibr" target="#b1">[2]</ref> to refine the initial seed from "Chang et al. + AdvCAM" or "SEAM + AdvCAM". <ref type="table" target="#tab_3">Table 4</ref> gives mIoU values for the initial seed and the pseudo ground truth mask obtained by combining each method with adversarial climbing. The use of AdvCAM improves the quality of the initial seed by an average of over 4%. Our approach does not require those initial seed generators to be modified or retrained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Manifold Visualization</head><p>For visualizing a trajectory of adversarial climbing at a feature-level, we used t-SNE dimensional reduction <ref type="bibr" target="#b37">[38]</ref>. We collect images that contain a single class of a cat or a bird and that are predicted by the classifier correctly. We then construct a set F containing the features of those images, before the final classification layer. We also choose a representative image of a cat, and another of a bird, and construct a set F containing the features of those two images and their 20 manipulated images by adversarial climbing. <ref type="figure">Figure 8</ref> presents t-SNE visualization of features in F ? F . We can see that adversarial climbing actually pushes the features away from the decision boundary boundary that separates the blue and green areas. In addition, despite 20 adversarial climbing steps, the manipulated features did not deviate significantly from the feature manifold of each class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have shown how adversarial manipulation can be used to expand the small discriminative regions of a target object, so as to obtain a better localization of that object. We manipulate images with a pixel-level perturbation, which is obtained from the gradient computed from the output of classifier with respect to the input image, which increase the classification score of the perturbed image. The attribution map of the manipulated image covers more of the target object. This is a post-hoc analysis of a trained classifier, and therefore no modification or re-training of the classifier is required. This allows AdvCAM to be readily integrated into existing methods. We have shown that AdvCAM can indeed be combined with recent weakly supervised semantic segmentation networks, and achieved new state-of-the-art performance on both weakly and semi-supervised semantic segmentation.  A. Appendix A.1. Implementation Details Details for Adversarial Climbing: Many recent studies <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b59">60]</ref> rely on the procedure of PSA <ref type="bibr" target="#b1">[2]</ref> and IRN <ref type="bibr" target="#b0">[1]</ref> for generating a CAM: a single image is flipped and resized with four different scales of {0.5, 1.0, 1.5, 2.0}, and the CAMs are extracted from those eight images. Those CAMs are aggregated into a single map by pixel-wise sum pooling. We manipulate those eight images independently for adversarial climbing.</p><p>Details for Semantic Segmentation: We used the Py-Torch implementation of DeepLab-v2-ResNet101 1 to train our segmentation network. We used multi-scale testing during inference time following <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b54">55]</ref>. Specifically, an input image is resized with four different scales of {0.5, 0.75, 1.0, 1.25}. These images are fed into the segmentation network independently, and the outputs are aggregated into a single map by pixel-wise max pooling, resulting in the final segmentation map. The experiments were performed on NVIDIA Tesla V100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Additional Analysis</head><p>Threshold analysis: As mentioned in Section 4.2 of the main paper, we report the best initial seed performance by applying a range of thresholds to separate the foreground and background in the map A. We present the effectiveness of this threshold by evaluating the initial seed, separated 1 https://github.com/kazuto1011/deeplab-pytorch by a range of thresholds, in terms of mIoU. <ref type="figure" target="#fig_0">Figure A1(a)</ref> shows the mIoU of the initial seed obtained from the 'CAM', 'AdvCAM without regularization', and 'AdvCAM with regularization'. We select t = 8 for 'AdvCAM without regularization' and t = 27 for 'AdvCAM with regularization', which are the best values of t for each setting according to <ref type="figure">Figure 6</ref>(a) in the main paper.</p><p>Effects of suppressing other classes: Section 3.4 in the main paper has proposed two regularization terms: 1) suppressing other classes and 2) inhibiting excessive concentration. The effectiveness of the latter was dealt with in-depth in the main paper (please see Section 5). We will now focus on the effectiveness of suppressing other classes. To isolate the effect of this regularization procedure, we exclude the masking technique in all experiments here. <ref type="figure" target="#fig_0">Figure A1(b)</ref> shows the mIoU of the initial seed for each adversarial iteration with and without the regularization of suppressing other classes. We can see that using this regularization technique provides better adversarial manipulation.</p><p>Comparison of per-class mIoU scores: <ref type="table" target="#tab_0">Table A1</ref> shows the per-class mIoU of our method and recently produced methods.</p><p>Additional mask examples on semantic segmentation. <ref type="figure" target="#fig_2">Figure A2</ref> shows more examples of the semantic masks from FickleNet <ref type="bibr" target="#b28">[29]</ref>, IRN <ref type="bibr" target="#b0">[1]</ref>, CCT <ref type="bibr" target="#b40">[41]</ref>, and our method.</p><p>Additional examples of localization maps by adversarial climbing. <ref type="figure" target="#fig_4">Figure A3</ref> shows additional examples of successive attribution maps obtained from images manipulated by iterative adversarial climbing.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Conceptual description of image manipulation methods for weakly supervised semantic segmentation: (a) erasure [21, 56, 62]; (b) FickleNet [29]; and (c) AdvCAM. (d) Examples of successive attribution maps obtained from iteratively manipulated images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 1(d) shows examples of CAMs obtained by applying this manipulation technique iteratively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Distributions of the pixel amplification ratio s i t for i ? R D and i ? R ND for 100 images, (a) without regularization and (b) with regularization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 2(a) shows that adversarial climbing makes both s i?RD t and s i?RND t</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Loss landscapes by manipulating images with weighted sums of the normal vector n and a random vector r for (a) adversarial climbing and (b) adversarial attack. The yellow star corresponds to the original image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>(a) An example image with its CAM and restricting mask M. (b) The initial CAM, and CAMs after 5, 10 and 20 steps of adversarial climbing, with and without regularization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Examples of predicted semantic masks for PASCAL VOC val images in weakly and semi-supervised manner. Effect of adversarial climbing and regularization on (a) the seed quality and (b) the proportion of noise. (c) Effect of the regularization coefficient ?. (d) Effect of the masking threshold ? . (d) Effect of the step size ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Examples of initial CAMs (the blue boxes) and successive localization maps obtained from images manipulated by iterative adversarial climbing, with the regularization procedure (top) and without (bottom). Adv Traj. Bird Adv Traj. Cat Bird Cat</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure</head><label></label><figDesc>A1: (a) Threshold analysis. (b) Effect of suppressing other classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure A2 :</head><label>A2</label><figDesc>Examples of predicted semantic masks for PASCAL VOC val images in weakly and semi-superivsed manner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure A3 :</head><label>A3</label><figDesc>Examples of initial CAMs and successive localization maps obtained from images manipulated by iterative adversarial climbing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>mIoU (%) of the initial seed (Seed), the seed with CRF (+CRF), and the pseudo ground truth mask (Mask) on PASCAL VOC 2012 train images.</figDesc><table><row><cell>Method</cell><cell cols="3">Seed + CRF Mask</cell></row><row><cell>Seed Refine with PSA [2]:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PSA CVPR '18 [2]</cell><cell>48.0</cell><cell>-</cell><cell>61.0</cell></row><row><cell>Mixup-CAM BMVC '20 [5]</cell><cell>50.1</cell><cell>-</cell><cell>61.9</cell></row><row><cell>Chang et al. CVPR '20 [6]</cell><cell cols="3">50.9 55.3 63.4</cell></row><row><cell>SEAM CVPR '20 [55]</cell><cell cols="3">55.4 56.8 63.6</cell></row><row><cell>AdvCAM (Ours)</cell><cell cols="3">55.6 62.1 68.0</cell></row><row><cell>Seed Refine with IRN [1]:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>IRN CVPR '19 [1]</cell><cell cols="3">48.8 54.3 66.3</cell></row><row><cell>MBMNet ACMMM '20 [36]</cell><cell>50.2</cell><cell>-</cell><cell>66.8</cell></row><row><cell>CONTA NeurIPS '20 [60]</cell><cell>48.8</cell><cell>-</cell><cell>67.9</cell></row><row><cell>AdvCAM (Ours)</cell><cell cols="3">55.6 62.1 69.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Weakly supervised semantic segmentation performance on PASCAL VOC 2012 val and test images.</figDesc><table><row><cell>Method</cell><cell>Sup.</cell><cell>val</cell><cell>test</cell></row><row><cell cols="2">Supervision: Stronger than image labels</cell><cell></cell><cell></cell></row><row><cell>DeepLab TPAMI '17 [8]</cell><cell>P</cell><cell cols="2">76.8 76.2</cell></row><row><cell>SDI CVPR '17 [26]</cell><cell>B</cell><cell>69.4</cell><cell>-</cell></row><row><cell>Song et al. CVPR '19 [50]</cell><cell>B</cell><cell>70.2</cell><cell>-</cell></row><row><cell>Supervision: Image-level tags</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Li et al. ICCV '19 [33]</cell><cell>I, S</cell><cell cols="2">62.1 63.0</cell></row><row><cell>FickleNet CVPR '19 [29]</cell><cell>I, S</cell><cell cols="2">64.9 65.3</cell></row><row><cell>Lee et al. ICCV '19 [30]</cell><cell>I, S, W</cell><cell cols="2">66.5 67.4</cell></row><row><cell>CIAN AAAI '20 [13]</cell><cell>I, S</cell><cell cols="2">64.3 65.3</cell></row><row><cell>Zhang et al. ECCV '20 [61]</cell><cell>I, S</cell><cell cols="2">66.6 66.7</cell></row><row><cell>Sun et al. ECCV '20 [52]</cell><cell>I, S</cell><cell cols="2">66.2 66.9</cell></row><row><cell>Fan et al. ECCV '20 [14]</cell><cell>I, S</cell><cell cols="2">67.2 66.7</cell></row><row><cell>Sun et al. ECCV '20 [52]</cell><cell>I, S, W</cell><cell cols="2">67.7 67.5</cell></row><row><cell>IRN CVPR '19 [1]</cell><cell>I</cell><cell cols="2">63.5 64.8</cell></row><row><cell>SSDD ICCV '19 [47]</cell><cell>I</cell><cell cols="2">64.9 65.5</cell></row><row><cell>SEAM CVPR '20 [55]</cell><cell>I</cell><cell cols="2">64.5 65.7</cell></row><row><cell>Chen et al. ECCV '20 [7]</cell><cell>I</cell><cell cols="2">65.7 66.6</cell></row><row><cell>Chang et al. CVPR '20 [6]</cell><cell>I</cell><cell cols="2">66.1 65.9</cell></row><row><cell>CONTA NeurIPS '20 [60]</cell><cell>I</cell><cell cols="2">66.1 66.7</cell></row><row><cell>AdvCAM (Ours)</cell><cell>I</cell><cell cols="2">68.1 68.0</cell></row></table><note>P?pixel-level mask, I?image class, B?box, S?saliency, W?web</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison of semi-supervised semantic segmentation methods on the PASCAL VOC 2012 val and test images.</figDesc><table><row><cell>Method</cell><cell>Training set</cell><cell>val</cell><cell>test</cell></row><row><cell>WSSL  ? [42]</cell><cell cols="3">1.5K P + 9.1K I 64.6 66.2</cell></row><row><cell>MDC  ? [57]</cell><cell cols="3">1.5K P + 9.1K I 65.7 67.6</cell></row><row><cell>Souly et al.  ? [51]</cell><cell cols="2">1.5K P + 9.1K I 65.8</cell><cell>-</cell></row><row><cell>FickleNet  ? [29]</cell><cell cols="2">1.5K P + 9.1K I 65.8</cell><cell>-</cell></row><row><cell>Song et al. [50]</cell><cell cols="2">1.5K P + 9.1K B 71.6</cell><cell>-</cell></row><row><cell>Luo et al. [37]</cell><cell cols="2">1.5K P + 9.1K I 76.6</cell><cell>-</cell></row><row><cell cols="3">CCT [41] (baseline) 1.5K P + 9.1K I 73.2</cell><cell>-</cell></row><row><cell>AdvCAM (Ours)</cell><cell cols="3">1.5K P + 9.1K I 77.8 76.9</cell></row></table><note>P?pixel-level mask, I?image class label, B?box, ? ? VGG backbone</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell cols="3">Effects of AdvCAM on different methods of gen-</cell></row><row><cell cols="3">erating the initial seed: mIoU of the initial seed (Seed) and</cell></row><row><cell cols="3">of the pseudo ground truth mask (Mask), for the PASCAL</cell></row><row><cell>VOC 2012 training images.</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Seed</cell><cell>Mask</cell></row><row><cell>Chang et al. [6]</cell><cell>50.9</cell><cell>63.4</cell></row><row><cell>+ AdvCAM</cell><cell>53.7 +2.8</cell><cell>67.5 +4.1</cell></row><row><cell>SEAM [55]</cell><cell>55.4</cell><cell>63.6</cell></row><row><cell>+ AdvCAM</cell><cell>58.6 +3.2</cell><cell>67.2 +3.6</cell></row><row><cell>IRN [2]</cell><cell>48.8</cell><cell>66.3</cell></row><row><cell>+ AdvCAM</cell><cell>55.6 +6.8</cell><cell>69.9 +3.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table A1 :</head><label>A1</label><figDesc>Comparison of per-class mIoU scores. bkg aero bike bird boat bottle bus car cat chair cow table dog horse motor person plant sheep sofa train tv mIOU Ours, semi) 94.3 93.6 65.7 90.3 54.2 74.4 91.7 85.6 91.7 28.2 88.1 67.4 86.2 88.5 89.4 82.6 62.2 87.2 47.6 80.5 65.3 76.9</figDesc><table><row><cell>Results on PASCAL VOC 2012 validation images:</cell></row><row><cell>AdvCAM (Ours, weak) 90.0 79.8 34.1 82.6 63.3 70.5 89.4 76.0 87.3 31.4 81.3 33.1 82.5 80.8 74.0 72.9 50.3 82.3 42.2 74.1 52.9 68.1</cell></row><row><cell>AdvCAM (Ours, semi) 94.4 91.7 65.6 89.1 72.4 72.8 93.4 86.0 90.4 37.5 90.6 58.6 84.5 88.9 83.3 84.9 62.0 81.6 49.5 85.9 71.8 77.8</cell></row><row><cell>Results on PASCAL VOC 2012 test images:</cell></row><row><cell>AdvCAM (Ours, weak) 90.1 81.2 33.6 80.4 52.4 66.6 87.1 80.5 87.2 28.9 80.1 38.5 84.0 83.0 79.5 71.9 47.5 80.8 59.1 65.4 49.7 68.0</cell></row><row><cell>AdvCAM (</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t remains fairly constant but s i?RND t still grows during adversarial climbing(Figure 2(b)).Figure 2shows that, adversarial climbing enhances non-discriminative features more than</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of instance segmentation with inter-pixel relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwoon</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwoon</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the robustness of semantic segmentation models to adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Miksik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">What&apos;s the point: Semantic segmentation with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Bearman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mixupcam: Weakly-supervised semantic segmentation via uncertainty regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ting</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaosong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robinson</forename><surname>Piramuthu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Weaklysupervised semantic segmentation via sub-category exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ting</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaosong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robinson</forename><surname>Piramuthu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Weakly supervised semantic segmentation with boundary exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Explanations can be manipulated and geometry is to blame</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann-Kathrin</forename><surname>Dombrowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximillian</forename><surname>Alber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Anders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Ackermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Kessel</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The pascal visual object classes (voc) challenge. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Cian: Crossimage affinity net for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Employing multi-estimations for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fooling neural network interpretations via adversarial model manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juyeon</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghwan</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesup</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Weakly supervised semantic segmentation using web-crawled videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghun</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Self-erasing network for integral object attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengtao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Weakly-supervised semantic segmentation network with deep seeded region growing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adversarial learning for semisupervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><forename type="middle">Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Ting Liou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen</forename><forename type="middle">Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Adversarial examples are not bugs, they are features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Simple does it: Weakly supervised instance and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Seed, expand and constrain: Three principles for weakly-supervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adversarial machine learning at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ficklenet: Weakly and semi-supervised semantic image segmentation using stochastic inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungbeom</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunji</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jangho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Frame-to-frame aggregation of active regions in web videos for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungbeom</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunji</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jangho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Robust tumor localization with pyramid grad-cam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jangho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungbeom</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chul-Kee</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11393</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Tell me where to look: Guided attention inference network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuan-Chuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attention bridging network for knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The secrets of salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning to detect a salient object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejian</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Weakly supervised segmentation with maximum bipartite graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weide</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung</forename><surname>Tzu-Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation via strong-weak dual-branch network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenfeng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deepfool: a simple and accurate method to fool deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alhussein</forename><surname>Seyed-Mohsen Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Robustness via curvature regularization, and vice versa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alhussein</forename><surname>Seyed-Mohsen Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Semisupervised semantic segmentation with cross-consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yassine</forename><surname>Ouali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?line</forename><surname>Hudelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myriam</forename><surname>Tami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised learning of a dcnn for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Adversarial robustness through local linearization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongli</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Gowal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishnamurthy</forename><surname>Dvijotham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Stanforth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Image synthesis with a single (robust) classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Gradcam: Visual explanations from deep networks via gradientbased localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Bootstrapping the performance of webly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Self-supervised difference detection for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wataru</forename><surname>Shimoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keiji</forename><surname>Yanai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Hide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Box-driven class-wise region masking and filling rate guided loss for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Semi supervised semantic segmentation using generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasim</forename><surname>Souly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Concetto</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Mining cross-image semantics for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Normalized cut loss for weakly-supervised cnn segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelaziz</forename><surname>Djelouah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Schroers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<title level="m">Robustness may be at odds with accuracy. ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Self-supervised equivariant attention mechanism for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yude</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meina</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Object region mining with adversarial erasing: A simple classification to semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Revisiting dilated convolution: A simple approach for weakly-and semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Adversarial examples for semantic segmentation and object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Causal intervention for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiansheng</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Splitting vs. merging: Mining object regions with discrepancy and intersection loss for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weide</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Adversarial complementary learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
