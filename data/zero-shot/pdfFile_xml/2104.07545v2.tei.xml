<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Learning for Generation with Long Source Sequences</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Rohde</surname></persName>
							<email>tobiasr@birch.ai</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxia</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
							<email>yinhan@birch.ai</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Seattle</roleName><forename type="first">Birch</forename><surname>Ai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">G</forename><surname>Allen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Learning for Generation with Long Source Sequences</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One of the challenges for current sequence to sequence (seq2seq) models is processing long sequences, such as those in summarization and document level machine translation tasks. These tasks require the model to reason at the token level as well as the sentence and paragraph level. We design and study a new Hierarchical Attention Transformer-based architecture (HAT) that outperforms standard Transformers on several sequence to sequence tasks. Furthermore, our model achieves state-of-theart ROUGE scores on four summarization tasks, including PubMed, arXiv, CNN/DM, SAMSum, and AMI. Our model outperforms document-level machine translation baseline on the WMT20 English to German translation task. We investigate what the hierarchical layers learn by visualizing the hierarchical encoder-decoder attention. Finally, we study hierarchical learning on encoder-only pre-training and analyze its performance on classification tasks. . 2016. Discourse parsing with attention-based hierarchical neural networks. In EMNLP, pages 362-371. . 2020. Multilingual denoising pre-training for neural machine translation. Transactions of the Association for Computational Linguistics, 8:726-742. Jiwei Tan, Xiaojun Wan, and Jianguo Xiao. 2017. Abstractive document summarization with a graphbased attentional neural model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sequence to sequence (seq2seq) models have been successfully used for a variety of natural language processing (NLP) tasks, including text summarization, machine translation and question answering. Often sequence to sequence models consist of an encoder that processes some source sequence and a decoder that generates the target sequence. Originally, <ref type="bibr" target="#b17">Sutskever et al. (2014)</ref> used recurrent neural networks as encoder and decoder for machine translation on the WMT-14 dataset. <ref type="bibr" target="#b1">Bahdanau et al. (2016)</ref> introduced the attention mechanism, where the decoder computes a distribution over the hidden states of the encoder and uses it to weigh the hidden states of the input tokens differently at each decoding step. <ref type="bibr">Vaswani et al. (2017)</ref> then introduced a new architecture for sequence to sequence * Work performed while Xiaoxia Wu interned at Birch AI. She is currently a postdoc fellow at The University of Chicago and Toyota Technological Institute at Chicago. modeling -the Transformer, which is based on the attention mechanism but not recurrent allowing for more efficient training.</p><p>While successful, both recurrent neural networks and Transformer-based models have limits on the input sequence length. When the input is long, the learning degrades particularly for tasks which require a comprehensive understanding of the entire paragraph or document. One of the main learning challenges for seq2seq models is that the decoder needs to attend to token level representations from the encoder to predict the next token, while at the same time it must learn from a large context.</p><p>A commonly used method for attempting to solve the long-sequence problem is hierarchical attention <ref type="bibr" target="#b21">(Yang et al., 2016)</ref>. This method was studied primarily on long sequence classification tasks, where the model learns a document representation which is used as the input to a classifier. Since then, many successful papers proposed methods using hierarchical attention (see Section 2 for full details). While hierarchical attention has been successfully applied to classification tasks, its potential for being applied to large document sequence to sequence tasks remains an interesting and open question.</p><p>In this paper, we present a hierarchical attention model based on the standard Transformer <ref type="bibr">(Vaswani et al., 2017)</ref> that produces sentence level representations, and combine them with token level representations to improve performance on long document sequence to sequence tasks. Our main contributions include 1) We design a hierarchical seq2seq attention network architecture named HAT (Hierachical Attention Transformer) (See <ref type="figure" target="#fig_0">Figure 1)</ref>. We conduct extensive experiments on various generation tasks for our proposed model in Sectios 4 and 5 and achieve new state-of-the-art results on several datasets.</p><p>2) In Sections 6 and 7, we study the generated  output of our architecture to further understand the benefits of hierarchical attention and compare it with those generated by plain seq2seq Transformer models. Furthermore, we analyze how the decoder makes use of the sentence level representations of the encoder by visualizing the hierarchical encoderdecoder attention.</p><p>3) Finally, we apply hierarchical attention to an encoder-only architecture and pre-train it on a Books and Wiki corpus similar to the one used in RoBERTa . We finetune our pre-trained model on several downstream tasks and analyze the performance in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Transformer models. The attention-based Transformer architecture <ref type="bibr">(Vaswani et al., 2017)</ref> currently dominates the state-of-the-art performance in many NLP tasks. This is largely due to the success of language model pre-training prior to fine-tuning the Transformer on the actual downstream task of interest. Pre-training has been applied in different ways to different variations of the Transformer architecture, including encoder-only pre-training <ref type="bibr">(BERT (Devlin et al., 2019)</ref>, RoBERTa , XLNet ), decoder-only pre-training (GPT <ref type="bibr" target="#b10">(Radford et al., 2019)</ref>), encoderdecoder pre-training (T5 <ref type="bibr" target="#b11">(Raffel et al., 2020)</ref>, <ref type="bibr">BART (Lewis et al., 2020)</ref>) and multilingual pretraining (MBART , <ref type="bibr">Roberta-XLM (Conneau et al., 2020)</ref>). Both classification and generation downstream task performance is improved significantly by initializing parameters from pre-trained models. To use a pre-trained model, the downstream task model architecture needs to be the same or similar to the one used in pre-training. Transformers have become the most popular architectures in NLP. However, one disadvantage of Transformers is that when the input sequence is long, the performance of the attention mechanism can become worse.</p><p>Long document modeling. Understanding and modeling large documents has been a longstanding challenge that has become increasingly demanded in practice <ref type="bibr" target="#b7">(Nakao, 2000;</ref><ref type="bibr" target="#b6">Mihalcea and Ceylan, 2007;</ref><ref type="bibr">Iyer et al., 2016;</ref><ref type="bibr" target="#b30">? elikyilmaz et al., 2018;</ref><ref type="bibr" target="#b9">Paulus et al., 2018;</ref><ref type="bibr">Tasnim et al., 2019;</ref><ref type="bibr">Gunel et al., 2019;</ref><ref type="bibr" target="#b22">Ye et al., 2019;</ref><ref type="bibr" target="#b2">Beltagy et al., 2020;</ref><ref type="bibr" target="#b25">Zhang et al., 2020)</ref>. Many methods have been proposed to improve performance on abstractive summarization tasks, which often have long source sequences. Recent work includes graph-based attention neural networks <ref type="bibr">(Tan et al., 2017)</ref>, discourse-aware attention models <ref type="bibr">Cohan et al. (2018)</ref> and decoder decomposition with policy learning <ref type="bibr">Kryscinski et al. (2018)</ref>. In addition,  suggest a discourse-aware neural summarization model with structural discourse graphs to capture the longrange dependencies among discourse units. <ref type="bibr" target="#b29">Zhu et al. (2020)</ref> design a hierarchical network for meeting summarization that hierarchically processes speaker turns and incorporates speaker roles.</p><p>Hierarchical learning. Hierarchical learning has been suggested by many researchers and it has been empirically shown to be effective in numerous diverse tasks outside of natural language processing, including policy learning <ref type="bibr" target="#b13">(Ryu et al., 2020)</ref>, visual relationship detection <ref type="bibr" target="#b4">(Mi and Chen, 2020)</ref>, partaware face detection <ref type="bibr" target="#b10">(Wu et al., 2019)</ref>, visuallyaware food recommendation <ref type="bibr">(Gao et al., 2020)</ref>, urban anomaly prediction <ref type="bibr" target="#b29">(Huang et al., 2020)</ref>, online banking fraud detection (Achituve et al., 2019) and discourse parsing <ref type="bibr">(Li et al., 2016)</ref>. Within the NLP domain, hierarchical learning has been successfully applied to text classification <ref type="bibr" target="#b21">(Yang et al., 2016)</ref>, machine translation <ref type="bibr" target="#b5">(Miculicich et al., 2018)</ref>, meeting summarization <ref type="bibr" target="#b29">(Zhu et al., 2020)</ref> and information extraction <ref type="bibr">(Gao et al., 2018;</ref><ref type="bibr" target="#b16">Song et al., 2017;</ref><ref type="bibr">Xing et al., 2018;</ref><ref type="bibr" target="#b27">Zhao et al., 2018;</ref><ref type="bibr">Han et al., 2018)</ref>. Particularly, <ref type="bibr" target="#b21">Yang et al. (2016)</ref>  In this work, we apply hierarchical learning to Transformer models for improving performance on generation tasks with long documents, including summarization and document-level machine translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>We modify the standard sequence to sequence transformer architecture <ref type="bibr">(Vaswani et al., 2017)</ref> by adding hierarchical attention for improved processing of long documents <ref type="figure" target="#fig_0">(Figure 1</ref>). The number of parameters for our large hierarchical model on summary tasks is 471M compared to the plain transformer 408M.</p><p>We use 12 encoder and decoder layers, a hidden size of 1024, 4096 for the dimension of the fullyconnected feed-forward networks and 16 attention heads in both the encoder and the decoder. Unlike the original Transformer, we use GELU activation instead of ReLU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data pre-processing</head><p>During pre-processing, we insert BOS tokens at the start of every sentence in each source document as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. We simply determine sentence boundaries by punctuation or by using prior sentence segmentation present in the documents. By using BOS tokens as hierarchical tokens, the hierarchical attention can benefit from the representations learned for the BOS token during pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Encoder</head><p>We use the same encoder as in Transformer. This produces an embedding for each input token. After those, we add an additional encoder layer (pink block in <ref type="figure" target="#fig_0">Figure 1</ref>) which only attends to the embeddings of the BOS tokens that we inserted during data-preprocessing. We refer to this layer as the hierarchical encoder layer, which produces another level of contextual representations for each of the BOS tokens, which can be interpreted as sentence level representations. We find that a single hierarchical layer works the best, although multiple hierarchical layers may be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Decoder</head><p>As in <ref type="bibr">Vaswani et al. (2017)</ref>, each layer first performs self attention over the previously generated tokens and then attends over the outputs of the final token level encoder layer, similarly to the vanilla Transformer. We add an attention module that attends over the BOS token embeddings from the hierarchical encoder layer.</p><p>Our architecture is specifically designed to better handle long sequences, thus we evaluate it on summarization and document level translation tasks, which tend to have long source sequences as the input. We also run experiments with non-generation tasks with an encoder-only hierarchical attention architecture (Section 7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Summarization Tasks</head><p>We characterize all the summarization tasks into several categories, test our architectures with different weight initializations and compare them with their non-hierarchical counterparts using the same weight initializations.</p><p>Long sequence datasets The PubMed and arXiv datasets <ref type="bibr">(Cohan et al., 2018)</ref> contain scientific articles from PubMed and arXiv respectively, and use the abstract of the articles as the target summary. The sequences in each of these datasets are long and need to be truncated to be processed by most transformers. Statistics on the PubMed and arXiv datasets are given in <ref type="table" target="#tab_2">Table 1</ref>.</p><p>We add a BOS token at the beginning of each sentence in the source sequences during data preprocessing. We use 3072 as the maximum source length, 512 as the maximum target length. Longer sequences are truncated. We follow BART (Lewis et al., 2020) and use GPT2 <ref type="bibr" target="#b10">(Radford et al., 2019)</ref> byte-pair encoding (BPE). We random initialize the hierarchical encoder layer, the hierarchical attention modules in the decoder layers and the additional positional embedding from 512 to 3072. We initialize all the remaining weights with pre-trained weights from BART. We also initialize a plain seq2seq model with BART pre-trained weights for direct comparison.</p><p>For training, we use a batch-size of 128. We set weight decay to 0.01 and use the Adam optimizer with (? 1 , ? 2 ) = (0.9, 0.999) and ? = 10 ?8 (Kingma and Ba, 2015). We train for 30000 steps and warm up the learning rate for 900 steps to 3 ? 10 ?5 and decay it linearly afterwards. We use a dropout rate of 0.1 for attention and all layers. We also use label smoothing with smoothing constant 0.1. We use mixed precision for training. We complete 30000 steps in approximately 115 hours (100 hours) for HAT (plain-transformer) on arXiv with 2 A100 GPUs. <ref type="bibr">1</ref> We train three models with 1 Only training time excluding validation and model saving. different batch sizes <ref type="bibr">(32, 64, and 128)</ref> and choose the best model based on minimizing label smoothing loss. We used the same training parameters for both plain and hierarchical models.</p><p>For generation, we use a beam width of 2, length penalty of 1 and minimum and maximum generation lengths of 72 and 966 respectively.</p><p>News datasets CNN/DailyMail <ref type="bibr">(Hermann et al., 2015)</ref> and XSum <ref type="bibr" target="#b8">(Narayan et al., 2018)</ref> are commonly used news summarization datasets. Both datasets are sourced from news articles, which frequently include a short summary in the article. Statistics for CNN/DM and Xsum are in <ref type="table" target="#tab_2">Table 1</ref>.</p><p>We use 1024 as maximum source length for article and 256 as maximum target length for the summary. Longer sequences are truncated. We apply the same data-processing and model initialization for these two datasets as we did for the long sequence datasets. We train on CNN/DM for 20000 steps with a batch-size of 64 and a peak learning rate of 3 ? 10 ?5 with linear decay afterwards. During generation, we use beam size 1, no length penalty and minimum and maximum generation lengths of 40 and 120, respectively. We use the same training and generation parameters for both the hierarchical seq2seq model and the plain seq2seq model. We grid search on batch sizes (32 and 64) and number of total steps (20k and 30k) and choose the best model by label smoothing loss.</p><p>Conversational datasets Since conversational summarization datasets are rare, we only consider the SAMSum (Gliwa et al., 2019) corpus. It consists of staged chat conversations between two people and corresponding abstractive summaries written by linguists. Statistics for SAMSum datset are presented in <ref type="table" target="#tab_2">Table 1</ref>.</p><p>During data-processing, we concatenate the role string with its utterance and add a BOS token at the beginning of each speaker turn. Then we concatenate all the turns together and use this as the source sequence. We add segmentation embeddings to our model, where we map the same role along with its utterance to the same segment embedding. The input to the encoder layers is the sum of the token embeddings, position embeddings and segment embeddings. We randomly initialize segment embedding parameters, and initialize the remaining parameters with the hierarchical seq2seq model trained on CNN-DM. For comparison, we also use train a plain seq2seq model initialized with weights</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instances</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Length</head><p>Output <ref type="table" target="#tab_2">Length  Dataset  Train Valid Test Words Tokens Words Tokens  PubMed  112K 6.6K 6.7K 3016  4016  178  258  ArXiv  203K 6.4K 6.4K 6055  8613  280  362  CNN-DM 287K 13K 11K  781  906  56  63  XSum  204K 11K 11K  431  488  23  27  SAMSum 14.7K 818  819  94  147  20  26  AMI  100  17  20  3156  4081  280  321  ISCI  43  10  6  6228  7913</ref> 466 576   <ref type="table" target="#tab_2">Table 1</ref>. We followed <ref type="bibr" target="#b15">(Shang et al., 2018)</ref> for splitting the data.</p><p>Since the meeting transcripts are transcribed from a speech to text model, we first built a small lexicon that filters out meaningless words. Then we add BOS tokens to each turn and concatenate all the turns together, which we use as input to the model. Following the conversational dataset approach, we add segment embeddings for each role. We follow the same weight initialization procedure as with the conversational datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Document level Machine Translation.</head><p>Historically, sentence level translation approaches have outperformed document level approaches for translating entire documents. This indicates that document level approaches are not able to incorporate the larger context of the document during translation. We test our hierarchical model on a translation task and see significant improvements over the non-hierarchical counterpart on English to German document translation.</p><p>Datasets We evaluate our architecture on the WMT20 English to German, English to Czech and TED17 Chinese to English translation tasks. Dataset statistics are shown in <ref type="table" target="#tab_3">Table 2</ref>. We only process 512 tokens at once due to memory constraints. Thus we split documents into chunks of at most 512 tokens. We only split at segment boundaries to maintain alignment between the source and target languages. We translate each chunk separately and afterwards concatenate the translated chunks to compute the BLEU score for the entire document. We use moses for preprocessing the English, German and Czech text and Jieba for tokenizing the Chinese text. We use the fastBPE implementation of <ref type="bibr" target="#b14">Sennrich et al. (2016)</ref> for byte-pair encoding with 40000 bpe codes and joined dictionaries for all tasks. For the WMT20 tasks, we use the WMT18 test data for validation and test the models on both the WMT19 and WMT20 test data. For TED17, we use the test and validation data prior to 2016 as validation data and test the model on the TED16 and TED17 test data.</p><p>Model and Optimization For English to German and English to Czech translation, we use the Transformer architecture <ref type="bibr">(Vaswani et al., 2017)</ref> with 6 encoder/decoder layers, a hidden size of 1024, 16 attention heads and 4096 for the dimen-sion of the feedforward networks. The hierarchical model has 260M parameters and the plain model has 222M parameters.</p><p>For Chinese to English translation we use the same architecture, but with a hidden size of 512, 4 attention heads and 1024 for the dimension of the feedforward netoworks. For the hierarchical models, we use one hierarchical encoder layer. The hierarchical model has 64M parameters and the plain model has 55M parameters.</p><p>For all training experiments we use the Adam optimizer with (? 1 , ? 2 ) = (0.9, 0.98) and ? = 10 ?6 . For training English to German translation, we use 1024 tokens per batch, 16 gradient accumulation steps and we train on 8 V100 GPUs. We minimize the label smoothed cross entropy loss with smoothing constant 0.1 and use dropout of 0.1 for regularization. We train with mixed precision for a total of 50000 steps and warm up the learning rate for 1250 steps to 10 ?4 and decay it linearly afterwards. Training takes about 25 hours for the hierarchical model and 17 hours for the plain model.</p><p>For training English to Czech translation, we use 1024 tokens per batch, 16 gradient accumulation steps and we train on 8 V100 GPUs. We use label smoothing with smoothing constant 0.2, a dropout of 0.2 and 0.1 weight decay. We train with mixed precision for a total of 15000 steps and warm up the learning rate to 5 ? 10 ?4 for 500 steps and decay it linearly to 0 afterwards. Training takes about 8 hours for the hierarchical model and 5 hours for the plain model.</p><p>For training English to Chinese translation, we use 8192 tokens per batch, 2 gradient accumulation steps and we train on one V100 GPU. We use label smoothing with smoothing constant 0.2, a dropout of 0.3 and 0.1 weight decay. We train with mixed precision for a total of 8000 steps and warm up the learning rate to 10 ?3 for 100 steps and decay it linearly to 0 afterwards. Training takes about 1.5 hours for the hierarchical model and 1 hour for the plain model.</p><p>For all languages, we use a beam width of 4 during generation and we generate until we encounter an EOS token. We manually tuned these parameters by choosing the best validation BLUE score. However, note that the above parameters were not extensively tuned and we do not use a pre-train.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>As shown in <ref type="table" target="#tab_5">Table 3</ref>, we achieve state-of-the-art results on the PubMed and arXiv datasets. As shown in <ref type="table" target="#tab_6">Table 4</ref>, our hierarchical seq2seq architecture outperforms its plain seq2seq peer initialized with the same pretrained weights on the news translation tasks XSum and CNN/DM and we achieve stateof-the-art results on the CNN/DM dataset. We outperform the previous baseline by 7 Rouge scores on SAMSum as shown in <ref type="table" target="#tab_7">Table 5</ref>. We also achieve state-of-the-art on ROUGE R2 on the AMI dataset, shown in <ref type="table" target="#tab_8">Table 6</ref>. File2rouge was used to evaluate Rouge score for all the results. <ref type="table" target="#tab_9">Table 7</ref> shows our document level machine translation results. Our hierarchical seq2seq architecture outperforms the plain seq2seq baseline in English to German (En-De) translation, while for English to Czech (En-Cs) and Chinese to English (Zh-En) there is no clear improvement. We hypothesize that this is because En-Cs and Zh-En contain significantly less documents than En-De (see <ref type="table" target="#tab_3">Table 2</ref>) and the hierarchical layers are not able to make use of the BOS embeddings which are randomly initialized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Analysis</head><p>The addition of hierarchical learning improves rouge scores over prior state-of-the-art methods for several summarization datasets. Here we compare the generated output of our hierarchical model with the non-hierarchical counterpart for three instances from the arXiv test data. We also include the introduction of each article. These can be found in Appendix A. Since there is often overlap between the abstract and the introduction, the models have learned to extract sentences from the introduction. We highlight the sentences extracted by the hierarchical model in blue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Ablation</head><p>Encoder-only transformer hierarchical attention. We evaluate our hierarchical attention model on several classification tasks. Instead of using our seq2seq architecture, we design a similar encoder-only architecture with hierarchical attention. This also allows us to easily pre-train the hierarchical layers.</p><p>Our architecture is based on the encoder of <ref type="bibr">(Vaswani et al., 2017)</ref>. We add a hierarchical attention module after the self attention module in each of the encoder layers. Similarly to how we prepro-PubMed arXiv R1 R2 RL R1 R2 RL PEGASUS <ref type="bibr">(Zhang et al., 2019) 45.97 20.15 41.34 44.21 16.95 38.83</ref> BigBird <ref type="bibr" target="#b23">(Zaheer et al., 2020)</ref> 46      cessed the summarization and translation datasets, we insert BOS tokens at the beginning of sentences. We follow RoBERTa  for pretraining our model by using the same dataset, preprocessing steps and pre-training objective. We evaluate the pre-trained model on three downstream tasks: SQuAD 2.0 <ref type="bibr" target="#b12">(Rajpurkar et al., 2018)</ref>, <ref type="bibr">MNLIm (Williams et al., 2018)</ref> and <ref type="bibr">RACE (Lai et al., 2017)</ref>. We observe that the pre-training converges faster to a better optimum with lower complexity than RoBERTa with the same hyperparameters. However, downstream task performances are not improved consistently.</p><p>The results are given in <ref type="table" target="#tab_11">Table 8</ref>. We observe that for SQuAD 2.0 and MNLI-m our hierarchical model does not perform better than the nonhierarchical model. However, the performance for RACE is significantly better, which suggests that there are some benefits in using hierarchical attention for classification tasks with long source sequences. Note that when fine-tuning on RACE, we had to disable dropout for the first epoch and then set it to 0.1, otherwise the model did not converge.  What has the hierarchical attention learned?</p><p>In order to better understand what the hierarchical model has learned, we plot a heatmap of the hierarchical attention between the decoder and the encoder. We use the best performing hierarchical model for this experiment <ref type="table" target="#tab_5">(Table 3)</ref>. We generate summaries for each sample article and record the hierarchical attention between the decoder and the BOS embeddings from the encoder at each step of generating the summary. For each of the 12 decoder layers and each of the 16 attention heads, we get a distribution over the BOS tokens for each generated summary tokens. To visualize the attention more concisely, we aggregate across the attention heads by choosing only the 16 BOS tokens with the highest weight for each generated token. We normalize the distribution such that the sum of the weights to 1. The hierarchical attention heatmaps for each layer are shown in <ref type="figure">Figures 2 and 3</ref>. We see that across different layers the model at-tends to different BOS tokens across the entire document. For each layer there are several horizontal lines, which indicates that some BOS tokens are assigned large weights at each step during generation. However, we also observe many individual points and discontinuous horizontal lines on the heatmaps, indicating that the model selectively attends to different BOS tokens across the entire document. We note that in the first few layers, the model seems to attend to the BOS tokens more uniformly while in the last few layers the model attends more heavily to certain BOS tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We designed a transformer based architecture with hierarchical attention and obtained improvements on several sequence to sequence generation tasks. We showed significant improvements on documentlevel machine translation for the WMT20 En-De translation task, as compared to our baseline. We did not see significant gains when applying hierarchical attention to encoder-only classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Future Work</head><p>Although we did not see significant gains when using a hierarchical encoder-only model for classification, we believe that a combination of modifying the architecture and the pre-training process might improve over current non-hierarchical models for classification tasks with long source sequences. For instance, one might introduce a pre-training objective specific to hierarchical attention such as predicting masked tokens in a sentence given only the representation of the BOS token corresponding to that sentence. We also believe the impact of dropout on hierarchical layers should be investigated more closely, since training on RACE diverged unless we disabled dropout during the first epoch (see Section 7).</p><p>convolutional neural networks typically consist of an input layer , a number of hidden layers , followed by a softmax classification layer . the input layer , and each of the hidden layers , is represented by a three -dimensional array with size , say , @xmath0 . the second and third dimensions are spatial . the first dimension is simply a list of features available in each spatial location . for example , with rgb color images @xmath1 is the image size and @xmath2 is the number of color channels . the input array is processed using a mixture of convolution and pooling operations . as you move forward through the network , @xmath3 decreases while @xmath4 is increased to compensate . when the input array is spatially sparse , it makes sense to take advantage of the sparsity to speed up the computation . more importantly , knowing you can efficiently process sparse images gives you greater freedom when it comes to preparing the input data . consider the problem of online isolated character recognition ; online means that the character is captured as a path using a touchscreen or electronic stylus , rather than being stored as a picture . recognition of isolated characters can be used as a building block for reading cursive handwriting , and is a challenging problem in its own right for languages with large character sets . each handwritten character is represented as a sequence of strokes ; each stroke is stored as a list of @xmath5-and @xmath6-coordinates . we can draw the characters as @xmath1 binary images : zero for background , one for the pen color . the number of pixels is @xmath7 , while the typical number of non -zero pixels is only @xmath8 , so the first hidden layer can be calculated much more quickly by taking advantage of sparsity . another advantage of sparsity is related to the issue of spatial padding for convolutional networks . convolutional networks conventionally apply their convolutional filters in valid mode they are only applied where they fit completely inside the input layer . this is generally suboptimal as makes it much harder to detect interesting features on the boundary of the input image . there are a number of ways of dealing with this . padding the input image @xcite with zero pixels . this has a second advantage : training data augmentation can be carried out in the form of adding translations , rotations , or elastic distortions to the input images . adding small amounts of padding to each of the convolutional layers of the network ; depending on the amount of padding added this may be equivalent to applying the convolutions in full mode . this has a similar effect to adding lots of padding to the input image , but it allows less flexibility when it comes to augmenting the training data . applying the convolutional network to a number of overlapping subsets of the image @xcite ; this is useful if the input images are not square . this can be done relatively computationally efficiently as there is redundancy in the calculation of the lower level convolutional filters . however , the ( often large ) fully connected classification layers of the network must be evaluated several times . sparsity has the potential to combine the best features of the above . the whole object can be evaluated in one go , with a substantial amount of padding added at no extra cost .  arXiv prediction and target -2 HAT spatial sparsity is an important feature of convolutional neural networks ( cnns ) . when the input array is sparse , it makes sense to take advantage of the sparsity to speed up the computation . more importantly , knowing you can efficiently process sparse images gives you greater freedom when it comes to preparing the input data . we describe a family of cnn architectures with many layers of max -pooling , and show how sparsity can be used to improve the performance of online character recognition and image recognition .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HAT2</head><p>When the input array is spatially sparse, it makes sense to take advantage of the sparsity to speed up the computation.More importantly, knowing you can efficiently process sparse images gives you greater freedom when it comes to preparing the input data. consider the problem of online isolated character recognition; online means that the character is captured as a path using a touchscreen or electronic stylus, rather than being stored as a picture.Recognition of isolated characters can be used as a building block for reading cursive handwriting, and is a challenging problem in its own right for languages with large character sets.Each handwritten character is represented as a sequence of strokes; each stroke is stored as an array of coordinates.We show that using sparsity allows a substantial increase in the spatial resolution, allowing us to obtain good results for challenging datasets such as casia-olhwdb1. 1.</p><p>Transformer convolutional neural networks ( cnns ) are powerful tools for the recognition of spatially sparse objects . when the input is sparse , the network can be used to speed up learning , and the training data can be augmented to improve performance . we describe a family of cnn architectures with many layers of max -pooling , and use them to perform online and offline character recognition and image recognition .</p><p>Target convolutional neural networks ( cnns ) perform well on problems such as handwriting recognition and image classification . however , the performance of the networks is often limited by budget and time constraints , particularly when trying to train deep networks . motivated by the problem of online handwriting recognition , we developed a cnn for processing spatially -sparse inputs ; a character drawn with a one -pixel wide pen on a high resolution grid looks like a sparse matrix . taking advantage of the sparsity allowed us more efficiently to train and test large , deep cnns . on the casia -olhwdb1.1 dataset containing 3755 character classes we get a test error of 3.82% . although pictures are not sparse , they can be thought of as sparse by adding padding . applying a deep convolutional network using sparsity has resulted in a substantial reduction in test error on the cifar small picture datasets : 6.28% on cifar-10 and 24.30% for cifar-100 . * keywords : * online character recognition , convolutional neural network , sparsity , computer vision question answering ( qa ) aims to automatically understand natural language questions and to respond with actual answers . the state -of -the -art qa systems usually work relatively well for factoid , list and definition questions , but they might not necessarily work well for real world questions , where more comprehensive answers are required . frequently asked questions ( faq ) based qa is an economical and practical solution for general qa @xcite . instead of answering questions from scratch , faq -based qa tries to search the faq archives and check if a similar question was previously asked . if a similar question is found , the corresponding answer is returned to the user . the faq archives are usually created by experts , so the returned answers are usually of higher -quality . the core of faq -based qa is to calculate semantic similarities between questions . this is a very challenging task , because two questions , which share the same meaning , may be quite different at the word or syntactic level . for example , how do i add a vehicle to this policy ? " and what should i do to extend this policy for my new car ? " have few words in common , but they share the same answer . in the past two decades , many efforts have been made to tackle this lexical gap problem . one type of methods tried to bridge the lexical gap by utilizing semantic lexicons , like wordnet @xcite . another method treated this task as a statistical machine translation problem , and employed a parallel question set to learn word -to -word or phrase -to -phrase translation probabilities @xcite . both of these methods have drawbacks . the first method is hard to adapt to many other languages , because the semantic lexicon is unavailable . for the second method , a large parallel question set is required to learn the translation probabilities , which is usually hard or expensive to acquire . to overcome these drawbacks , we utilize distributed word representations to calculate the similarity between words , which can be easily trained by only using amount of monolingual data . in this paper , we propose a novel word -alignment -based method to solve the faq -based qa tasks . the characteristics of our method include :</p><p>( 1 ) a neural network model for calculating question similarity with word alignment features . for an input question and a candidate question , the similarities of each word pairs ( between the two questions ) are calculated first , and then the best word alignment for the two questions is computed . we extract a vector of dense features from the word alignment , then import the feature vector into a neural network and calculate the question similarity in the network s output layer .</p><p>( 2 ) a bootstrap -based feature extraction method . the faq archives usually contain less than a few hundred questions , and in order to avoid overfitting , we are unable to use too many sparse features . therefore , we come up with this method to extract a small set of effective sparse features according to our system s ranking results .</p><p>( 3 ) a learning -to -rank algorithm for training . the faq -based qa task is essentially a ranking task , our model not only needs to calculate a proper similarity for each question pair , but also needs to rank the most relevant one on top of the other candidates . so we propose a learning -to -rank method to train parameters more suitable for ranking . experimental results , conducted on faq archives from three languages , demonstrate that our method is very effective . we also evaluate our method on the answer sentence selection task . experimental results on the standard trec data set show that our method outperforms all previous state -of -the -art systems . arXiv prediction and target -3 HAT in this paper , we propose a novel word -alignment -based method to solve the frequently asked questions ( faq ) -based question answering ( qa ) tasks . the characteristics of our method include : ( 1 ) a neural network model for calculating question similarity with word alignment features . for an input question and a candidate question , the similarities of each word pairs ( between the two questions ) are calculated first , and then the best word alignment for the two question is computed . we extract a vector of dense features from the word alignment , then import the feature vector into a neural networks and calculate the question similarity in the network s output layer .</p><p>( 2 ) a bootstrap -based feature extraction method . the faq archives usually contain less than a few hundred questions , and in order to avoid overfitting , we are unable to use too many sparse features . therefore , we come up with this method to extract a small set of effective sparse features according to our system s ranking results .</p><p>( 3 ) a learning -to -rank algorithm for training . since faq -based qa task is essentially a ranking task , our model not only needs to calculate a proper similarity for each question pair , but also needs to rank the most relevant candidate on top of the other candidates . so we propose an explicit learning -to -rank method to train parameters more suitable for ranking . experimental results , conducted on faq archive from three languages , demonstrate that our method is very effective . we also evaluate our method on the answer sentence selection task , and experimental results on the standard trec data set show that our system outperforms all previous stateof -the -art systems .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HAT2</head><p>In this paper, we propose a novel word-alignment-based method to solve the frequently asked questions (faq) based question answering (qa) tasks.The characteristics of our method include: (1) a neural network model for calculating question similarity with word alignment features. for an input question and a candidate question, the similarities of each word pairs (between the two questions)Are calculated first, and then the best word alignment for the two question is computed.We extract a vector of dense features from the word alignment, then import the feature vector into the neural network and calculate the question similarity in the network s output layer.</p><p>(2) a bootstrap-based feature extraction method to extract a small set of effective lexical features according to the model s ranking results.</p><p>(3) a learning-to-rank algorithm for training.The faq-based qa task is essentially a ranking task, our model not only needs to calculate a proper similarity for each question pair, but also needs to rank the most relevant candidate on top of the other candidates.So we propose to train parameters more suitable for ranking.Experimental results, conducted on faq archives from three languages, demonstrate that our method is very effective.We also evaluate our method on the answer sentence selection task.Experiment results on the standard trec data set show that our system outperforms all previous state-of-the-art systems.</p><p>Transformer in this paper , we propose a novel word -alignment -based method to solve the frequently asked questions ( faq ) based question answering ( qa ) tasks . the characteristics of our method include : ( 1 ) a neural network model for calculating question similarity with word alignment features . for an input question and a candidate question , the similarities of each word pairs ( between the two questions ) are calculated first , and then the best word alignment for the two question is computed . we extract a vector of dense features from the word alignment , then import the feature vector into a neural networks and calculate the question similarity in the network s output layer .</p><p>( 2 ) a bootstrap -based feature extraction method . the faq archives usually contain less than a few hundred questions , and in order to avoid overfitting , we are unable to use too many sparse features . therefore , we come up with this method to extract a small set of effective sparse features according to our system s ranking results .</p><p>( 3 ) a learning -to -rank algorithm for training . thefaq -based qa task is essentially a ranking task , our model not only needs to calculate a proper similarity for each question pair , but also needs to rank the most relevant one on top of the other candidates . so we propose an explicit learning -to -rank method to train parameters more suitable for ranking . experimental results , conducted on faq archive from three languages , demonstrate that our method is very effective . we also evaluate our method on the answer sentence selection task . experimental result on the standard trec data set shows that our system outperforms all previous state -ofthe -art systems .</p><p>Target in this paper , we propose a novel word -alignment -based method to solve the faq -based question answering task . first , we employ a neural network model to calculate question similarity , where the word alignment between two questions is used for extracting features . second , we design a bootstrap -based feature extraction method to extract a small set of effective lexical features . third , we propose a learning -to -rank algorithm to train parameters more suitable for the ranking tasks . experimental results , conducted on three languages ( english , spanish and japanese ) , demonstrate that the question similarity model is more effective than baseline systems , the sparse features bring 5% improvements on top-1 accuracy , and the learning -to -rank algorithm works significantly better than the traditional method . we further evaluate our method on the answer sentence selection task . our method outperforms all the previous systems on the standard trec data set . </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The architecture of HAT: Hierarchical Attention Transformer. The blocks shaded in pink are the additional hierarchical layers added to the existing Transformer (Vaswani et al., 2017) architecture. The extra BOS tokens for hierarchical learning are highlighted in purple. The figure is based on Figure 1 in Vaswani et al. (2017).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:2104.07545v2 [cs.CL] 16 Sep 2021</figDesc><table><row><cell cols="2">1X</cell><cell cols="3">Hierarchical</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Learning</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Input</cell><cell></cell><cell></cell><cell></cell></row><row><cell>[bos 1] +</cell><cell cols="2">sentence 1</cell><cell>+</cell><cell>[bos 2] +</cell><cell>sentence 2</cell><cell>+?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Stats for each of the summarization datasets. Both average number of words and number of BPE tokens are presented. Words are counted before tokenization.</figDesc><table><row><cell></cell><cell cols="2">Documents</cell><cell></cell><cell cols="4">Source Length Target Length</cell></row><row><cell>Dataset</cell><cell cols="7">Train Valid Test Words Tokens Words Tokens</cell></row><row><cell cols="4">WMT20 En-De 363K 122 130</cell><cell>354</cell><cell>446</cell><cell>331</cell><cell>460</cell></row><row><cell cols="2">WMT20 En-Cs 42K</cell><cell cols="2">107 130</cell><cell>738</cell><cell>903</cell><cell>641</cell><cell>862</cell></row><row><cell cols="2">TED17 Zh-En 1906</cell><cell>97</cell><cell>12</cell><cell>3775</cell><cell>2680</cell><cell>2107</cell><cell>2590</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Meeting datasets The AMI(Carletta et al.,  2006)  andISCI (Janin et al., 2003)  corpora consist of staged meetings which are transcribed and annotated with abstractive summaries. The meeting transcripts are extremely long and the turn-based structure of the meetings makes these datasets particularly suitable for the hierarchical architecture, since speaker turns can be marked by hierarchical tokens. Statistics for AMI and ISCI datasets are illustrated in</figDesc><table /><note>Statistics for the WMT20 En-De, WMT20 En-Cs and TED17 Zh-En translation datasets. Both average number of words and number of BPE tokens are presented.from a plain seq2seq trained on CNN-DM.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>.32 20.65 42.33 46.63 19.02 41.77 LSH (Huang et al., 2021) 48.12 21.06 42.72 48.24 20.26 41.78 Transformer-BART 48.35 21.43 36.90 46.54 18.82 42.00 HAT-BART 48.36 21.43 37.00 46.68 19.07 42.17</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Results on summarization tasks with long source sequences. PEGASUS results are from BigBird<ref type="bibr" target="#b23">(Zaheer et al., 2020)</ref> paper. BigBird uses source sequence length of 4096, LSH (Huang et al., 2021) uses 7168, while Transformer-BART and HAT-BART use 3072 due to memory constraints. Transformer-BART and HAT-BART were trained using the same parameter settings.</figDesc><table><row><cell></cell><cell cols="3">CNN/DailyMail</cell><cell></cell><cell>XSum</cell></row><row><cell></cell><cell>R1</cell><cell>R2</cell><cell>RL</cell><cell>R1</cell><cell>R2</cell><cell>RL</cell></row><row><cell cols="7">PEGASUS (Zhang et al., 2019) 44.16 21.56 41.30 47.60 24.83 39.64</cell></row><row><cell>BigBird (Zaheer et al., 2020)</cell><cell cols="6">43.84 21.11 40.74 47.12 24.05 38.80</cell></row><row><cell>BART (Lewis et al., 2020)</cell><cell cols="6">44.16 21.28 40.9 45.14 22.27 37.25</cell></row><row><cell>Transformer-BART</cell><cell cols="6">44.45 21.27 41.51 45.26 22.19 37.04</cell></row><row><cell>HAT-BART</cell><cell cols="6">44.48 21.31 41.52 45.92 22.79 37.84</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell>SAMSum</cell><cell></cell></row><row><cell>R1</cell><cell>R2</cell><cell>RL</cell></row></table><note>Results on standard news summarization tasks. PEGASUS results are from Zaheer et al. (2020). Big- Bird (Zaheer et al., 2020), Transformer-BART and HAT-BART use use a source sequence length of 1024. BART and HAT-BART were trained on the same parameters settings.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>AMI</cell><cell></cell><cell></cell><cell>ISCI</cell></row><row><cell></cell><cell>R1</cell><cell>R2</cell><cell>RL</cell><cell>R1</cell><cell>R2</cell><cell>RL</cell></row><row><cell cols="3">HMNet (Zhu et al., 2020) 53.02 18.57</cell><cell>-</cell><cell cols="2">46.28 10.60</cell><cell>-</cell></row><row><cell>Transformer-CNNDM</cell><cell cols="6">52.06 19.27 50.02 43.77 11.65 41.64</cell></row><row><cell>HAT-CNNDM</cell><cell cols="6">52.27 20.15 50.57 43.98 10.83 41.36</cell></row></table><note>Results on conversational summarization tasks. The plain Transformer model (Transformer-CNNDM) was initialized by the BART CNN/DM model. The hierarchical model (HAT-CNNDM) was initialized by the hierarchical seq2seq model trained on CNN/DM.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Results on meeting summarization tasks. The plain Transformer model (Transformer-CNNDM) was initialized by the BART CNN/DM model. The hierarchical model (HAT-CNNDM) was initialized by the hierarchical Transformer model trained on CNN/DM.</figDesc><table><row><cell></cell><cell cols="6">WMT20 WMT19 WMT20 WMT19 TED17 TED16</cell></row><row><cell></cell><cell>En-De</cell><cell></cell><cell>En-Cs</cell><cell></cell><cell cols="2">Zh-En</cell></row><row><cell>Transformer (no pretrain)</cell><cell>27.1</cell><cell>32.5</cell><cell>25.0</cell><cell>18.6</cell><cell>23.2</cell><cell>23.1</cell></row><row><cell>HAT (no pretrain)</cell><cell>27.4</cell><cell>34.5</cell><cell>24.2</cell><cell>19.2</cell><cell>24.0</cell><cell>22.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table /><note>Results on the WMT20/19 En-De, En-Cs and TED17/16 Zh-En translation tasks. The numbers shown are BLEU scores computed with sacrebleu. The En-De and En-Cs models are trained on the WMT20 training data and we use the WMT18 test data for validation. The Zh-En models are trained on the TED17 training data and we use the test data from TED11-TED15 and validation data from TED10 as validation data. All models are initialized randomly with the same training setup.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Hierarchical learning for encoder only.</figDesc><table /><note>1 The results are taken from Table 1 (DOC-SENTENCES) in Liu et al. (2019)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>in section [ deepcnet]-[deepcnin ] we describe a family of convolutional networks with many layers of max -pooling . in section [ sparsity][nn ] we describe how sparsity applies to character recognition and image recognition . in section [ results ] we give our results . in section [ sec : conclusion ] we discuss other possible uses of sparse cnns .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>Introduction of sample article from arXiv test data (2).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10</head><label>10</label><figDesc></figDesc><table><row><cell>: ROUGE(HAT): 37.25/12.25/34.01; ROUGE(Transformer): 36.84/7.96/31.58</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 11 :</head><label>11</label><figDesc>Introduction of sample article from arXiv test data (3).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 12 :</head><label>12</label><figDesc>ROUGE(HAT): 58.71/37.41/55.37; ROUGE(Transformer): 58.99/38.07/55.15 B Appendix Figure 2: Hierarchical Attention heatmaps for arXiv test article 2 (Table 9 and 10). For each summary token, we only show the top 16 BOS tokens across each head.Figure 3: Hierarchical Attention heatmaps for arXiv test article 3(Table 11 and12). For each summary token, we only show the top 16 BOS tokens across each head.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A Appendix arXiv test introduction -2</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Interpretable online banking fraud detection based on hierarchical attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarit</forename><surname>Idan Achituve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Kraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goldberger</surname></persName>
		</author>
		<idno type="DOI">10.1109/MLSP.2019.8918896</idno>
	</analytic>
	<monogr>
		<title level="m">MLSP</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hierarchical graph attention network for visual relationship detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13886" to="13895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Document-level neural machine translation with hierarchical attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lesly</forename><surname>Miculicich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhananjay</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1325</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2947" to="2954" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Explorations in automatic book summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Ceylan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="380" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An algorithm for one-page summarization of a long text based on thematic hierarchy detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshio</forename><surname>Nakao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Don&apos;t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1206</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1797" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A deep reinforced model for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-totext transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Know what you don&apos;t know: Unanswerable questions for squad</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="784" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-agent actor-critic with hierarchical graph attention network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heechang</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayong</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinkyoo</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="7236" to="7243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised abstractive meeting summarization with multisentence compression and budgeted submodular maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guokan</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wensi</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Tixier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Polykarpos</forename><surname>Meladianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis</forename><surname>Vazirgiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Pierre</forename><surname>Lorr?</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1062</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="664" to="674" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hierarchical contextual attention recurrent neural network for map query suggestion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haishan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongfei</forename><forename type="middle">Mark</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2017.2700392</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1888" to="1901" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<title level="m">the AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Discourse-aware neural extractive text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5021" to="5031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: human language technologies</title>
		<meeting>the 2016 conference of the North American chapter of the association for computational linguistics: human language technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04070</idno>
		<title level="m">Bp-transformer: Modelling long-range context via binary partitioning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kumar Avinava Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifan</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Pegasus: Pre-training with extracted gap-sentences for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Summarizing chinese medical answer with graph convolution networks and question-focused dual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.2</idno>
	</analytic>
	<monogr>
		<title level="m">EMNLP (Findings)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="15" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deconvolutional paragraph representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4172" to="4182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Document embedding enhanced event detection with hierarchical and supervised attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="414" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Abstractive meeting summarization via hierarchical adaptive segmental network learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changjie</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3308558.3313619</idno>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3455" to="3461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A hierarchical network for abstractive meeting summarization with cross-domain pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruochen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="194" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep communicating agents for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Asli ? Elikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1662" to="1675" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
