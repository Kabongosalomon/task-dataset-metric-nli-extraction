<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Few Could Be Better Than All: Feature Sampling and Grouping for Scene Text Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingqun</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NetEase</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqing</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongye</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NetEase</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkun</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NetEase</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanglong</forename><surname>Hu</surname></persName>
							<email>guanglong.hu@163.com</email>
							<affiliation key="aff0">
								<orgName type="institution">NetEase</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
							<email>xbai@hust.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Few Could Be Better Than All: Feature Sampling and Grouping for Scene Text Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, transformer-based methods have achieved promising progresses in object detection, as they can eliminate the post-processes like NMS and enrich the deep representations. However, these methods cannot well cope with scene text due to its extreme variance of scales and aspect ratios. In this paper, we present a simple yet effective transformer-based architecture for scene text detection. Different from previous approaches that learn robust deep representations of scene text in a holistic manner, our method performs scene text detection based on a few representative features, which avoids the disturbance by background and reduces the computational cost. Specifically, we first select a few representative features at all scales that are highly relevant to foreground text. Then, we adopt a transformer for modeling the relationship of the sampled features, which effectively divides them into reasonable groups. As each feature group corresponds to a text instance, its bounding box can be easily obtained without any post-processing operation. Using the basic feature pyramid network for feature extraction, our method consistently achieves state-of-the-art results on several popular datasets for scene text detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Scene text detection has been an active research field for a long time, because of its wide range of practical applications, such as scene understanding, automatic driving, and photo translation. As a key prior component of scene text reading, scene text detection aims to precisely locate text in scene images. Despite the noticeable improvement achieved by existing methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b66">67]</ref>, it is still a challenging task due to the variety of scene text, e.g. different scales, complicated illumination, perspective distortion, * Corresponding Author multi-orientations, and complex shapes. Moreover, most scene text detection methods depend on complicated processing to generate or refine the predicted results, such as anchor generation, non-maximum suppression (NMS) <ref type="bibr" target="#b36">[37]</ref>, binarization <ref type="bibr" target="#b20">[21]</ref>, or contour extraction <ref type="bibr" target="#b42">[43]</ref>.</p><p>Inspired by the advantages of the transformer <ref type="bibr" target="#b45">[46]</ref> in natural language processing (NLP), lots of works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b72">73]</ref> introduce it into vision tasks to extract global-range features and model long-distance dependencies in images, while showing promising performance. Especially in object detection, DETR-based methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b72">73]</ref> successfully use transformers to remove the complicated hand-designed processes (e.g. NMS and anchor generation) from the former object detection frameworks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b39">40]</ref>.</p><p>Although transformers bring advantages in global-range feature modeling to DETR-based frameworks <ref type="bibr" target="#b2">[3]</ref>, they may suffer from handling the small objects and the high computational complexity. For instance, a recent DETR-based scene text detector <ref type="bibr" target="#b38">[39]</ref> cannot achieve the satisfactory detection accuracy on the ICDAR2015 dataset <ref type="bibr" target="#b14">[15]</ref> and ICDAR2017-MLT dataset <ref type="bibr" target="#b35">[36]</ref>, since the text instances in these two datasets have much larger variance of scales and aspect ratios. It is often insufficient for transformers to capture small text on the feature map at small scales, while the time cost of a DETR-based method with multi-scale feature maps is unpredictable. Essentially, unexpected background noise in higher-resolution feature maps would significantly increase the computational cost and disturb the transformer modeling. Though, some recent works <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b72">73]</ref> improve the efficiency of transformer-based object detectors by optimizing the attention operations, they fail to achieve the competitive results in scene text detection (refer to the results reported in Tab. 6).</p><p>In this paper, we propose a simple yet effective transformer-based architecture for scene text detection. We argue that feature learning with the relationship of all pixels is not necessary, as foreground text instances only occupy a few small and narrow regions in scene images. Intuitively, we firstly sample and collect the features that are highly relevant to scene text as illustrated in <ref type="figure" target="#fig_0">Fig. 1(a)</ref> <ref type="bibr">(b)</ref>. Then, we adopt a transformer for modeling the relationship of the sampled features so that they can be properly grouped. As shown in <ref type="figure" target="#fig_0">Fig. 1(c)(d)</ref>, benefiting from the powerful attention mechanism of the transformer, each feature group will correspond to a text instance, which is quite convenient for predicting its bounding box.</p><p>Different from the previous scene text detection methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b70">71]</ref> that usually learn the deep representations of scene text images in a holistic manner with CNNs, our detection method based on only a few representative features has three prominent advantages: 1) it can significantly eliminate the redundant background information, which is beneficial for improving the effectiveness and efficiency of the detection process; 2) Using a transformer to group the sampled features, we can obtain more accurate grouping results and bounding boxes without any post-processing operation; 3) As the feature sampling and grouping are implemented in an end-to-end fashion, the two stages can jointly improve the final detection performance. To verify the effectiveness of the proposed feature sampling-and-grouping scheme, we conduct extensive experiments on several popular datasets <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b64">65]</ref> for scene text detection, consistently achieving the state-of-of-art results. In addition, the comparison with the recent transformer-based detectors <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b72">73]</ref> also proves the effectiveness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Lots of works on scene text detection have been proposed before, which can be roughly divided into two categories: bottom-up methods and top-down methods.</p><p>Bottom-up methods firstly detect/segment the basic components or pixels of scene text, which are then formed into bounding boxes with some heuristic operations. In an early method, CTPN <ref type="bibr" target="#b44">[45]</ref> develops a vertical anchor mechanism to predict sequential proposals, and naturally connects them into bounding boxes by a recurrent neural network. To better detect long and dense text, SegLink <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b43">44]</ref> detects components and links of each text instance, and combines them together to generate the final detection results. In addition, the fundamental components can be defined as characters with affinity boxes (e.g. CRAFT <ref type="bibr" target="#b1">[2]</ref>) or center points with radius (e.g. TextSnake <ref type="bibr" target="#b27">[28]</ref>). These methods are more flexible in detecting text with various shapes, as long as the components can be detected and grouped into final results. However, it suffers from missing components and background noise, and the final detection results are susceptible to the grouping post-process. Our proposed method, which is also a bottom-up method, can predict the bounding boxes by sampling and grouping at the feature level while not relying on any post-processing.</p><p>Top-down methods directly predict bounding boxes of scene text at the word or line level. Inspired by the popular object detectors <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b39">40]</ref>, some methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b30">31]</ref> adjust default anchors into quadrilaterals or rotated bounding boxes to fit the multi-orientations and various aspect ratios of scene text. EAST <ref type="bibr" target="#b70">[71]</ref> directly regresses the coordinates of multi-oriented bounding boxes on the entire feature map. To directly detect curved text in the wild, recent methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b73">74]</ref> adopt Bezier curves or Fourier signatures for locating scene text, and apply extra processes (e.g. Bezier-Align, Inverse Fourier Transformation, and NMS) to generate the final detection results. These top-down methods are usually more straightforward than the bottom-up ones, but they still need some hand-designed processes, such as anchor generation, NMS, and binarization.</p><p>Inspired by the power of transformers in natural language processing, the pioneer work, DETR <ref type="bibr" target="#b2">[3]</ref>, presents a novel transformer-based architecture for object detection. It discards several hand-designed processes employed in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b39">40]</ref>, while achieving promising performance. Although a recent method <ref type="bibr" target="#b38">[39]</ref> has tried to apply the DETRbased architecture to scene text detection, it can not achieve a satisfying detection performance on ICDAR2015 <ref type="bibr" target="#b14">[15]</ref> and ICDAR2017-MLT <ref type="bibr" target="#b35">[36]</ref>. Since scene text is more challenging than common objects for its extreme variance of scales  <ref type="figure" target="#fig_3">Figure 2</ref>. The overview of our proposed transformer-based architecture. It consists of a backbone network, a multi-scale feature sampling network, and a feature grouping network. Specifically, multi-scale feature maps are first produced from the backbone network. Next, a multi-scale text extractor is used to predict the confidence scores of the representative text regions at the pixel level. Then, we select text point features with top-N scores and concatenate them with position embeddings. After that, we adopt a transformer to model the relationship between the sampled features and implicitly group them into fine representations by the attention mechanism. Finally, the detection results are obtained from the prediction heads. and aspect ratios, transformers cannot obtain sufficient information from a single scale feature map. The multi-scale scheme can somewhat cope with this problem, but it incurs a huge computational overhead for transformers. Different from those DETR-based methods <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b72">73]</ref> focusing on improving the attention units, we propose to eliminate redundant background information directly and select a few important features <ref type="bibr" target="#b71">[72]</ref> from multi-scale feature maps. Thus, both computational overhead and the quality of sampled features can be taken into account, which facilitates transformers being better employed for text detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, we first introduce the overall architecture of the proposed scene text detection method. Then, we elaborate on the proposed feature sampling-and-grouping scheme and further analyze the advantages of feature sampling in transformer modeling. Finally, we describe the details about the training of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network Architecture</head><p>As shown in <ref type="figure" target="#fig_3">Fig. 2</ref>, our proposed transformer-based architecture is composed of a backbone network, a feature sampling network, and a feature grouping network.</p><p>The backbone is the basic feature pyramid network (FPN) <ref type="bibr" target="#b21">[22]</ref> equipped with ResNet-50 <ref type="bibr" target="#b11">[12]</ref>. The produced feature maps F in three different scales (i.e. 1/4, 1/8, 1/16) are used for feature sampling.</p><p>In our feature sampling network, the three feature maps are first down-sampled to smaller scales (i.e. 1/8, 1/16, 1/32) by a Coord-Convolution layer <ref type="bibr" target="#b22">[23]</ref> and a constrained deformable pooling layer. Then, several convolution layers are employed to generate confidence score maps to distinguish representative text regions. After that, we only select the features with top-N k scores in each scale layer k, and gather them into a sequence form with a shape ( k N k , C), where C is the channel number.</p><p>In our feature grouping network, the sampled features are first concatenated with position embeddings. Then, we adopt transformer encoder layers to model their relationships, and implicitly aggregate the features from the same text instance. Finally, scores and coordinates of bounding boxes (or polygons) are obtained via a text/non-text classification head and a text detection head, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature Sampling</head><p>Despite the novel structure and promising performance in object detection, transformer-based methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b38">39]</ref> can not perform well on scene text detection due to the extreme variance of scales and aspect ratios. Following previous text detectors <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b47">48]</ref>, we use multi-scale features from the FPN to boost the detection performance. Nevertheless, such a scheme incurs unbearable computational cost and much longer convergence time for transformers. We observe that foreground text instances only occupy small and narrow regions, and useful information for localizing text is relatively sparse. Hence, we propose a feature sampling network to decrease redundant background noise involved by multi-scale features, reducing the computational complexity and facilitating feature learning for transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Scale Text Extractor</head><p>To sample representative features from foreground text, we apply a simple multiscale text extractor to predict the confidence scores for text regions at the pixel level. Following CoordConv <ref type="bibr" target="#b22">[23]</ref>, we first concatenate each feature map with two extra channels of normalized coordinates to introduce location information. Let F denote the feature maps from the FPN in different scales (i.e. 1/4, 1/8, 1/16), and</p><formula xml:id="formula_0">F = {f k ? R H k ?W k ?C |k = 0, 1, 2}.</formula><p>(1)</p><p>Then the position information is injected via</p><formula xml:id="formula_1">f k = Conv(f k ? C k ),<label>(2)</label></formula><p>where ? stands for the concatenation operation, and C k ? R H k ?W k ?2 denotes the normalized coordinates. Inspired by deformable ROI pooling <ref type="bibr" target="#b4">[5]</ref>, we specifically design a constrained one to down-sample the multi-scale feature maps. Since the text area is relatively concentrated, the predicted offsets in deformable pooling with further distance will introduce irrelevant information into the pooled features. Thus, we add a learnable scaling parameter to constrain the predicted offsets, and pool f k tof k with smaller scales (i.e. 1/8, 1/16, 1/32).</p><p>Finally, we construct a simple scoring net S composed of convolution layers and a Sigmoid function to generate the confidence score maps for representative text regions at all scales. To better distinguish the importance of pixels at different positions in each text instance, different scores over positions are used for supervision. To generate the score maps, we adjust the Gaussian heatmap generation in general object detection <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16]</ref> for text instances in the word level. Specifically, a two-dimensional Gaussian distribution is implemented to generate the ground truth S t = {S t k |k = 0, 1, 2} for S, ensuring that the central part of each text instance has the highest importance score, and the scores gradually decrease from the center to contours.</p><p>Feature Sampling To reduce the redundant background noise, we design a strategy for selecting representative features that are highly relevant to foreground text. These features, containing rich geometric and context information of foreground text, would be sufficient for text localization.</p><p>Let S denote the predicted score maps, and</p><formula xml:id="formula_2">S = {S k ? R H ? k ?W ? k |S k = S(f k ), k = 0, 1, 2}. (3)</formula><p>Then, we sort scores in S k , and select features with top-N k scores inf k of each scale, respectively. The selected features are gathered intoF ? R N ?C for the incoming transformer modeling:</p><formula xml:id="formula_3">F = [f n ? R C |n = 0, 1, ..., N ],<label>(4)</label></formula><p>where N = 2 k=0 N k , and N k is the number of selected features in different scales. Thus, the number of enormous features at all scales can be significantly reduced. The primary selected features are probably from foreground text regions, which would contain sufficient geometric and context information for text detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Feature Grouping</head><p>Through feature selection, only a few representative features that are highly relevant to foreground text are concatenated for the incoming transformer modeling. To reserve the position information of the sampled features, we add the position embeddings intoF. Then, we adopt a transformer structure to implicitly aggregate features from the same text instance by attention mechanism. The basic form is a stacked network with four transformer encoder layers, which are composed of self-attention modules, feedforward layers, and layer normalization. Following <ref type="bibr" target="#b45">[46]</ref>, we construct our self-attention module as</p><formula xml:id="formula_4">Attn( F) = sof tmax( Q( F)K( F) T ? C ? )V ( F),<label>(5)</label></formula><p>where F ? R N ?C ? denotes the sampled features with position embeddings, and C ? is the channel number. Q, K and V denote the different linear layers. For previous methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b31">32]</ref>, the core issue of applying the attention operation on a feature map x ? R H?W ?C ? is the computational complexity on all spatial locations. In the original DETR <ref type="bibr" target="#b2">[3]</ref> encoder, the complexity of attention operation is O((HW ) 2 C ? ), which is quadratic with the spatial size. However, in our method, it is only related to the number N of selected features F, and the complexity becomes O(N 2 C ? ). In our implementation, the selected number N 2 ? (HW ) 2 , and thus the complexity of our transformer could be significantly reduced.</p><p>Finally, the output text features are fed into two prediction heads for classification and text detection. The text detection head is composed of fully-connected layers and a Sigmoid function. It can regress the coordinates of rotated bounding boxes in the form of B(x, y, h, w, ?) or 8 control points of Bezier-Curve <ref type="bibr" target="#b24">[25]</ref> for arbitrary-shaped text. x, y, h, w, and ? are the coordinates of the center point, height, width, and angle, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Optimization</head><p>The proposed model is trained in an end-to-end manner, and the objective function consists of three parts as follows:</p><formula xml:id="formula_5">L = ? c L class + ? d L det + ? f L f s ,<label>(6)</label></formula><p>where L class is the loss for classification, L det is the loss for text detection, and L f s is the loss for feature selection. ? c , ? d , and ? f are scaling factors. Following DETR <ref type="bibr" target="#b2">[3]</ref>, we adopt Hungarian algorithm for pair-wise matching before calculating losses for L class and L det . <ref type="figure">Figure 3</ref>. The qualitative results of our proposed method in different cases, including multi-oriented text, long text, multi-lingual text, low-resolution text, curved text, dense text. For curved text detection, the Bezier curves' control points are drawn in red.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss for classification</head><p>We adopt the Cross Entropy loss for text/non-text classification after pair-wise matching by Hungarian algorithm. It can be formulated as <ref type="bibr" target="#b6">(7)</ref> where N is the total number of selected features, g x represents the label of sample x and p x represents the predicted probability. The elements with denote the probabilities or labels of the matched samples after pair-wise matching.</p><formula xml:id="formula_6">L class = 1 N x ?[ g x ?log( p x )+(1? g x )?log(1? p x )],</formula><p>Loss for text detection For multi-oriented text detection, we adapt the Gaussian Wasserstein Distance (GWD) loss <ref type="bibr" target="#b57">[58]</ref> into a scale-invariant form to better balance the loss weights of text with different scales. Due to the extreme variance of scales, the loss of small text has a negligible influence on the gradient back-propagation compared with the loss of large text. Hence, we adjust the GWD loss as follows:</p><formula xml:id="formula_7">L det = 1 N r x (1 ? 1 ? + f (d 2 ( ux | tx| , tx | tx| )) ),<label>(8)</label></formula><p>where u x denotes the predicted rotated bounding box, t x denotes the target one, and | * | denotes its area. N r is the number of bounding boxes after pair-wise matching. The elements with denote the matched bounding boxes or the target ones after pair-wise matching. f (?) represents a non-linear function, and ? is a hyper-parameter to modulate the loss. d 2 will be explained in the Appendix. According to the GWD loss <ref type="bibr" target="#b57">[58]</ref>, we set f (d 2 ) = log (d 2 + 1) and ? = 3. By normalizing u x and t x with the area of t x , we can decrease the negative effect of the scale imbalance.</p><p>For arbitrary-shaped text detection, we adopt the losses for Bezier-Curve in ABC-Net <ref type="bibr" target="#b24">[25]</ref>. Thus, the prediction head for text detection is changed to two heads for predicting both bounding boxes and the control points of Bezier curves, respectively. In the bounding box prediction head, the center point coordinates, box width and box height are predicted for each bounding boxB(x, y, h, w). In the Bezier curve prediction head, it predicts the coordinates of 8 control points for each text instance.</p><p>Loss for feature selection We apply a smooth L1 loss for optimizing the importance score maps in our feature selection as follows:</p><formula xml:id="formula_8">L f s = 1 N f k L1 smooth {S k , S t k }, k = 0, 1, 2,<label>(9)</label></formula><p>where N f is the total size of all score maps. S k and S t k are the predicted score map and the target map, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first introduce the datasets and implementation details in our experiments. Then, we present the evaluation results on public benchmarks and an ablation study on feature sampling. Finally, we compare our proposed method with some popular transformer-based detection methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>SynthText <ref type="bibr" target="#b8">[9]</ref> is a large synthetic dataset including 800k images. It is only used to pre-train our models.</p><p>ICDAR 2015 (IC15) <ref type="bibr" target="#b14">[15]</ref> contains 1000 training images and 500 testing images in English, most of which are severely distorted or blurred. All images are annotated with quadrilateral boxes at the word level.</p><p>MLT-2017 (MLT17) <ref type="bibr" target="#b35">[36]</ref>  validation images, and 9000 testing images. All images are annotated with quadrilateral boxes at the word level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MSRA-TD500</head><p>[64] is a multi-lingual text dataset in Chinese and English. It includes 300 training images and 200 testing images with multi-oriented long text. Following previous works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28]</ref>, we include HUST-TR400 <ref type="bibr" target="#b62">[63]</ref> as the extra training data in the fine-tuning stage.</p><p>MTWI <ref type="bibr" target="#b13">[14]</ref> is a large-scale dataset for Chinese and English web text reading. It contains some challenging cases, such as complex layout, small text, and watermarks. There are 10000 training images and 10000 images for testing, and all text instances are annotated at the line level. <ref type="bibr" target="#b3">[4]</ref> is a dataset that contains text of various shapes, including horizontal, multi-oriented, and curved. It contains 1255 training images and 300 testing images, and the text instances are labeled at the word level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Total-Text</head><p>CTW1500 <ref type="bibr" target="#b64">[65]</ref> is a curved text dataset, which consists of 1000 training images and 500 testing images. The text instances are annotated at the text-line level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Our model for oriented text detection is denoted as Ours (RBox), and that for arbitrary-shaped text detection is denoted as Ours (Bezier). Ours (RBox) is first pre-trained on SynthText for 150 epochs, and then fine-tuned on each corresponding real-world dataset for another 100 epochs. Ours (Bezier) follows the experiment settings of ABC-Net <ref type="bibr" target="#b24">[25]</ref>, and adds its Bezier Curve Synthetic Dataset for pretraining. We optimize our models by AdamW <ref type="bibr" target="#b28">[29]</ref> with a weight decay of 1e ?4 and a momentum of 0.9. The initial learning rate for pre-training and fine-tuning is 1e ?3 and 5e ?4 , re-spectively. Both of them will decay to 1e ?4 after the 40th epoch. More details can be referred to Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation on Benchmarks</head><p>To compare with previous scene text detectors, we evaluate our proposed method on several popular benchmarks for scene text detection. We adopt the best model configuration in the #5 of Tab. 4 for evaluating on all benchmarks. As shown in <ref type="figure">Fig. 3</ref>, we provide some qualitative results in different cases, including multi-oriented text, long text, multilingual text, small text, low-resolution text, and curved text.</p><p>Multi-oriented text detection We evaluate our method for multi-oriented text on the IC15 dataset and the MSRA-TD500 dataset, which contain lots of small, low-resolution, and long text instances. As shown in Tab. 1, our model outperforms previous state-of-the-art method by 0.9% on both IC15 and MSRA-TD500. Compared with the former DETR-based method <ref type="bibr" target="#b38">[39]</ref>, our proposed model shows a much better detection performance (89.1% vs. 83.7%) on small and blurry text of IC15. Compared with previous CNN-based methods on MSRA-TD500, our method outperforms them by at least 1.7% in terms of f-measure, owing to the advantages of transformers in extracting global-range features and long-distance dependencies.</p><p>Curved text detection To prove our method's effectiveness on curved text, we evaluate it on two popular curved text benchmarks, i.e. the Total-Text dataset and the CTW1500 dataset. As shown in Tab. 1, our method obtains 0.2% improvement in terms of f-measure compared with the stateof-the-art method TextBPN <ref type="bibr" target="#b67">[68]</ref>. With the help of Bezier-Curve <ref type="bibr" target="#b24">[25]</ref>, our method could generate polygons for curved text, which can not be precisely detected by the former DETR-based method <ref type="bibr" target="#b38">[39]</ref>. Moreover, our method with Bezier-Curve could also achieve state-of-the-performance performance on the IC15 and MSRA-TD500 datasets.</p><p>Multi-lingual text detection To demonstrate the robustness of our model for different languages, we evaluate it on two large-scale scene text datasets (i.e. the MLT17 test dataset and the MTWI dataset). As shown in Tab. 2, compared with the state-of-the-art model <ref type="bibr" target="#b51">[52]</ref>, our model obtains 3.1%, 0.4%, and 1.5% improvements in terms of precision, recall, and f-measure, respectively. We also evaluate our model on the MTWI dataset, which contains multi-lingual text from web images. Our method achieves the best performance 75.2% in terms of f-measure with a competitive inference speed (21.5 FPS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Experiments on Feature Sampling</head><p>To demonstrate the effectiveness of our proposed feature sampling scheme, we conduct several experiments with different sampling configurations on the IC15 dataset and the MLT17 validation dataset. As shown in #1, #2, and #5 of Tab. 4, our method can significantly improve the performance with the help of higher-resolution feature maps. For IC15, sampling features at all scales outperforms the other two configurations by 22.1% and 6.9%, respectively. Consistently, it achieves 15.6% and 6.1% performance gain compared with others on MLT17. In addition, we conduct four configurations to explore the effects of sampling numbers from #3 to #6 in Tab. 4. We observe that the performance can increase with more sampling features, but stagnates in the last. The models with fewer sampled features can not perform well, because these features do not contain enough geometric and context information of all text instances. From #5 and #6, we find the performance slightly decreases as the sampling number increases, which may introduce more redundant features and incur negative effects.</p><p>To further evaluate the impact of sampling points, we we adopt an adaptive sampling scheme for training in #7. For every training image, we sort all features from the foreground text area by the predicted scores, and sample a fixed percentage (25%) of them with top scores. In this way, the sampling number is adaptive to the foreground feature number, and the performance of adaptive sampling is close to #5 and #6. Hence, our method is not sensitive when the sampling number is larger than that of #5. Moreover, we try to use all the features in different scales for the transformer modeling, but encounter the issue of "Out Of Memory" during training. Assuming the size of input images is 1024 ? 1024, the sizes of L0, L1, and L2 would be 32 ? 32, 64 ? 64, and 128 ? 128, respectively. The whole features mixed with background are difficult to model, and lead to a huge computational cost which is nearly 1400 times more than that of #5. Thus, our feature sampling is effective to decrease complexity for multi-scale feature maps and preserve the important information for scene text detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparisons with Transformer-Based Detection Methods</head><p>In this part, we compare our model with some popular transformer-based methods (i.e. DETR <ref type="bibr" target="#b2">[3]</ref>, Deformable DETR <ref type="bibr" target="#b72">[73]</ref>, and Conditional DETR <ref type="bibr" target="#b32">[33]</ref>) in object detection. We use their official codes and follow our training settings for fair comparisons. Noticeably, we adjust their codes  <ref type="figure">Figure 4</ref>. The convergence curves for DETR, Deformable DETR, Conditional DETR and Ours (RBox) on SynthText. The training and validation set is split from SynthText with a ratio 8:2. We train the previous methods by adjusting their official codes for multioriented text detection and follow the same settings as ours.</p><p>for multi-oriented text detection by adding angle regression and using our loss function.</p><p>Since pre-training on SynthText is a necessary step in previous methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b66">67]</ref>, we first compare the convergence speed on SynthText. We train most models excluding DETR with the same training settings as ours, but train DETR for 350 epochs for its low convergence speed. As illustrated in <ref type="figure">Fig. 4</ref>, the convergence speed of our method is much faster than DETR, because ours can significantly reduce redundant information and ease transformer modeling. Compared with the other two methods <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b72">73]</ref> focusing on increasing the efficiency of the attention units, our feature sampling-and-grouping scheme has a simpler pipeline but demonstrates a competitive convergence speed with a better detection performance. After fine-tuning, our proposed model obtains the best detection performance in terms of f-measure on IC15 and MLT17 as shown in Tab. <ref type="bibr" target="#b5">6</ref>.</p><p>In addition, we compare the FLOPs, the number of model parameters, and the inference speed with the previous transformer-based methods. For a fair comparison, we resize both sides of input images to 640 for all models to calculate the FLOPs, and use the same images from the IC15 test dataset to measure the inference speed by FPS. The number of object queries is set to 100 for previous methods, and we adopt the #5 configuration in Tab. 4 for ours. As shown in Tab. 7, our proposed transformerbased architecture has a lower computational cost in terms of FLOPs and a faster inference speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Transformer Structure</head><p>Despite the state-of-the-art performance achieved by our basic model architecture, we replace the basic transformer  <ref type="table">Table 7</ref>. Comparisons with transformer-based methods on FLOPs, the number of parameters, and the inference speed. For FLOPs, both sides of input images are set to 640. For FPS, we evaluate all models on the IC15 test dataset with the same inference setting of ours. The number of object queries is set to 100 for previous methods, and we adopt the #5 configuration in Tab. 4 for ours.</p><p>encoder layers with those in the modern transformer structure, i.e. Swin-Transformer <ref type="bibr" target="#b26">[27]</ref>, for further improvement. Different from applying Swin-Transformer for images, we only use four swin-transformer blocks for our feature grouping. Since it is designed for 2-D feature maps, we feed the feature map into the swin-transformer stage while masking out the unsampled features. Owing to the power of Swin-Transformer layers, our model obtains 0.4% and 0.2% performance gain on the IC15 and the MLT17 datasets as shown in Tab. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Rotated Object Detection</head><p>Our proposed method not only achieves state-of-the-art performance on scene text detection, but also performs well on oriented object detection. To prove the effectiveness of our method, we adapt it to oriented object detection and evaluate it on a popular dataset for oriented object detection in aerial images, i.e., DOTA-v1.0 <ref type="bibr" target="#b50">[51]</ref>. DOTA-v1.0 is one of the largest dataset for oriented object detection in aerial   images, and it contains 15 common categories, 2806 images and 188282 instances.</p><p>In the training, we use the same loss function as the loss for multi-oriented text detection. The feature sampling scheme is consistent with the configuration #5. Following the pre-processing in previous methods <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b61">62]</ref>, we split the training images of DOTA-v1.0 into 1024 ? 1024 subimages with an overlap of 200 pixels. We train our model for 100 epochs with an initial learning rate 1e ?4 , and decay it at 50th and 80th epoch, respectively.</p><p>As shown in Tab. 8, we compare our model with previous oriented object detection approaches in both singlescale and multi-scale testing manners. For a fair comparison, our method achieves the best performance among the single-stage approaches, and outperform KLD [62] by 1.32 AP 50 . By multi-scale testing, our model also achieves the competitive result 79.59 in terms of AP 50 with refine-stage and two-stage approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.">Limitation</head><p>For our feature sampling-and-grouping scheme, it is hard to deal with the "text overlapping" cases, which mean two text instances overlap each other. Although our feature grouping network can model the relationship of the sampled features, the features of the overlapping text are quite complex and tangled. Thus, our proposed method sometimes fails in these cases, which are shown in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we present a simple yet effective transformer-based architecture for scene text detection. Different from previous methods in scene text detection, our method leverages only a few representative features containing sufficient geometric and context information of foreground text. It is able to effectively reduce the redundant background noise and overcome the complexity limitation of the self-attention module. With the power of transformers, we can obtain more accurate bounding boxes without any post-processing. Through extensive experiments on several benchmarks, we demonstrate the effectiveness of our proposed method by consistently achieving state-of-theart results on both multi-oriented text datasets and arbitraryshaped text datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Network Architecture</head><p>Our proposed transformer-based architecture is composed of a backbone network, a feature sampling network, and a feature grouping network.</p><p>The backbone is the basic feature pyramid network (FPN) <ref type="bibr" target="#b21">[22]</ref> equipped with ResNet-50 <ref type="bibr" target="#b11">[12]</ref> as shown in <ref type="figure" target="#fig_4">Fig. 6</ref>, The produced feature maps in three different scales (i.e. 1/4, 1/8, 1/16) are used for feature sampling.</p><p>As shown in <ref type="figure" target="#fig_5">Fig. 7</ref>, each feature map is first fed into a Coord-Convolution layer <ref type="bibr" target="#b22">[23]</ref> to involve position information for the incoming presentation in our feature sampling network. Next, it is down-sampled by a constrained deformable pooling adjusted from <ref type="bibr" target="#b4">[5]</ref>. In our implementation, the predicted offsets are obtained by ?p ij = ? ? ? p ij ? (W k , H k ), where ? = Sigmoid(Avg(f ij )) is a learnable scaling parameter to modulate the predicted offset and f ij is the feature vector at (i, j). The other symbol definitions are consistent with the original ROI deformable pooling <ref type="bibr" target="#b4">[5]</ref>. Then, a convolution layer with a 1 ? 1 kernel size and a Sigmoid function are employed to generate confidence score maps to distinguish representative text regions. After that, we select the features with top-N k scores in each scale layer k, and gather them into a sequence form with a shape ( k N k , C), where C = 256 is the channel number.</p><p>In our feature grouping network, the sampled features are first concatenated with position embeddings. Then, we adopt four basic transformer encoder layers as those in DETR <ref type="bibr" target="#b2">[3]</ref> to model the feature relationship, and implicitly aggregate the features from the same text instance. Finally, scores and coordinates of rotated bounding boxes are obtained via a text/non-text classification head and a bounding box prediction head, which are composed of full-connected layers and Sigmoid functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Scale-Invariant GWD Loss</head><p>To regress the coordinates of rotated bounding boxes, we adapt the Gaussian Wasserstein Distance (GWD) loss <ref type="bibr" target="#b57">[58]</ref> into a scale-invariant form to better balance the loss weights of text with different scales. Following the GWD loss, we first convert the rotated bounding box B(x, y, h, w, ?) into a 2-D Gaussian distribution representation N (m, ?), where m = (x, y) and ? is formulated as ? = (10) Then, we use the Wasserstein distance between two instances to formulate d 2 as   Due to the extreme variance of scales, the loss of small text has a negligible influence on the gradient backpropagation compared with the loss of large text. Hence, we adjust the GWD loss into a scale-invariant form as follows:</p><formula xml:id="formula_9">d 2 =? m 1 ?m 2 ? 2 2 +Tr ? 1 + ? 2 ? 2(? 1/2 1 ? 2 ? 1/2 1 ) 1/2 .<label>(11)</label></formula><formula xml:id="formula_10">L rbox = 1 N r x (1 ? 1 ? + f (d 2 ( ux | tx| , tx | tx| )) ),<label>(12)</label></formula><p>where u x denotes the predicted rotated bounding box, t x denotes the target one, and | t x | denotes its area. N r is the number of bounding boxes after pair-wise matching. The <ref type="figure">Figure 8</ref>. The bad cases of "text overlapping" in our method. The red bounding boxes denote the wrong predictions, and the green ones are the right predictions. elements with denote the matched bounding boxes or the target ones after pair-wise matching. f (?) represents a non-linear function, and ? is a hyper-parameter to modulate the loss. According to the GWD loss <ref type="bibr" target="#b57">[58]</ref>, we set f (d 2 ) = log (d 2 + 1) and ? = 3. By normalizing u x and t x with the area of t x , we can decrease the negative effect of the scale imbalance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Training</head><p>In the training period, the data argumentation for training data includes: (1) Random Rotation, flipping, and perspective transformation; (2) Color argumentation; (3) Random cropping. In addition, both sides of the training images are randomly resized in the range between 640 ? 640 and 1680 ? 1680 with an interval of 64. In our loss function, we use ? c , ? d , and ? f to adjust the influences of different losses. Specifically, we set ? c to 0.5 and ? d to 1. For ? f , we initialize it to 1e ?2 , and decay it by a factor 0.1 at the 35th and 45th epoch, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Inference</head><p>In the inference period, we keep the aspect ratio of test images and resize the shorter sides to 768 (for TD500 and MTWI) or 1024 (for others), while the upper limit of the longer sides is 2048. Moreover, we can easily obtain the detection results without any complex post-processing. By setting a proper threshold, we only keep the predicted boxes with scores higher than the threshold. Specifically, we set it to 0.45 for the IC15 dataset, and 0.5 for others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Qualitative Results</head><p>As shown in <ref type="figure">Fig. 9</ref>, we provide more qualitative results for visualization, including multi-oriented text, long text, multi-lingual text, small text, dense text, and curved text. Moreover, we also provide some bad cases of our method shown in <ref type="figure">Fig. 8</ref>. The red bounding boxes are the wrong predictions. It is hard for our method to deal with the case of "text overlapping", because the features of the overlapping text instances are quite complex and tangled. Our feature grouping module sometime fails in these cases.</p><p>As shown in <ref type="figure" target="#fig_0">Fig. 10</ref>, we show the feature grouping results of the predicted rotated bounding boxes in red. We visualize the attention weights for one text instance's features in the last transformer layer. The weight value increases from 0 to 1 as the color changes from blue to red. It means that the output features for text instances in red bounding boxes are mainly aggregated from the inner features (red ones).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Constrained Deformable Pooling</head><p>To demonstrate the effectiveness of our constrained deformable pooling, we construct an ablation study on the IC15 and the MLT17 datasets. As shown in Tab. 10, our constrained deformable pooling outperforms average pooling and the original deformable pooling. It achieves 89.1% and 79.5% f-measure on the IC15 and the MLT17 datasets, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Loss for Rotated Bounding Boxes</head><p>As shown in Tab. 11, we compare the original GWD <ref type="bibr" target="#b57">[58]</ref> loss with our proposed scale-invariant form on the IC15 and the MLT17 datasets. Our scale-invariant GWD loss outperforms the original one by 0.7% and 0.5% on the IC15 and the MLT17 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Compared with Non-Transformer Structure</head><p>To evaluate sampling and grouping with non-transformer methods, we replace our transformer module with GCN <ref type="bibr" target="#b16">[17]</ref> (FPN+GCN) and FC layers (FPN+FC). As shown in Tab. 9, these two settings achieve lower f-measure <ref type="figure">Figure 9</ref>. The qualitative results of our proposed method in different cases, including multi-oriented text, long text, multi-lingual text, low-resolution text, curved text, dense text. For curved text detection, the Bezier curves' control points are drawn in red. <ref type="figure" target="#fig_0">Figure 10</ref>. The visualization of feature sampling and grouping. We visualize the attention weights for one text instance's features in the last transformer layer. The weight value increases from 0 to 1 as the color changes from blue to red. The output feature for the text instance in a red bounding box is mainly aggregated from the inner text point features. than ours. This phenomenon validates the effectiveness of our proposed sampling and grouping framework based on transformers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The illustration of feature sampling and grouping. (a) The confidence score map for text regions indicates the pixel importance for text detection. (b) The text features at red points containing geometric and context information of foreground text are selected by scores. (c) The sampled features from the same text instance are implicitly grouped at the feature level by a transformer. (d) The bounding boxes can be easily obtained from the grouped features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>The qualitative results on DOTA-v1.0 testing set. It contains 15 common categories, such as large-vehicle, small-vehicle, plane, swimming-pool, ship, tennis-court, etc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>w 2 cos 2 ? + h 2 sin 2 ? w?h 2 cos ? sin ? w?h 2 cos ? sin ? w 2 sin 2 ? + h 2 cos 2 ? 2 .</head><label>2</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>The structure of our feature pyramid network equipped with ResNet-50.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>The pipeline of feature sampling for each feature map f k .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Input Image Backbone Network Multi-Scale Text Extractor Selecting Points with Top-N Scores ? ? ? Prediction Heads Position Embeddings + Text Point Features Feature Grouping ? ? ? Multi-scale Feature Maps Detection Results (a) Representative Text Regions (b) Selected Text Points</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>87.3 89.1 86.8 73.4 79.5 Swin Transformer Layer 90.9 88.1 89.5 87.2 73.4 79.7 The experiment on the transformer layers in our feature grouping network.</figDesc><table><row><cell>Transformer Layer</cell><cell>P</cell><cell>IC15 R</cell><cell>F</cell><cell>P</cell><cell cols="2">MLT17 val R</cell><cell>F</cell></row><row><cell cols="2">Basic Layer 90.8 Methods P</cell><cell>IC15 R</cell><cell>F</cell><cell cols="2">P</cell><cell cols="2">MLT17 val R</cell><cell>F</cell></row><row><cell>DETR* [3]</cell><cell cols="7">87.9 75.4 81.2 84.6 63.4 72.5</cell></row><row><cell cols="8">Deformable DETR* [73] 88.3 84.7 86.5 86.5 69.3 77.0</cell></row><row><cell cols="8">Conditional DETR* [33] 87.5 81.8 84.6 85.9 67.8 75.8</cell></row><row><cell>Raisi et al. [39]</cell><cell cols="3">89.8 78.3 83.7</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell>Ours (RBox)</cell><cell cols="7">90.9 87.3 89.1 86.8 73.4 79.5</cell></row><row><cell cols="8">Table 6. Comparisons with transformer-based methods on the</cell></row><row><cell cols="8">IC15 test dataset and the MLT17 validation dataset. * indicates</cell></row><row><cell cols="8">the methods are trained by adjusting their official codes for multi-</cell></row><row><cell>oriented text detection.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="6">FLOPs Params FPS</cell></row><row><cell cols="3">DETR [3] Deformable DETR [73] 36.8G 38.9G Conditional DETR [33] 42.2G</cell><cell cols="2">41.3M 39.8M 43.2M</cell><cell></cell><cell>9.7 7.6 9.1</cell></row><row><cell>Ours (RBox)</cell><cell cols="2">35.9G</cell><cell cols="4">38.3M 12.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>81.40 74.30 47.70 70.30 64.90 67.80 70.00 90.80 79.10 78.20 53.60 62.90 67.00 64.20 50.2068.20    RoI-Trans.<ref type="bibr" target="#b6">[7]</ref> R-101 ? 88.64 78.<ref type="bibr" target="#b51">52</ref> 43.44 75.92 68.81 73.68 83.59 90.74 77.27 81.46 58.39 53.54 62.83 58.93 47.67 69.56 SCRDet [60] R-101 ? 89.98 80.65 52.09 68.36 68.36 60.32 72.41 90.85 87.94 86.86 65.02 66.68 66.25 68.24 65.21 72.61 Gliding Vertex [54] R-101 89.64 85.00 52.26 77.34 73.01 73.14 86.82 90.74 79.02 86.81 59.55 70.91 72.94 70.86 57.32 75.02 CenterMap OBB [47] R-101 ? 89.83 84.41 54.60 70.25 77.66 78.32 87.19 90.66 84.89 85.27 56.46 69.23 74.13 71.56 66.06 76.03 FPN-CSL [57] R-152 ? 90.25 85.53 54.64 75.31 70.44 73.51 77.62 90.84 86.15 86.69 69.60 68.04 73.83 71.10 68.93 76.17 89.93 84.45 53.77 74.35 71.52 78.31 78.12 91.14 87.35 86.93 65.64 65.17 75.35 79.74 63.31 76.34 82.12 54.78 70.86 78.93 83.00 88.20 90.90 87.50 84.68 63.97 67.69 74.94 68.84 52.28 75.87 R-101 88.86 83.48 55.27 76.92 74.27 82.10 87.52 90.90 85.56 85.33 65.51 66.82 74.36 70.15 57.28 76.28 R-50 ? 89.84 85.43 61.09 79.82 79.71 85.35 88.82 90.88 86.68 87.73 72.21 70.80 82.42 78.18 74.11 80.87 R-101 ? 90.26 84.74 62.01 80.42 79.04 85.07 88.52 90.85 87.24 87.96 72.26 70.03 82.93 78.46 68.05 80.52 89.08 80.41 52.41 70.02 76.28 78.11 87.21 90.89 84.47 85.64 60.51 61.52 67.82 68.02 50.09 73.50 DCL [56] R-152 ? 89.26 83.60 53.54 72.76 79.04 82.56 87.31 90.67 86.59 86.98 67.49 66.88 73.29 70.56 69.99 77.37 89.28 84.11 56.95 79.21 80.18 82.93 89.21 90.86 84.66 87.61 71.66 68.23 78.58 78.20 65.55 79.15 89.90 84.91 59.21 78.74 78.82 83.95 87.41 89.89 86.63 86.69 70.47 70.87 76.96 79.40 78.62 80.17 R-152 ? 89.92 85.13 59.19 81.33 78.82 84.38 87.50 89.80 87.33 87.00 72.57 71.35 77.12 79.34 78.68 80.63 .71 50.10 68.75 78.20 76.05 84.58 89.41 86.15 85.28 63.15 60.90 75.06 71.51 67.45 75.28 R-50 ? 88.91 85.23 53.64 81.23 78.20 76.99 84.58 89.50 86.84 86.38 71.69 68.06 75.95 72.23 75.42 78.32 85.31 56.39 76.45 74.55 83.46 87.78 90.86 85.85 85.28 64.52 67.82 77.72 74.32 67.80 77.90 R-50 ? 89.81 85.19 61.35 76.18 79.29 84.81 88.26 90.86 87.55 87.42 66.89 70.10 78.40 79.28 68.48 79.59</figDesc><table><row><cell>Method ICN [1] ? RSDet-II [38] Backbone MS PL R-101 R-152 ? Oriented R-CNN [53] R-50 89.46 Refine-stage Two-stage CFC-Net [34] R-101 ? RIDet [35] R-50 ? 89.31 80.77 54.07 76.38 79.81 81.99 89.13 90.72 83.58 87.22 64.42 67.56 78.08 79.17 62.07 77.62 BD BR GTF SV LV SH TC BC ST SBF RA HA SP HC AP 50 S 2 A-Net [10] R-101 ? R 3 Det-GWD [59] R-152 ? 89.66 84.99 59.26 82.19 78.97 84.83 87.70 90.21 86.54 86.85 73.04 67.56 76.92 79.22 74.92 80.19 R 3 Det-KLD [61] R-50 ? Single-stage PolarDet [69] R-101 ? 89.65 87.07 48.14 70.97 78.53 80.34 87.45 90.76 85.63 86.87 61.64 70.32 71.92 73.09 67.15 76.64 RDD [70] R-101 ? 89.15 83.92 52.51 73.06 77.81 79.00 87.08 90.62 86.72 87.15 63.96 70.29 76.98 75.79 72.15 77.75 GWD [58] R-152 ? 89.06 84.32 55.33 77.53 76.95 70.28 83.95 89.75 84.51 86.06 73.47 67.77 72.60 75.76 74.17 77.43 KLD [62] R-50 88.91 83Ours (RBox) R-50 90.36</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 .</head><label>8</label><figDesc>Detection results on the DOTA-v1.0 testing set. R-50, R-101, and R-152 denote ResNet-50, ResNet-101, and ResNet-152, respectively. MS indicates that multi-scale testing is used. Red and blue indicate the top two performances.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 .</head><label>9</label><figDesc>The ablation study on feature grouping with nontransformer structures.</figDesc><table><row><cell>Method</cell><cell>Sampling Number</cell><cell>IC15</cell><cell>F-measure TD500</cell><cell>MTWI</cell></row><row><cell>FPN+FC</cell><cell>64+128+256</cell><cell>85.7</cell><cell>85.5</cell><cell>70.6</cell></row><row><cell>FPN+GCN</cell><cell>64+128+256</cell><cell>87.9</cell><cell>87.0</cell><cell>72.5</cell></row><row><cell>Ours (RBox)</cell><cell>64+128+256</cell><cell>89.1</cell><cell>88.1</cell><cell>75.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 .</head><label>10</label><figDesc>Deformable Pooling 89.9 87.3 88.6 86.8 72.8 79.2 Ours (RBox) 90.9 87.3 89.1 86.8 73.4 79.5The abalation study on the constrained deformable pooling. "P", "R", and "F" represent Precision, Recall, and F-measure, respectively.Table 11. The ablation study on the loss for rotated bounding boxes.</figDesc><table><row><cell>Methods</cell><cell>P</cell><cell>IC15 R</cell><cell>F</cell><cell>P</cell><cell>MLT17 val R</cell><cell>F</cell></row><row><cell>Average Pooling</cell><cell cols="6">89.5 87.2 88.3 86.6 72.6 79.0</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards multi-class object detection in unconstrained remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleonora</forename><surname>Seyed Majid Azimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Bahmanyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>K?rner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reinartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACCV</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Character region awareness for text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngmin</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bado</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwalsuk</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Total-text: A comprehensive dataset for scene text detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;apos;</forename><surname>Chee Kheng Ch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chee Seng</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dynamic detr: End-toend object detection with dynamic attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning roi transformer for oriented object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qikai</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Align deep features for oriented object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TGARS</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Most: A multi-oriented scene text detector with localization refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humen</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqing</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongpan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Icpr2018 contest on robust reading for multi-type web images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongpan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianwen</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Icdar 2015 competition on robust reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimosthenis</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluis</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anguelos</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masakazu</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><forename type="middle">Ramaseshan</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mask textspotter v3: Segmentation proposal network for robust scene text spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Textboxes++: A single-shot oriented scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Textboxes: A fast text detector with a single deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Real-time scene text detection with differentiable binarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyi</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An intriguing failing of convolutional neural networks and the coordconv solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piero</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><forename type="middle">Petroski</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Abcnet: Real-time scene text spotting with adaptive bezier-curve network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianwen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Omnidirectional scene text detection with sequential-free box discretization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianwen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lele</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaqiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhepeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Textsnake: A flexible representation for detecting text of arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangbang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqiang</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-oriented scene text detection via corner localization and region segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Arbitraryoriented scene text detection via rotation proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyuan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3111" to="3122" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Conditional detr for fast training convergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Depu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejia</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Conditional detr for fast training convergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Depu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejia</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cfc-net: A critical feature capturing network for arbitrary-oriented object detection in remote-sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingjuan</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote SensingF</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Optimization for arbitrary-oriented object detection via representation invariance loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingjuan</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Icdar2017 robust reading challenge on multi-lingual scene text detection and script identification-rrc-mlt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nibal</forename><surname>Nayef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imen</forename><surname>Bizid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunsoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimosthenis</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umapada</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Rigaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Chazalon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Efficient nonmaximum suppression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Neubeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning modulated loss for rotated object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silong</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08299</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Transformer-based text detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zobeir</forename><surname>Raisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">A</forename><surname>Naiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georges</forename><surname>Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Wardell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">S</forename><surname>Zelek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Faster r-cnn: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Detecting oriented text in natural images by linking segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Rethinking transformer-based set prediction for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcao</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Topological structural analysis of digitized binary images by border following. Computer vision, graphics, and image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Suzuki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="32" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Seglink++: Detecting dense and arbitrary-shaped scene text by instance-aware component grouping. PR, 96:106954</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongpan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Detecting text in natural image with connectionist text proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning center probability map for detecting objects in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng-Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TGARS</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="4307" to="4323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Shape robust text detection with progressive scale expansion network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Efficient and accurate arbitrary-shaped text detection with pixel aggregation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoge</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Contournet: Taking a further step toward accurate arbitrary-shaped scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengting</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Dota: A large-scale dataset for object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Sequential deformation for accurate scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanyu</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangrui</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyu</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Min</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Oriented r-cnn for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiabao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiwen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page" from="2021" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Gliding vertex on the horizontal bounding box for multi-oriented object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingtao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qimeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1452" to="1459" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Textfield: Learning a deep direction field for irregular scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongpan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5566" to="5579" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Dense label encoding for boundary discontinuity free rotation detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liping</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="2021" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Arbitrary-oriented object detection with circular smooth label</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Rethinking rotated object detection with gaussian wasserstein distance loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, 2021. 5, 9</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Rethinking rotated object detection with gaussian wasserstein distance loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page" from="2021" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Scrdet: Towards more robust detection for small, cluttered and rotated objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jirui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning high-precision bounding box for rotated object detection via kullbackleibler divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jirui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning high-precision bounding box for rotated object detection via kullbackleibler divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jirui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A unified framework for multioriented text detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4737" to="4749" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Detecting texts of arbitrary orientations in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Liu Yuliang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Lianwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Shuaitao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02170</idno>
		<title level="m">Detecting curve text in the wild: New dataset and new solution</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Look more than once: An accurate detector for text of arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengquan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyi</forename><surname>En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Deep relational reasoning graph network for arbitrary shape text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi-Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie-Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongfa</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu-Cheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Adaptive boundary proposal network for arbitrary shape text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi-Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongfa</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu-Cheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Polardet: A fast, more precise detector for rotated target in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenshen</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingjia</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenming</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyu</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="5821" to="5851" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Single-stage rotation-decoupled detector for oriented object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Ao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page">3262</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">East: an efficient and accurate scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuchang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Ensembling neural networks: many could be better than all. Artificial intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">137</biblScope>
			<biblScope unit="page" from="239" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Fourier contour embedding for arbitrary-shaped text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghui</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianwen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
