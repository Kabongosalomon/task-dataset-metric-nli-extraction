<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BLOCK: Bilinear Superdiagonal Fusion for Visual Question Answering and Visual Relationship Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Ben-Younes</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Sorbonne Universit?</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<postCode>LIP6, F-75005</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Heuritech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Cadene</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Sorbonne Universit?</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<postCode>LIP6, F-75005</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">CEDRIC -Conservatoire National des Arts et M?tiers</orgName>
								<address>
									<postCode>75003</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Sorbonne Universit?</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<postCode>LIP6, F-75005</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">BLOCK: Bilinear Superdiagonal Fusion for Visual Question Answering and Visual Relationship Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multimodal representation learning is gaining more and more interest within the deep learning community. While bilinear models provide an interesting framework to find subtle combination of modalities, their number of parameters grows quadratically with the input dimensions, making their practical implementation within classical deep learning pipelines challenging. In this paper, we introduce BLOCK, a new multimodal fusion based on the block-superdiagonal tensor decomposition. It leverages the notion of block-term ranks, which generalizes both concepts of rank and mode ranks for tensors, already used for multimodal fusion. It allows to define new ways for optimizing the tradeoff between the expressiveness and complexity of the fusion model, and is able to represent very fine interactions between modalities while maintaining powerful mono-modal representations. We demonstrate the practical interest of our fusion model by using BLOCK for two challenging tasks: Visual Question Answering (VQA) and Visual Relationship Detection (VRD), where we design end-to-end learnable architectures for representing relevant interactions between modalities. Through extensive experiments, we show that BLOCK compares favorably with respect to state-of-the-art multimodal fusion models for both VQA and VRD tasks. Our code is available at https://github.com/Cadene/block. bootstrap.pytorch.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In many of the most recent tasks tackled by artificial intelligence, multiple sources of information need to be taken into account for decision making. Problems such as visual question answering <ref type="bibr" target="#b12">(Goyal et al. 2017)</ref>, visual relationship detection <ref type="bibr" target="#b26">(Lu et al. 2016</ref>), cross-modal retrieval  or social-media post classification <ref type="bibr" target="#b8">(Duong, Lebret, and Aberer 2017)</ref> require, at a certain level and to some extent, the fusion between multiple modalities.</p><p>For classical mono-modal tasks, linear models constitute a handful building block to transform the input x and to match it with the desired output y. When dealing with two modalities, not only do we need to properly transform each input x 1 and x 2 into a representation that fits the problem, we also want to model the interactions between these modalities. A natural candidate to extend linear models for Copyright c 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. two inputs are bilinear models. However, the number of parameters of a bilinear model is quadratic in the input dimensions: as a linear model is characterized by a matrix W ? R dim(x)?dim(y) , a bilinear model is defined by a tensor T ? R dim(x1)?dim(x2)?dim <ref type="bibr">(y)</ref> . When the input dimensions grow (thousand or more), learning a full tensor T becomes quickly intractable. The main issue is to reduce and control the numbers of parameters representing T . Usually, when working with linear models, restricting the complexity is done by constraining the rank of the matrix W . Unfortunately, it is much more complicated when it comes to bilinear models, since it requires notions of multi-linear algebra. While a lot of work has been done in extending the notion of complexity to higher-order tensors <ref type="bibr" target="#b14">(Harshman et al. 2001)</ref>, <ref type="bibr" target="#b3">(Carroll and Chang 1970)</ref>, <ref type="bibr" target="#b29">(Tucker 1966)</ref>, it is not clear whether the simple extension of rank should be used to constrain a bilinear model.</p><p>In this paper, we tackle the general problem of learning end-to-end bilinear models. We propose BLOCK, a Block Superdiagonal Fusion framework for multimodal representation based on the block-term tensor decomposition <ref type="bibr" target="#b7">(De Lathauwer 2008)</ref>. As it has been studied in the signal processing literature <ref type="bibr" target="#b5">(Cichocki et al. 2015)</ref>, the focus was on having uniqueness properties to ensure a physically interpretable decomposition. We study here this decomposition under a machine learning perspective instead, and use it as a fully learnable tensor of parameters. Interestingly, this decomposition leverages the notion of block-term ranks to define a tensor's complexity. It encapsulates both concepts of rank and mode ranks, at the basis of Candecomp/PARAFAC <ref type="bibr" target="#b14">(Harshman et al. 2001</ref>) and Tucker decomposition <ref type="bibr" target="#b29">(Tucker 1966)</ref>. This complexity analysis is capitalized on to provides a new way to control the tradeoff between the expressiveness and complexity of the fusion model. More precisely, BLOCK enables to model very rich (i.e. full bilinear) interactions between groups of features, while the block structure limits the whole complexity of the model, which enables to keep expressive (i.e. high dimensional) mono-moodal representations.</p><p>The BLOCK model is used for solving two challenging applications: Visual Question Answering (VQA) and Visual Relationship Detection (VRD). For both tasks, the number of blocks and the size of each projection in the BLOCK fusion will be adapted to balance between fine interaction modeling and low number of parameters. We embed our bilinear BLOCK fusion strategy into deep learning architectures ; through extensive experiments, we validate the relevance of the approach as we provide an extensive and systemic comparison of many state-of-the-art multimodal fusion techniques. Moreover, we obtain very competitive results on three commonly used datasets: VQA 2.0 <ref type="bibr" target="#b12">(Goyal et al. 2017)</ref>, TDIUC <ref type="bibr" target="#b17">(Kafle and Kanan 2017)</ref> and the VRD dataset <ref type="bibr" target="#b26">(Lu et al. 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BLOCK fusion model</head><p>In this section, we present our BLOCK fusion strategy and discuss its connection to other bilinear fusion methods from the literature.</p><p>A bilinear model takes as input two vectors x 1 ? R I and x 2 ? R J , and projects them to a K-dimensional space with tensor products:</p><formula xml:id="formula_0">y = T ? 1 x 1 ? 2 x 2 (1) where y ? R K . Each component of y is a quadratic form of the inputs: ?k ? [1, K], y k = I i=1 J j=1 T ijk .x 1 i .x 2 j<label>(2)</label></formula><p>A bilinear model is completely defined by its associated tensor T ? R I?J?K , the same way as a linear model is defined by its associated matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">BLOCK model</head><p>In order to reduce the number of parameters and constrain the model's complexity, we express T using the block-term decomposition. More precisely, the decomposition of T in rank (L,M,N) terms is defined as:</p><formula xml:id="formula_1">T := R r=1 D r ? 1 A r ? 2 B r ? 3 C r (3) where ?r ? [1, R], D r ? R L?M ?N , A r ? R I?L , B r ? R J?M and C r ? R K?N .</formula><p>This decomposition is called block-term because it can be written as</p><formula xml:id="formula_2">T = D bd ? 1 A ? 2 B ? 3 C<label>(4)</label></formula><p>where A = [A 1 , ..., A R ] (same for B and C), and D bd ? R LR?M R?N R the block-superdiagonal tensor of {D r } 1?r?R , as illustrated in <ref type="figure">Figure 1</ref>. Applying this structural constraint to T in Eq.</p><p>(1), we can express y with respect to x 1 and x 2 . Letx 1 = Ax 1 ? R LR andx 2 = Bx 2 ? R M R . These two projections are merged with a fusion parametrized by the block-superdiagonal tensor D bd . Each block in this tensor merges together chunks of size L fromx 1 and of size M fromx 2 to produce a vector of size N :</p><formula xml:id="formula_3">z r = D r ? 1x 1 rL:(r+1)L ? 2x 2 rM :(r+1)M<label>(5)</label></formula><p>wherex i:j is a vector of dimension j ? i containing the corresponding values inx. Finally, all the z r vectors are concatenated to produce z ? R N R . The final prediction vector is y = Cz ? R K . <ref type="figure">Figure 1</ref>: BLOCK framework. The third-order interaction tensor is decomposed in R rank-(L,M,N) terms. We give here the two equivalent representations of the block-term decomposition, presented in this paper. On the left, we show the formulation with the block-superdiagonal tensor decomposition that corresponds to Eq. (4). On the right, we express it as a sum of small decompositions, as written in Eq.</p><p>(3). Through the R number of blocks and the dimensions of the A, B and C projections, we can handle the trade-off between model complexity and expressivity.</p><p>To further reduce the number of parameters in the model, we add a constraint on the rank of each third order slices matrices of the blocks D r , as it was done in some recent VQA applications (see Section 3).</p><p>Discussion When working with a linear model, a usual technique to restrict the hypothesis space and number of parameters is to constrain the rank of its associated matrix. The formal notion of matrix rank quantifies how complex a linear model is allowed to be. However, when it comes to restricting the complexity of a bilinear model, multiple algebraic concepts can be used. We give two examples that are related to the block-term decomposition.</p><p>The Candecomp/PARAFAC (CP) decomposition <ref type="bibr" target="#b3">(Carroll and Chang 1970)</ref>, <ref type="bibr" target="#b14">(Harshman et al. 2001</ref>) of a tensor T ? R I?J?K is the linear combination of rank-1 terms</p><formula xml:id="formula_4">T := R r=1 a r ? b r ? c r<label>(6)</label></formula><p>where ? denotes the outer product, and the vectors a r ? R I , b r ? R J and c r ? R K represent the elements of the decomposition. The rank of T is defined by the minimal number R of triplet vectors so that the Eq. (6) is true. Thus, restricting the hypothesis space for T to the set of tensors defined by Eq. (6) guarantees that the rank is upper-bounded by R. Applying this constraint on T , Eq. (1) is simplified into</p><formula xml:id="formula_5">y = C x 1 A * x 2 B<label>(7)</label></formula><p>where</p><formula xml:id="formula_6">A = [a 1 , ..., a R ] ? R I?R , B = [b 1 , ..., b R ] ? R J?R and C = [c 1 , .</formula><p>.., c R ] ? R K?R , and * denotes element-wise product. This decompositioncan be seen as a special case of the block-term decomposition where L = M = N = 1, reducing D bd to a super-diagonal identity tensor.</p><p>Another way to restrict a three-way tensor's complexity is through its mode ranks. The rank-(L,M,N) Tucker decomposition <ref type="bibr" target="#b29">(Tucker 1966)</ref> of T ? R I?J?K is defined as</p><formula xml:id="formula_7">T := D ? 1 A ? 2 B ? 3 C (8) where D ? R L?M ?N , A ? R I?L , B ? R J?M and C ? R K?N .</formula><p>This decomposition assumes a constraint on the three unfolding matrices of T , such that Rank (T JK?I ) = L, Rank (T KI?J ) = M and Rank (T IJ?K ) = N (following the notations in (De Lathauwer 2008)).</p><p>Applying this constraint to T , Eq. (1) can be re-written as:</p><formula xml:id="formula_8">y = C D ? 1 x 1 A ? 2 x 2 B<label>(9)</label></formula><p>This decomposition can be seen as a special case of the block-term decomposition where there is only R = 1 block in the core tensor. As was studied by (De Lathauwer 2008), the notion of tensor complexity should be expressed not only in terms of rank or mode ranks, but using the number of blocks and the mode-n ranks of each block. It appears that CP and Tucker decompositions are two extreme cases, where only one of the two quantities is used. For the CP, the number of blocks corresponds to the rank, but each block is of size (1,1,1). The monomodal projections can be high-dimensional and thus integrate rich transformation of the input, but the interactions between both projections is relatively poor as a dimension from one space is only allowed to interact with another. For the Tucker decomposition, there is only one block of size (L,M,N). The interaction modeling is very rich since all inter-correlations between feature dimensions of the different modalities are considered. However, this quantity of possible interactions limits the dimensions of the projected space, which can cause a bottleneck in the model. BLOCK being built on the block-term decomposition, we constrain the tensor using a combination of both concepts, which provides a richer modeling of the interactions between modalities. This richness is ensured by the R tensors D r , each parametrizing a bilinear function that takes as inputs chunks ofx 1 andx 2 . As this interaction modelling is done by chunks and not for every possible combination of components inx 1 andx 2 , we can reach high dimensions in the projections A and B without exploding the number of parameters in D bd . This property of having a fine interaction modeling between high dimensional projections is very desirable in our context where we need to model complex interactions between high-level semantic spaces. As we show in the experiments, performance of a bilinear model strongly depends on both the number and the size of the blocks D r that parametrize the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BLOCK fusion for VQA task</head><p>The task of Visual Question Answering <ref type="bibr" target="#b1">(Antol et al. 2015)</ref>, <ref type="bibr" target="#b12">(Goyal et al. 2017</ref>) has been a fertile playground for researchers to investigate on how bilinear models should be used. In the classical setup for VQA, shown in <ref type="figure">Figure 2</ref>, image and question have to be merged with a multimodal fusion technique, which can be implemented as an instance of bilinear model. The answer is predicted through a classification layer that follows the fusion module. In MCB <ref type="bibr" target="#b11">(Fukui et al. 2016)</ref>, the bilinear interaction is simplified using a sketching technique. However, more recent techniques tackle this complexity issue from a tensor decompositions standpoint: MLB <ref type="bibr" target="#b18">(Kim et al. 2017) and</ref><ref type="bibr">MUTAN (Ben-Younes et al. 2017</ref>) constrain the tensor of parameters using respectively the CP and Tucker decomposition. In MFB <ref type="bibr" target="#b31">(Yu et al. 2017b)</ref>, the tensor is viewed as a stack of matrices, and a classical matrix rank constraint is imposed on each of them. Finally in MFH <ref type="bibr" target="#b32">(Yu et al. 2018)</ref>, multiple MFB blocks are cascaded to model higher-order interactions between inputs. Most recent techniques embed some of these fusion strategies into more elaborated architecture that involve multiple types of visual features <ref type="bibr" target="#b16">(Jiang et al. 2018)</ref>, or specific modules <ref type="bibr" target="#b33">(Zhang, Hare, and Prgel-Bennett 2018)</ref> designed for precise question types. In this section, we compare BLOCK to the other multimodal fusion techniques, and show how it surpasses them both in terms of performance and number of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">VQA architecture</head><p>Our VQA model is based on a classical attentional architecture <ref type="bibr" target="#b11">(Fukui et al. 2016)</ref>, enriched by our proposed merging scheme. Our fusion model is shown in <ref type="figure">Figure 2</ref>. We use the Bottom-up image features provided by (Teney et al. 2018), consisting of a set of detected objects and their representation (see <ref type="bibr" target="#b9">Durand et al. 2017)</ref> for further insights on detection and localization). To get a vector embedding of the question, words are preprocessed and then fed into a pretrained Skip-thought encoder . The outputs of this language model are used to produce a single vector representing the whole question, as in <ref type="bibr" target="#b32">(Yu et al. 2018)</ref>. We use a BLOCK fusion to merge the question and image representations. The question vector is used as a context to guide the visual attention. Saliency scores are produced using a BLOCK fusion between each image vector and the question embedding.</p><p>Details: For the BLOCK layers, we set L = M = N = 80, R = 20 and constrain the rank of each mode-3 slices of each block to be less than 10. We found these hyperparameters with a a cross-validation on the val set. As in <ref type="bibr" target="#b32">(Yu et al. 2018)</ref>, we consider the 3000 most frequent answers. As in (Ben-Younes et al. 2017), we use a cross-entropy loss with answer sampling. We jointly optimize the parameters of our VQA model using Adam <ref type="bibr" target="#b19">(Kingma and Ba 2015)</ref> with a learning rate of 1e ?4 , without learning rate decay or gradient clipping, and with a batch size of 200. We early stop the training of our models according to their accuracy on a holdout set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fusion analysis</head><p>In <ref type="table" target="#tab_0">Table 1</ref>, we compare BLOCK to 8 different fusion schemes available in the literature on the commonly used VQA 2.0 Dataset <ref type="bibr" target="#b12">(Goyal et al. 2017</ref>). This dataset is composed of 658,111 image-question-answers triplets for training and validation (trainval set), and 447,793 triplets for evaluation (test-std set). We train on trainval minus a small subset used for early-stopping, and report the performance on test-dev set. For each fusion strategy, we run a grid search over its hyperparameters and keep the model that performs best on our validation set. We report the size of the model, <ref type="figure">Figure 2</ref>: Architecture for VQA that embeds the BLOCK bilinear fusion. To make this system more efficient, we integrate the fusion in an attentional framework.</p><p>corresponding to the number of parameters between the attended image features, the question embedding, and the answer prediction. We briefly describe the different fusion schemes used for the comparison:</p><p>-(1) the two vectors are projected on a common space, and their summation is projected to predict the answer; -(2) the vectors are concatenated and passed at the input of a 3-layer MLP; -(3) a bilinear interaction based on a count-sketching technique that projects the outer product of between inputs on a multimodal space; -(4) a bilinear interaction where the tensor is expressed as a Tucker decomposition; -(5) a bilinear interaction where the tensor is expressed as a CP decomposition; -(6) a bilinear interaction where each 3rd mode slice matrix of the tensor is constrained by its rank; -(7) a bilinear interaction where the tensor is expressed as a Tucker decomposition, and where its core tensor has the same rank constraint as <ref type="formula" target="#formula_4">(6)</ref>; -(8) a higher order fusion composed of cascaded <ref type="formula" target="#formula_4">(6)</ref>; -(9) our BLOCK fusion. From the results in <ref type="table" target="#tab_0">Table 1</ref>, we see that the simple sum fusion (1) provides a very low baseline. We also note that the MLP (2) doesn't provide the best results, despite its non-linear structure. As the MLP should be able to find that two different modalities are used and that it needs to look for interactions between them, this is in practice difficult to obtain. Instead, top performing methods are based on a bilinear model. The structure imposed on the parameters highly influences the final performance. We can see that (3), which simplifies the bilinear model using random projections, has efficiency issues due to the count-sketching technique. These issues are alleviated in the other bilinear methods, which use the tensor decomposition framework to practically implement the interaction. Our BLOCK method (9) gives the best results. As we saw, the block-term decomposition generalizes both CP and Tucker decompositions, which is why it is not surprising to see it surpass them. Moreover, the fact that it integrates the 3rd order slices rank con-straint gives it the advantages of (6) and <ref type="formula" target="#formula_5">(7)</ref>. Interestingly, it even surpasses (8) which is based on a higher-order interaction modeling, while using 30M less parameters. This strongly indicates that controlling a bilinear model through its block-term ranks provides an efficient trade-off between modeling capacities and number of parameters. To further validate this hypothesis, we evaluate a BLOCK fusion with only 3M parameters. This model obtained 64.91%. Unsurprisingly, it does not surpasses all the methods against which we compare. However, it obtains competitive results, improving over 5 out of 8 methods that all use far more parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison to leading VQA methods</head><p>We compare our model with state-of-the-art VQA architecture on two datasets: the widely used VQA 2.0 Dataset <ref type="bibr" target="#b12">(Goyal et al. 2017</ref>) and TDIUC <ref type="bibr" target="#b17">(Kafle and Kanan 2017)</ref>. On this more recent dataset, evaluation metrics are provided to assess the robustness of the model with respect to answer imbalance, as well as to account for performance homogeneity across the difference question types.</p><p>As we show in <ref type="table" target="#tab_1">Table 2</ref>, our model is able to outperform the preceding ones on TDIUC by a large margin for every metrics, especially those which account for bias in the data. We notably report a gain of +1.7 in accuracy, +3.95 in A-MPT, +5.05 in H-MPT, +16.12 in A-NMPT, +15.45 in H-NMPT, over the best scoring model in each metric. The high results in the harmonic metrics (H-MPT and H-NMPT) suggest that BLOCK performs well across all question types, while the high scores in the normalized metrics (A-NMPT and H-NMPT) denote that our model is robust to answer imbalance type of bias in the dataset.</p><p>In <ref type="table">Table 3</ref>, we see that our fusion model obtains competitive results on VQA 2.0 compared to previously published methods. As we are outperformed by <ref type="bibr" target="#b33">(Zhang, Hare, and Prgel-Bennett 2018)</ref>, whose proposition rely on a completely different architecture, we believe that both our contributions are orthogonal. Still, our model performs better than (Teney et al. 2018) and <ref type="bibr" target="#b32">(Yu et al. 2018)</ref>, with whom we share the global VQA architecture. In further details, we point out that BLOCK surpasses <ref type="bibr" target="#b32">(Yu et al. 2018</ref>) reaching a +1.78 improvement in the overall accuracy on test-dev, even though the latter encompasses the current state-of-the-art fusion scheme. Furthermore, we use the same image features than <ref type="bibr" target="#b28">(Teney et al. 2018)</ref> and are able to achieve a +2.26 gain on test-dev and +2.25 on test-std.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">VRD task</head><p>The task of Visual Relationship Detection aims at predicting triplets of the type "subject-predicate-object" where subject and object are localized objects, and predicate is a label corresponding to the relationship that links them (for example: "man-riding-bicycle", "woman-holding-phone"). To predict this relationship, multiple types of information are available, for both the subject and object regions: classes, bounding box coordinates, visual features, etc. However, this context being more recent than VQA, fusion techniques are less formalized and more ad-hoc. In (Hanwang Zhang 2017), the  relation is predicted by a substractive fusion between subject and object representations, each consisting in a linear function of relative coordinates, class distributions and visual features.  predicts the relationship by a complex message passing structure between subject and object representations, and (Dai, Zhang, and Lin 2017) uses a formulation inspired from Conditional Random Fields to perform joint recognition between the subject, object and predicate classes. We adopt in the following a very simple architecture, to put emphasis on the fusion module between different information sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">VRD Architecture</head><p>Our VRD architecture is shown in <ref type="figure">Figure 3</ref>. It takes as inputs a subject and an object bounding box. Each of them is represented as their 4-dimensional box spatial coordinates x s s and x s o (normalized between 0 and 1), their object classes x c s and x c o , and their semantic visual features x f s and x f o . To predict the relationship predicate, we use one fusion module for each type of features following Eq. (10).</p><formula xml:id="formula_9">x = [f s (x s s , x s o ) , f c (x c s , x c o ) , f f x f s , x f o ]<label>(10)</label></formula><p>where f can be implemented as BLOCK, or any other multimodal fusion. Each fusion module outputs a vector of dimension d, all concatenated into a 3d-dimensional vector that will serve as an input to a linear layer predictor y = W x. The system is trained with back-propagation on a binary-crossentropy loss. An other important component is the object detector. As usually done, we first train a Faster-RCNN on the object <ref type="figure">Figure 3</ref>: Architecture of our visual relationship detection system boxes of the VRD dataset. For Predicate prediction, we use it as a features extractor given the ground truth bounding boxes. For Phrase detection and Relationship detection, we use it to extract the bounding boxes with their associated features.</p><p>For a system to perform well on Phrase and Relationship detection, it should have been also trained on pairs of (subject, object) boxes that are not linked together by any relationship. During training, we randomly sample half of all possible negative pairs, and assign them an all-zeros label vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Dataset</head><p>The VRD dataset <ref type="bibr" target="#b26">(Lu et al. 2016</ref>) is composed of 5,000 images with 100 object categories and 70 predicates. It contains 37,993 relationships with 6,672 unique triplets and an average of 24.25 predicates per object category. The dataset <ref type="table">Table 3</ref>: State-of-the-art results on VQA2 testing sets. The models were trained on the union of VQA 2.0 trainval split and VisualGenome ) train split. All is the overall OpenEnded accuracy (higher is better). Yes/no, Numbers and Others are subsets that correspond to answers types. Only single model scores are reported. * scores reported from <ref type="bibr" target="#b12">(Goyal et al. 2017</ref>  <ref type="bibr" target="#b32">(Yu et al. 2018)</ref> 65.80 -------Counter <ref type="bibr">(Zhang, Hare, and Prgel-Bennett 2018) 68.09 83.14 51.62 58.97 68.41 83.56 51.39 59.11 BLOCK 67.58 83.6 47.33 58.51 67.92 83.98 46.77 58.79</ref> is divided between 4,000 images for training and 1,000 for testing. Three different settings are commonly used to evaluate a model on VRD: (1) Predicate prediction: the coordinates and class labels are given for both subject and object regions. This setup allows to assess the model's ability to predict a relationship, regardless of the object detection stage.</p><p>(2) Phrase detection: a predicted triplet &lt;subject, predicate, object&gt; matches a ground-truth if the three labels match and if the union region of its bounding boxes matches the union region of the ground-truth triplet, with IoU above 0.5. (3) Relationship detection: more challenging than (2), this one requires that both subject and object intersect with an IoU higher than 0.5. For each of these settings, performance is usually measured with Recall@50 and Recall@100</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Fusion analysis</head><p>To show the effectiveness of the BLOCK bilinear fusion, we run the same type of experiment we did in the previous section. For each fusion technique, we use the architecture described in <ref type="figure">Equation 10</ref> where we replace f by the corresponding bilinear function. As we did for VQA, we cross-validate the hyperparameters of each fusion technique and keep the best model each time. In <ref type="table" target="#tab_3">Table 4</ref>, we see that BLOCK still outperforms all previous methods on each of the three tasks. We can remark that for this task, the non linear MLP perform relatively well compared to the other methods. It is likely that an MLP can model the interactions at stake for VRD more easily than those for VQA. However, we can improve over this strong baseline using a BLOCK fusion.</p><p>In the next experiments, we validate the power of our BLOCK fusion, and analyze how it behaves under different setups. We randomly split the training set into three train/val sets, and plot the mean and standard deviation of the re-call@50 calculated over them. In <ref type="figure" target="#fig_0">Figure 4a</ref>, we fix the dimension of the block-superdiagonal tensor to RL = RM = RN = 500 and vary the number of blocks used to fill this tensor. When R = 1, which corresponds to the Tucker de- composition, the number of parameters in the core tensor is equal to 500 3 = 125M , making the system arduously trainable on our dataset. On the opposite, when R = 500, the number of parameters is controlled, but the mono-modal projections are only allowed to interact through an elementwise multiplication, which makes the interaction modeling relatively poor. The block-term decomposition provides an in-between working regime, reaching an optimum when R ? 20.</p><p>In <ref type="figure" target="#fig_0">Figure 4b</ref>, we keep the number of parameters fixed. As the number of chunks increases, the dimensions of the mono-modal projections also increases. Once again, an optimum is reached when R ? 20. These results confirm our hypothesis that the way the parameters are distributed within the tensor, in terms of size and number of blocks, has a real impact on the system's performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison to leading VRD methods</head><p>In <ref type="table" target="#tab_4">Table 5</ref>, we compare our system to the state-of-the-art methods on VRD. On predicate prediction, our fusion outperforms all previous methods on R@50, including <ref type="bibr" target="#b30">(Yu et al. 2017a</ref>) that uses external data. On R@100, the BLOCK fusion is only marginally outperformed by <ref type="bibr" target="#b30">(Yu et al. 2017a</ref>),  but we perform better than all methods that don't use extra data. These results validate the efficiency of the blockterm decomposition to predict a predicate by fusing information coming from ground truth subject and object boxes. On phrase detection, our BLOCK fusion achieves better results than all previous models in R@50. Notably, the scores obtained for phrase detection are lower than for predicate prediction, since the ground truth regions are not provided in this setup. Finally, on relationship detection, BLOCK surpasses all previous methods without extra data in R@50, and gives similar performance than <ref type="bibr" target="#b6">(Dai, Zhang, and Lin 2017)</ref> in R@100. The scores for relationship detection are lower than for phrase detection: in this setup, a prediction is positive if both subject and object boxes match the ground truth. On contrary, in phrase detection, the comparison between prediction and ground truth is done on the union between subject and object regions. Lastly, unlike some of the methods reported in <ref type="table" target="#tab_4">Table 5</ref>, we do not fine-tune or adapt the detection network to the visual relationship tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we introduce BLOCK, a bilinear fusion model whose tensor of parameters is structured using the blockterm decomposition. BLOCK aims at optimizing the trade-off between complexity and modeling capacities, and combines the strengths of the CP and Tucker decompositions. It offers the possibility to model rich interactions between groups of features, while still using high-dimensional monomodal representations. We apply BLOCK for two challenging computer vision tasks: VQA and VRD, where the parameters of our BLOCK fusion model are learned. Comparative experiments show that BLOCK improves over previous fusion schemes including linear, bilinear and non-linear models. We also show that BLOCK is able to maintain competitive performances with very compact parametrization.</p><p>In future works, we plan to extend the BLOCK idea to other applications. In particular, we want to explore the use of multiple input and output modalities, and to apply BLOCK for interpreting and explaining the behaviour of the multimodal deep fusion model <ref type="bibr" target="#b10">(Engilberge et al. 2018;</ref><ref type="bibr" target="#b4">Carvalho et al. 2018)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>Performance of BLOCK with respect to the structure of the block-superdiagonal tensor. Scores are given on a holdout validation set. 4a: The size of the core tensor is fixed to RL = RM = RN = 500. 4b: The total number of parameters in the block-diagonal tensor is fixed to 555K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of the fusion schemes on VQA2 test-dev set. |?| is the number of parameters learned in the fusion modeling. All is the overall Open Ended accuracy (higher is better). Yes/no, Numbers and Others are subsets that correspond to answers types. In the descriptions, the letter B corresponds to a bilinear model.</figDesc><table><row><cell></cell><cell>Description</cell><cell>Reference</cell><cell>|?|</cell><cell>All</cell><cell cols="2">Yes/no Number Other</cell></row><row><cell>(1)</cell><cell>Linear</cell><cell>Sum</cell><cell cols="3">8M 58.48 71.89</cell><cell>36.56</cell><cell>52.09</cell></row><row><cell>(2)</cell><cell>Non-linear</cell><cell>Concat MLP</cell><cell cols="3">13M 63.85 81.34</cell><cell>43.75</cell><cell>53.48</cell></row><row><cell>(3)</cell><cell>B + count-sketching</cell><cell>MCB (Fukui et al. 2016)</cell><cell cols="3">32M 61.23 79.73</cell><cell>39.13</cell><cell>50.45</cell></row><row><cell>(4)</cell><cell>B + Tucker decomp.</cell><cell>Tucker (Ben-Younes et al. 2017)</cell><cell cols="3">14M 64.21 81.81</cell><cell>42.28</cell><cell>54.17</cell></row><row><cell>(5)</cell><cell>B + CP decomp.</cell><cell>MLB (Kim et al. 2017)</cell><cell cols="3">16M 64.88 81.34</cell><cell>43.75</cell><cell>53.48</cell></row><row><cell cols="2">(6) B + low-rank on the 3rd mode slices</cell><cell>MFB (Yu et al. 2017a)</cell><cell cols="3">24M 65.56 82.35</cell><cell>41.54</cell><cell>56.74</cell></row><row><cell>(7)</cell><cell>Combination of (4) and (6)</cell><cell cols="4">MUTAN (Ben-Younes et al. 2017) 14M 65.19 82.22</cell><cell>42.1</cell><cell>55.94</cell></row><row><cell>(8)</cell><cell>Higher order fusion</cell><cell>MFH (Yu et al. 2018)</cell><cell cols="3">48M 65.72 82.82</cell><cell>40.39</cell><cell>56.94</cell></row><row><cell>(9)</cell><cell>B + Block-term decomposition</cell><cell>BLOCK</cell><cell cols="3">18M 66.41 82.86</cell><cell>44.76</cell><cell>57.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>State-of-the-art comparison on the TDIUC testing set. * scores reported from (Kafle and Kanan 2017).</figDesc><table><row><cell>Model</cell><cell cols="5">Accuracy A-MPT H-MPT A-NMPT H-NMPT</cell></row><row><cell>Most common answer (Kafle and Kanan 2017)</cell><cell>51.15</cell><cell>31.11</cell><cell>17.53</cell><cell>15.63</cell><cell>0.83</cell></row><row><cell>Question only (Kafle and Kanan 2017)</cell><cell>62.74</cell><cell>39.31</cell><cell>25.93</cell><cell>21.46</cell><cell>8.42</cell></row><row><cell>NMN* (Andreas et al. 2016)</cell><cell>79.56</cell><cell>62.59</cell><cell>51.87</cell><cell>34.00</cell><cell>16.67</cell></row><row><cell>MCB* (Fukui et al. 2016)</cell><cell>81.86</cell><cell>67.90</cell><cell>60.47</cell><cell>42.24</cell><cell>27.28</cell></row><row><cell>RAU* (Noh and Han 2016)</cell><cell>84.26</cell><cell>67.81</cell><cell>59.00</cell><cell>41.04</cell><cell>23.99</cell></row><row><cell>BLOCK</cell><cell>85.96</cell><cell>71.84</cell><cell>65.52</cell><cell>58.36</cell><cell>39.44</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparative study of the different multimodal fusion strategies on the VRD test-set. The reported metrics are the Recall@K in %.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Predicate</cell><cell cols="2">Phrase</cell><cell cols="2">Relationship</cell></row><row><cell></cell><cell>Description</cell><cell>|?|</cell><cell cols="2">Prediction</cell><cell cols="2">Detection</cell><cell cols="2">Detection</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">R@50 R@100 R@50 R@100 R@50 R@100</cell></row><row><cell>(1)</cell><cell>Linear</cell><cell cols="2">2.5M 82.99</cell><cell>89.68</cell><cell>14.44</cell><cell>16.94</cell><cell>9.73</cell><cell>11.34</cell></row><row><cell>(2)</cell><cell>Non-linear</cell><cell>2M</cell><cell>84.47</cell><cell>91.6</cell><cell>21.9</cell><cell>24.69</cell><cell>15.79</cell><cell>17.83</cell></row><row><cell>(3)</cell><cell>B + count-sketching</cell><cell>2M</cell><cell>82.23</cell><cell>89.07</cell><cell>13.42</cell><cell>15.8</cell><cell>9.17</cell><cell>10.79</cell></row><row><cell>(4)</cell><cell>B + Tucker decomp.</cell><cell>3M</cell><cell>83.25</cell><cell>89.77</cell><cell>11.23</cell><cell>14.09</cell><cell>7.37</cell><cell>9.00</cell></row><row><cell>(5)</cell><cell>B + CP decomp.</cell><cell>4M</cell><cell>85.96</cell><cell>91.66</cell><cell>23.67</cell><cell>26.50</cell><cell>16.41</cell><cell>18.59</cell></row><row><cell cols="4">(6) B + low-rank on the 3rd mode slices 15M 85.21</cell><cell>91.06</cell><cell>25.31</cell><cell>28.03</cell><cell>17.83</cell><cell>19.77</cell></row><row><cell>(7)</cell><cell>Combination of (4) and (6)</cell><cell cols="2">30M 85.65</cell><cell>91.33</cell><cell>25.77</cell><cell>28.65</cell><cell>18.53</cell><cell>20.38</cell></row><row><cell>(8)</cell><cell>Higher order fusion</cell><cell cols="2">16M 85.58</cell><cell>91.3</cell><cell>26.09</cell><cell>28.73</cell><cell>18.81</cell><cell>20.63</cell></row><row><cell>(9)</cell><cell>Block-term decomposition</cell><cell>5M</cell><cell>86.58</cell><cell>92.58</cell><cell>26.32</cell><cell>28.96</cell><cell>19.06</cell><cell>20.96</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>State-of-the-art results on the VRD testing set. The reported metrics are the Recall@K in %.</figDesc><table><row><cell>Model</cell><cell>External data</cell><cell cols="2">Predicate Prediction</cell><cell cols="2">Phrase Detection</cell><cell cols="2">Relationship Detection</cell></row><row><cell></cell><cell></cell><cell cols="6">R@50 R@100 R@50 R@100 R@50 R@100</cell></row><row><cell>Yu et. al (Yu et al. 2017a)</cell><cell></cell><cell>85.64</cell><cell>94.65</cell><cell>26.32</cell><cell>29.43</cell><cell>22.68</cell><cell>31.89</cell></row><row><cell>Li et. al (Li et al. 2017)</cell><cell></cell><cell>-</cell><cell>-</cell><cell>22.78</cell><cell>27.91</cell><cell>17.32</cell><cell>20.01</cell></row><row><cell>Liang et. al (Liang, Lee, and Xing 2017)</cell><cell></cell><cell>-</cell><cell>-</cell><cell>21.37</cell><cell>22.60</cell><cell>18.19</cell><cell>20.79</cell></row><row><cell>Zhang et. al (Hanwang Zhang 2017)</cell><cell></cell><cell>44.76</cell><cell>44.76</cell><cell>19.42</cell><cell>22.42</cell><cell>14.07</cell><cell>15.20</cell></row><row><cell>Lu et. al (Lu et al. 2016)</cell><cell></cell><cell>47.87</cell><cell>47.87</cell><cell>16.17</cell><cell>17.03</cell><cell>13.86</cell><cell>14.70</cell></row><row><cell>Peyre et. al (Peyre et al. 2017)</cell><cell></cell><cell>52.6</cell><cell>52.6</cell><cell>17.9</cell><cell>19.5</cell><cell>15.8</cell><cell>17.1</cell></row><row><cell>Dai et. al (Dai, Zhang, and Lin 2017)</cell><cell></cell><cell>80.78</cell><cell>81.90</cell><cell>19.93</cell><cell>23.45</cell><cell>17.73</cell><cell>20.88</cell></row><row><cell>BLOCK</cell><cell></cell><cell>86.58</cell><cell>92.58</cell><cell>26.32</cell><cell>28.96</cell><cell>19.06</cell><cell>20.96</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work has been supported within the Labex SMART supported by French state funds managed by the ANR within the Investissements dAvenir programme under reference ANR-11-LABX-65.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">VQA: Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Mutan: Multimodal tucker fusion for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cad?ne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Analysis of individual differences in multidimensional scaling via an n-way generalization of &quot;eckart-young&quot; decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cross-modal retrieval in the cooking context: Learning semantic text-image embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cad?ne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Soulier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Tensor decompositions for signal processing applications: From twoway to multiway component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cichocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Mandic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Caiafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Lathauwer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>IEEE Signal Processing Magazine</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Detecting visual relationships with deep relational networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Decompositions of a higher-order tensor in block terms -part ii: Definitions and uniqueness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>De Lathauwer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1033" to="1066" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Multimodal classification for analysing social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">T</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lebret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aberer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ECML-PKDD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">WILDCAT: weakly supervised learning of deep convnets for image classification, pointwise localization and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Finding beans in burgers: Deep semantic-visual embedding with localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engilberge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chevallier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Making the v in vqa matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Visual translation embedding network for visual relation detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zawlin</forename><surname>Kyaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><forename type="middle">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><forename type="middle">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Foundations of the parafac procedure: Models and conditions for an &quot;explanatory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Harshman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ladefoged</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Reichenbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">I</forename><surname>Jennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Terbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Comrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Bentler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yamane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vaughan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jahnke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
	<note>multimodal factor analysis</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multimodal learning and reasoning for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ilievski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="551" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Pythia v0.1: The winning entry to the vqa challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/pythia" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An analysis of visual question answering algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hadamard Product for Low-rank Bilinear Pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>On</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 5th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Vipcnn: Visual phrase guided convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>In CVPR</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep variationstructured reinforcement learning for visual relationship and attribute detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Deeper lstm and normalized cnn visual question answering model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deformable part-based fully convolutional network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Visual relationship detection with language priors</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Training recurrent answering units with joint loss minimization for vqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03647</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Tips and tricks for visual question answering: Learnings from the 2017 challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peyre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Iccv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Weaklysupervised learning of visual relations</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Some mathematical notes on threemode factor analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="279" to="311" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Visual relationship detection with internal and external linguistic knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Multi-modal factorized bilinear pooling with co-attention learning for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Beyond bilinear: Generalized multi-modal factorized highorder pooling for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>IEEE TNNLS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to count objects in natural images for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bennett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
