<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Unified Model for Opinion Target Extraction and Target Sentiment Prediction *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Systems Engineering and Engineering Management</orgName>
								<orgName type="department" key="dep2">Machine Intelligence Technology</orgName>
								<orgName type="department" key="dep3">Alibaba DAMO Academy</orgName>
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<addrLine>Kong 2 R&amp;D Center Singapore</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
							<email>l.bing@alibaba-inc.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piji</forename><surname>Li</surname></persName>
							<email>pijili@tencent.com</email>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
							<email>wlam@se.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Systems Engineering and Engineering Management</orgName>
								<orgName type="department" key="dep2">Machine Intelligence Technology</orgName>
								<orgName type="department" key="dep3">Alibaba DAMO Academy</orgName>
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<addrLine>Kong 2 R&amp;D Center Singapore</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Unified Model for Opinion Target Extraction and Target Sentiment Prediction *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Target-based sentiment analysis involves opinion target extraction and target sentiment classification. However, most of the existing works usually studied one of these two sub-tasks alone, which hinders their practical use. This paper aims to solve the complete task of target-based sentiment analysis in an end-to-end fashion, and presents a novel unified model which applies a unified tagging scheme. Our framework involves two stacked recurrent neural networks: The upper one predicts the unified tags to produce the final output results of the primary target-based sentiment analysis; The lower one performs an auxiliary target boundary prediction aiming at guiding the upper network to improve the performance of the primary task. To explore the inter-task dependency, we propose to explicitly model the constrained transitions from target boundaries to target sentiment polarities. We also propose to maintain the sentiment consistency within an opinion target via a gate mechanism which models the relation between the features for the current word and the previous word. We conduct extensive experiments on three benchmark datasets and our framework achieves consistently superior results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Target-Based Sentiment Analysis (TBSA) aims to detect the opinion targets explicitly mentioned in sentences and predict the sentiment polarities over the opinion targets <ref type="bibr" target="#b19">(Liu 2012;</ref><ref type="bibr">Pontiki 2014)</ref>. For example, in the sentence "USB3 Peripherals are noticably less expensive than the Thunder-Bolt ones", the user mentions two opinion targets, namely, "USB3 Peripherals" and "ThunderBolt ones", and expresses positive sentiment over the first, and negative sentiment over the second.</p><p>Traditionally, this task can be broken into two sub-tasks, namely, opinion target extraction and target sentiment classification. The goal of opinion target extraction is to detect the opinion target mentions in the text, and it has been extensively studied <ref type="bibr" target="#b22">(Qiu et al. 2011;</ref><ref type="bibr" target="#b17">Liu, Xu, and Zhao 2013;</ref><ref type="bibr" target="#b18">Liu, Xu, and Zhao 2014;</ref><ref type="bibr" target="#b16">Liu, Joty, and Meng 2015;</ref><ref type="bibr" target="#b28">Yin et al. 2016;</ref><ref type="bibr" target="#b25">Wang et al. 2016a;</ref><ref type="bibr" target="#b4">He et al. 2017;</ref><ref type="bibr" target="#b11">Li and Lam 2017;</ref><ref type="bibr" target="#b13">Li et al. 2018b;</ref>. The second sub-task, i.e., target sentiment classification, performs as a multiplier for the usefulness of the extracted target mentions, as it can predict the sentiment polarity of the given opinion targets. This sub-task has also received a lot of attention in recent years <ref type="bibr" target="#b0">(Dong et al. 2014;</ref><ref type="bibr" target="#b24">Tang, Qin, and Liu 2016;</ref><ref type="bibr" target="#b25">Wang et al. 2016b;</ref><ref type="bibr" target="#b19">Ma et al. 2017;</ref><ref type="bibr" target="#b0">Chen et al. 2017;</ref><ref type="bibr" target="#b25">Tay, Luu, and Hui 2017;</ref><ref type="bibr" target="#b20">Ma, Peng, and Cambria 2018;</ref><ref type="bibr" target="#b3">Hazarika et al. 2018;</ref><ref type="bibr" target="#b12">Li et al. 2018a;</ref><ref type="bibr" target="#b25">Wang et al. 2018;</ref><ref type="bibr" target="#b27">Xue and Li 2018;</ref><ref type="bibr" target="#b5">He et al. 2018;</ref><ref type="bibr" target="#b14">Li et al. 2019</ref>). However, most existing methods solving the second sub-task assume that the target mentions are given, which limits their practical use. To sum up, all the above works aim at solving only one of the sub-tasks. In order to apply these existing methods in practical settings, i.e., not only extracting the targets, but also predicting the target sentiment, one typical way is to pipeline the methods of the two sub-tasks together.</p><p>As observed in some other tasks <ref type="bibr" target="#b6">(Jing et al. 2003;</ref><ref type="bibr" target="#b21">Ng and Low 2004;</ref><ref type="bibr" target="#b1">Finkel and Manning 2009;</ref><ref type="bibr" target="#b21">Miwa and Sasaki 2014)</ref>, if two sub-tasks have strong couplings (e.g, NER and relation extraction), a more integrated model is usually more effective than a pipline solution. For the TBSA task, previous researchers have attempted two approaches to a more integrated solution <ref type="bibr" target="#b21">(Mitchell et al. 2013;</ref><ref type="bibr" target="#b29">Zhang, Zhang, and Vo 2015)</ref>. One approach is to make the models of the two sub-tasks jointly trained, which utilizes a set of target boundary tags (e.g., B, I, E, S and O) and a set of sentiment tags (e.g. POS, NEG, NEU). The "joint" row of <ref type="table">Table 1</ref> gives an example of the tagging scheme in this approach. Another approach is to totally dismiss the boundary of the two sub-tasks, which utilizes a set of specially-designed tags (we name it "unified tagging scheme"), namely, B-{POS, NEG, NEU}, I-{POS, NEG, NEU}, E-{POS, NEG, NEU}, S-{POS, NEG, NEU}, denoting the beginning of, inside of, end of, and single-word opinion target with positive, negative or neutral sentiment respectively, and O denoting NULL sentiment. An example is given in the "unified" row in <ref type="table">Table 1</ref>. Unfortunately, these initial attempts did not result in a more integrated model that can outperform the pipeline approaches.</p><p>Although the importance of solving the complete TBSA task remains significant, existing studies are relatively less</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>The AMD Turin Processor seems to always perform much better than Intel . <ref type="table">Table 1</ref>: Tagging schemes used in the integrated approaches. "Joint" and "Unified" refers to joint and unified approaches respectively.</p><formula xml:id="formula_0">Joint O B I E O O O O O O O S O O POS POS POS O O O O O O O NEG O Unified O B-POS I-POS E-POS O O O O O O O S-NEG O</formula><p>and their findings <ref type="bibr" target="#b21">(Mitchell et al. 2013;</ref><ref type="bibr" target="#b29">Zhang, Zhang, and Vo 2015)</ref>, to some extent, discouraged other researchers to do further explorations. However, we think that research efforts should be paid to explore a more integrated model for solving this task, because its two sub-tasks are highly coupled together and the potential of a more integrated model is promising.</p><p>In this paper, we investigate the complete task of TBSA and design a novel unified framework to handle it in an end-to-end fashion. The proposed framework involves two stacked Recurrent Neural Networks (RNN). The upper one produces the final tagging results of the TBSA task based on the unified tagging scheme. The lower one performs an auxiliary prediction of target boundaries with the aim for guiding and providing the information to the upper RNN. Such design is based on the observation that under the unified tagging scheme, the span information is exactly identical to that under the boundary tagging scheme. Refer to the example in <ref type="table">Table 1</ref>, if a word is at the beginning of a target mention under the boundary scheme, i.e., having the tag B, it should also be at the beginning under the unified scheme, i.e., having the tag B-POS. In order to explore such inter-scheme tag dependency, we propose to guide the prediction of the upper RNN for the complete TBSA task with the boundary prediction from the auxiliary task, corresponding to the lower RNN. Specifically, we design a component to encode the dependencies into a transition matrix and use the matrix to map the probability distribution of the boundary prediction to the unified tag space of the TBSA task. Then, we determine the proportions of the obtained boundary-based probability scores in the tagging decision and consolidate them with the probability scores from the upper RNN for final predictions.</p><p>We also propose to maintain the consistency of the sentiment of individual words within the same target mention based on a simple gate mechanism. The gate mechanism is designed to explicitly consolidate the features of the current word and the previous word. Since both of the gate here and the transition matrix above need to take reliable boundary prediction for performing well, improving the reliability of such prediction in the lower RNN is supposed to be useful for the complete TBSA task. Therefore, we introduce another component to estimate the potential of a word to be a target word. Note that as defined by the task <ref type="bibr">(Pontiki 2014;</ref><ref type="bibr">Pontiki 2015;</ref><ref type="bibr">Pontiki 2016)</ref>, an opinion target should always co-occur with opinion words, thus, the words close to the opinion words are more likely to be target words and we obtain additional supervision signals for refining boundary information based on this assumption.</p><p>In the experiments, our framework outperforms the state-of-the-art methods and the strongest sequence taggers on several benchmark datasets. We conducted detailed ablation studies to quantitatively demonstrate the effectiveness of the designed components. With some case analysis, we show how our framework can handle some difficult cases with the help of the designed components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our Proposed Framework Task Definition</head><p>We formulate the complete Target-Based Sentiment Analysis (TBSA) task as a sequence labeling problem, and employ a unified tagging scheme: Y S = {B-POS, I-POS, E-POS, S-POS, B-NEG, I-NEG, E-NEG, S-NEG, B-NEU, I-NEU, E-NEU, S-NEU, O}. Except O, each tag contains two parts of tagging information: the boundary of target mention, and the target sentiment. For example, B-POS denotes the beginning of a positive target mention, and S-NEG denotes a single-word negative opinion target. For a given input sequence X = {x 1 , . . . , x T } with length T , our goal is to predict a tag sequence</p><formula xml:id="formula_1">Y S = {y S 1 , . . . , y S T }, where y S i ? Y S .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Description</head><p>Overview As shown in <ref type="figure">Figure 1</ref>, on the top of two stacked RNNs with LSTM cells, our framework designs three tailormade components, depicted in detail with the callouts, to explore three important intuitions in the task of TBSA. Specifically, the upper LSTM S is for the complete TBSA task and it predicts the unified tags as output, while the lower LSTM T is for the auxiliary task and predicts the boundary tags of target mentions. The boundary prediction from LSTM T is used to guide LSTM S to make better predictions over the unified tags for the complete task. The three key components are named Boundary Guidance (BG) component, Sentiment Consistency (SC) component and Opinion-Enhanced (OE) Target Word Detection component. The BG component takes the advantages of the boundary information provided by the auxiliary task to guide the LSTM S for predicting the unified tags more accurately. The SC component is empowered with a gate mechanism to explicitly integrate the features of the previous word into the current prediction, aiming at maintaining the sentiment consistency within a multi-word opinion target. In order to provide boundary information of higher quality, the OE component, following the oberservation that "opinion targets and opinion words always co-occur", performs another auxiliary binary classification task to determine if the current word is a target word. <ref type="figure">Figure 1</ref>: Architecture of the proposed framework.</p><formula xml:id="formula_2">1 ? ? ? ? ? ? ? 1 ? ? 1 ? 2 ? ? ? 1 ? ? 2 ? ? y 1 y y 2 OE BG SC 2 OE BG SC ? ? ? ? ? ? OE BG SC ? 2 ? ? ? ?1 + ? ? softmax ? + ? 1 ? ? 1 1 1 1 ? ? 1</formula><p>Target Boundary Guided TBSA We employ LSTM S with softmax decoding layer for the prediction of the tag sequence. It is observed that the boundary tag can provide important clues for the unified tag prediction. For example, if the current boundary tag is B, denoting the beginning of an opinion target, then the corresponding unified tag can only be B-POS, B-NEG or B-NEU. Thus, we introduce an additional network LSTM T for the target boundary prediction, where the valid tag set Y T is {B, I, E, S, O}. We link these two LSTM layers so that the hidden representations generated by the LSTM T can be directly fed to LSTM S as guidance information. Specifically, their hidden representa-</p><formula xml:id="formula_3">tions h T t ? R dim T h and h S t ? R dim S h at the t-th time step (t ? [1, T ])</formula><p>are calculated as follows:</p><formula xml:id="formula_4">h T t = [ ? ??? ? LSTM T (x t ); ? ??? ? LSTM T (x t )], h S t = [ ? ??? ? LSTM S (h T t ); ? ??? ? LSTM S (h T t )], t ? [1, T ].</formula><p>(1)</p><p>The probability scores z T t ? R |Y T | over the boundary tags are calculated by a fully-connected softmax layer:</p><formula xml:id="formula_5">z T t = p(y T t |x t ) = Softmax(W T h T t ).<label>(2)</label></formula><p>where the Softmax denotes the softmax activation function and W T is the model parameter. Similarly, the scores over the unified tags z S t ? R |Y S | are obtained as below:</p><formula xml:id="formula_6">z S t = p(y S t |h T t ) = Softmax(W S h S t ).<label>(3)</label></formula><p>As mentioned above, the boundary information is supposed to be useful for improving the performance of LSTM S . <ref type="bibr" target="#b29">(Zhang, Zhang, and Vo 2015)</ref> incorporated such boundary information by adding hard boundary constraints in the decoding step of the CRFs model. However, their prediction results are not promising. One reason is that their model employs a hard constraint which is prone to propagating the errors from the tagger of the boundary detection task and thus it decreases the performance of the TBSA tagger. Different from their way of imposing hard constraints, our proposed BG component can absorb the boundary information via boundary guided transition and automatically determine its proportions in the final tagging decision based on the confidence of the target boundary tagger. Firstly, the BG component encodes the constraints into a transition matrix W tr ? R |Y T |?|Y S | . As we have no prior knowledge about the transition probabilities between the boundary tags and the unified tags, we initially set them equally as follows:</p><formula xml:id="formula_7">W tr i,j = 1 |Bi| , if j ? B i 0, Otherwise<label>(4)</label></formula><p>where B i is the set of valid unified tags coherent with the boundary tag i. In this transition matrix, a non-zero element, e.g., W tr B,B-POS , denotes the probabilities of the unified tags given the boundary tag, and a zero element, e.g., W tr B,I-NEG , suggests that the unified tag cannot be inferred through this transition. After encoding the constraints, the next step is to guide the unified tag prediction with the boundary information. We directly propagate such information to the TBSA tagger by mapping the probability scores of the boundary tag z T t to the unified tag space. The transition-based sentiment score z S t ? R |Y S | is obtained as follows:</p><formula xml:id="formula_8">z S t = (W tr ) z T t<label>(5)</label></formula><p>where the transition operation is equivalent to the linear combination of the row vectors in the transition matrix W tr . Assuming z T t = [1, 0, 0, 0, 0] (i.e., taking the tag B), the result of the transition is exactly the row vector W tr B,: . As the unified tag can be partially derived from the boundary tag, a natural question is how to determine the proportions of the transition-based unified tagging scores z S t . Intuitively, if the target boundary score z T t is nearly uniform, suggesting that the boundary tagger is not confident to its prediction, the obtained distribution over the unified tags, i.e., z S t , will also be close to a uniform distribution and has little meaningful information for the sentiment prediction. To avoid such uninformative boundary transitions, we calculate a proportion score ? t ? R based on the confidence c t of the target bound-ary tagger:</p><formula xml:id="formula_9">c t = (z T t ) z T t ? t = c t (6)</formula><p>where the hyper-parameter denotes the maximum proportions that the boundary-based scores z S t occupy in the tagging decision. Obviously, c t will be down-weighted if the boundary scores are uniformly distributed. The maximum confidence value is reached if z T t is a one-hot vector. The final scores are obtained by combining the boundary-based and model-based unified tagging scores:</p><formula xml:id="formula_10">z S t = ? t z S t + (1 ? ? t )z S t .<label>(7)</label></formula><p>Maintaining Sentiment Consistency In the traditional target sentiment classification task, the sentiments towards the different words in a given multi-word opinion target are assumed to be identical. However, in the complete TBSA task, such sentiment consistency is not guaranteed since the task is formulated as a sequence tagging/labeling problem. Taking the sentence in <ref type="table">Table 1</ref> as an example, there is still some possibility that the word "Processor" is labeled with an E-NEG tag due to the independent tagging decisions made by LSTMs. To maintain the sentiment consistency within the same opinion target, we propose to predict the current unified tag using both of the features from the current and the previous time steps. Specifically, we design a Sentiment Consistency (SC) component with a gate mechanism to combine these two feature vectors:</p><formula xml:id="formula_11">h S t = g t h S t + (1 ? g t ) h S t?1 g t = ?(W g h S t + b g )<label>(8)</label></formula><p>where W g and b g are learnable parameters of the SC component, and denotes the element-wise multiplication. ? is the sigmoid function. Through the gating, the previous features are considered in the current predictions and such indirect bi-gram dependency can help reduce the probability that the words within the same target hold different sentiments.</p><p>Auxiliary Target Word Detection A good boundary tagger for opinion targets is crucial for producing the boundary information of high quality. Here, we introduce the OE component to learn a more robust boundary tagger from another view of the training data. As defined in (Pontiki 2014; Pontiki 2015; Pontiki 2016), opinion targets are always collocated with opinion words. Inspired by this, we regard the word as a target word if there is at least one opinion word within the context window of fixed-size s of this word. Then, we train an auxiliary token-level classifier for discriminating target words and non-target words based on the distantly supervised labels and the boundary representations h T t are further refined with such supervision signals. The computational process of the OE component is below:</p><formula xml:id="formula_12">z O t = Softmax(W o h T t ) y O t = arg max y z O t<label>(9)</label></formula><p>where W o is the model parameter.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Training</head><p>All the components in our framework are differentiable, thus, the whole framework can be efficiently trained with gradient-based methods. Word/Token-level cross-entropy error is employed as the loss function:</p><formula xml:id="formula_13">L I = ? 1 T T t=1 I(y I,g t ) ? log(z I t )<label>(10)</label></formula><p>where I is the symbol of task indicator and its possible values are T , S, and O. I(y) represents the one-hot vector with the y-th component being 1 and y I,g t is the gold standard tag for the task I at the time step t. Then, the losses from the main TBSA task and the two auxiliary tasks are aggregated to form the training objective J (?) of the framework:</p><formula xml:id="formula_14">J (?) = L S + L T + L O .<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Dataset</head><p>Our model is evaluated on two product review datasets from SemEval ABSA challenges (Pontiki 2014; Pontiki 2015; Pontiki 2016) and the Twitter dataset. <ref type="table" target="#tab_1">Table 2</ref> gives the statistics of these benchmark datasets. D L (SemEval 2014) contains reviews from the laptop domain and the train-test split is the same as the original dataset. D R is the union set of the restaurant datasets from SemEval ABSA challenge 2014, 2015 and 2016. The new training dataset is obtained by merging the three years' training datasets and the new testing set is built in the same way. D T consists of tweets collected by <ref type="bibr" target="#b21">(Mitchell et al. 2013)</ref>. The ground truth of the opinion target mentions and their sentiments are provided in these datasets. For D L and D R , we regard 10% randomly held-out training data as the development set. For D T , we report the ten-fold cross validation results, as done in <ref type="bibr" target="#b21">(Mitchell et al. 2013;</ref><ref type="bibr" target="#b29">Zhang, Zhang, and Vo 2015)</ref>, since there is no standard train-test split for this dataset. The gold standard boundary annotations are available for the auxiliary target boundary prediction task. For another auxiliary task, namely, opinion-based target word detection, we employ the existing opinion lexicon 1 to provide the opinion words.  The evaluation metric measures the standard precision (P), recall (R) and F1 score based on the exact match, which means that an output segment is considered to be correct only if it exactly matches with the gold standard span of the target mention and the corresponding sentiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compared Models</head><p>We compare our framework with the following methods:</p><p>? CRF-{pipeline, joint, unified} <ref type="bibr" target="#b21">(Mitchell et al. 2013)</ref>:</p><p>Conditional Random Fields (CRF) based sequence tagger 2 . "pipeline" denotes the pipeline approach. "joint" and "unified" are the models following the joint tagging scheme and unified tagging scheme respectively.</p><p>? NN-CRF-{pipeline, joint, unified} <ref type="bibr" target="#b29">(Zhang, Zhang, and Vo 2015)</ref>: Enhanced CRF models 3 armed with word embeddings and neural network feature extractors.</p><p>? HAST-TNet: HAST <ref type="bibr" target="#b13">(Li et al. 2018b</ref>) and TNet <ref type="bibr" target="#b12">(Li et al. 2018a</ref>) are the current state-of-the-art models on the tasks of target boundary detection and target sentiment classification respectively. HAST-TNet is the pipline approach of these two models. We use the officially released codes 4 to produce the results.</p><p>? LSTM-unified: the standard LSTM model adopting the unified tagging scheme.</p><p>? LSTM-CRF-1 <ref type="bibr" target="#b10">(Lample et al. 2016)</ref>: LSTM model with CRF decoding layer and no feature engineering is needed. We run the officially released code 5 and utilize the unified tag set to reproduce the results.</p><p>? LSTM-CRF-2 (Ma and Hovy 2016): LSTM-CRF-2 is similar to LSTM-CRF-1. The difference is that LSTM-CRF-2 employs CNN rather than LSTM to learn the character-level word representations. We run the released code 6 to reproduce the results.</p><p>? LM-LSTM-CRF : Language model enhanced LSTM-CRF model. It is a competitive model in several sequence tagging tasks. We rerun their code 7 and report the tagging results based on the unified tagging scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Settings</head><p>Word Embeddings We use GloVe.840B.300d 8 released by <ref type="bibr" target="#b21">(Pennington, Socher, and Manning 2014)</ref> to initialize the word embeddings, fine-tuned during training. The embeddings of the out-of-vocabulary words are sampled from the uniform distribution U(-0.25, 0.25) <ref type="bibr" target="#b8">(Kim 2014)</ref>.</p><p>Weight Initializations The weight matrices in the LSTM units are initialized by following the Glorot Uniform strategy <ref type="bibr" target="#b2">(Glorot and Bengio 2010)</ref> and the others are randomly sampled from the uniform distribution U(-0.2, 0.2). Besides, all biases are initialized as 0's.</p><p>Optimization Our models are trained up to 50 epochs with Adam (Kingma and Ba 2014), with ? 1 = ? 2 = 0.9, and the initial learning rate ? 0 = 10 ?3 . The decay rate is kept the same as the setting in <ref type="bibr" target="#b10">(Lample et al. 2016)</ref>. We apply dropout on word embeddings and the ultimate features for prediction. The dropout rates are empirically set as 0.5. The model obtaining the best F1 score on the development set is selected for producing the testing results.</p><p>Others Both of the dimension of the hidden representations dim T h and dim S h are 50. The maximum proportion of the boundary-based scores is 0.5. The size of the context window s in the opinion-based target word detection component is 3. The tuning details of and s are given later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Analysis</head><p>Main Results <ref type="table" target="#tab_3">Table 3</ref> presents our comparisons with other methods for the complete TBSA task. To make the comparison fair, we use GloVe.840B.300d as the pre-trained word embeddings for all the baselines requiring word embedding input on all of the datasets. Besides, we align the train/dev/test configurations for all methods. The experimental results suggest that our proposed framework consistently gives the best F1 score across all datasets and significantly outperforms the strongest baselines in most cases.</p><p>Compared to HAST-TNet, the pipeline of two state-ofthe-art models, our proposed framework achieves 2.6%, 2.4% and 0.40% absolute gains on D L , D R and D T respectively, suggesting that a carefully-designed integrated model can be more effective than the pipeline approaches on the TBSA task. Three competitive unified sequence taggers (see the third block in <ref type="table" target="#tab_3">Table 3</ref>) are also introduced into the comparative study. Again, our framework outperforms the best of them by 1.7%, 3.4% and 0.5% on the benchmark datasets. We notice that the improvement of our framework on the Twitter dataset is marginal in contrast with the unified baselines. The small gap is reasonable since these models employ additional component (e.g., LSTM or CNN) to learn the character-level word representations, whose capability for representing out-of-vocabulary words has been verified in <ref type="bibr" target="#b23">(Santos and Zadrozny 2014;</ref><ref type="bibr" target="#b7">Kim et al. 2016)</ref>, while our framework only utilizes the word-level features provided by the pre-trained word embeddings. Similar observation is captured in the comparison with HAST-TNet. We attribute this to the superior modeling power of the CNN applied in TNet when processing the ungrammatical sentences such as tweets and micro-blogs, as pointed out in <ref type="bibr" target="#b12">(Li et al. 2018a)</ref>.</p><p>We also notice that the performances of the CRF-based models, especially the recall (R) scores, are quite poor. Armed with the pre-trained word embeddings and neural network feature extractor, the models are slightly improved but the scores are still not promising.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of the Proposed Components</head><p>To investigate the effectiveness of the designed components, we conduct ablation study on the proposed framework and the results are listed in the last block of the <ref type="table" target="#tab_3">Table 3</ref>. Let us start the discussion from the base model, namely, the stacked LSTMs. We find that the base model always gives superior performance compared to the LSTM-unified. This result indicates that the boundary information predicted by the auxiliary LSTM indeed increases the F1 score of the complete TBSA task. With the help of the BG component, the performances are improved more significantly and the way we impose the boundary constraints proves effective for yielding more true positives. Another interesting finding is that introducing the component SC or OE individually into the "Base model + BG" does not bring in too much gains on F1 measure and even hurts the prediction performance on D R . But putting them together, i.e., the "Full model", leads to the new state-of-the-art result. This result illustrates the necessity of both of the SC and OE components in the boundary guided TBSA. Considering the "Base model + BG + SC", the quality of the boundary information may not be accurate without the clues from the OE component, and thus, the SC component tends to incorrectly align the sentiments of both the target words and non-target words. For the "Base model + BG + OE", the quality of the boundary information obtained from the LSTM T is improved but the sentiments of the words within the same target are not fully consistent compared to the "Full model" armed with SC component. In summary, the SC component and the OE component are complementary to some extent when they are added into the boundary-guided "Base model + BG". <ref type="table">Table 4</ref> gives some prediction examples of the base model (i.e., the stacked LSTMs) and the models empowered with our proposed components. As observed in the first input and the second input, the "Base model" correctly predicts the target boundary but it fails to produce the right target sentiments, suggesting that linking the two LSTMs for the target boundary prediction and the TBSA task is still insufficient for exploiting the boundary information to improve the performance of the complete TBSA. The "Base model+BG" and the "Full model", where the boundary constraints are properly imposed via our BG component, can correctly handle these two cases. Although the boundary information can guide the model to predict the sentiment more accurately, there is the possibility that only using the BG component (i.e., "Base model+BG") inherits the errors from the lower boundary detection task, e.g., the third and the fourth input. Thus, the boundary information of high quality is crucial for improving the upper TBSA task and our OE component can serve as a simple but effective solution. Besides, we find that maintaining sentiment consistency within the same target mention, especially for whose with several words (e.g., "portobello and asparagus mole" in the last input), is difficult for the "Base model" and "Base model+BG", while our "Full model" alleviates this issue by employing the SC component to make predictions based on the features from the current and the previous time step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case Analysis</head><p>Impact of and s Here, we investigate the impacts of the maximum proportion of the boundary-based scores and the window size s on the prediction performance. Specifically, the experiments are conducted on the development set of D R , the largest benchmark dataset. We vary from 0.3 to 0.7, increased by 0.1, and two extreme values 0.0 and 1.0 are also included. The range of the window size s is 1 to 5. According to the results given in <ref type="figure" target="#fig_0">Figure 2</ref>, we observe that the best results are obtained at =0.5. The value basically affects the importance of the sentiment scores from the BG component in the final tagging decision and 0.5 is a good trade-off between absorbing boundary information and eliminating noises. We also observe that a moderate value of s (i.e., s = 3) is the best for the TBSA task, probably because too large s may enforce the model to attend the larger  <ref type="table">Table 4</ref>: Case analysis. The "Target" column contains the results from the auxiliary task of target boundary detection. The "Complete" column presents the output of the complete TBSA task, but note that we only show the sentiment part of the unified labels (i.e., POS, NEG, and NEU) and use brackets to indicate the boundary. The marker denotes the incorrect prediction.</p><p>context and increase the possibility of associating with irrelevant opinion words, on the other hand, too small s is likely not sufficient to involve the potential opinion words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Works</head><p>As mentioned in Introduction, Target-based Sentiment Analysis are usually divided into two sub-tasks, namely, the Opinion Target Extraction task (OTE) and the Target Sentiment Classification (TSC) task. Although these two subtasks are treated as separate tasks and solved individually in most cases, for more practical applications, they should be solved in one framework. Given an input sentence, the output of a method should contain not only the extracted opinion targets, but also the sentiment predictions towards them. Some previous works attempted to discover the relationship between these two sub-tasks and gave a more integrated solution for solving the complete TBSA task. Concretely, <ref type="bibr" target="#b21">(Mitchell et al. 2013</ref>) employed Conditional Random Fields (CRF) together with hand-crafted linguistic features to detect the boundary of the target mention and predict the sentiment polarity. <ref type="bibr" target="#b29">(Zhang, Zhang, and Vo 2015)</ref> further improved the performance of the CRF based method by introducing a fully connected layer to consolidate the linguistic features and word embeddings. However, they found that a pipeline method can beat both of the model with joint training and the unified model. In this paper, we reexamine the task, and proposed a new unified solution which outperforms all previous reported methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>We investigate the complete task of Target-Based Sentiment Analysis (TBSA), which is formulated as a sequence tagging problem with a unified tagging scheme in this paper. The basic architecture of our framework involves two stacked LSTMs for performing the auxiliary target boundary detection and the complete TBSA task respectively. On top of the base model, we designed two components to take the advantage of the target boundary information from the auxiliary task and maintain the sentiment consistency of the words within the same target. To ensure the quality of the boundary information, we employ an auxiliary opinion-based target word detection component to refine the predicted target boundaries. Experimental results and case studies well illustrate the effectiveness of our proposed framework, and a new state-of-the-art result of this task is achieved. We publicly release our implementation at https://github.com/lixin4ever/E2E-TBSA.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>F1 scores (%) on the development set of D R with different and s values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Statistics of the datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Main results of the complete TBSA task. "Base model" refers to the stacked LSTMs. The markers and refer to our full model significantly outperforms HAST-TNet and LM-LSTM-CRF respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>And the fact that it comes with an [i5 processor] POS definitely speeds things up i5 processor [processor] POS () i5 processor [i5 processor] POS i5 processor [i5 processor] POS 2. There were small problems with [mac office] NEG . mac office [mac] NEG () mac office [mac office] NEG mac office [mac office] NEG 3. The [teas] POS are great and all the [sweets] POS are homemade teas, sweets [teas] POS , [sweets] POS POS 5. I blame the [Mac OS] NEG . Mac OS [Mac NEG OS NEU ] () Mac OS [Mac NEG OS POS ] () Mac OS [Mac OS] NEG 6. Also, I personally wasn't a fan of the [portobello and asparagus mole] NEG .</figDesc><table><row><cell>Input</cell><cell>Target</cell><cell>Base model Complete</cell><cell>Target</cell><cell cols="2">Base model + BG Complete</cell><cell>Target</cell><cell>Full model Complete</cell></row><row><cell cols="5">1. teas, sweets, homemade ()</cell><cell>[teas] POS , [sweets] POS , [homemade] POS ()</cell><cell cols="2">teas, sweets</cell><cell>[teas] POS , [sweets] POS</cell></row><row><cell>4. I love the [form factor] POS</cell><cell cols="7">NONE form factor [form factor] portobello NONE NONE NONE [portobello NEG portobello [portobello NEG portobello</cell></row><row><cell></cell><cell>and</cell><cell>and NEG</cell><cell>and</cell><cell></cell><cell>and NEG</cell><cell>and</cell><cell>[portobello and</cell></row><row><cell></cell><cell>asparagus</cell><cell>asparagus NEG</cell><cell cols="2">asparagus</cell><cell>asparagus NEU</cell><cell>asparagus</cell><cell>asparagus mole] NEG</cell></row><row><cell></cell><cell>mole</cell><cell>mole NEU ] ()</cell><cell>mole</cell><cell></cell><cell>mole NEU ] ()</cell><cell>mole</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://www.m-mitchell.com/code/index.html 3 https://github.com/SUTDNLP/NNTargetedSentiment 4 Available at: https://github.com/lixin4ever/HAST and https://github.com/lixin4ever/TNet respectively. 5 https://github.com/glample/tagger</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/XuezheMax/NeuroNLP2 7 https://github.com/LiyuanLucasLiu/LM-LSTM-CRF 8 https://nlp.stanford.edu/projects/glove/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Adaptive recursive neural network for target-dependent twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="49" to="54" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Joint parsing and named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="326" to="334" />
		</imprint>
	</monogr>
	<note>and Manning</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AISTATS</title>
		<imprint>
			<biblScope unit="page" from="249" to="256" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Glorot and Bengio</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Modeling inter-aspect dependencies for aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Hazarika</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="266" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An unsupervised neural attention model for aspect extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="388" to="397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exploiting document knowledge for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="579" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Howtogetachinesename(entity): Segmentation and combination issues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Jing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<meeting><address><addrLine>Ba</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Lample</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep multitask learning for aspect term extraction with memory interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Transformation networks for target-oriented sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="946" to="956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Aspect term extraction with history attention and selective transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploiting coarse-to-fine task transfer for aspectlevel sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Empower sequence labeling with task-aware neural language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fine-grained opinion mining with recurrent neural networks and word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joty</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1433" to="1443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Syntactic patterns versus word alignment: Extracting opinion targets from online reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1754" to="1763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Extracting opinion targets and opinion words from online reviews with graph co-ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="314" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Interactive attention networks for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="4068" to="4074" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Targeted aspect-based sentiment analysis via embedding commonsense knowledge into an attentive lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Chinese part-of-speech tagging: One-at-a-time or all-at-once? wordbased or character-based?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<editor>EMNLP. [Miwa and Sasaki</editor>
		<imprint>
			<publisher>Pennington, Socher, and Manning</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="27" to="35" />
		</imprint>
	</monogr>
	<note>EMNLP. Pontiki 2014] Pontiki, M. 2014. Semeval-2014 task 4: Aspect based sentiment analysis. In SemEval</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Opinion word expansion and target extraction through double propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Semeval-2015 task 12: Aspect based sentiment analysis</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="19" to="30" />
		</imprint>
	</monogr>
	<note>Semeval-2016 task 5: Aspect based sentiment analysis</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning character-level representations for part-ofspeech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Santos and Zadrozny</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Aspect level sentiment classification with deep memory network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Liu ; Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to attend via word-aspect associative fusion for aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luu</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui ;</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="957" to="967" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Double embeddings and cnn-based sequence labeling for aspect extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Aspect based sentiment analysis with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2514" to="2523" />
		</imprint>
	</monogr>
	<note>and</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised word and dependency path embeddings for aspect term extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2979" to="2985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Neural networks for open domain targeted sentiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vo ; Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Vo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="612" to="621" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
