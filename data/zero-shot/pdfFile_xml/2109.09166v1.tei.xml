<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised 3D Pose Estimation for Hierarchical Dance Video Recognition *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Hu</surname></persName>
							<email>xiaodan8@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
							<email>n-ahuja@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised 3D Pose Estimation for Hierarchical Dance Video Recognition *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Dance experts often view dance as a hierarchy of information, spanning low-level (raw images, image sequences), mid-levels (human poses and bodypart movements), and high-level (dance genre). We propose a Hierarchical Dance Video Recognition framework (HDVR). HDVR estimates 2D pose sequences, tracks dancers, and then simultaneously estimates corresponding 3D poses and 3D-to-2D imaging parameters, without requiring ground truth for 3D poses. Unlike most methods that work on a single person, our tracking works on multiple dancers, under occlusions. From the estimated 3D pose sequence, HDVR extracts body part movements, and therefrom dance genre. The resulting hierarchical dance representation is explainable to experts. To overcome noise and interframe correspondence ambiguities, we enforce spatial and temporal motion smoothness and photometric continuity over time. We use an LSTM network to extract 3D movement subsequences from which we recognize dance genre. For experiments, we have identified 154 movement types, of 16 body parts, and assembled a new University of Illinois Dance (UID) Dataset, containing 1143 video clips of 9 genres covering 30 hours, annotated with movement and genre labels. Our experimental results demonstrate that our algorithms outperform the state-ofthe-art 3D pose estimation methods, which also enhances our dance recognition performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Dance represents a special genre of human activity. Our goal in this paper is development of algorithms to understand dance videos. We combine estimation of body movements with their feasibility as a part of dance. This enables interpretation of dance videos using not only constraints posed by the data but also those by the domain knowledge.</p><p>A variety of proposed methods have also focused on * The support of the Office of Naval Research under grant N00014-20-1-2444 and USDA National Institute of Food and Agriculture under grant 2020-67021-32799/1024178 are gratefully acknowledged. dance videos <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>. Most of these rely on kinect sensors to obtain depth information <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. <ref type="bibr" target="#b2">[3]</ref> classifies Indian dances by extracting patches centered at body's joint locations and using an LSTM network for classification. <ref type="bibr" target="#b3">[4]</ref> proposes to perform Laban Movement Analysis (in terms of dance domain constructs of Body, Effort, Shape and Space) to then describe human motion from a pose sequence. <ref type="bibr" target="#b4">[5]</ref> compares the effects of using three different representations -raw images, optical flow and multi-person pose data -on their proposed dance dataset proving that visual information is not sufficient to classify motion-heavy categories. There are several approaches to action recognition that first estimate poses <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref>. <ref type="bibr" target="#b6">[7]</ref> creates a coaching system for personalized athletic training based on pose correctness. <ref type="bibr" target="#b7">[8]</ref> improves action recognition performance by improving pose estimation accuracy using additional spatial and temporal constraints. However, <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> both estimate only the 2D poses, leading to difficulties ambiguity when the movements are along the viewing direction . <ref type="bibr" target="#b8">[9]</ref> estimates both 2D and 3D Poses as well as image features to predict actions from all three. <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref> limit their representation for action recognition to pose sequences without including any higher level semantics that may define action. Moreover, these methods also require pose annotations in training videos. <ref type="bibr" target="#b5">[6]</ref> embeds RGB and optical-flow values into a single two-inone stream network for more efficient dance genre classification. In addition to the features such as pose and optical flow used in these works, in this paper we use dance domain representations to tune feature analysis to dance instead of being generic.</p><p>When people dance, they follow a carefully choreographed sequence of 3D movements, where each movement is hierarchically composed of simpler movements, ending in basic movements. Each basic movement is composed of a sequence of poses representing a specific dance pattern. For brevity, in what follows, we will refer to basic movements by simply movements, We identify movements of 16 main body parts e ? E illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>. following Labanotation <ref type="bibr" target="#b9">[10]</ref>, a well-known notation system used to record and archive human motion. Then in <ref type="table" target="#tab_0">Table 1</ref> we At the second level, our algorithm simultaneously estimates the 2D posep i t and 3D pos? P i t of each dancer i(i = 0, ..., N ? 1) at each frame, as well as the camera projection parameters. Our algorithm works under occlusions, e.g., among dancers. At the third level, each dance movement? e t of each body part e ? E (defined over a sequence of frames) is recognized and its location, given by, e.g., its starting frame t and length are estimated, based on the poses estimated for the previous frames. At the fourth level, the dance genre? is recognized based on the movements {? e t }e?E of all body parts.</p><p>list the basic movements y e ? Y e for each body part e ? E, again following <ref type="bibr" target="#b9">[10]</ref> and defined in terms of homogeneity of motion direction, and level which are frequently used to describe the dance in dance domain. Our dance recognition model adopts this hierarchy used by dance experts, which starts with the 3D pose sequence of the dancer, combines subsequences of joint displacements into dance movements, and finally infers dance genre from the sequences of the movements of joints. To help the model segment the pose sequence into the basic movements, we manually annotate the starting and ending positions of such movements for each body part for a subset of videos in the UID dataset. Our framework takes a raw dance video sequence {I t } T ?1 t=0 as input, estimates posesp t for each frame I t , recognizes the movement? e t (over multiple frames) of each body part e based on its past pose sequence, and then predicts the dance genre? t from the movement sequence. Experiments show that our hierarchical feature analysis is an effective way to recognize dance and our method outperforms state-of-theart on F-score.</p><p>The main contributions of this paper are as follows:</p><p>? We propose the first dance video understanding framework that analyzes the videos hierarchically -from the bottom level of video frames, through the middle level of human poses, to the highest level of movements and associated dance genres.</p><p>? Our algorithm tracks and outputs 2D pose of each dancer in each frame in the presence of occlusions among dancers.</p><p>? We propose an unsupervised 3D pose estimation algorithm that starts with the estimated 2D pose sequence, and simultaneously and iteratively updates 2D poses, 3D poses and 3D-to-2D projection parameters using a single camera without using ground-truth for these poses or parameters. Our 3D pose network achieves state-of-the-art performance by incorporating kinematic constraints of a 34-DOF human skeletal model and temporal smoothness of motion.</p><p>? We have curated a large dance video data set, containing pose in ground truths for each video frame as well as for each movement, which we will share with the community for further exploration. <ref type="figure" target="#fig_0">Figure 1</ref> describes the components of our approach to dance video recognition and the hierarchy they form. Our approach can be summarized in the following steps:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Computational Approach</head><p>Step 1: For each input frame I t , the model estimates the 2D pose p i t for dancer i appearing in I t . The model tracks approximate locations of the dancers {i} N ?1 i=0 throughout the video via their bounding boxes {B i t } T ?1 t=0 .</p><p>Step 2: At each frame, the model provides an estimatep i t of the 2D pose p i t of the dancer associated with each tracked box B i t (Section 2.1).</p><p>Step 3: The model then estimates 3D posesP i t from the estimated 2D onesp i t , by using an unsupervised 3D pose estimation method (Section 2.2).</p><p>Step 4: The model uses the LSTM network to recognize the movement {? e t } T ?1 t=0 of each body part e ? E (e.g., head, torso, etc.) from the trajectories {{P j t } j?Je } T ?1 t=0 of all the joints j ? J e connected to the body part e, where J e ? E (Section 2.3). We represent any given state of a dance as a set of body part configurations and the entire dance as a sequence of such sets.</p><p>Step 5: For recognition, we first concatenate the movements {{? e t } e?E }} T ?1 t=0 of all body parts, and input it to an LSTM network to recognize the dance genre? (Section 2.4). The rest of this section introduces the components of this hierarchy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">2D Pose Estimation by Tracking</head><formula xml:id="formula_0">{B i t } T ?1 t=0 = {(x i t , y i t , w i t , l i t )} T ?1 t=0 of the i th dancer Output: a sequence of poses {p i t } T ?1 t=0 of the i th dancer while new frame I t available do</formula><p>Estimate poses // Perform OpenPose for i th dancer do Select pose? from C poses overlapped with the bounding box B i t based on histogram match end end coordinates of each body joint. Classical pose estimation methods such as pictorial structures framework and deformable part models largely rely on hand-designed features to determine body joint locations. Recently, deep learningbased approaches have achieved a major breakthrough in solving the problems in multi-person pose estimation (e.g., how to group keypoints for different people). They can be divided into top-down <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> and bottom-up <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref>. The former employ detectors to first locate person instances and then their individual joints; the latter first estimate all joint locations within the image and then assign the joints to the associated person. Although these methods provide superior pose estimates, they have two major shortcomings critical to our task. Firstly, most of the pose estimation methods cannot track a dancer through the video when there are multiple dancers present because they perform pose estimation from individual images, ignoring the temporal information. Besides, the methods perform training mostly on large datasets wherein the dance parts are very small, with a single person, limited pose variety, and clean background. and therefore cannot guarantee accuracy on real world dance videos. The method we propose can track se-lected dancers, detect estimation errors, and correct them automatically. Object Tracking: As explained in Algorithm 1, our tracking algorithm is built upon the LDES tracker <ref type="bibr" target="#b15">[16]</ref>. Since occlusion between dancers is a serious problem, our algorithm centrally addresses it. Following are the three stages of our algorithm: (1) Use the LDES tracker to track each i th dancer when the dancer has no overlap with other dancers, while maintaining a color histogram h i t and a bounding box</p><formula xml:id="formula_1">B i t = (x i t , y i t , w i t , l i t ) for the dancer.</formula><p>(2) Detect occurrence of overlap by detecting failure of the tracker as indicated by a significant difference between the directions of motion before and after overlap. (3) Predict the time and the location of the dancer when the overlap may be expected to end, from the location and velocity observed just before the beginning of the overlap. Since multiple dancers may be detected in the vicinity of the predicted location in the predicted frame, select the one that provides the best histogram match, and update h i t and B i t accordingly. Tracking Based 2D Pose Estimation: As explained in Algorithm 2, we obtain the initial 2D poses by using the OpenPose method <ref type="bibr" target="#b14">[15]</ref>. After we obtain the bounding box B i t for each dancer i at the end of the overlap, the box B i t may overlap with multiple boxes simultaneously, indicating multiple 2D pose estimation results. We select that posep i t whose histogram is most similar to the onep i t?1 seen in the previous frame. (Algorithm 2). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">3D Pose Estimation</head><formula xml:id="formula_2">i = t ? ? to t + ? do Generate 3D poseP k i = G(? k ) Estimate 2D posep k i = ?(P k i ; ? k ) Compute error e k i = ||p k i ? p i || 2 2</formula><p>Optimize ? * k , ? * k end end end Select the 3D pose corresponding to seed k * = argmi? k t+? i=t?? ek i as the initialized pose</p><formula xml:id="formula_3">Algorithm 4: 3D Pose Estimation Input: a sequence of video frames {I t } T ?1 t=0 , 2D poses {p t } T ?1 t=0 and initial 3D poses {P t } T ?1 t=0 of a dancer Output: a sequence of estimated 3D poses {P t } T ?1 t=0 of the dancer while new frame I t available do Estimate 3D poseP t Project to 2D posep t Compute loss L = ?(||p t ?p t?1 || 2 2 + ?||P t ? P t?1 || 2 2 ) + ||p t ? p t || 2 2 + ||P t ?P t || 2 2</formula><p>Update ? 2D and ? 3D end Towards our objective of using dance representations close to those used by experts, we need to use 3D, instead of 2D, pose sequences. Similarly for recognition using the language of dance experts, we need to extract descriptors of 3D movements from the 2D pose sequences, which constitute our method's next stage. Computationally too, 3D poses contain more information than 2D poses, and thus lead to more accurate dance recognition. However, predicting 3D poses from 2D poses is an ill-posed problem like other 2Dto-3D problems. The state-of-the-art methods <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref> use a two-step pipeline for solving it: first detect 2D poses from video frames, and then predict 3D poses by learning the correspondences of 2D and 3D key points. <ref type="bibr" target="#b19">[20]</ref> provides a simple yet effective baseline proving that the 2D to 3D task can be solved with a remarkably low error rate. <ref type="bibr" target="#b20">[21]</ref> learns a mapping from a distribution of 2D poses to a distribution of 3D poses using an adversarial training approach. However, <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> estimate 3D poses from 2D poses estimated from individual 2D frames, which ignores the temporal continuity information. <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> use temporal correspondences of 2D keypoints to both learn the joint angles as well as predict the joint locations. They compute loss in terms of the distance between these key points and those back-projected using the estimated 3D pose. They enforce such geometric consistency to progressively refine the estimates of 3D poses. However, these methods are based on the assumption that the input 2D poses are accurate. <ref type="bibr" target="#b22">[23]</ref> proposes a 2D pose correction module which uses a temporal CNN to refine the 2D initial inputs. However, this assumes that ground-truth 2D poses are available to train the correction module. These assumptions are often restrictive in practice, and do not hold for our dance videos which are collected from the internet. <ref type="bibr" target="#b23">[24]</ref> relates detected 2D poses across frames based on tracking-by-detection and then recovers 3D pose in a Bayesian framework. However, their MAP estimation is not robust if the video is long or background changes dramatically. <ref type="bibr" target="#b24">[25]</ref> proposes a method to cope with , we initialize their 3D poses and camera perspective projection parameters, P * t and ? * 2D , as shown in <ref type="figure" target="#fig_2">Fig. 2 (top)</ref> and Algorithm 3. Finally, a neural network is trained to estimate the 3D poses {Pt} T ?1 t=0 , which incorporates kinematic constraints and spatiotemporal smoothness of motion, as described in <ref type="bibr">Algorithm 4.</ref> occlusion. They first infer 3D locations of the visible body joints and then reconstruct the occluded joint locations using learned pose priors and a kinematic skeletal model. <ref type="bibr" target="#b25">[26]</ref> fit a parametric human model (SMPL) to observed image key points and segments along with some additional constraints. However, <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> require 3D pose labels and/or shape to supervise the training, which are not available for our "in the wild" video dataset. <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> estimate 3D pose from in-the-wild images without 3D pose annotations, but they require either additional 2D pose datasets or a multiview setting. To avoid these requirements and the need for groudtruth 2D pose, and to improve computational robustness, we propose an algorithm that integrates 3D pose estimation with 2D pose correction, which can be trained to converge on both estimates simultaneously while also estimating the camera projection parameters consistently.</p><p>We use the Denavit-Hartenberg (DH) parameters ? k = {? k , d k , a k , ? k } to represent the 3D pose. A 3D poseP t is generated by passing ? k to the 34-DOF kinematic model G as follows:</p><formula xml:id="formula_4">P k i = (J 0 , J 1 , ..., J 24 ) (1) J j = G(?, d, a, ?) = T ? T d T a T ? J j?1<label>(2)</label></formula><p>where   <ref type="table" target="#tab_6">Table 7</ref> in Appendix. The bounds of the joint rotation offset angles ? and bone length b are defined in <ref type="table">Table 8 in Appendix.</ref> where T ? , T d , T a and T ? J j?1 are transition matrices, and J j is the 3D location of the joint j.</p><formula xml:id="formula_5">T ? T d T a T ? =<label>cos</label></formula><p>We initialize the desired estimates of 3D poseP t and the 3D-to-2D projection parameter ? 2D t with multiple randomly selected seed pairs {? * k , ? k } (to sample the search space), as explained in <ref type="figure" target="#fig_2">Figure 2</ref> (top) and Algorithm 3. ? k = {f k , c k } are the perspective projection parameters. At frame t, we sample K seeds of the DH parameters to generate 3D poses {P k i } t+? i=t?? in a sliding window of size 2? centered at t and 3D-to-2D projection parameter ? 2D . By comparing the reconstructed 2D posep = ?(P t ; ? 2D ) projected from the generated 3D poseP t with the input 2D pose p i estimated in 2.1, we optimize the DH parameters ? k generating the 3D poseP k i while enforcing: (a) constraints that govern the joint rotation offset angles ? k , (b) consistency with the known bone lengths b k and (c) temporal smoothness of both the 2D and 3D poses. This is achieved by training with a loss function consisting of two parts: (1) temporal smoothness of both the 2D pose and 3D pose:</p><formula xml:id="formula_6">?(||p t ?p t?1 || 2 2 + ?||P t ?P t?1 || 2 2 ). (2) preservation of 3D- to-2D projection (imaging) property: ||?(P t ; ? 2D ) ? p t || 2</formula><p>2 . The coefficients ? and ? are chosen to be inversely proportional to the error: the larger the error, smaller the weight of the window. We also enforce constancy of the 3D to 2D projection parameters by smoothing it over a time window. At each time step t, we update the 3D poseP t and the projection parameter ? 3D . From among the solutions obtained using the different seeds, the pair {P * t ; ? * 2D } corresponding to the seed offering the least error is selected.</p><p>As shown in <ref type="figure" target="#fig_2">Figure 2</ref> (bottom), after obtaining the initial 3D pose P * t and the 3D-to-2D projection parameters ? * 2D t from the 3D Pose Initialization block, we train temporal convolutional networks to learn the mapping from the input 2D poses {p t } to the 3D ones {P t }. We use <ref type="bibr" target="#b16">[17]</ref> as our baseline networks. During the training, in addition to the consistency between 2D and 3D poses at all times, we again enforce temporal smoothness of motion with the loss function defined as follows:</p><formula xml:id="formula_7">L = ||p t ?p t || 2 2 +||p t ?p t?1 || 2 2 +||P t ?P * t || 2 2 +||P t ?P t?1 || 2 2 ) (3) wherep t = ?(P t ; ? * 2D ).See details in Algorithm 4.</formula><p>To further improve the accuracy when limited labeled 3D ground-truth pose data are available, we introduce a semisupervised training version of the proposed pose estimation method. A supervised loss is trained by using the available labeled ground truth 3D poses P t as target, and the loss in Equation <ref type="formula">(3)</ref> is implemented using the remaining unlabeled data. Here, the predicted 3D posesP t are projected back to 2D joint coordinates for consistency with the 2D input p t . Similar to the training strategy in <ref type="bibr" target="#b16">[17]</ref>, we jointly optimize the supervised component with our unsupervised component during training, with the labeled data occupying the first half of a batch, and the unlabeled data occupying the second half.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Body Part Movement Recognition</head><p>For each body part e, we train an LSTM-based model to recognize its (basic) movement. During training, the input is a sequence of 3D poses {{p j t } j?Je } T ?1 t=0 of all the joints j ? J e connected to the body part e and the output is a sequence of predicted movement labels {? e t } T ?1 t=0 connected to e. Since this is a multi-label classification problem, which means the poses {p j t } j?Je connected to the body part e may map to multiple movement labels? e t of e at the same time, we use the Binary Cross Entropy (BCE) loss between predicted movements {? e t } T ?1 t=0 and the target movement labels {y e t } T ?1 t=0 . This loss is minimized during the training to obtain the optimal model. During testing, the trained model of each e ? E takes a sequence of 3D poses {{p j t } j?Je } T ?1 t=0 of all the joints connected to e as input, and predicts the movement {? e t } T ?1 t=0 of e.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Dance Genre Recognition</head><p>Analogous to the approach in Section 2.3, we train an LSTM model to take a sequence of movement labels {{? e t } e?E } T ?1 t=0 of all the body parts e ? E as input. We use the output of the last time step from the last layer as the prediction of the dance genre?. For loss function, we use cross entropy between the predicted dance genre? and the target dance genre g. We describe the movement and dance genre recognition in detail in Algorithm 5 and Algorithm 6 in the supplementary document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data and Experiment Setting</head><p>University of Illinois Dance (UID) Dataset. One major challenge for dance recognition lies in the lack of training data. We have curated UID video dataset containing 9 types of dances (Ballet, Belly dance, Flamenco, Hip Hop, Rumba, Swing dance, Tango, Tap dance and Waltz) with details listed in <ref type="table" target="#tab_2">Table 2</ref>. <ref type="figure" target="#fig_7">Figure 4</ref> and 5 show sample frames and information about in our dataset for each dance genre. The videos contain situations of varying difficulty, from simple ones such as tutorial videos with clean background, to hard videos, having interacting dancers, noisy background and varying lights.       <ref type="bibr" target="#b28">[29]</ref>. ( ) uses ground truth 2D poses. Methods using different supervision level are divided by horiontal line. Our proposed method (semi-supervised) achieves the lowest error against the fully supervised methods. Moreover, our unsupervised pose estimation method can achieve the same level of performance as the state-of-the-art supervised/semi-supervised methods.</p><p>mean Euclidean distance between the predicted 3D poses {P t } T ?1 t=0 and the target 3D poses {P t } T ?1 t=0 . We use F-score to measure the accuracy of our movement and dance recognition approaches on our UID dataset. Experiment Setting. We evaluate our unsupervised 3D pose estimation approach on both the UID video dataset and AIST++ dance dataset <ref type="bibr" target="#b28">[29]</ref>. The AIST++ Dataset contains 1,408 multi-view dance sequences from 10 dance genres with hundreds of choreographies, provides 3D human key-  <ref type="bibr" target="#b32">[33]</ref> evaluated on S9 and S11. ( ) uses ground truth 2D poses. Based on the method's supervision level, five labelled subjects (S1, S5, S6, S7, S8) are used to train the supervised methods, four labelled subjects (S5, S6, S7, S8) and one unlabelled subject (S1) are used to train the semi-supervised methods, and five unlabelled subjects (S1, S5, S6, S7, S8) for the rest methods (e.g., unsupervised). Our proposed method (semi-supervised) achieves the second lowest error against the fully supervised methods. Without the need of additional 2D/3D data, our unsupervised pose estimation method can achieve the same level of performance as the state-of-the-art methods. point annotations and camera parameters for 10.1M images, and covers 30 different subjects in 9 views. We did our experiments with a subset of AIST++, containing 200 videos ( 0.4M frames). 30% of the videos with ground-truth 3D poses are used as labeled data to train the supervised methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref> and semi-supervised methods ( <ref type="bibr" target="#b16">[17]</ref> and our method). 10% of the videos are used for testing. The remaining video samples are used as unlabeled data for training the semi-supervised methods.</p><p>For consistency with other work <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>, we train and evaluate on 3D poses in camera space. In the 3D Pose Initialization component, we use Adam <ref type="bibr" target="#b34">[35]</ref> optimizer to optimize the estimated 3D poses in Algorithm 3 for 50 epochs. The temporal window size ? = 3 and the number of seeds K = 2. After obtaining the best initial 3D poses and camera projection parameters (focal lengths and principal points), we use <ref type="bibr" target="#b16">[17]</ref> as the baseline to train the 3D pose estimation network for 200 epochs. <ref type="figure" target="#fig_9">Figure 6</ref> shows qualitative results of our 3D pose method on both the UID dataset and the AIST++ dataset <ref type="bibr" target="#b28">[29]</ref>. The   <ref type="table">Table 5</ref>. F-scores for body part movements recognition from estimated 2D poses (Sec 2.1) and estimated 3D poses (Sec 2.2) as inputs. Recognition improves as a result of using our estimated 3D poses. Note that the performances for several parts are comparable with existing results. This is because the dancers are at a large distance, diminishing the extra power offered by the 3D information. This situation changes in <ref type="table">Table 6</ref>.  <ref type="table">Table 6</ref>. Ablation study using different components as inputs. The 3D pose, in general, provides higher accuracy for genre recognition than 2D pose. Combination of the two, 2D and 3D level estimates, achieves better performance than either alone. timated 3D poses well match the known human skeletal structure and are smooth between frames. To quantitatively evaluate our method, we train our model and three stateof-the-art methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref> on the AIST++ dataset and calculate the mean per-joint position errors (MPJPE). We also evaluated our model on the Human 3.6M dataset <ref type="bibr" target="#b32">[33]</ref>. <ref type="table" target="#tab_3">Table 3</ref> and <ref type="table">Table 4</ref> shows that our unsupervised pose estimation method is comparable with the supervised methods. Moreover, our semi-supervised version achieves the best and second best performance on the AIST++ dataset <ref type="bibr" target="#b28">[29]</ref> and 3.6M dataset <ref type="bibr" target="#b32">[33]</ref>, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">3D Poses</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Movement and Dance Genre Recognition</head><p>Recognition results for body part movements and dance genre recognition on the UID dataset are given in <ref type="table">Tables  5 and 6</ref>. We use the 3D poses estimated using our un-supervised method as the input for recognition since our UID collects videos in the wild and hence does not provide ground-truth 3D annotations for training the proposed semi-supervised version. The movements of different body parts can help with dance understanding from the viewpoint of dance experts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions and Future Work</head><p>In conclusion,we have presented an approach to dance videos understanding that follows a hierarchical representation used by experts to describe dances. We have presented an approach to extract the primitives occurring at each level of the representation, from raw videos, to 3D pose, to movements, to dance genre. We have presented the challenges we have encountered and how we have addressed them using new constraints and algorithms. Note that the training in our current dance video recognition framework is not fully unsupervised. We plan to develop a fully unsupervised pipeline that could be jointly trained for pose estimation and genre recognition. In addition, we plan to synthesize dances using the representations we have extracted. We also plan to use the judgments of expert viewers on the quality of the synthesized dance videos as qualitative metrics of the representations extracted by our algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material for Unsupervised 3D Pose Estimation for Hierarchical Dance Video Recognition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. UID Dataset</head><p>The proposed UID video dataset can be found at https://drive.google.com/drive/folders/ 1-SdWYxIorbhQzi9Bp_HpJf25_ieMjoh5?usp=sharing. The dataset folder contains 9 sub-folders, each having ?30 to ?300 videos, showing one of the following 9 dance types: Ballet, Belly dance, Flamenco, Hip Hop, Rumba, Swing dance, Tango, Tap dance and Waltz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Demo Videos</head><p>In <ref type="figure" target="#fig_9">Figure 6</ref> in Section 3 of the paper, we have shown estimated 3D poses by drawing skeletal figures in the estimated poses and overlaying them on the corresponding images of the dancers. These single frame overlays help us verify the placement of the skeleton within the body parts in only four frames in a video (containing ?100 to ?800 frames). However, the contributions of our paper also include enforcement of temporal smoothness constraints, and estimation of complex 3D poses. Here we therefore include videos that show the overlays of the skeletons in all frames of the videos. Viewing these videos shows the temporal smoothness of pose estimates as well as their continuous alignment with the dancer's poses achieved by our method, which cannot be seen from the static depictions in the paper. Further, the pose and alignment quality can now be seen for the entire range of complexities associated with the poses assumed by the dancer throughout the video instead of with the selected few frames in the paper.</p><p>The videos we use to show our results are selected from the test set in the UID dataset. The selected videos can be found at https://drive.google.com/drive/folders/1X5K2U1Eq1QlcU8GmM_gHFVv75VkBzcoV?usp= sharing. The right side of each video shows videos of skeletons representing 2D projections (2D poses)p t of the estimated 3D posesP t by themselves. These 2D poses are estimated using the estimated 3D-to-2D projection parameter ? * 2D . To bring out the poses, they are shown from a closer and different viewpoint than used to capture the original video. On the left side, we show the same 2D poses, using the viewpoint used to capture the original video, and overlaid on the original video frames. We can see that the skeletons align well with the complex movements of the dancers, such as spinning, Pointe (fully extended feet), and Tour Jet? (a high turning leap). Also, we can see that transitions between poses in adjacent frames are smooth, e.g., without large, abnormal displacements between the locations of the same joint in successive frames.</p><p>Finally, names of the recognized 3D movements? e t of each body part e ? E are shown at the bottom of the video. The recognized movements? e t can be seen to well match the dancers' movements in the original video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Algorithms in Detail</head><p>Algorithms 1*, 2*, 3* and 4* here are the detailed versions of Algorithms 1, 2, 3 and 4. The movement and dance genre recognition is described in detail in Algorithm 5 and 6. <ref type="table" target="#tab_6">Table 7</ref> and 8 show the the values of the DH parameters, and the bounds of the joint rotation offset angles and bone length of our 34-DOF digital dancer model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3*: 3D Pose Initialization</head><p>Input: a sequence of 2D poses {p t } N ?1 t=0 of a dancer Output: a sequence of 3D poses {P t } N ?1 t=0 of the dancer Set the temporal window size to be 2? Denote total number of segments as s = N 2? for t = ? to N ? ? do for k = 0 to K ? 1 do Try new seed for DH parameters ? k = {? k , d k , a k , ? k } and perspective projection parameters</p><formula xml:id="formula_8">? k = {f k , c k } for i = t ? ? to t + ? do Generate 3D poseP k i = G(? k ) Estimate 2D posep k i = ?(P k i ; ? k ) Compute error e k i = ||p k i ? p i || 2 2</formula><p>Optimize ? * k , ? * k = argmin   <ref type="figure" target="#fig_1">Figure 3</ref>. The joint rotation angle ? along z axis, the distance d along z axis, and the offset distance a along x axis are determined by the joint rotation offsets ? = (?0, ..., ?32) and bone lengths b = (b0, ..., b6), where their bounds are defined in <ref type="table">Table 8</ref>.</p><formula xml:id="formula_9">? k ,? k e k i AssignP * k i = G(? * k ) Update ? k ? ? * k Update ? k ? 1 i?t+?</formula><p>Rotation ?1 ?2 ?3 ?7 ?8 ?9 ?10 ?13 ?14 ?15 ?16 ?19 ?20 ?21 ?22 ?23 ?26 ?27 ?28 ?29 ?30   <ref type="table">Table 8</ref>. The bounds of the joint rotation offset angles ? = (?0, ..., ?32) and bone length ratios b = (b0, ..., b6) defined for our digital dancer model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Overview of the model architecture. Given a sequence of video frames {It} T ?1 t=0 , the model analyzes the content in a hierarchical manner, from the low levels (pose estimation &amp; tracking) to the cognitive levels (movement and dance genre recognition). The input sequence {It} T ?1 t=0 forms the first (bottom) level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 3 :</head><label>3</label><figDesc>3D Pose Initialization Input: a sequence of 2D poses {p t } N ?1 t=0 of a dancer Output: a sequence of 3D poses {P t } N ?1 t=0 of the dancer Set the temporal window size to be 2? Denote total number of segments as s = N 2? for t = ? to N ? ? do for k = 0 to K ? 1 do Try new seed for DH parameters ? k and perspective projection parameters ? k for</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Overview of Proposed 3D pose estimation method. Given a sequence of video frames {It} T ?1 t=0 , the dancers are tracked by our tracking algorithm in Algorithm 1 and each of their 2D poses {p i t } N ?1 i=0 are estimated by our tracking based 2D pose estimation algorithm in Algorithm 2. Then based on the 2D poses {p i t } N ?1 i=0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>? ? sin ? cos ? sin ? sin ? r cos ? sin ? cos ? cos ? ? cos ? sin ? r sin ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Our 34-DOF digital dancer model. The values of the DH parameters ? = {?, d, a, ?} of this model are listed in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Dance</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Evaluation Protocols. we use the widely used mean perjoint position error (MPJPE) in millimeters to calculate the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 .</head><label>4</label><figDesc>Sample frames for 9 types of dances in the University of Illinois Dance (UID) Dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 .</head><label>5</label><figDesc>Distribution of the numbers and durations of clips for each genre in the UID Dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 .</head><label>6</label><figDesc>2D poses (top row) reconstructed from the estimated 3D poses align well with the dancer's movement. The es-Visualization results on sample videos from the AIST++ dataset [34] and our proposed University of Illinois Dance (UID) dataset. The top row shows the reconstructed 2D poses from the estimated 3D poses and the bottom row shows the estimated 3D poses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Movements (2D Pose as input) 0.73 3D Pose + Movements (3D Pose as input) 0.86</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>2 2 1 e=0 } T ? 1</head><label>211</label><figDesc>t l=t?? ? * k l end end end Select the seed k * = argmi? k t+? i=t?? ek i AssignP t , ? 2D t ?P * k * t , ? k * Algorithm 4*: 3D Pose Estimation Input: a sequence of video frames {I t } T ?1 t=0 , 2D poses {p t } T ?1 t=0 and initial 3D poses {P t } T ?1 t=0 of a dancer Output: a sequence of estimated 3D poses {P t } T ?1 t=0 of the dancer while new frame I t available do Estimate 3D posePt = ?(p t ; ? 3D ) Project to 2D posep t = ?(P t ; ? 2D ) Compute loss L = ?(||p t ?p t?1 || 2 2 + ?||P t ?P t?1 || 2 2 ) + ||p t ? p t || 2 2 + ||P t ?P t || Update ? 2D ? ? 2D ? ? ?L ?? 2D Update ? 3D ? ? 3D ? ? ?L ?? 3Dend Algorithm 5: Movement Identification Input: a sequence of poses {p e t = {(x j t ,? j t )} |Je| j=0 } T ?1 t=0 where T denotes the total number of frames and J e denotes the set of body joints connected to the body part e; a sequence of corresponding movement labels {? e t } T ?1 t=0 of the body part e Output: predicted movement labels {? e t } T ?1 t=0 for epoch = 0 to N ? 1 do {? e t } T ?1 t=0 = LSTM({p e t } T ?1 t=0 ) L = BCELoss({? e t } T ?1 t=0 , {? e t } T ?1 t=0 ) Update LSTM until converge end Algorithm 6: Dance Classification Input: a sequence of movement labels {{? e t } |E|?t=0 of all the body parts e ? E; and the ground-truth dance genre label g of the sequence Output: predicted dance genre label? for epoch = 0 to N ? 1 d? g = LSTM({{? e t } |E|?1 e=0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Selected examples of movement labels of each body part. To save space, only the movements of the left body parts are shown in the table. The movements of the right body parts are the same as the left ones. There are 16 body parts and 154 movement labels in total.</figDesc><table><row><cell>Body Part</cell><cell>Examples of Movement Label</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Max clip length 824s Max # of clips / class 304 Summary of the characteristics of the UID dataset.</figDesc><table><row><cell>Genres</cell><cell>9</cell><cell>Total Duration</cell><cell>108,089s</cell></row><row><cell>Total # of Clips</cell><cell cols="2">1143 Total # of Frames</cell><cell>2,788,157</cell></row><row><cell cols="2">Min clip length 4s</cell><cell cols="2">Min # of clips / class 30</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table><row><cell>Method</cell><cell>Supervision</cell><cell>Extra Data</cell><cell>MPJPE (mm)(?)</cell></row><row><cell>Martinez [20] ICCV'17</cell><cell>Supervised</cell><cell>-</cell><cell>110.0</cell></row><row><cell>Wandt [21] CVPR'19</cell><cell>Supervised</cell><cell>-</cell><cell>323.7</cell></row><row><cell>Pavllo [17] CVPR'19</cell><cell>Supervised</cell><cell>-</cell><cell>77.6</cell></row><row><cell cols="2">Pavllo [17] CVPR'19( ) Semi-Sup.</cell><cell>No</cell><cell>446.1</cell></row><row><cell>Ours</cell><cell>Semi-Sup.</cell><cell>No</cell><cell>73.7</cell></row><row><cell>Zhou [27] ICCV'17</cell><cell>Weakly-Sup.</cell><cell>Yes</cell><cell>93.1</cell></row><row><cell>Kocabas [28] CVPR'19</cell><cell>Self-Sup.</cell><cell>Multiview</cell><cell>87.4</cell></row><row><cell>Ours</cell><cell cols="2">Unsupervised No</cell><cell>246.4</cell></row></table><note>Comparison of 3D pose estimation results using Protocol 1: Mean Per-Joint Position Error (MPJPE) on AIST Dance Video Dataset</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>The DH parameters ? = {?, d, a, ?} for the 34-DOF human model as shown in</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">} T ?1 t=0 )</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">L = CrossEntropyLoss(?, g)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Update LSTM until converge</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>end</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Joint ?</cell><cell>d</cell><cell>a</cell><cell>?</cell><cell cols="2">Joint ?</cell><cell>d</cell><cell>a</cell><cell>?</cell><cell cols="2">Joint ?</cell><cell>d</cell><cell>a</cell><cell>?</cell></row><row><cell>0</cell><cell cols="2">180 + ?0 0</cell><cell>0</cell><cell>90</cell><cell>12</cell><cell>90 + ?11</cell><cell>b4h</cell><cell>0</cell><cell>90</cell><cell>24</cell><cell>0 + ?23</cell><cell>0</cell><cell>0</cell><cell>90</cell></row><row><cell>1</cell><cell cols="2">?90 + ?0 0</cell><cell>0</cell><cell>90</cell><cell>13</cell><cell>90 + ?12</cell><cell>0</cell><cell>0.6b4h</cell><cell>0</cell><cell>25</cell><cell>0 + ?24</cell><cell>b8h</cell><cell>0</cell><cell>-90</cell></row><row><cell>2</cell><cell>90 + ?1</cell><cell>0</cell><cell cols="3">b0h -90 14</cell><cell>0 + ?13</cell><cell>0</cell><cell>0</cell><cell cols="2">-90 26</cell><cell cols="2">?90+?25 0</cell><cell>0.1b8h</cell><cell>0</cell></row><row><cell>3</cell><cell>0 + ?2</cell><cell>0</cell><cell>0</cell><cell>90</cell><cell>15</cell><cell cols="2">?90+?14 0</cell><cell>0</cell><cell>90</cell><cell>27</cell><cell>0 + ?26</cell><cell>0</cell><cell>0</cell><cell>-90</cell></row><row><cell>4</cell><cell>90 + ?3</cell><cell>0</cell><cell>0</cell><cell>90</cell><cell>16</cell><cell>90 + ?15</cell><cell cols="2">?b3h 0</cell><cell>90</cell><cell>28</cell><cell cols="2">?90+?27 0</cell><cell>0</cell><cell>90</cell></row><row><cell>5</cell><cell>90 + ?4</cell><cell cols="2">b1h 0</cell><cell>90</cell><cell>17</cell><cell>0 + ?16</cell><cell>0</cell><cell>0</cell><cell cols="2">-90 29</cell><cell>0 + ?28</cell><cell>b6h</cell><cell>0</cell><cell>-90</cell></row><row><cell>6</cell><cell>90 + ?5</cell><cell>0</cell><cell>0</cell><cell>90</cell><cell>18</cell><cell>90 + ?17</cell><cell cols="2">?b4h 0</cell><cell>90</cell><cell>30</cell><cell>90 + ?29</cell><cell>0</cell><cell>?b7h</cell><cell>0</cell></row><row><cell>7</cell><cell>90 + ?6</cell><cell>0</cell><cell cols="2">b1h 0</cell><cell>19</cell><cell cols="2">?90+?18 0</cell><cell cols="2">?0.6b4h 0</cell><cell>31</cell><cell>0 + ?30</cell><cell>0</cell><cell>0</cell><cell>90</cell></row><row><cell>8</cell><cell>0 + ?7</cell><cell>0</cell><cell>0</cell><cell>90</cell><cell>20</cell><cell>0 + ?19</cell><cell>0</cell><cell>0</cell><cell cols="2">-90 32</cell><cell>0 + ?31</cell><cell cols="2">?b8h 0</cell><cell>-90</cell></row><row><cell>9</cell><cell>90 + ?8</cell><cell>0</cell><cell>0</cell><cell>90</cell><cell>21</cell><cell cols="2">?90+?20 0</cell><cell>0</cell><cell>90</cell><cell>33</cell><cell cols="2">?90+?32 0</cell><cell cols="2">?0.1b8h 0</cell></row><row><cell>10</cell><cell>90 + ?9</cell><cell cols="2">b3h 0</cell><cell>90</cell><cell>22</cell><cell>0 + ?21</cell><cell cols="2">?b6h 0</cell><cell>-90</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>11</cell><cell>0 + ?10</cell><cell>0</cell><cell>0</cell><cell cols="2">-90 23</cell><cell>90 + ?22</cell><cell>0</cell><cell>b7h</cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Boneneck b0 head b1 shoulder b2 uparm b3 lowarm b4 hip b5 upleg b6 lowleg b7 toe b8</figDesc><table><row><cell></cell><cell></cell><cell>0</cell><cell>0</cell><cell>? 1.6</cell><cell>? 1.6</cell><cell>0</cell><cell>0</cell><cell>? 4</cell><cell>? 1.3</cell><cell>? 2</cell><cell>? 1.3</cell><cell>? 2</cell><cell>? 4</cell><cell>? 1.3</cell><cell>? 2</cell><cell>? 1.3</cell><cell>? 2</cell></row><row><cell cols="2">Average 0.25</cell><cell>0.08</cell><cell>0.06</cell><cell>0.17</cell><cell></cell><cell>0.17</cell><cell></cell><cell>0.04</cell><cell></cell><cell>0.21</cell><cell cols="2">0.21</cell><cell></cell><cell>0.04</cell><cell></cell></row><row><cell>Std</cell><cell>0.05</cell><cell>0.05</cell><cell>0.05</cell><cell>0.05</cell><cell></cell><cell>0.05</cell><cell></cell><cell>0.05</cell><cell></cell><cell>0.05</cell><cell cols="2">0.05</cell><cell></cell><cell>0.05</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t , y i t , w i t , l i t ) ? location ofk th patch end end end Algorithm 2*: Tracking Based 2D Pose Estimation Input: a sequence of video frames {I t } T ?1 t=0 and a sequence of bounding boxes{B i t } T ?1 t=0 = {(x i t , y i t , w i t , l i t )} T ?1 t=0 of the i th dancerOutput: a sequence of poses {p i t } T ?1 t=0 of the i th dancer while new frame I t available do Estimate poses // Perform OpenPose for i th dancer do Select C poses {p i,c t } C?1 c=0 overlapped with the bounding box B i t c = argmax c (correlation(h i,c t , h i t?1 )) where h i,c t is the histogram of the pose p i,c t p i t ? p i,? t end end</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1*: Object Tracking</head><p>Input: a sequence of video frames </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Anastasios Doulamis, and Grammalidis Nikos. Folk dance pattern recognition over depth images acquired via kinect sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eftychios</forename><surname>Protopapadakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grammatikopoulou</surname></persName>
		</author>
		<idno>XLII-2/W3:587-593, 02 2017. 1</idno>
	</analytic>
	<monogr>
		<title level="j">IS-PRS</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hybrid activity recognition for ballroom dance exercise using video and wearable sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Matsuyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hiroi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yonezawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kawaguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIEV and icIVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A deep learning pipeline for Indian dance style classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Swati Dewan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navjyoti</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">10696</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spatio-temporal laban features for dance style recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Let&apos;s dance: Learning from online dance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Hickson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patsorn</forename><surname>Sangkloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhavishya</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irfan</forename><forename type="middle">A</forename><surname>Essa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dance with flow: Twoin-one stream action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaojiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An approach to pose-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="915" to="922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ai coach: Deep human pose estimation and analysis for personalized athletic training assistance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianke</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">2d/3d pose estimation and action recognition using multitask deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Diogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Labanotation: The System of Analyzing and Recording Movement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann Hutchinson</forename><surname>Guest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Routledge</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2005-02-15" />
		</imprint>
	</monogr>
	<note>4th Edition</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>PAMI</publisher>
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pifpaf: Composite fields for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kreiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Higherhrnet: Scale-aware representation learning for bottom-up human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Openpose: Realtime multi-person 2d pose estimation using part affinity fields. PAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Robust estimation of similarity transformation for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhefeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hantang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">man pose machines with self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1069" to="1082" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>3d hu</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Vibe: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Repnet: Weakly supervised training of an adversarial reprojection network for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Anatomy-aware 3d human pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhili</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep kinematics analysis for monocular 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="896" to="905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Monocular 3d pose estimation and tracking by detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">XNect: Real-time multi-person 3D motion capture with a single RGB camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Monocular 3d pose and shape estimation of multiple people in natural scenes -the importance of multiple scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeta</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: A weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Selfsupervised learning of 3d human pose using multi-view geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salih</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Aist dance video database: Multi-genre, multi-dancer, and multi-camera database for dance information processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhei</forename><surname>Tsuchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoru</forename><surname>Fukayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masahiro</forename><surname>Hamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masataka</forename><surname>Goto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Society for Music Information Retrieval Conference</title>
		<meeting>the 20th International Society for Music Information Retrieval Conference<address><addrLine>Delft, Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="501" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised geometry-aware representation learning for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised 3d pose estimation with geometric self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Drover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stojanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Unsupervised cross-modal alignment for multi-person 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jogendra</forename><surname>Nath Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambareesh</forename><surname>Revanur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Govind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Waghmare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rahul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R Venkatesh</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Babu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learn to dance with aist++: Music conditioned 3d dance generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruilong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
