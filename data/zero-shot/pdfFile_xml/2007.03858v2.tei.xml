<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PaMIR: Parametric Model-Conditioned Implicit Representation for Image-based Human Reconstruction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">PaMIR: Parametric Model-Conditioned Implicit Representation for Image-based Human Reconstruction</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Body Pose</term>
					<term>Human Reconstruction</term>
					<term>Surface Representation</term>
					<term>Parametric Body Model</term>
					<term>Implicit Surface Function !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modeling 3D humans accurately and robustly from a single image is very challenging, and the key for such an ill-posed problem is the 3D representation of the human models. To overcome the limitations of regular 3D representations, we propose Parametric Model-Conditioned Implicit Representation (PaMIR), which combines the parametric body model with the free-form deep implicit function. In our PaMIR-based reconstruction framework, a novel deep neural network is proposed to regularize the free-form deep implicit function using the semantic features of the parametric model, which improves the generalization ability under the scenarios of challenging poses and various clothing topologies. Moreover, a novel depth-ambiguity-aware training loss is further integrated to resolve depth ambiguities and enable successful surface detail reconstruction with imperfect body reference. Finally, we propose a body reference optimization method to improve the parametric model estimation accuracy and to enhance the consistency between the parametric model and the implicit function. With the PaMIR representation, our framework can be easily extended to multi-image input scenarios without the need of multi-camera calibration and pose synchronization. Experimental results demonstrate that our method achieves state-of-the-art performance for image-based 3D human reconstruction in the cases of challenging poses and clothing types. <ref type="figure">Fig. 1</ref>. Example results reconstructed by our method. Our method is able to infer the underlying body shape, the 3D surface geometry and its texture given a single RGB image.</p><p>non-parametric methods still suffer from challenging poses and self-occlusions due to the lack of semantic information and depth ambiguities.</p><p>Recently, DeepHuman [5] pioneered in conditioning the non-parametric volumetric representation on the parametric SMPL model, and demonstrates robust reconstruction for challenging poses and diverse garments. However, it struggles to recover high-quality geometric details due to the resolution limitation of the regular occupancy volume.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>I MAGE-BASED parsing of human bodies is a popular topic in computer vision and computer graphics. Among all the tasks of image-based human parsing, recovering 3D humans from a single RGB image attracts more and more interests given its wide applications in VR/AR content creation, image and video editing, telepresence and virtual dressing. However, as an ill-posed problem, recovering 3D humans from a single RGB image is very challenging due to the lack of depth information and the variations of body shapes, poses, clothing types and lighting conditions.</p><p>Benefiting from the huge progress of deep learning techniques, recent studies have tried to address these challenges using learning-based methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. According to their 3D representations, these methods can be roughly classified into two categories: parametric methods and non-parametric methods. Parametric methods like HMD <ref type="bibr" target="#b2">[3]</ref> and Tex2Shape <ref type="bibr" target="#b3">[4]</ref> utilize a statistical body template model (e.g., SMPL <ref type="bibr" target="#b8">[9]</ref>) as the geometrical prior, and learn to deform the template according to the silhouette and the shading information. However, the low dimensional parametric model limits the performance when handling different clothing types such as long dresses and skirts. Nonparametric methods use various free-form representations, including voxel grids <ref type="bibr" target="#b0">[1]</ref>, silhouettes <ref type="bibr" target="#b1">[2]</ref>, implicit fields <ref type="bibr" target="#b7">[8]</ref> or depth maps <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, to represent 3D human models. Although being able to describe arbitrary clothes, current More importantly, conditioning the non-parametric representation on SMPL models raises a crucial problem: how can we obtain accurate SMPL model estimation given a single image at testing time? In DeepHuman, the SMPL models at testing time are estimated using learning-based methods and/or optimization-based methods. Although the recent years have witness a renaissance in the context of singleimage body shape estimation <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, we observe that there is still an accuracy gap between the training and testing stages: during training, we can obtain well-aligned SMPL models for the training data by fitting the SMPL template to the high quality scans, while for testing, the SMPL estimations provided by the state-of-theart single-image methods <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> still cannot align with the keypoints or the silhouettes as perfectly as the SMPL models in the training dataset. As a result, the neural network trained with the groundtruth image-SMPL pairs cannot generalize well to the testing images which have no ground-truth SMPL annotations. A simple solution is to replace the ground-truth SMPL models with the predicted ones while still using the ground-truth surface scans for training supervision, preventing the network from heavy dependence on accurate SMPL annotations. However, due to depth ambiguities, it is impossible to guarantee good alignments between the predicted SMPL models and the ground-truth scans along the depth axis. Consequently, by doing so we are forcing the networks to make "correct" geometry inference given a "wrong" SMPL reference along the depth axis, which will generate even worse results.</p><p>To achieve high-quality geometric detail reconstruction while maintaining robustness to challenging pose and clothing styles, in this paper, we propose Parametric Model-Conditioned Implicit Representation, dubbed PaMIR, to incorporate the parametric SMPL model and the free-form implicit surface function into a unified learning and optimization framework <ref type="figure" target="#fig_0">(Fig.2)</ref>. The implicit surface function overcomes the resolution limit of volumetric representation, and enables detailed surface reconstruction capability. To fill the accuracy gap between the ground truth training data and the inaccurate testing input, we further propose a new depth-ambiguity-aware training loss and a body reference optimization module in the PaMIR-based framework, and achieve surface detail reconstruction even under imperfect body reference initialization. Specifically, the three technical contributions are summarized as follow:</p><p>? PaMIR. Our PaMIR representation has the ability to condition the implicit field on the SMPL prediction, which is realized by a novel network architecture converting an image feature map and the corresponding SMPL feature volume into an implicit surface representation. The SMPL feature volume is directly encoded from the SMPL prediction provided by GraphCMR <ref type="bibr" target="#b12">[13]</ref>, and serves as a soft constraint for handling challenging poses and/or self-occlusions. ? Training Losses. We use the predicted SMPL models at both the training and the testing stages. In order to make the network to be more robust against the inaccurate body reference along the depth direction, instead of using a traditional reconstruction loss, we propose a depth-ambiguity-aware reconstruction loss that adaptively adjusts the reconstruction supervision based on the current SMPL estimation while respecting 2D image observations. ? Body Reference Optimization. We also propose a body reference optimization method for testing process to further refine the body reference. Our optimization method directly utilizes the network itself as well as its outputs to construct an efficient energy function for body fitting. The optimization further alleviates the error caused by the inaccurate initial SMPL estimation, and consequently closing the accuracy gap of SMPL annotation between training and testing. Overall, in our framework, the underlying body tem-plate and the outer surface are mutually beneficial: on one hand, benefiting from the proposed depth-ambiguity-aware reconstruction loss, even the imperfect body template can be used to provide strong semantic information for implicit surface reconstruction; on the other hand, the deep implicit function of the outer surface is also used to optimize the underlying body template by minimizing the body fitting error directly.</p><p>Furthermore, thanks to the usage of the common underlying SMPL model as body reference, our PaMIR representation implicitly builds a correspondence relationship across different models. As a result, our method can be easily extended to multi-image setups like multi-view inputs or video inputs without the requirement for calibration and synchronization. Experiments show that our method is able to reconstruct high-quality human models with various body shapes, poses and clothes, and outperforms stateof-the-art methods in terms of accuracy, robustness and generalization capability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Human Reconstruction from multi-view images. Previous studies focused on using multi-view images for human model reconstruction <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>. Shape cues like silhouette, stereo and shading have been integrated to improve the reconstruction performance <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. State-of-the-art multi-view real-time systems include <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>. Extremely high-quality reconstruction results have also been demonstrated with tens or even hundreds of cameras <ref type="bibr" target="#b23">[24]</ref>. To capture detailed motions of multiple interacting characters, more than six hundred cameras have been used to overcome the self-occlusion challenges <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>.</p><p>In order to reduce the difficulty of system setup, human model reconstruction from sparse camera views has recently been investigated by using CNNs for learning silhouette cues <ref type="bibr" target="#b26">[27]</ref> and stereo cues <ref type="bibr" target="#b27">[28]</ref>. These systems require about 4 camera views for a coarse-level surface detail capture. Note also that although temporal deformation systems using lightweight camera setups <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref> have been developed for dynamic human model reconstruction using skeleton tracking ( <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b31">[32]</ref>) or human mesh template deformation <ref type="bibr" target="#b29">[30]</ref>, these systems require a pre-scanned subject-specific 3D template for deformation optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image-based Parametric Body Estimation. Dense 3D</head><p>parsing from a single image has attracted substantial interest recently because of the emergence of human statistical models like SCAPE <ref type="bibr" target="#b32">[33]</ref> and SMPL <ref type="bibr" target="#b8">[9]</ref>. For example, by fitting the SMPL model to the 2D keypoint detections <ref type="bibr" target="#b33">[34]</ref> and other dense shape cues <ref type="bibr" target="#b34">[35]</ref>, the shape and pose parameters can be automatically obtained from a single image <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b35">[36]</ref>. Instead of optimizing mesh and skeleton parameters, recent approaches proposed to train deep neural networks that directly regress the 3D shape and pose parameters from a single image <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. The estimation accuracy of these methods are further improved by performing fitting optimization after network inference <ref type="bibr" target="#b36">[37]</ref>, introducing model optimization into the training loop <ref type="bibr" target="#b37">[38]</ref>, incorporating adversarial prior in temporal domain <ref type="bibr" target="#b38">[39]</ref>, or combining global and local features to produce fine-grained human body poses <ref type="bibr" target="#b40">[40]</ref>. However, the parametric body models can only capture the shape and pose of a minimally clothed body, thus lack the ability to represent a 3D human model with more general clothing layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-parametric</head><p>Human Reconstruction from a Single Image. Regarding single-image human model reconstruction using non-parametric models, recent studies have adopted techniques based on silhouette estimation <ref type="bibr" target="#b1">[2]</ref>, template-based deformation <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, depth estimation <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> and volumetric reconstruction <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b4">[5]</ref> . Although they have achieved promising results, typical limitations still exist when using a single 3D representation: silhouette-based methods like <ref type="bibr" target="#b1">[2]</ref> is suffer from lack of details and view inconsistency, template-based deformation methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> are unable to handle loose clothes, depth-based methods <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> cannot handle self-occlusions naturally, and volumetric methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b4">[5]</ref> cannot recover high-frequency details due to their cubically growing memory consumption. Implicit Representation. How to represent 3D surface is a core problem in 3D learning. Explicit representations like point clouds <ref type="bibr" target="#b41">[41]</ref>, <ref type="bibr" target="#b42">[42]</ref>, voxel grids <ref type="bibr" target="#b43">[43]</ref>, <ref type="bibr" target="#b44">[44]</ref>, triangular meshes <ref type="bibr" target="#b45">[45]</ref>, <ref type="bibr" target="#b46">[46]</ref> have recently been explored to replicate the success of deep learning techniques on 2D images. However, the loss of structure information in point cloud, the memory requirement of regular voxel grid, and the fixed topology of meshes make these explicit representations unsuitable to represent arbitrary high-quality 3D surfaces in neural networks. Implicit surface representations <ref type="bibr" target="#b47">[47]</ref>, <ref type="bibr" target="#b48">[48]</ref>, on the other hand, overcome these challenges and demonstrate the best flexibility and expressiveness for topology representation. It defines a surface as the level set of a function of occupancy probability or surface distance. Recent works <ref type="bibr" target="#b49">[49]</ref>, <ref type="bibr" target="#b50">[50]</ref>, <ref type="bibr" target="#b51">[51]</ref>, <ref type="bibr" target="#b52">[52]</ref>, <ref type="bibr" target="#b53">[53]</ref> have shown promising results on generative models for 3D object shape inference based on deep implicit representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Implicit Representation for Human Reconstruction.</head><p>The success of deep implicit representations in general object modeling has inspired research in 3D human reconstruction <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b54">[54]</ref>, <ref type="bibr" target="#b55">[55]</ref>, <ref type="bibr" target="#b56">[56]</ref>, <ref type="bibr" target="#b57">[57]</ref>. For example, PIFu <ref type="bibr" target="#b7">[8]</ref> proposed to regress an deep implicit function using pixelaligned image features and is able to reconstruct highresolution results, which is then accelerated to real-time framerate in <ref type="bibr" target="#b56">[56]</ref>. PIFuHD <ref type="bibr" target="#b54">[54]</ref> extended PIFu to capture more local details by adding a fine-level feature extraction network. However, both PIFu and PIFuHD are prone to reconstruction artifacts in cases of challenging poses and self-occlusions. In contrast, our method achieve more robust performance under these challenging scenarios by introducing SMPL model as an additional semantic condition for the implicit representation. ARCH <ref type="bibr" target="#b55">[55]</ref> and IP-Net <ref type="bibr" target="#b57">[57]</ref> are two concurrent works that also use SMPL model for semantic reconstruction. In particular, ARCH <ref type="bibr" target="#b55">[55]</ref> proposed to regress animation-ready 3D avatars in a canonical pose (A-pose), but fails to generate accurate results, especially for loose clothes and human-object interaction (e.g., holding a camera with two hands as in <ref type="figure">Fig.8</ref>) because it is ambiguous to determine the position of the objects and accessories in A-pose. IP-Net <ref type="bibr" target="#b57">[57]</ref> jointly predict the outer 3D surface of a human model, the inner body surface and the body part labels, but is restricted to point cloud inputs. In contrast, our method can naturally handle human-object interaction and can be flexibly adopted in both single-image and multiimage setups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SURFACE REPRESENTATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Deep Implicit Function</head><p>A deep implicit function defines a surface as the level set of an occupancy probability function F , e.g. F (p) = 0.5, where p ? R 3 denotes a 3D point and F : R 3 ? [0, 1] is represented by a deep neural network. In practice, in order to represent the surface of a specific object using a general neural network, the function F also takes a condition variable (e.g., image feature of the object) as input and thus can be written as:</p><formula xml:id="formula_0">F (p, c) : R 3 ? X ? [0, 1]<label>(1)</label></formula><p>where c ? X is the condition variable that encodes the overall shape of a specific surface and can be custom designed in accordance of applications <ref type="bibr" target="#b49">[49]</ref>, <ref type="bibr" target="#b50">[50]</ref>, <ref type="bibr" target="#b51">[51]</ref>. Intuitively, F predicts the continuous inside/outside probability field of a 3D model, in which iso-surface can be easily extracted In PIFu <ref type="bibr" target="#b7">[8]</ref>, the authors combined the condition variable with the point coordinate and formulate a pixel-aligned implicit function as:</p><formula xml:id="formula_1">F (C(p)) : R 3 ? [0, 1]</formula><p>(2)</p><formula xml:id="formula_2">C(p) = (S (F I , ?(p)) , Z(p)) T<label>(3)</label></formula><p>where F I = E I (I) represents the image feature map from the deep image encoder E I (?), ?(p) the 2D projection of p on the feature map F I , S (?, ?) is the sampling function used to sample the value of F I at pixel ?(p) using bilinear interpolation, and Z(p) is the depth value of p in the camera coordinate space. A weak perspective camera is assumed in both PIFu and this paper. Intuitively, given a pixel on the image, PIFu casts a ray through that pixel along the direction of z-axis and estimates the values of occupancy probability on that ray based on the local image feature of the given pixel. Thanks to the usage of pixel-level features as condition variable, PIFu can reconstruct fine-scale detailed surfaces that are well aligned to the input images. PIFu <ref type="bibr" target="#b7">[8]</ref> demonstrates high-quality human digitization for fashion poses, like standing or walking. However, we argue that purely relying on 2D image feature is insufficient to handle severe occlusions and large pose variations for single-image 3D human recovery, especially when ideal high quality dataset (covering sufficient poses, shapes and cloth types) is inaccessible. Specifically, the reasons are twofolds. On one hand, under the scenarios of self-occlusions, there may be multiple peaks of occupancy probability along the z-rays, but it is hard to infer these types of changes consistently and accurately only based on the local pixellevel 2D features. On the other hand, without the awareness of the underlying body shape and pose, the reconstruction results are prone to seriously incomplete or asymmetric bodies, like breaking arms and unequal legs, in the cases of challenging poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Parametric body Model</head><p>Inspired by DeepHuman <ref type="bibr" target="#b4">[5]</ref>, we integrate the parametric body model, SMPL <ref type="bibr" target="#b8">[9]</ref>, to regularize the human reconstruction. SMPL is a function M (?) that maps pose ? and shape ? to a mesh of n S vertices:</p><formula xml:id="formula_3">M (?, ?) = W (T (?, ?), J(?), W)) T (?, ?) = T + B s (?) + B p (?)<label>(4)</label></formula><p>where linear blend-skinning W (?) with skinning weights W poses the T-pose template T + B s (?) + B p (?) based on its skeleton joints J(?). As shown in DeepHuman, a SMPL model fitted to the input image can serve as a strong geometry prior, and guarantee pose and shape aware human model reconstructions. However, estimating SMPL model given a single image is a fundamentally ill-posed problem. Recent studies have explored to learn from largescale datasets to address this challenge <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b40">[40]</ref>. In this work we use GCMR <ref type="bibr" target="#b12">[13]</ref>, one of the state-of-the-art networks as the backbone for body shape inference. Note that GCMR can be replaced with other equivalent networks such as SPIN <ref type="bibr" target="#b37">[38]</ref> for single images and VIBE <ref type="bibr" target="#b38">[39]</ref> for videos, as we do not make any assumptions on how the initial body is estimated and our method is able to deal with inaccurate body initialization thanks to our body reference optimization (Sec.4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">PaMIR: Parametric Model-Conditioned Implicit Representation</head><p>To combine the strengths of parametric body models and non-parametric implicit field, we introduce Parametric Model-Conditioned Implicit Representation (PaMIR). Specifically, in PaMIR, we define the definition of C(p) in Eqn.(3) as:</p><formula xml:id="formula_4">C(p) = (S (F I , ?(p)) , S (F V , p)) T<label>(5)</label></formula><p>where S (F I , ?(p)) has the same definition as Eqn. <ref type="formula" target="#formula_2">(3)</ref>, while F V is the feature volume and S (F V , p) represents the voxelaligned volumetric feature at p sampled from F V . The feature volume F V is obtained by firstly converting the SMPL mesh into an occupancy volume V O through mesh voxelization and then encoding it through a 3D encoder</p><formula xml:id="formula_5">E V , i.e., F V = E V (V O )</formula><p>. Given the per-pixel feature vector S (F I , ?(p)) of p as well as its per-voxel feature vector S (F V , p), we learn an implicit function F (C(p)) in Eqn. <ref type="formula">(2)</ref> that can classify whether p is inside or outside the surface. Note that although the introduction of 3D dense feature volume may increase memory consumption, in practice, we found that a relatively small feature volume is enough for soft semantic regularization. Moreover, we omit Z(p) in C(p) since S (F V , p) already contains 3D coordinate information. The illustration of the proposed PaMIR representation and the corresponding network is shown in <ref type="figure" target="#fig_0">Fig.2</ref>.</p><p>In PaMIR, the free-form implict representation is regularized by the semantic features of the parametric model. As we can see in the following sections, introducing SMPL model as a body reference has several advantages: 1) Pose Generalization. For surface geometry reconstruction, the SMPL input can be regarded as an initial guess for the network, which helps to factor out pose changes </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Depth-ambiguity-aware Reconstruction Loss</head><p>Reconstruction Loss</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GCMR PaMIR Network</head><p>Sampling</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Traditional Reconstruction Loss</head><p>Ground-truth SMPL NOT USED <ref type="figure">Fig. 3</ref>. Comparison of the traditional reconstruction loss and our proposed depth-ambiguity-aware reconstruction loss. Unlike the traditional reconstruction loss, we shift the point samples according to the depth difference between the ground-truth SMPL and the predicted one. In this way the network is able to learn to infer an implicit field registered with the predicted 3D pose.</p><p>of the subject and eliminate depth ambiguities, thus making the network mainly focusing on surface detail reconstruction. As a result, our method has more robust performance compared with PIFu especially under challenging poses. 2) Multi-modal Prediction. Unlike PIFu <ref type="bibr" target="#b7">[8]</ref> or Occupancy Network <ref type="bibr" target="#b51">[51]</ref> that only condition on the input images, our PaMIR representation also conditions on the underlying bodies. Thus, in contrast to reconstruct one model for each input image, our method can reconstruct various plausible models given different but plausible body poses ( <ref type="figure" target="#fig_0">Fig.12</ref>). 3) Easy Extension to Multi-image Setups. With the common underlying SMPL model as a body reference, our PaMIR representation also implicitly builds a correspondence relationship across different models and images. As a result, our method can be easily extended to multi-image settings like multi-view input and video input without explicit calibration and synchronization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview</head><p>Our PaMIR-based 3D human reconstruction is implemented as a neural network. An overview of the network is illustrated in <ref type="figure" target="#fig_0">Fig.2</ref>. Given a single image I as input, our method first feeds it into the GCMR network to estimate an initial SMPL model, which is then converted into an occupancy volume through voxelization. In the feature extraction step, the input image is encoded into a feature map F I by a 2D convolution network, while the occupancy volume are encoded into a feature volume F V by a 3D convolution network. For each point in the 3D space, its pixel-aligned image feature S (F I , ?(p)) and voxel-aligned volume feature S (F V , p) are sampled in the feature map and the feature volume, respectively. The two feature vectors are then concatenated and translated to an occupancy probability value by a feature-to-occupancy decoder as formulated in Eqn. <ref type="formula">(2)</ref> and <ref type="formula" target="#formula_4">(5)</ref>.</p><p>To alleviate the reliance on accurate SMPL annotations, we use the training images, the SMPL models estimated by GCMR and the corresponding ground-truth meshes to train the other parts of the network. To deal with the depth inconsistency between the predicted SMPL models and the ground-truth scans, we carefully design a depth-ambiguityaware reconstruction loss as elaborated in Sec.4.2. Moreover, for inference, we propose body reference optimization, and optimize the human body template as well as the implicit function in an iterative manner in Sec.4.3. To obtain the final reconstruction results, we densely sample the occupancy probability field over the 3D space and extract the isosurface of the probability field at threshold 0.5 using the Marching Cube algorithm. Texture inference can be performed in a similar way to geometry inference (Sec.4.4). Our framework can be easily extended to multi-image setups, which is described in Sec.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Depth-ambiguity-aware Reconstruction Loss</head><p>To train our network, we sample 3D points in 3D space around the human model, infer their occupancy probabilities and construct a per-point reconstruction loss. The traditional reconstruction loss is defined as the mean square error between the predicted occupancy probability of the point samples and the ground-truth ones. However, we argue that with a single-image setup, depth ambiguity is inevitable for many body poses and hence it is unpractical to force the network output to be perfectly identical with the ground-truth. Furthermore, if we utilize the predicted SMPL models as reference of 3D shape and pose, using traditional reconstruction loss will lead to negative impact on the reconstruction accuracy because the predicted models may not be well aligned with the ground-truth along the z-axis. To deal with this issue, we propose a novel depth-ambiguity-aware reconstruction loss defined as:</p><formula xml:id="formula_6">L R = 1 n p np i=1 |F (C(p i + ?p i )) ? F * (p i )| 2 ,<label>(6)</label></formula><p>where n p is the number of point samples, p i a 3D point sample indexed by i, F * (p i ) the ground-truth occupancy value of p i , and ?p i = (0, 0, ?z i ) T is the compensating translation along z-axis. ?z i is calculated on-the-fly during network training using the explicit correspondences between the predicted SMPL model and the ground-truth SMPL model according to</p><formula xml:id="formula_7">?z i = j?N (i) ? j?i ? i Z(v j ) ? Z(v * j ) ,<label>(7)</label></formula><p>, PaMIR Network ( , 0.5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inside/ Outside Penalty Function</head><p>Backward Pass Forward Pass Network Inference SMPL Instantiation <ref type="figure">Fig. 4</ref>. Illustration of body reference optimization. The estimated SMPL parameters are firstly decoded into a SMPL mesh and then the occupancy values of its vertices are infered by our PaMIR network. Through minimizing the body fitting loss, the SMPL parameters are optimized iteratively.</p><p>where N (i) is the nearest SMPL vertex set of p i and |N (i)| = 4, ? j?i the corresponding blending weight, ? i the weight normalizer, v j and v * j are the j-th vertex of the predicted SMPL model and the corresponding ground-truth SMPL model, respectively. The blending weight is defined according to the distance between the point sample p i and its neighboring SMPL vertex v j :</p><formula xml:id="formula_8">? j?i = exp ? p i ? v j 2? 2 , ? i = j?N (i) ? j?i .<label>(8)</label></formula><p>Intuitively, with the depth-ambiguity-aware loss, we are guiding the network to output plausible surface corresponding to the predicted SMPL model but not the exact groundtruth occupancy volume. In this way the network is able to learn to infer an implicit field registered with the predicted 3D pose. The illustration of depth-ambiguity-aware reconstruction loss is shown in <ref type="figure">Fig.3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Body Reference Optimization</head><p>Although our training scheme already prevents our network from being heavily dependent on the accuracy of SMPL estimation, the inconsistency between the image observation and the SMPL estimation may still lead to reconstruction artifacts. Fortunately, the predictions of our method can be further refined at inference time. Specifically, we can improve the accuracy of SMPL estimation by minimizing:</p><formula xml:id="formula_9">(? * , ? * ) = arg min ?,? L B + ? R L REG<label>(9)</label></formula><p>where L B is the body fitting loss used to encourage the alignment of the predicted implicit function and SMPL model and L REG is a regularization term penalizing the difference between (? * , ? * ) and the initial prediction. The body fitting loss is defined as following: where n S is the number of SMPL vertices, v j is the j-th predicted SMPL vertex and q(?) is the penalty loss defined as:</p><formula xml:id="formula_10">L B = 1 n S n S j=1 q (F (c(v j )) ? 0.5)<label>(10)</label></formula><formula xml:id="formula_11">q(x) = |x| x ? 0, 1 ? |x| x &lt; 0,<label>(11)</label></formula><p>where ? = 5. In other words, p(?) applies more penalization on the vertices that fall outside of the predicted surface while allowing the body to shrink into the surface in case of loose clothes like skirts and dresses. The regularization term is defined as</p><formula xml:id="formula_12">L REG = |? ? ? init | 2 2 + |? ? ? init | 2 2 ,<label>(12)</label></formula><p>where (? init , ? init ) is the initial SMPL parameters estimated by GCMR. Note that other constraints like 2D keypoint detection results can also be added to further improve the fitting performance. The insight behind the formulation of body fitting loss is that the predicted SMPL may not be perfectly aligned with the image observation and consequently, the output implicit function is a compromise between these two information. Thus, by minimizing the body fitting loss, we can eliminate the inconsistency between the SMPL prediction and the image observation. As a result, more consistent SMPL prediction as well as more accurate surface inference can be obtained. The idea of our body reference optimization is illustrated in <ref type="figure">Fig.4</ref>.</p><p>Although body model optimization has been proposed in SMPLify <ref type="bibr" target="#b9">[10]</ref> and HoloPose <ref type="bibr" target="#b36">[37]</ref>, our optimization scheme are substantially different from that in previous works. Existing methods fit body models to image observation such as 2D keypoints detection, 3D keypoints estimation and/or dense correspondences, while ours directly utilizes the human reconstruction network to fit SMPL model into the implicit surface via minimizing the loss in Eqn. <ref type="bibr" target="#b9">(10)</ref>. Our scheme guarantees that the image observation, the estimated body model and the reconstructed outer surface are aligned with each other, and eliminates the requirements of sparse/dense keypoint detection (although they can be used as additional constraints). We visualize the optimization process in <ref type="figure" target="#fig_2">Fig.5</ref>. In this figure, the initial pose of the right arm is incorrect, which leads to reconstruction artifacts. However, as the optimization is carried out, the right arm gradually moves towards the right position. In the meantime, the reconstructed mesh becomes more and more plausible. To clarify, the reconstruction results in this experiment do not contradict the results in <ref type="figure" target="#fig_6">Fig.14:</ref> in this experiment, the right arm is almost invisible and consequently it is difficult for the network to infer the correct geometry with an inaccurate arm pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Texture Inference</head><p>Following the practice of Texture Field <ref type="bibr" target="#b58">[58]</ref> and PIFu <ref type="bibr" target="#b7">[8]</ref>, we also regard the surface texture as a vector function defined in the space near the surface, which can support texturing of shapes with arbitrary topology and self-occlusion. To perform texture inference, we make some simple modification to our network. Specifically, we define the output of the decoder in <ref type="figure" target="#fig_0">Fig.2</ref> as an RGB? vector field instead of a scalar field. The RGB value is the network prediction of the color of a specific point on the mesh surface, while the alpha channel is used to blend the predicted value with the observed one:</p><formula xml:id="formula_13">C(p i ) = ? * S (I, ?(p)) + (1 ? ?) * C (p i )<label>(13)</label></formula><p>where C (p i ) is the color prediction provided by the network, i.e., the first three channels of the decoder output, ? is the last channel of the decoder output and C(p i ) is the final color prediction. Thus, the reconstruction loss in Eqn.6 is modified accordingly to:</p><formula xml:id="formula_14">L R = 1 n p np i=1 |C(p i + ?p i , c(p i + ?p i )) ? C * (p i )| + |C (p i + ?p i , c(p i + ?p i )) ? C * (p i )| ,<label>(14)</label></formula><p>where C * (p i ) is the ground-truth vertex texture and L1 loss is used to avoid color over smoothing. Intuitively, the network learns to infer the color of the whole surface and</p><p>Single-image Result Multi-image Result <ref type="figure">Fig. 7</ref>. Comparison between the single-image result and the multi-image result. By adding four more frames (without calibration and synchronization infomation) as input, our method is able to recover the surface details on the back. In contrast, the back area is over-smoothed in the single-image setting. Zoom in for better view.</p><p>also determine which part of the surface is visible so that we can directly sample color observation on the image. We show the effect of the alpha channel in <ref type="figure" target="#fig_3">Fig.6</ref>. Note that we do not decompose shading component from the original color observation to obtain the "real" surface texture. To obtain shading-free texture, one can use state-of-the-art albedo estimation method like <ref type="bibr" target="#b59">[59]</ref> to firstly remove shading from the input images and then feed the processed images to our texture inference network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXTENSION TO MULTI-IMAGE SETUP</head><p>With the common underlying SMPL model as body reference, our PaMIR representation also implicitly builds a correspondence relationship across different models. As a result, our method can be easily extended to multi-image settings like multi-view input or video input. Specifically, given N I images of the same person I t , t = 0, 1, ..., N I ? 1, we first use the optimization method described in Sec.4.3 to obtain the SMPL models aligned to each input images, M (? t , ? t ). Note that the optimization step is performed on each individual image independently; for future work more constraints like shape consistency and pose consistency constraints can be incorporated to the energy function. With these SMPL template parameters we can establish correspondences across two images. For example, for a point p i in the model space of the reference image I 0 , its corresponding point p (t) i in the model space of I t can be calculated as</p><formula xml:id="formula_15">p (t) i = M 0?t i p i , M 0?t i = j?N (pi) ? j?i ? i M (t) j (? t ) M (0) j (? 0 ) ?1<label>(15)</label></formula><p>where M</p><p>j (? 0 ) and M (t) j (? t ) are the linear blending skinning (LBS) matrices of the j-th SMPL vertex in the reference frame and the t-th frame, respectively, N (p i ), ? j?i and ? i share the same definition as Eqn. <ref type="bibr" target="#b6">(7)</ref>.</p><p>To perform geometry inference using multiple image frames, we follow the practice of PIFu <ref type="bibr" target="#b7">[8]</ref> and decompose <ref type="figure">Fig. 8</ref>. Our results on natural images. From left to right: the 1st is the input images, the 2nd and 3rd column show the SMPL models estimated by our method (network inference + optimization), the 4th to 6th demonstrates our geometry reconstruction results, and the last three column demonstrates the texture inference results. The results demonstrate the ability of our method to reconstruct high-quality models and its robust performance to tackle various human poses and clothing styles.</p><p>the feature-to-occupancy network F (?) in Eqn.2 into two components:</p><formula xml:id="formula_17">F = F 2 ? F 1<label>(16)</label></formula><p>where F 1 is a feature embedding network while F 2 is an occupancy reasoning network. For a 3D point p i in the reference frame I 0 , we first sample its feature vector C (0) (p i ) as described in Sec.3.3. Then we calculate the corresponding point p (t) i in the t-th frame and also sample its corresponding feature vector C (t) (p (t) i ), where t = 1, 2, ..., N I ? 1. The feature embedding network F 1 takes these feature vectors and encodes them into latent feature embedding vectors ? (k) i , k = 0, 1, ..., N I ? 1. The latent embedding vectors across different frames are aggregated into one embeddin? ? i using mean pooling, which is then mapped to the target implicit field F (p i ) by the occupancy reasoning network F 2 . The multi-image network is fine-tuned from the singleimage network using multi-view rendering of 3D human models.</p><p>Note that PIFu also demonstrates results given mutliview inputs. However, the multi-view input should be well calibrated and synchronized. In contrast, neither calibration or synchronization is necessary in our method because we can utilize the SMPL estimation to build the correspondence across different views. Moreover, our method can handle the cases where body poses are not identical across images, e.g., video input. This feature enables video-based personalized avatar creation. A similar application has been demonstrated in <ref type="bibr" target="#b60">[60]</ref>, but our method can support more challenging cloth topologies and recover more surface details. Overall, in terms of multi-image setup, our method is more general and practical than state-of-the-art methods. A <ref type="figure">Fig. 9</ref>. Multi-image human model reconstruction on VideoAvatar dataset: (a) the first column is the first images of the input sequences, (b) the reconstructed models by our method, (c) results by <ref type="bibr" target="#b60">[60]</ref>, (d) Results by <ref type="bibr" target="#b61">[61]</ref>. Our method can reconstruct full-body models with high-resolution details, proving the capability of our PaMIR-based reconstruction method.</p><formula xml:id="formula_18">(a) (b) (c) (d)</formula><p>comparison between the single-image result and the multiimage result is presented in <ref type="figure">Fig.7</ref>. As shown in the figure, by adding four more video frames as input, our method is able to recover the surface details on the back that is invisible in the first frame.</p><p>Since we consider only pose changes across views and neglect surface detail deformations (e.g., wrinkle movements) across frames, challenging pose deviation and inconsistent geometry between frames will cause inaccurate feature fusion and reconstruction artifacts. Besides, the multiimage network is trained using only multi-view rendering of human models without pose deviations. For future work, we can introduce 4D data to resolve these issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>In this section we evaluate our approach. Details about the implementation and the evaluation dataset are given in Sec.6.1 and Sec.6.2 respectively. In Sec.6.3 we demonstrate that our method is able to reconstruct human models with challenging poses. We then compare our method against state-of-the-art methods in Sec.6.4. We also evaluate our contributions both qualitatively and quantitatively in Sec.6.5. The quantitative evaluation results are given in Tab.3. We mainly use geometry reconstruction performance for evaluation, which is our main focus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Implementation Details</head><p>Network Architecture. For image feature extraction, we adapt the same 2D image encoders in PIFu <ref type="bibr" target="#b7">[8]</ref> (i.e., Hourglass Stack <ref type="bibr" target="#b62">[62]</ref> for geometry and CycleGAN <ref type="bibr" target="#b63">[63]</ref> for texture), which take as input an image of 512?512 resolution and outputs a 256-channel feature map with a resolution of 128?128. For volumetric feature extraction, we use a 3D convolution network which consists of two convolution layers and three residual blocks. Its input resolution is 128?128?128 and its output is a 32-channel feature volume with a resolution of 32?32?32. We replace batch normalization with group normalization to improve the training stability. The feature decoder is implemented as a multi-layer perceptron, where the number of neurons is <ref type="figure" target="#fig_0">(288, 1024, 512, 256, 128, ch)</ref>, where ch = 1 for the geometry network while ch = 4 for the texture network.</p><p>Training Data. To achieve state-of-the-art reconstruction quality, we collect 1000 high-quality textured human scans with various clothing, shapes, and poses from Twindom 1 .  <ref type="figure">Fig. 10</ref>. Comparison between the single-image reconstruction results and the multi-image results from input images with moderate pose deviations. In these cases, our method is still able to integrate information from different frames.</p><p>We randomly split the Twindom dataset into a training set of 900 scans and a testing of 100 scans. To augement pose variety, we also randomly sample 600 models from DeepHuman <ref type="bibr" target="#b4">[5]</ref> dataset. We render the training models from multiple view points using Lambertian diffuse shading and spherical harmonics lighting <ref type="bibr" target="#b64">[64]</ref>. We render the images with a weak perspective camera and image resolution of 512 ? 512. To obtain the ground-truth SMPL annotations for the training data, we apply MuVS <ref type="bibr" target="#b65">[65]</ref> to the multi-view images for pose computation and then solve for the shape parameters to further register the SMPL to the scans. For point sampling during training, we use the same scheme proposed in PIFu <ref type="bibr" target="#b7">[8]</ref>, in which the authors combine uniform sampling and adaptive sampling based on the surface geometry and use embree algorithm <ref type="bibr" target="#b66">[66]</ref> for occupancy querying. Network training. We use Adam optimizer for network training with the learning rate of 1 ? 10 ?3 , the batch size of 3, the number of epochs of 10, and the number of sampled points of 5000 per subject. The learning rate is decayed by the factor of 0.1 at every 10000-th iteration. We also combine predicted and ground-truth SMPL models during network training. Specifically, in each batch, we randomly select one image to replace the predicted SMPL model with the ground-truth when constructing the depth-ambiguityaware reconstruction loss. In this way we can guarantee the best performance is obtained once the underlining SMPL becomes more accurate. The multi-image network is finetuned from the models trained for single-view network with three random views of the same subject using a learning rate of 2 ? 10 ?5 and a batch size of 1. Before training the PaMIR network, we first fine-tune the pre-trained GCMR network on our training set.</p><p>Network testing. When testing, our network only requires an RGB image as input and outputs both the parametric model and the reconstructed surface with texture. To maximize the performance, We run the body reference optimization step for all results unless otherwise stated. Fifty iterations are needed for the optimization and take about 40 seconds.</p><p>Network complexity. We report the module complexity in Tab.1. To reconstruct the 3D human model given an RGB image, we use the hierarchical SDF querying method proposed in OccNet <ref type="bibr" target="#b51">[51]</ref> to reduce network querying times. Overall, taking the body reference optimization step into account, it takes about 50s to reconstruct the geometry of the 3D human model and 1s to recover its surface color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Evaluation Dataset</head><p>For qualitative evaluation on single-image reconstruction, we utilize real-world full-body images collected from the DeepFashion dataset <ref type="bibr" target="#b68">[67]</ref> and from the Internet. We remove the background of these real-world image using neural semantic segmentation <ref type="bibr" target="#b69">[68]</ref> followed by Grabcut refinement <ref type="bibr" target="#b70">[69]</ref>. For qualitative evaluation on multi-image reconstruction, we use the open-source dataset from VideoAvatar <ref type="bibr" target="#b60">[60]</ref> which contains 24 RGB sequences of different subjects turning 360 degree with a rough A-pose in front of a camera. The testing split of the Twindom dataset is used for quantitative comparison. In addition, for quantitative comparison and evaluation we also use the BUFF dataset <ref type="bibr" target="#b71">[70]</ref>, which provides 26 4D human sequences with various clothes and over 9000 scans in total. To reduce computation burden, We select 300 scans of different poses from the buff dataset. We render both the Twindom testing data and the BUFF data from 12 views spanning every 30 degrees in yaw axis using the same method in Sec.6.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Results</head><p>We demonstrate our approach for single-image 3D human reconstruction in <ref type="figure">Fig.1 and Fig.8</ref>. The input images in <ref type="figure">Fig.1</ref> and <ref type="figure">Fig.8</ref> covers various body poses (dancing, Kungfu, sitting and running), and also covers different clothes (loose pants, skirts, sports suits and casual clothes). The results <ref type="figure">Fig. 11</ref>. Qualitative comparison against state-of-the-art methods for single-image human model reconstruction: (a) input images, (b) results by HMD <ref type="bibr" target="#b2">[3]</ref>, (c) Tex2Shape <ref type="bibr" target="#b3">[4]</ref>, (d) Moulding Humans <ref type="bibr" target="#b5">[6]</ref>, (e) DeepHuman <ref type="bibr" target="#b4">[5]</ref>, (f) PIFu <ref type="bibr" target="#b7">[8]</ref>, (g) ours.</p><formula xml:id="formula_19">(a) (b) (c) (d) (e) (f) (g)</formula><p>demonstrate the ability of our method to reconstruct highquality 3D human models and its robust performance to tackle various human poses and clothing styles. We also test our performance for multi-image human model reconstruction using the VideoAvatar dataset in <ref type="figure">Fig.9</ref>. We uniformly sample 5 frames for each sequence for surface reconstruction. Note that for each subject, the poses in different frames are different due to the body movements and no camera extrinsic parameter is provided, so PIFu is not suitable to reconstruct human models in this case. In contrast, our method can reconstruct full-body models with high-resolution details, proving the generalization capability of our PaMIR representation. We also present two example results of applying multi-image feature fusion on images with moderate pose deviation in <ref type="figure">Fig.10</ref>. As the figure shows, our method is still able to reconstruct the overall shapes of the subjects in these cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Comparison</head><p>We qualitatively compare our method with several state-ofthe-art methods including HMD <ref type="bibr" target="#b2">[3]</ref>, Tex2Shape <ref type="bibr" target="#b3">[4]</ref>, Moulding Humans <ref type="bibr" target="#b5">[6]</ref>, DeepHuman <ref type="bibr" target="#b4">[5]</ref> and PIFu <ref type="bibr" target="#b7">[8]</ref>. Among them, HMD <ref type="bibr" target="#b2">[3]</ref> and Tex2Shape <ref type="bibr" target="#b3">[4]</ref> are parametric methods based on SMPL <ref type="bibr" target="#b8">[9]</ref> model deformation, PIFu <ref type="bibr" target="#b7">[8]</ref> uses a deep implicit function as geometry representation, Moulding Humans <ref type="bibr" target="#b5">[6]</ref> uses the combination of a front depth map and a back depth map for representation, and DeepHuman <ref type="bibr" target="#b4">[5]</ref> combines volumetric representation with the SMPL model. Note that Tex2Shape only computes shapes, so we use the pose parameters obtained by our method to skin the its results. For simplicity, we omit comparisons with the works that have already been compared like BodyNet <ref type="bibr" target="#b0">[1]</ref> and SiCloPe <ref type="bibr" target="#b1">[2]</ref>. In our experiments, DeepHuman, PIFu and Moulding Humans are all retrained on the our dataset, while parametric methods like HMD and Tex2shape are not because our dataset contains loose garments like dresses which will deteriorate the performance of those methods.</p><p>We conduct qualitative comparison in <ref type="figure">Fig.11</ref>. As shown in the figure, HMD and Tex2Shape have difficulties dealing with loose clothes and cannot reconstruct the surface geometry accurately; Moulding Human <ref type="bibr" target="#b5">[6]</ref> fails to handle challenging poses (due to the lack of semantic constraints), and produces broken body parts when self-occlusions occur, which is the essential limitation of its double depth representation; DeepHuman <ref type="bibr" target="#b4">[5]</ref> cannot recover high-frequency geometrical details although it succeeds to reconstruct the rough shapes from the images; PIFu <ref type="bibr" target="#b7">[8]</ref> struggles to reconstruct the models in challenging poses and also suffer from self-occlusions. In contrast, our method is able to reconstruct plausible 3D human models under challenging body poses and various clothing styles. In terms of surface quality and pose generalization capacity, our method is superior to other state-of-the-art methods.</p><p>We quantitatively compare our method with the stateof-the-art methods using both Twindom testing dataset and BUFF rendering dataset to evaluate the geometry reconsturction accuracy. Similar to the experiments in PIFu <ref type="bibr" target="#b7">[8]</ref>, we use point-to-surface error as well as the Chamfer distance as error metric. The numerical results are presented in  Tab.2. The quantitative comparison shows that our method outperforms the state-of-the-art methods in terms of surface reconstruction accuracy. We also provide the errors when ground-truth SMPL annotations are available to present the upper limit of our reconstruction accuracy if the SMPL estimation is perfect. Overall, our method is more general, more robust and more accurate than HMD <ref type="bibr" target="#b2">[3]</ref>, Moulding Humans <ref type="bibr" target="#b5">[6]</ref>, DeepHuman <ref type="bibr" target="#b4">[5]</ref> and PIFu <ref type="bibr" target="#b7">[8]</ref>.</p><p>For multi-image setups, we also conduct qualitative comparison against state-of-the-art methods <ref type="bibr" target="#b60">[60]</ref> and <ref type="bibr" target="#b61">[61]</ref>. The method proposed in <ref type="bibr" target="#b60">[60]</ref> is an optimization-based method which deforms the SMPL template according to the silhouettes. The optimization is performed on the whole video sequence. In contrast, the method in <ref type="bibr" target="#b61">[61]</ref> is a learningbased method that deforms the SMPL template based on only 8 views of images. The comparison results are shown in <ref type="figure">Fig.9</ref>. From the results we can see that although all methods are able to reconstruct the overall shapes correctly, our method is able to recover more surface details than the other two methods. This is because our non-parametric representation allows more flexible surface reconstruction. We do not perform quantitative comparison because there is no such benchmark available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.1">PaMIR Representation</head><p>The superiority of our PaMIR representation is already proved through the comparison experiments in Sec.6.3 and 6.4. In this evaluation, we demonstrate the advantage of our PaMIR representation from another aspect: it can support multi-modal outputs. To be more specific, our method can output multiple possible human models corresponding to different body pose hypotheses. Two examples are shown in <ref type="figure" target="#fig_0">Fig.12</ref>. In both experiments we manually adjust one part of the body in order to generate multiple pose hypotheses. As a result, the reconstruction results change in accordance with the input SMPL models. In contrast, non-parametric method like PIFu <ref type="bibr" target="#b7">[8]</ref> or Moulding Humans <ref type="bibr" target="#b5">[6]</ref> can only generate a specific mesh for each image, showing their limited and overfit capability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.2">Depth-ambiguity-aware Loss</head><p>To conduct ablation study on our depth-ambiguity-aware loss, we implement a baseline network that is trained using the traditional reconstruction loss and compare it against our network. After the same number of training epochs, we test both network on real-world input images. Two example results are shown in <ref type="figure" target="#fig_5">Fig.13</ref>. As we can see in the figure, with our depth-ambiguity-aware reconstruction loss, the network is able to learn to reconstruct 3D human models that have consistent poses with the estimated SMPL models. On the contrary, the models reconstructed by the baseline network without our depth-ambiguity-aware loss are less consistent with the SMPL models and have more artifacts. The quantitative study is presented in Tab.3. From the numeric comparison between the 3rd and 4th rows of Tab.3, we can conclude that our depth-ambiguity-aware loss avoids the negative impact of the depth inconsistency between predicted SMPL models and ground-truth scans, and improves the accuracy of the reconstruction results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.3">Training Scheme</head><p>We also implement another baseline network that is trained using the ground-truth SMPL annotations to evaluate our training scheme. We compare this baseline network with our network on different input images. In <ref type="figure" target="#fig_6">Fig.14,</ref> we present some cases in which the predicted SMPL models are not well aligned with image keypoints and silhouettes. As we can see in <ref type="figure" target="#fig_6">Fig.14,</ref> under the scenarios of inaccurate SMPL estimation, the baseline network fails to reconstruct complete full-body models, while our network is able to provide plausible results in accordance with image observations. Note that this feature is of vital importance. For instance, in the last example of <ref type="figure" target="#fig_6">Fig.14</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.4">Reference Body Optimization</head><p>To evaluate the effectiveness of our reference body optimization step, we compare the body fitting results before and after optimization using the evaluation images in Sec.6.5.3. The results are presented in <ref type="figure" target="#fig_2">Fig.15</ref>. As shown in the figure, the optimization step can further register the SMPL model to the image observation, resulting into more accurate body pose estimation. This is also proven in the quantitiative evaluation in Tab.4. From the numerical results in the last two rows of Tab.3, we can also see that the mesh reconstruction is also improved after reference body optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.5">Multi-image Setup</head><p>To evaluate the detail changes using more or less images, we conduct a quantitative evaluation in Tab.5. Specifically, we perform feature fusion with 1, 2, 3 and 4 views of the same model and extract the meshes. Then we measure the normal reprojection error <ref type="bibr" target="#b7">[8]</ref> from 8 uniform viewpoints to evaluate the detail improvements. The numerical results presented in Tab.5 suggest that more geometric details are recovered after fusing information from multiple input images.</p><p>Before After Before After Before After <ref type="figure" target="#fig_2">Fig. 15</ref>. Evaluation of reference body optimization. Note that after body reference optimization, the SMPL models are more accurately aligned with the image observations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION</head><p>Conclusion. Modeling 3D humans accurately and robustly from a single RGB image is an extremely ill-posed problem due to the varieties of body poses, clothing types, view points and other environment factors. Our key idea to overcome these challenges is factoring out pose estimation from surface reconstruction. To this end, we have contributed a deep learning-based framework to combine the parametric SMPL model and the non-parametric deep implicit function for 3D human model reconstruction from a single RGB image. Benefiting from the proposed PaMIR representation, the depth-ambiguity-aware reconstruction loss and the reference body optimization algorithm, our method outperforms state-of-the-art methods in terms of both robustness and surface details. As shown in the supplementary video, we can obtain temporally consistent reconstruction results by applying our method to video frames individually. We believe that our method will enable many applications and further stimulate the research in this area. From a more general perspective of view, we have made a step forward towards integrating semantic information into the free-form implicit fields which have attracted more and more attention from the research community for its flexibility, representation power and compact nature. For example, similar topics include the reconstruction of hand and face using such PaMIR representation. We also believe our semantic implicit fields would become one important future topic and inspire many other studies in 3D vision. For example, the multi-modality of our method <ref type="figure" target="#fig_0">(Fig.12</ref>) can inspire research on image-based geometry editing; the practice of integrating geometry template into implicit functions can be used to improve the robustness of other 3D recovery tasks. Moreover, the semantic implicit functions can also be adopted in research on 3D perception, parsing and understanding. Limitation and Future Work. Our method needs highquality human scans for training. However, it is highly costly and time consuming to obtain a large-scale dataset of high-quality human scans. Moreover, the currently available scanners for human bodies require the subject to keep static poses in a sophisticated capturing environment, which makes them incapable to capture real-world human motions in the wild and consequently our training data is biased towards simple static poses like standing. Therefore, although the proposed method already makes a step forward in terms of generalization capability, it still fails in the cases of extremely challenging poses, see <ref type="figure" target="#fig_3">Fig.16</ref>. One important future direction is to alleviate the reliance on ground-truth by exploring large-scale image and video dataset for unsupervised training.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Overview of the PaMIR representation and the corresponding network architecture. Given an input image, we first estimate a corresponding SMPL model in the SMPL estimation step. In the following step, the image and the SMPL model are converted into a feature map and a feature volume respectively. Conditioned on the feature vectors sampled in the feature sampling step, the implicit function value for each 3D point can be obtained in the final step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Visualization of the body optimization process. The leftmost column: the input image. The 2nd to 4th columns: the reconstruction results before reference body optimization, the intermediate status of optimization and the results after optimization. We present the SMPL models in the top row and the reconstructed models in the bottom row. The median row shows the occupancy probability of SMPL vertices: blue color represents F (C(v)) = 1.0, while red means F (C(v)) = 0.0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Illustration of our texture inference process. The texture network first recovers the color on the whole surface as well as an alpha channel which is used to blend the predicted color with the observation. In this way, the input image is maximally utilized and more texture details are recovered (Please zoom in to compare the texture on the shirt).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 12 .</head><label>12</label><figDesc>Two examples of multi-modal output. Given an input image (a), our method can output different possible reconstruction results (top row of (b)(c)) corresponding to different body pose hypothesis (bottom row of (b)(c) ). From the overlapped figure (the last columns of (b)(c)), we can see that the right arm and the left leg of reconstructed mesh adjusts accordingly with the right arm and the left leg of the body hypothesis, while other parts of the mesh keep consistent. Better view in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 13 .</head><label>13</label><figDesc>Qualitative evaluation on depth-ambiguity-aware training loss: (a) the input images, (b) the corresponding SMPL models estimated by GCMR, (c) our reconstruction results, (d) the reconstruction results of the baseline network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 14 .</head><label>14</label><figDesc>Qualitative evaluation on our training scheme: (a) the input images, (b) the estimated SMPL models rendered on top of the input images, (c) our reconstruction results, (d) the reconstruction results of the baseline network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 16 .</head><label>16</label><figDesc>Failure cases. For extremely challenging poses, our method fails to generate plausible reconstruction results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1</head><label>1</label><figDesc>The number of parameters and execution time of each network module</figDesc><table><row><cell>Module</cell><cell cols="2">#Parameters Execution Time</cell></row><row><cell>GCMR</cell><cell>46,874,690</cell><cell>0.15s  *</cell></row><row><cell>Geometry Network</cell><cell>27,225,105</cell><cell>0.25s  ?</cell></row><row><cell>Texture Network</cell><cell>13,088,268</cell><cell>0.29s  ?</cell></row><row><cell></cell><cell></cell><cell></cell></row></table><note>* Measured using one RGB image.? Measured using one RGB image and 10k query points.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2</head><label>2</label><figDesc>Numerical comparison results.</figDesc><table><row><cell>Dataset</cell><cell>BUFF</cell><cell></cell><cell></cell><cell>TWINDOM</cell></row><row><cell>Method</cell><cell cols="4">P2S (cm) Chamfer (cm) P2S (cm) Chamfer (cm)</cell></row><row><cell>HMD [5]</cell><cell>2.48</cell><cell>3.92</cell><cell>2.50</cell><cell>4.01</cell></row><row><cell>Moulding Humans [6]</cell><cell>2.25</cell><cell>2.68</cell><cell>2.84</cell><cell>3.35</cell></row><row><cell>DeepHuman [5]</cell><cell>2.15</cell><cell>2.80</cell><cell>2.35</cell><cell>2.97</cell></row><row><cell>PIFu [8]</cell><cell>1.93</cell><cell>2.22</cell><cell>2.34</cell><cell>2.65</cell></row><row><cell>Ours</cell><cell>1.52</cell><cell>1.92</cell><cell>1.80</cell><cell>2.12</cell></row><row><cell>Ours using ground-truth SMPL</cell><cell>0.709</cell><cell>0.936</cell><cell>0.744</cell><cell>1.00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3 Numerical</head><label>3</label><figDesc>Ablation Study.</figDesc><table><row><cell>Method  *</cell><cell>P2S (cm)</cell></row><row><cell>GT SMPL</cell><cell>1.71</cell></row><row><cell>Pred SMPL wo/ DAAL</cell><cell>1.81</cell></row><row><cell>Pred SMPL w/ DAAL</cell><cell>1.60</cell></row><row><cell>Full (Pred SMPL w/ DAAL + Optimization)</cell><cell>1.52</cell></row><row><cell cols="2">*  GT = ground-truth, Pred = predicted, DAAL = depth-ambiguity-</cell></row><row><cell>aware loss</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>, the right forearm of the baseline model is completely missing. In this case, the reference body optimization step could not work because no matter how the right forearm moves, the occupancy probability value of its vertices are always closed to zero. The quantitative study is presented in Tab.3. From the numerical comparison between the 2nd and 4th rows of Tab.3, we can conclude that our training scheme improves the accuracy of our reconstruction results at inference time when no accurate SMPL annotation is available.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4</head><label>4</label><figDesc>Mean Per Joint Position Error (MPJPE, unit: cm) Before/After Body Reference Optimization.</figDesc><table><row><cell>Dataset</cell><cell cols="2">BUFF TWINDOM</cell></row><row><cell>Before Optimization</cell><cell>2.65</cell><cell>2.85</cell></row><row><cell>After Optimization</cell><cell>2.49</cell><cell>2.74</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 5</head><label>5</label><figDesc>Normal Reprojection Error with Inputs Images From Various Numbers of View Points.</figDesc><table><row><cell>View Num</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell></row><row><cell cols="5">Normal Reprojection Error 0.161 0.147 0.139 0.135</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">BodyNet: Volumetric inference of 3D human body shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Siclope: Silhouette-based clothed people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Natsume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Morishima</surname></persName>
		</author>
		<idno>abs/1901.00049</idno>
		<ptr target="http://arxiv.org/abs/1901.00049" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Detailed human shape estimation from a single image by hierarchical mesh deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tex2shape: Detailed full human body geometry from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Alldieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Magnor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deephuman: 3d human reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Moulding humans: Non-parametric 3d human shape estimation from single images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gabeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-S</forename><surname>Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Facsimile: Fast and accurate scans from an image in less than a second</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mavroidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Natsume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Morishima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">SMPL: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics (Proc. SIGGRAPH Asia)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="248" />
			<date type="published" when="2015-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="561" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Neural body fitting: Unifying deep learning and model based human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional mesh regression for single-image human shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Shape-aware human pose and shape reconstruction using multi-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Denserac: Joint 3d pose and shape estimation by dense render-and-compare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Virtualized reality: Constructing virtual worlds from real scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE MultiMedia</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="34" to="47" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Surface capture for performance-based animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Starck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="21" to="31" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A point-cloud-based multiview stereo algorithm for free-viewpoint video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="407" to="418" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Shadingbased dynamic shape refinement from multi-view video under general illumination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Varanasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1108" to="1115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scalable 3d video of dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Waschb?sch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>W?rmlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cotting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sadlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Visual Computer</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8-10</biblScope>
			<biblScope unit="page" from="629" to="638" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dynamic shape capture using multi-view photometric stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vlasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Baran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Debevec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Popovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rusinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matusik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="174" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fusion4d: real-time performance capture of challenging scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khamis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Degtyarev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Fanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kowdle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">O</forename><surname>Escolano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">114</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Motion2fusion: realtime volumetric performance capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Fanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khamis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kowdle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tankovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">High-quality streamable free-viewpoint video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Collet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sweeney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gillett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Evseev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Calabrese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">69</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Panoptic studio: A massively multiview system for social motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nobuhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3334" to="3342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Total capture: A 3d deformation model for tracking faces, hands, and bodies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Volumetric performance capture from minimal camera viewpoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Volino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collomosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="591" to="607" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep volumetric video from very sparse multiview performance capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Legendre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="351" to="369" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Articulated mesh animation from multi-view silhouettes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vlasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Baran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Popovic</surname></persName>
		</author>
		<idno>97:1-97:9</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Performance capture from sparse multi-view video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>De Aguiar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stoll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="98" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Motion capture using joint skeleton tracking and surface estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stoll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aguiar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Seidel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1746" to="1753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Markerless motion capture of multiple characters using multiview image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stoll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T-PAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2720" to="2735" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">SCAPE: shape completion and animation of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="408" to="416" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1302" to="1310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Densepose: Dense human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>G?ler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unite the people: Closing the loop between 3d and 2d human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4704" to="4713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Holopose: Holistic 3d human reconstruction in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Guler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3D human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10" />
			<biblScope unit="page" from="2380" to="7504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">VIBE: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="5252" to="5262" />
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pose2pose: 3d positional pose-guided 3d rotational pose prediction for expressive 3d human pose and mesh estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00593</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning efficient point cloud generation for dense 3d object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multiview convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Atlasnet: A papier-m\?ach\&apos;e approach to learning 3d surface generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Groueix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05384</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pixel2mesh: Generating 3d mesh models from single rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Reconstruction and representation of 3d objects with radial basis functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Beatson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Cherrie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Fright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 28th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="67" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A volumetric method for building complex models from range images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
		<idno type="DOI">10.1145/237170.237269</idno>
		<ptr target="https://doi.org/10.1145/237170.237269" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Annual Conference on Computer Graphics and Interactive Techniques, ser. SIGGRAPH &apos;96</title>
		<meeting>the 23rd Annual Conference on Computer Graphics and Interactive Techniques, ser. SIGGRAPH &apos;96<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="303" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deepsdf: Learning continuous signed distance functions for shape representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lovegrove</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning implicit fields for generative shape modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5939" to="5948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Occupancy networks: Learning 3d reconstruction in function space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oechsle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Implicit functions in feature space for 3d shape reconstruction and completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chibane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Alldieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Local implicit grid representations for 3d scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niessner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Pifuhd: Multi-level pixel-aligned implicit function for high-resolution 3d human digitization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Arch: Animatable reconstruction of clothed humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Monocular real-time volumetric performance capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Olszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV) 2020</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Combining implicit function learning and parametric models for 3d human reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Bhatnagar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Texture fields: Learning texture representations in function space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oechsle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Strauss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE International Conf. on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Relighting humans: occlusion-aware inverse rendering for full-body human images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kanamori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Endo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Asia 2018 Technical Papers</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page">270</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Video based reconstruction of 3d people models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Alldieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Magnor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning to reconstruct people in clothing from a single RGB camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Alldieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Magnor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Bhatnagar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Unpaired image-toimage translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning from synthetic humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4627" to="4635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Towards accurate markerless human shape and pose estimation over time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Embree: A kernel framework for efficient cpu ray tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Wald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Benthin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ernst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="143" />
			<date type="published" when="2014-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
				<idno type="DOI">http:/doi.acm.org/10.1145/2601097.2601199</idno>
		<ptr target="http://doi.acm.org/10.1145/2601097.2601199" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Deepfashion: Powering robust clothes recognition and retrieval with rich annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Look into person: Joint body parsing &amp; pose estimation network and a new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T-PAMI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Grabcut -interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2004-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Detailed, accurate, human shape estimation from clothed 3D scan sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pujades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
