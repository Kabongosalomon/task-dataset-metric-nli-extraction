<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Knowledge Representing: Efficient, Sparse Representation of Prior Knowledge for Knowledge Distillation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Liu</surname></persName>
							<email>liujunjie@canon-ib.com.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Canon Information Technology (Beijing) Co., LTD</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongchao</forename><surname>Wen</surname></persName>
							<email>wendongchao@canon-ib.com.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Canon Information Technology (Beijing) Co., LTD</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxing</forename><surname>Gao</surname></persName>
							<email>gaohongxing@canon-ib.com.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Canon Information Technology (Beijing) Co., LTD</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tao</surname></persName>
							<email>taowei@canon-ib.com.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Canon Information Technology (Beijing) Co., LTD</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tse-Wei</forename><surname>Chen</surname></persName>
							<email>twchen@ieee.org</email>
							<affiliation key="aff1">
								<orgName type="department">Device Technology Development Headquarters</orgName>
								<address>
									<country>Canon Inc</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kinya</forename><surname>Osa</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Device Technology Development Headquarters</orgName>
								<address>
									<country>Canon Inc</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masami</forename><surname>Kato</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Device Technology Development Headquarters</orgName>
								<address>
									<country>Canon Inc</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Knowledge Representing: Efficient, Sparse Representation of Prior Knowledge for Knowledge Distillation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite the recent works on knowledge distillation (KD) have achieved a further improvement through elaborately modeling the decision boundary as the posterior knowledge, their performance is still dependent on the hypothesis that the target network has a powerful capacity (representation ability). In this paper, we propose a knowledge representing (KR) framework mainly focusing on modeling the parameters distribution as prior knowledge. Firstly, we suggest a knowledge aggregation scheme in order to answer how to represent the prior knowledge from teacher network. Through aggregating the parameters distribution from teacher network into more abstract level, the scheme is able to alleviate the phenomenon of residual accumulation in the deeper layers. Secondly, as the critical issue of what the most important prior knowledge is for better distilling, we design a sparse recoding penalty for constraining the student network to learn with the penalized gradients. With the proposed penalty, the student network can effectively avoid the over-regularization during knowledge distilling and converge faster. The quantitative experiments exhibit that the proposed framework achieves the state-ofthe-arts performance, even though the target network does not have the expected capacity. Moreover, the framework is flexible enough for combining with other KD methods based on the posterior knowledge.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The deep neural network has achieved the significant improvement in different fields with years, but it also requires higher computational and memory costs. For the purpose to apply these networks to the real-time industrial tasks, the neural network compression <ref type="bibr" target="#b3">[4]</ref> is arguably Knowledge aggregation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sparse recoding</head><p>Teacher Net Student Net <ref type="figure">Figure 1</ref>. The pipeline of knowledge representing algorithm: The prior knowledge in teacher network is represented by the knowledge aggregation scheme into higher abstract level. Then the sparse recoding penalty is further used to regularize the gradients in student network for efficient learning these prior knowledge.</p><p>the most crucial strategy. As for the network compression problem, the typical solutions are designed to slim <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b36">37]</ref> the network directly, or quantify their parameters distributions <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25]</ref>, and filter the redundant layer dimensions <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21]</ref>. In contrast to these techniques which aim at directly compressing the network while preserving its performance as much as possible, an alternative solution is to preset a smaller target network as the student, and employ the knowledge from the larger network as teacher to improve student's performance. Therefore, knowledge distillation <ref type="bibr" target="#b10">[11]</ref> (KD) is proposed. The KD mainly assumes the samples distribution is anisotropy <ref type="bibr" target="#b0">[1]</ref>, but annotations of the samples are not able to represent this intrinsic. Based on the hypothesis, these methods evaluate the samples in the teacher network to produce the decision boundary as a strong posterior distribution, and then use to regularize the gradients optimization of student network. While this helps prevent the student network from being over-fitting, the extra risk of non-convergence is introduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:1911.05329v1 [cs.CV] 13 Nov 2019</head><p>A possible solution is to refine the posterior distribution from the teacher network, in order to provide more valuable knowledge for better distilling. The Neuron Selectively Transfer (NST) <ref type="bibr" target="#b11">[12]</ref> is proposed to align the distribution selectively with the Maximum Mean Discrepancy (MMD) metric, and the generative adversarial network with KD (KDGAN) <ref type="bibr" target="#b29">[30]</ref> is further used to produce a more robust decision boundary for student classifier. However, considering the student network which contains a very limited capacity -the representation ability, this limitation gradually becomes a major bottleneck in network training to further improve the performance of knowledge distillation. In a word, the fine-grained posterior distribution is usually underemployed.</p><p>With the constraint from network capacity, an instinctive approach is to introduce the parameters distribution <ref type="bibr" target="#b5">[6]</ref> from the teacher network as the prior knowledge <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b33">34]</ref>. For the typical one, Romero et al. <ref type="bibr" target="#b25">[26]</ref> constructs the Hint layer to estimate a parameters distribution with less filter numbers, through using the intermediate features representation of the teacher, and it uses these knowledge to guide the update of student parameters. However, the Hint layer suffers from the over-regularization if the teacher network is too deep.</p><p>In this paper, we produce a KD solution mainly focusing on modeling the prior knowledge, while avoiding the negative impacts from over-regularization, and the solution is flexible enough, for combining with other KD methods based on the posterior knowledge. Specially, we propose a knowledge representing (KR) framework, which aims at representing the prior knowledge at more abstract level, and taking full advantage of these knowledge. For answering the question of how to represent the prior knowledge, a knowledge aggregation scheme is firstly suggested. Inspired by the theory of optimal transportation <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>, the scheme is designed to alleviate the phenomenon of residual accumulation in the deeper layers. Then, as for the most critical issue of what the dominant prior knowledge is for better distilling, a sparse recoding penalty is proposed. Through employing a learnable threshold in the penalty, it can enhance the gradients of dominant neurons and smooth inactive ones. With these two proposed terms, the proposed framework can prompt the student network to preserve the key features of teacher network, even without a strong representation ability.</p><p>Our paper makes the following contributions:</p><p>? A new penalty is proposed to constrain the optimization of knowledge distillation. It helps the student network to avoid the over-regularization and converge faster. Moreover, the penalty can be further applied on other network optimization problems.</p><p>? A new scheme is suggested for aggregating the prior knowledge. It is able to produce more abstract features and alleviate the phenomenon of residual accumulation.</p><p>? According to the proposed framework, the more flexible architecture is allowable for both teacher and student network, without the constraints from model depths or filter scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The latest deep networks are usually accompanied with carefully designed modules <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> and enormous parameters. Though the performance of targeted tasks is obviously being improved, the computation and memory cost gradually become the challenge to employ these networks in real-life applications <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b27">28]</ref>. Comparing to the traditional neural network compression methods <ref type="bibr" target="#b3">[4]</ref> which focus on compressing the original network directly, a solution with the knowledge distillation to compress the deep network attracts more attention from research community in recent years, such as in the tasks of image recognition <ref type="bibr" target="#b33">[34]</ref>, object detection <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22]</ref>, or recommender systems <ref type="bibr" target="#b38">[39]</ref>, as the flexibility to obtain an arbitrary architecture of target network. In summary, the KD methods can be categorized into two main groups: 1) Distilling the posterior distribution from training data: Considering the possibility to extract the knowledge in an ensemble (teacher) into a single model (student), Hinton et al. <ref type="bibr" target="#b10">[11]</ref> introduces the idea of knowledge distillation as a regularizer. Through employing a penalized version <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b32">33]</ref> of final features of the teacher network, a joint learning is processed with the knowledge from posterior distribution. For refining the posterior distribution to provide more valuable knowledge, the Neuron Selectively Transfer (NST) <ref type="bibr" target="#b11">[12]</ref> is proposed to align the distribution selectively with the Maximum Mean Discrepancy (MMD) metric. Furthermore, considering the sample bias is unavoidable, the generative adversarial networks for knowledge distillation (KDGAN) <ref type="bibr" target="#b29">[30]</ref> is further used to produce a more robust posterior distribution for student classifier. However, these methods haven't take the capacity of student network into consideration, so the fine-grained posterior distribution is underemployed.</p><p>2) Distilling the prior distribution from model parameters: An alternative approach is to introduce the parameters distribution from teacher network as the prior knowledge <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36]</ref>. Romero et al. <ref type="bibr" target="#b25">[26]</ref> designs the Hint layer to estimate the parameters distribution by using the intermediate hidden layers from the teacher, and used the Hint layer to guide the distillation. Net2Net <ref type="bibr" target="#b2">[3]</ref> suggests a functionpreserving transform for extracting the prior knowledge from teacher network to initialize the parameters of the student network. And Yim et al. <ref type="bibr" target="#b33">[34]</ref> suggests a representation operator named FSP matrix. It uses not only the parameters distribution but the intermediate features from the neighbor layers. However, these methods either are constrained by the depth of teacher network, or suffer from the overregularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>For obtaining a student network that faithfully preserves the key representation ability of the teacher, Sec. 3.1 presents the objective function of the knowledge representing framework. Accordingly, we firstly answer the key problem of what the most important prior knowledge is for distilling in Sec. 3.2, through introducing the mathematical expression of the sparse recoding penalty. Then, we suggest how to represent the prior knowledge from the teacher network, with a knowledge aggregation scheme in Sec. 3.3. Finally, Sec. 3.4 shows the optimization procedure of the objective function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Knowledge Representing</head><p>As one of the most typical feature representation technique, the deep model produces the decision boundary through modeling the data distribution with the parameters in layers. Given a trained decision boundary y t (x, W t ), where y t is generated by teacher network with data distribution x and the parameters W t , the objective of knowledge distillation is to find the parameters W s for the student network. Specially, with the W s and x, the y s from student network is jointly optimized with the y t . Through minimizing the dissimilarity of two decision boundaries, the objective function of knowledge distillation is defined as:  where ? represents the metric for evaluating the similarity between the y t and y s , and the cross entropy, KDGAN <ref type="bibr" target="#b29">[30]</ref>, or NST <ref type="bibr" target="#b11">[12]</ref> are allowable. Different from the KD methods only evaluating the decision boundary, we further introduce a penalty ?(?) in Eq. 1, in order to measure the representation ability of student network. However, if the representation ability of student network is weak, the fine-grained posterior distribution will be underemployed. Then, we extend the objective function Eq. 1 through further introducing the prior knowledge W t from the teacher network, and the objective function is:</p><formula xml:id="formula_0">arg min W s N i=1 ?(y t i (x i , W t ), y s i (x i , W s )) + ??(W s ) (1)</formula><formula xml:id="formula_1">arg min W s ,W t N i=1 ?(y t i (x i , W t ), y s i (x i , W s )) + ?(W t , W s ) + ??(W t , W t ) + ??(W s )<label>(2)</label></formula><p>Instead of directly employing the parameter distributions W t from the teacher network as prior knowledge, we firstly represent these distributions as more abstract level, and a knowledge aggregation scheme ?(?) is suggested to aggregate W t into theW t . With the prior knowledgeW t , the ?(W t , W s ) is used to guide the update of parameters distributions W s for the student. Moreover, we propose a sparse recoding penalty to specify the ?(?). Through enhancing the magnitude of dominant gradients and filtering the inactive ones, the optimizer no longer requires the parameters distribution W s of student network to strictly close to the teacher one, and prompts the student network to firstly learn with the most valuable knowledge. In summary, the optimization procedure is represented in Algorithm 1, and we leave over the details in following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Sparse Recoding Penalty</head><p>As demonstrated by previous works <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38]</ref>, prompting the neurons connection being sparse is beneficial for obtaining a well generalization ability. However, such penalties are designed to directly clip the parameters distribution, and the extra risk of over-regularization is introduced. After we analyze the distribution of prorogated gradients in the previous KD methods, we found that major reason for the convergence of oscillatory is that the gradients are not discriminative enough, especially in the student network with a weak representation ability.</p><p>Therefore, we propose a sparse recoding penalty ?(?), which can penalize the prorogated gradients during the training of deep network. Given an input parameters tensor W , it enhances the high gradients g j of dominant neurons, and filters the low gradients of inactive neurons. The function is defined as:</p><formula xml:id="formula_2">?(W ) = j ? 0 (g j )<label>(3)</label></formula><p>where</p><formula xml:id="formula_3">? 0 (g) = 1 ? (|g| + g 2 ), if |g| ? ? 0, otherwise<label>(4)</label></formula><p>where ? 0 (?) is a piecewise function that enhances the gradients when |g| ? ?, and smooths the |g| by zero in others. The ? is a learnable threshold within the update of gradient optimization, and it is initialized with the mean value of parameters distribution. For fairly comparing with other penalties, the <ref type="figure" target="#fig_0">Fig.2</ref> shows the curves of ?(?) by comparing with the L 1 and L 2 norms. It exhibits that ?(?) is a more strict sparse constraint. Moreover, with different parameter setting, properties of the sparse recoding penalty are shown in the figure, and we leave over the further discussion in experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Deep Knowledge Aggregation</head><p>For representing the prior knowledge as more abstract level, we design a deep knowledge aggregation scheme through stacking the neighbor layers in a very deep network. Specially, with the analysis of prior knowledge distilling in previous methods, we notice that the optimization errors between two networks will be accumulated from layers, since the higher layer in teacher network usually contains a strong representation ability. However, the situation is simply regarded as the phenomenon of gradient vanishing, and cause an over-regularization if the teacher network is too deep. So we name this phenomenon as the residual accumulation, and the proposed scheme will mainly considers this phenomenon. Based on the theory of optimal transportation <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>, the scheme try to reduce the residual accumulation during gradient optimization, through minimizing the inter-domain transportation cost. Given a P 1 and P 2 being two distribution space with probability measures ? and ? respectively, the transportation T preserving P 1 ? P 2 has equal total measure ?(T (p 1 )) = ?(p 2 )</p><p>where p 1 and p 2 is any measurable subset of P 1 and P 2 . Then the total transportation cost for sending p 1 ? P 1 to p 2 ? P 2 by transportation cost ? (p 1 , p 2 ) can be defined by</p><formula xml:id="formula_5">min T :P1?P2 P1 ? (p 1 , T (p 1 ))d?(p 1 )<label>(6)</label></formula><p>With minimizing the total transportation cost, the distribution P 2 progressively approximates P 1 on measures ?. Assuming a series of neighbor layers k , ..., n as set ? n k , for sending parameter distribution W ? n k toW with measurable subset w ? W k ,..., n , the deep knowledge aggregation scheme merges the neighbouring layers to form the higher abstract parameters knowledge. In this case, the function ?(?) is formulated as</p><formula xml:id="formula_6">?(W , W ? n k ) = min T :W ? n k ?W W ? n k ? (w, T (w))d?(w) (7)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Optimization</head><p>Instead of directly optimizing the proposed objective function, we design an joint optimization method as the alternative solution. In details, our method uses two stages optimization to alternatingly solve the Eq. 2.</p><p>OptimizingW t with W t ? n k Given an elaborate teacher network with parameter distribution W t ? n k , we first aggregate the knowledgeW t with T (W t ? n k ) in here as:</p><formula xml:id="formula_7">arg mi? W t N i=1 ?(y t i (x i , W t ), y s i (x i , W s )) + ? W t ? n k ? (w t , T (w t ))d?(w t )<label>(8)</label></formula><p>As the Eq. 8 involves a transportation cost and the definition of probability measures, it is difficult to directly integrate with gradient descent optimizer. In this case, we use the feature representation F W as an approximation probability measures, which means the set of features maps F generated by parameters set W . If the transportation cost ? (?) is defined as the simple L 2 distance, we revise the Eq. 8 as:</p><formula xml:id="formula_8">arg mi? W t N i=1 ?(y t i (x i , W t ), y s i (x i , W s )) + ??(W t ? n k ) F W t ? n k ? FW t 2<label>(9)</label></formula><p>where ? is a predefined parameter to control the penalty from optimal transportation. The ?(W t ? n k ) as a measures function is used to penalize more on the layer with higher accumulation error, and the standard deviation is employed here. Moreover, we remove the part of terms during the derivation for Eq. 8 for fast computation. Then, the solution ofW t can be obtained by gradient descent optimization.</p><p>Optimizing W s withW t Given an aggregate knowledg? W t , our goal here is further to solve the W s on student network with sparse recoding penalty, as:</p><formula xml:id="formula_9">arg min W s ?(W t , W s ) + ? j ? 0 (g s j )<label>(10)</label></formula><p>where ? 0 (g s j ) is designed for prompting the student network to firstly learn with the penalized gradients, and the parameter ? is predefined to control the importance of the sparse recoding penalty.</p><p>Instead of directly solving the global optimum for objective function Eq. 2, the two sub-objective functions Eq. 9 and Eq. 10 are designed to overcome the conflict between optimizing the prior knowledge and posterior knowledge simultaneously. Through alternatively minimizing the distribution dissimlarity ?(W t , W s ) and ?(y t , y s ), the optimization for Eq. 2 is regarded as an joint optimization procedure. Once the posterior knowledge is dominant during optimization, the optimizer for prior knowledge will penalize the total loss more, and the opposite is also. The gradient is only allowed to descend on the direction that makes both two optimizers are optimal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we evaluate the proposed knowledge distillation framework with several benchmark datasets. For the base of experiments, we use the deep residual network <ref type="bibr" target="#b7">[8]</ref> as the network architecture, and the excerpt of the proposed framework in this architecture is shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. The c in residual module means the number of aggregated convolution layers. For the problem of optimizing these layers with different spatial scales, the identity mapping (ID) layer <ref type="bibr" target="#b34">[35]</ref> is employed also. To ensure a fair comparison, the same data augment strategies are used. Moreover, we employ the similar settings of learning rates, optimization iterations and computation precision (32 float points). The implementation details will be shown in corresponding subsections.</p><p>In Sec. 4.1, through comparing with the typical penalties, the property of the sparse recoding penalty is analysed. Then, through comparing with the state-of-the-arts, we evaluate the performance of student networks in general image recognition tasks, and further explore their generalization ability in a revised dataset TCIFAR-100, as de-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Analysis of Proposed Penalty</head><p>As for the sparse recoding penalty, its property through comparing with typical methods is analysed, and we further explore the reason of why the proposed penalty is able to boost the convergence of knowledge distilling. Based on the experiment result, we address that the proposed penalty can be applied on other network optimization problems if the gradients distribution is not discriminative enough.</p><p>Penalty Property Given a specific parameters distribution, the traditional penalties <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b37">38]</ref> form a convex function and obtain the maximal reward in the unique extreme. It penalizes the parameter with higher value to reduce the total loss, for encouraging the value of parameter to close to 0. In contrast to these methods, the sparse recoding penalty is designed to penalize the gradients directly. For the propagated gradients, it filters the gradients with an equal reward within the learnable threshold, in order to slow down the update of inactive neurons. For the gradients out of the threshold, it boosts the update to highlight the dominant neurons. For validating our hypothesis, we visualize the convolutional kernels with the constraint by different penalties in image recognition tasks. The <ref type="figure">Fig. 4</ref> shows that the sparse recoding penalty can prompt the parameters distribution of the network to be more sparse, through directly regularizing the optimized gradients.</p><p>Convergence We have observed fast convergence in our experiment result. In <ref type="figure">Fig. 5</ref>, it illustrates the training loss on MNIST over the beginning 20,000 iteraitons. The student network with sparse recoding penalty is better than the traditional penalties. We think one possible reason is that the proposed penalty is designed to penalize the gradients firstly, so it can produce a bigger step for gradients descent <ref type="bibr" target="#b37">[38]</ref>  <ref type="figure">Figure 4</ref>. With the penalized gradients, the sparse recoding penalty is also to produce more discriminative parameters distribution (best see in color).</p><p>in the beginning of network training. Moreover, we evaluate the different types for initializing the parameters distribution in the experiment, and we also found the similar conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Performance Analysis</head><p>In this section, we firstly conduct the experiments in the image recognition task on CIFAR-10, CIFAR-100 <ref type="bibr" target="#b17">[18]</ref> and ILSVRC 2012 <ref type="bibr" target="#b4">[5]</ref>, in order to evaluate the performance of the proposed knowledge representing framework with the state-of-the-arts. Then, we design a TCIFAR-100 based on CIFAR-100, for further verifying their generalization ability. As the focus of this experiment is analysing the performance of student network with a small capacity, so we reserve the comparison on different tasks as future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">CIFAR-10</head><p>The CIFAR-10 is an image recognition dataset <ref type="bibr" target="#b17">[18]</ref> which includes 50,000 training images and 10,000 test images, and per training class has 5,000 images while test class has 1000 <ref type="bibr" target="#b37">[38]</ref> [31] <ref type="figure">Figure 5</ref>. Convergence speed; the traditional penalties <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b37">38]</ref> and the sparse recoding penalty (best see in color).</p><p>images. For all images, they store in RGB format with size of 32?32. We use a trained teacher network with 26 layers, which is structured as 5 residual modules. For student network, it contains 8 layers with 2 residual modules, which has roughly 1/3 parameters of the teacher. In details, with the same parameters settings and training strategies, we reduce about 1/3 number of the filters on each layer for the student network, in order to evaluate the case if the target network contains a weak representation ability. And we set the c of knowledge aggregation as 3, which aggregates each three layer of teacher network into higher abstract level for one layer in student network.</p><p>Accuracy Params Teacher ResNet-26 91.91 ? 0.36M Student  87.91 ? 0.12M FitNet <ref type="bibr" target="#b25">[26]</ref> 88.57 ? 0.12M FSP <ref type="bibr" target="#b33">[34]</ref> 88.70 ? 0.12M Proposed-Dense 89.11 ? 0.09M Proposed 90.65 ? 0.09M NTS <ref type="bibr" target="#b11">[12]</ref> 88.98 ? 0.12M KDGAN <ref type="bibr" target="#b29">[30]</ref> 88.62 ? 0.12M Proposed + KDGAN <ref type="bibr" target="#b29">[30]</ref> 91.35 ? 0.09M <ref type="table">Table 1</ref>. ResNet-8 in CIFAR-10 Classification rates(%). Proposed: the KR framework. Proposed-Dense: the KR framework but removing the sparse recoding penalty.</p><p>In Tab. 1, it summarizes the obtained results. Based on the proposed framework, the student network which contains less parameters wins the methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b33">34]</ref> focusing on prior knowledge with a significant improvement. For the state-of-the-arts <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">30]</ref> by modeling the posterior knowledge, the proposed framework also achieves the comparable performance. For the self-comparison, we remove the sparse recoding penalty in KR framework and name it as the KR-Dense. And the experiment proves the importance to sparsely penalize the gradients during the distilling optimization, if the student network only has a small capacity. Besides, through combining with the KDGAN <ref type="bibr" target="#b29">[30]</ref>, a further improvement confirms that our method is flexible for the extension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">CIFAR-100</head><p>The CIFAR-100 is an augmented version of CIFAR-10. It contains the same amount of images and size of CIFAR-10, which includes 50,000 training images and 10,000 test images, so only has 100 samples per class. Similar the setting to CIFAR-10, we use a trained teacher network with 32 layers as 6 residual modules, and student is composed of 14 layers as 3 residual modules. Besides, the reduction of about 1/3 filter number is still used, and c is set as 3.</p><p>Tab. 2 shows results of student network with evaluated Accuracy Params Teacher ResNet-32 64.06 ? 0.46M Student  58.65 ? 0.19M FitNet <ref type="bibr" target="#b25">[26]</ref> 61.28 ? 0.19M FSP <ref type="bibr" target="#b33">[34]</ref> 63.33 ? 0.19M Proposed Method 63.95 ? 0.17M NTS <ref type="bibr" target="#b11">[12]</ref> 63.78 ? 0.19M KDGAN <ref type="bibr" target="#b29">[30]</ref> 63.96 ? 0.19M Proposed Method + KDGAN <ref type="bibr" target="#b29">[30]</ref> 63.98 ? 0.17M <ref type="table">Table 2</ref>. ResNet-14 in CIFAR-100 Classification rates(%). With the similar network architecture, we further reduce the output channels in each layer for saving the total parameters. methods. Though the proposed method achieves the comparable performance than the state-of-the-arts <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">30]</ref> with less parameters, the improvement for our method is not obvious. We think one possible reason is that the ResNet-14 has a stronger representation ability that the ResNet-8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">ILSVRC 2012</head><p>The ILSVRC 2012 classification challenge involves the recognition task to classify one image into 1,000 leaf-node categories in the ImageNet hierarchy <ref type="bibr" target="#b18">[19]</ref>. It has about 1.2 million images for training, 50,000 for validation and 100,000 testing images. Although training the very deep network on such enormous datasets to achieve satisfied performance has been a solvable issue, how to obtain the comparable performance with a tiny network by the knowledge distillation still confuses the research community, especially for the methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b33">34]</ref> with prior knowledge. We think the major reason is that the depth of teacher network in ILSVRC 2012 is very deep, so the student network in these methods seriously suffers from the overregularization. Tab. 3 shows the errors of Top-1 and Top-5. With the c which is set as 4 in knowledge aggregation scheme, we found the situation of over-regularization is alleviated, and it prompts the KR framework to achieve the better performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Generalization Ability</head><p>We further explore the generalization ability of previous methods and the proposed framework. Based on the data resource from CIFAR-100, we reproduce the CIFAR-100 as the TCIFAR-100 with the data distortion strategies. In details, each image in training and test set is distorted by the artifacts, from a gaussian distribution (? = 1) with the random sample. The <ref type="figure" target="#fig_3">Fig. 6</ref> shows the examples. In Tab. 4, it shows the proposed framework achieves a significant improvement than state-of-the-arts. We believe the KR framework is able to produce a student network with stronger generalization ability, since the joint optimization prevents the optimizer from being trapped in local extremum. 54.37 ? 0.19M FitNet <ref type="bibr" target="#b25">[26]</ref> 56.77 ? 0.19M FSP <ref type="bibr" target="#b33">[34]</ref> 57.31 ? 0.19M Proposed Method 60.03 ? 0.17M NTS <ref type="bibr" target="#b11">[12]</ref> 57.88 ? 0.19M KDGAN <ref type="bibr" target="#b29">[30]</ref> 58.15 ? 0.19M Proposed Method + KDGAN <ref type="bibr" target="#b29">[30]</ref> 60.33 ? 0.17M <ref type="table">Table 4</ref>. ResNet-14 in TCIFAR-100 Classification rates(%). The transformed CIFAR-100 dataset is reproduced by the CIFAR-100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Optimization Discussion</head><p>In this section, we further discuss the implementation details of optimizing the proposed framework, and analysis the optimization procedure with different settings. Implementation Details As for the training on CIFAR-10 and CIFAR-100, the learning rate for Eq. 9 is set as 0.1, and was changed to 0.01, and 0.001 at two steps (30k and 48k) respectively. The optimizer for Eq. 10 started at a smaller learning rate 0.01, but also is reduced according to similar strategies. For the ILSVRC 2012, the learning rate for Eq. 9 is set as 0.1 with a ploy decreasing in each 6 epoch, and the optimizer for Eq. 10 started at learning rate 0.005. The weight decay of 0.00001 and momentum of 0.9 are all used. For the works related to quantization strategies <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b24">25]</ref>, we try to evaluate the performance if combining these works with our framework. Since the quantization techniques transfer the parameters distribution into a discrete space, we found the optimization will be seriously impacted and convergence performance also be influenced. However, this analysis is out of the scope of this paper, so it is left as future work.</p><p>Joint Optimization For optimizing theW t with W t by Eq. 9 and the W s withW t by Eq. 10, we use two different optimizers to separately training these two sub-objective functions. Moreover, we tried different initialization techniques for parameters, and we found the objective function is harder to converge, if the initialization onW t is very different from W s . We also consider the types for different optimizers <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b28">29]</ref>. Through changing the two optimizers as Adam <ref type="bibr" target="#b16">[17]</ref> or RMS <ref type="bibr" target="#b28">[29]</ref>, we found it caused a performance oscillation but less than 1%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a knowledge representing (KR) framework mainly focusing on modeling the parameters distribution as prior knowledge. We suggest a knowledge aggregation scheme to represent the parameters knowledge from teacher network into more abstract level, for alleviating the phenomenon of residual accumulation in the deeper layers. We also design a sparse recoding penalty for constraining the student network to learn with the penalized gradients. It helps the student network to avoid the over-regularization during knowledge distilling and converge faster. In conclusion, the proposed framework can prompt the student network to preserve the key features of teacher network, even though the student network does not have a strong representation ability.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>The properties of proposed sparse recoding penalty (best viewed in color): (a) the sparse recoding penalty is able to approximate more strict sparseness; (b) the properties with different setting of ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The excerpt of proposed framework on the residual network. scribed in Sec. 4.2. Finally, the discussions about the optimization procedure of the proposed framework is shown in Sec. 4.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>left CIFAR-100; right TCIFAR-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Aggregation knowledgeW t and optimized weights of student network W s Initialization:W t , W s , M axIter; while less than the MaxIter do OptimizingW</figDesc><table /><note>Algorithm 1: Training the Knowledge Representing Algorithm Input: Weights W t of teacher network and W s of student network Output:t with W t ; Knowledge aggregation for teacher in Eq. 9 ; Optimizing W s withW t ; Sparse recoding for student in Eq. 10 ; end</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We thanks all reviewers for providing the constructive suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bayesian dark knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Anoop Korattikara Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">P</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3438" to="3446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning efficient object dectection models with knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guobin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Net2net: Accelerating learning via knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A survey of model compression and acceleration for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09282</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Predicting parameters in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><surname>Shakibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><forename type="middle">De</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2148" to="2156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laurens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Channel pruning for accelerating very deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1389" to="1397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Knowledge distillation with adversarial samples supporting decision boundary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsik</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for the Advance of Artificial Intelligence</title>
		<meeting>Association for the Advance of Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="38" to="39" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Like what you like: Knowledge distill via neuron selectivity transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01219</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Data-driven sparse structure selection for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="304" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Quantized neural networks: Training neural networks with low precision weights and activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="6869" to="6898" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Quantization and training of neural networks for efficient integer-arithmetic-only inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Skirmantas</forename><surname>Kligys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2704" to="2713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Stochastic gradient tricks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bottou</forename><surname>Leon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">7700</biblScope>
			<biblScope unit="page">430445</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Thinet: A filter level pruning method for deep neural network compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyao</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5058" to="5066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Face model compression by distilling knowledge from neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for the Advance of Artificial Intelligence</title>
		<meeting>Association for the Advance of Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjovsky</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chintala</forename><surname>Soumith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bottou</forename><surname>Leon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A geometric view of optimal transportation and generative model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kehua</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tung</forename><surname>Shing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David Xianfeng</forename><surname>Yau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Xnor-net: Imagenet classification using binary convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="525" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fitnets: Hints for thin deep nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Towards a mathematical understanding of the difficulty in learning with feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Rmsprop: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Kdgan: Knowledge distillation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhong</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="783" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning structured sparsity in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunpeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2074" to="2082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Beyond sparsity: Tree regularization of deep models for interpretability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonali</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurizio</forename><surname>Parbhoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Zazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Finale</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doshi-Velez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for the Advance of Artificial Intelligence</title>
		<meeting>Association for the Advance of Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning loss for knowledge distillation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chang</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A gift from knowledge distillation: Fast optimization, network minimization and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junho</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donggyu</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihoon</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning strict identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikumar</forename><surname>Ramalingam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">l1-regularized neural networks are improperly learnable in polynomial time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="993" to="1001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Rocket launching:a universal and efficient framework for training well-performing light net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guorui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runpeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gai</forename><surname>Kun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for the Advance of Artificial Intelligence</title>
		<meeting>Association for the Advance of Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
