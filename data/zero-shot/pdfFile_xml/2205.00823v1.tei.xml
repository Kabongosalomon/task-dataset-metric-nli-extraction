<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CenterCLIP: Token Clustering for Efficient Text-Video Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher>ACM</publisher>
				<availability status="unknown"><p>Copyright ACM</p>
				</availability>
				<date>July 11-15, 2022. 2022. July 11-15, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhao</surname></persName>
							<email>zhaoshuaimcc@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CCAI</orgName>
								<orgName type="institution" key="instit2">Zhejiang University Hangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
							<email>zhulinchao7@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">ReLER Lab, AAII</orgName>
								<orgName type="institution">University of Technology Sydney Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">CCAI</orgName>
								<address>
									<addrLine>Zhejiang University Hangzhou</addrLine>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<email>yangyics@zju.edu.cn</email>
							<affiliation key="aff3">
								<orgName type="institution">CCAI</orgName>
								<address>
									<addrLine>Zhejiang University Hangzhou</addrLine>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhao</surname></persName>
							<affiliation key="aff4">
								<orgName type="laboratory">ACM Reference Format</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
							<affiliation key="aff4">
								<orgName type="laboratory">ACM Reference Format</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Wang</surname></persName>
							<affiliation key="aff4">
								<orgName type="laboratory">ACM Reference Format</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<affiliation key="aff4">
								<orgName type="laboratory">ACM Reference Format</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CenterCLIP: Token Clustering for Efficient Text-Video Retrieval</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;22)</title>
						<meeting>the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;22) <address><addrLine>Madrid, Spain; Madrid, Spain; New York, NY, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>ACM</publisher>
							<biblScope unit="volume">12</biblScope>
							<date type="published">July 11-15, 2022. 2022. July 11-15, 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3477495.3531950</idno>
					<note>ACM ISBN 978-1-4503-8732-3/22/07. . . $15.00 [CLASS] embedding</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Text-video retrieval</term>
					<term>CLIP</term>
					<term>transformer</term>
					<term>token clustering</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, large-scale pre-training methods like CLIP have made great progress in multi-modal research such as text-video retrieval. In CLIP, transformers are vital for modeling complex multi-modal relations. However, in the vision transformer of CLIP, the essential visual tokenization process, which produces discrete visual token sequences, generates many homogeneous tokens due to the redundancy nature of consecutive and similar frames in videos. This significantly increases computation costs and hinders the deployment of video retrieval models in web applications. In this paper, to reduce the number of redundant video tokens, we design a multi-segment token clustering algorithm to find the most representative tokens and drop the non-essential ones. As the frame redundancy occurs mostly in consecutive frames, we divide videos into multiple segments and conduct segment-level clustering. Center tokens from each segment are later concatenated into a new sequence, while their original spatial-temporal relations are well maintained. We instantiate two clustering algorithms to efficiently find deterministic medoids and iteratively partition groups in high dimensional space. Through this token clustering and center selection procedure, we successfully reduce computation costs by removing redundant visual tokens. This method further enhances segment-level semantic alignment between video and text representations, enforcing the spatio-temporal interactions of tokens from within-segment frames. Our method, coined as CenterCLIP, surpasses existing state-of-the-art by a large margin on typical textvideo benchmarks, while reducing the training memory cost by 35% and accelerating the inference speed by 14% at the best case. The code is available at https://github.com/mzhaoshuai/CenterCLIP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Information systems ? Novelty in information retrieval.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: t-SNE <ref type="bibr" target="#b49">[50]</ref> visualization of video token embeddings of CLIP. The shown similar image patches within a cluster are from different temporal frames in the same video. Best viewed in color with 300% zoom.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Text-video retrieval is less studied than the commonly known textimage retrieval task as the intricate context of the video, especially when the length of the video is very long or the temporal variation of the video is large. With the explosive growth of video content on mobile phones and Internet during the past decade, text-video retrieval becomes increasingly popular. People also desire a better text-video retrieval system as searching for videos of interest already becomes a part of daily lives of most people. Recently, with the success of large-scale contrastive languageimage pre-training methods like CLIP <ref type="bibr" target="#b38">[39]</ref>, text-video retrieval also has made great progress. To be specific, CLIP4clip <ref type="bibr" target="#b31">[32]</ref> transfers the knowledge of CLIP to text-video retrieval tasks, surpassing the previous state-of-the-art methods by a large margin (e.g., more than 30% improvement of the recall metric on ActivityNet <ref type="bibr" target="#b11">[12]</ref>). This demonstrates the power of billion-scale image-text pairs pretraining via contrastive learning. In CLIP, a vision transformer <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b50">51]</ref> is adopted for visual representation learning. Typically, in vision transformer, visual tokenization, i.e., linear projection of non-overlapped image patches to an embedding space, is a necessary component to produce discrete visual token sequences. Then token sequences can be processed by the multi-head self-attention (MHSA) in transformer blocks as the same manner of dealing with text sequences in the original transformer <ref type="bibr" target="#b50">[51]</ref>.</p><p>When the input of the vision transformer becomes videos, the visual tokenization procedure produces many homogeneous tokens due to the redundancy nature in continuously changing frames. In <ref type="figure">Figure 1</ref>, we extract the token embedding of CLIP from different frames in the same video and visualize them by t-SNE <ref type="bibr" target="#b49">[50]</ref>. From the visualization, we can see those token embeddings from different frames form many tight clusters. Image patches with similar texture features correspond to immediate data points within a certain cluster. It is also clear that the number of clusters and the average number of tokens in clusters are not small, i.e., there are many similar token embedding in high-dimensional space. As a result, repeated computation of these homogeneous tokens in CLIP inevitably introduces a lot of unnecessary computation costs and hinders the training and deployment of video retrieval models in web applications. To resolve the above problem, in this work, we propose to distinguish the most representative tokens, i.e., the center token of each cluster in <ref type="figure">Figure 1</ref>, and only use these typical tokens for visual representation learning as these tokens contribute most to the discriminative feature representation learning.</p><p>We introduce a multi-segment token clustering algorithm to find the most representative tokens to reduce computation costs, and achieve segment-level semantic alignment of video and text representation. An input video is divided into multiple temporal segments. Each segment contains the same number of consecutive frames. Given the token embeddings of these frames, a clustering algorithm is performed on each segment independently. After clustering, only center tokens of clusters are reserved and non-center tokens are dropped to avoid duplicated computation of similar tokens. This significantly reduces computation costs. Center tokens from the same temporal segment are then concatenated into a new visual sequence and arranged according to their original spatial-temporal positions, i.e., tokens whose image patches occur earlier in the video would appear at the earlier position of the new visual sequence. Then the new visual sequence is processed by the standard transformer blocks. This enables the model to learn segment-level video representation via attention among tokens from within-segment frames. These segment-level video representations are aligned with the text through contrastive learning.</p><p>In this work, we introduce two instances of clustering algorithm in multi-segment token clustering. One is k-medoids equipped with a deterministic centroids initialization method, i.e., KKZ initialization <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b47">48]</ref>, to ensure the clustering results are consistent through multiple runs. A good initialization also helps the clustering algorithm converge fast. The other is spectral clustering which suits high-dimensional data points clustering. With our multi-segment token clustering algorithm, CenterCLIP achieves state-of-the-art performance on four common benchmarks: MSR-VTT <ref type="bibr" target="#b59">[60]</ref>, MSVD <ref type="bibr" target="#b6">[7]</ref>, LSMDC <ref type="bibr" target="#b41">[42]</ref>, and ActivityNet <ref type="bibr" target="#b11">[12]</ref>. We achieve significant improvement of retrieval metrics on all these four datasets compared to the baseline. At the same time, we achieve a decent reduction in memory cost and speed up the inference process. Specifically, on ActivityNet, we achieve a 35% reduction in memory cost and 14% speedup of inference speed compared to the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>Contrastive Vision-Language Pre-Training. Since the success of derivative works of Contrastive Language-Image Pre-Training (CLIP) <ref type="bibr" target="#b38">[39]</ref> in different areas <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b58">59]</ref>, visual representation learning under text supervision attracts widespread attention. Huge models pre-trained on billion-scale image-text pairs from web like WenLan <ref type="bibr" target="#b17">[18]</ref>, Google's ALIGN <ref type="bibr" target="#b20">[21]</ref>, and Microsoft's Florence <ref type="bibr" target="#b64">[65]</ref> emerged. In the language-video understanding area, there are similar works like Frozen in Time <ref type="bibr" target="#b1">[2]</ref> and HowTo100M <ref type="bibr" target="#b34">[35]</ref>. However, the scale of language-video pre-training is much smaller than language-image pre-training as the former is much more expensive. Following CLIP4clip <ref type="bibr" target="#b31">[32]</ref>, we transfer the knowledge of CLIP to the text-video retrieval task in this work. Text-video Retrieval. Text-video retrieval is more complex than commonly studied text-image retrieval as the additional temporal dimension introduces complex context information. Previously, standard language-video learning methods tend to design dedicated fusion manners for cross-model learning from offline extracted video and text features <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b63">64]</ref>. Recently, the paradigm of end-to-end large-scale pre-training plus task-specific finetune becomes more and more popular for language-video understanding, e.g., HowTo100M <ref type="bibr" target="#b34">[35]</ref>, MIL-NCE <ref type="bibr" target="#b33">[34]</ref>, ActBERT <ref type="bibr" target="#b68">[69]</ref>, VideoBERT <ref type="bibr" target="#b48">[49]</ref>, MMT <ref type="bibr" target="#b12">[13]</ref>, and HERO <ref type="bibr" target="#b27">[28]</ref>. These methods achieve promising results on many language-video tasks and demonstrate the effectiveness of pre-training. Our work is also in this line, the difference is that we inherit the knowledge from CLIP <ref type="bibr" target="#b38">[39]</ref>, which is pre-trained on image-text pairs rather than video-text pairs. Efficient Transformer. Recently, transformer becomes the unified model for many vision and text tasks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b50">51]</ref>. However, there are many time-consuming operations in transformers such as self-attention and softmax operations. Some works try to reduce the complexity of self-attention for very long sequences or remove the softmax operation, e.g., Performer <ref type="bibr" target="#b8">[9]</ref>, Linear Transformer <ref type="bibr" target="#b21">[22]</ref>, Linformer <ref type="bibr" target="#b54">[55]</ref>, Reformer <ref type="bibr" target="#b24">[25]</ref>, Sparse Transformer <ref type="bibr" target="#b7">[8]</ref>, Routing Transformer <ref type="bibr" target="#b43">[44]</ref>, Longformer <ref type="bibr" target="#b2">[3]</ref>, and Galerkin Transformer <ref type="bibr" target="#b5">[6]</ref>. Very recently, in computer vision, people also notice that not all tokens matter for the final performance of the model. To reduce the computation cost, researchers try to learn a few most representative visual tokens <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b56">57]</ref>, learn to rank all tokens and select the most important ones <ref type="bibr" target="#b52">[53]</ref>, and learn to mask the unimportant tokens <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b60">61]</ref>. Compared to these mentioned methods, we are parameterfree and introduce segment level semantic alignment of text and video representation for text-video retrieval.  <ref type="figure">Figure 2</ref>: The overall framework of CenterCLIP and multi-segment clustering strategies. In this case, the video is divided into three segments and each contains three frames. Clustering is performed independently on tokens of each segment, and center tokens of all clusters from one segment are selected and concatenated into a new sequence. Via attention on this new sequence, the visual model is able to learn features that contain segment-level video semantics. This helps the whole text-video retrieval model to achieve segment-level semantic alignment between video and text while reducing computation costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminary</head><p>Given a video set V and text set T , the goal of text-video retrieval is to learn a score function f , which gives a high similarity score f ( , ) if a video ? V and a text ? T are highly relevant and a low similarity score for an irrelevant video-text pair. Then we can rank videos according to the query text (text to video retrieval) or rank texts according to the query video (video to text retrieval).</p><p>Following the typical multi-modal retrieval frameworks <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b38">39]</ref>, our text-video retrieval model is composed of a text encoder g and a video encoder h. Given a text and a video , g( ) and h( ) produce the normalized high-dimensional feature of the input, where ? 2 normalization is often considered in final feature encoding. Then the similarity score of this text-video pair ( , ) is</p><formula xml:id="formula_0">f ( , ) = h( ) g( ).</formula><p>(</p><p>In training, a video-text pair ( , ) is treated as the positive if and are corresponded. All other instances of video or text in the mini-batch are treated as the negative. The text and video encoders are optimized in an end-to-end manner via normalized softmax loss <ref type="bibr" target="#b65">[66]</ref>. The overall loss L is the average of video-to-text classification loss (L 2 ) and text-to-video classification loss (L 2 ):</p><formula xml:id="formula_2">L 2 = ? 1 ?? log exp(h( ) g( )/ ) =1 exp(h( ) g( )/ ) ,<label>(2)</label></formula><formula xml:id="formula_3">L 2 = ? 1 ?? log exp(g( ) h( )/ ) =1 exp(g( ) h( )/ ) ,<label>(3)</label></formula><formula xml:id="formula_4">L = 1 2 (L 2 + L 2 ),<label>(4)</label></formula><p>where is the mini-batch size and is the temperature to scale the logits. It is worth noting that is crucial because both h( ) and g( ) are normalized. We set it as a trainable parameter following the CLIP model. During training, our model is initialized from the pre-trained weight of CLIP. We describe the details of the text encoder and video encoder below.</p><p>3.1.1 Text encoder. We instantiate the text encoder using the text model of CLIP. It is a transformer <ref type="bibr" target="#b50">[51]</ref> with the architecture modifications described in BERT <ref type="bibr" target="#b39">[40]</ref>, i.e., only encoder and no decoder. A transformer model typically consists of repeated blocks (layers) of multi-head self-attention (MHSA) and feed-forward networks (FFN). We use a transformer with 12 layers and 512 width with 8 attention heads, where the width is the dimension of the query, key, and value feature. The text tokenizer is a lower-cased byte pair encoding (BPE) <ref type="bibr" target="#b45">[46]</ref> with a 49 152 vocab size. The text sequence is padded with [SOS] and [EOS] tokens.</p><p>[SOS] and [EOS] is padded at the beginning and end of the text sequence, respectively.</p><p>The final text feature representation is the activation from the last layer of the transformer that corresponds to the [EOS] token. This text representation is later normalized by layer normalization and linearly projected into the joint video-text embedding space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Video encoder.</head><p>Our video encoder is a vision transformer (ViT), which first successfully applied transformers in vision tasks. The architecture of ViT is the same as the transformer in natural language processing, except ViT introduces an additional visual tokenization process to convert images into discrete sequences. When feeding images or videos into a ViT, we first convert the nonoverlapped image patches into visual sequences, where a [CLASS] token is prepended to the beginning of sequences as BERT <ref type="bibr" target="#b39">[40]</ref>.</p><p>Then the output of [CLASS] token at the final layer is extracted as the visual representation. In this work, we adopt a 2D linear projection to project image patches of different frames into an embedding space independently following the practice of CLIP4clip <ref type="bibr" target="#b31">[32]</ref>. For convenience, we name this linear transformation process as visual tokenization. Generally, we use a ViT-B/32 model <ref type="bibr" target="#b10">[11]</ref> with 12 layers and 512 width with 8 attention heads. ViT-B/32 means the nonoverlapped input image patch size is 32 ? 32. When applying the visual tokenization process to videos, it inevitably produces many redundant tokens as shown in <ref type="figure">Figure 1</ref>. Generally, an input video consists of many temporal related frames: When performing text-video retrieval on long videos, the total number of tokens for a video is large. For example, videos in the ActivityNet <ref type="bibr" target="#b11">[12]</ref> dataset usually have a few minutes duration. In this case, | | will be easily larger than 1 000. The redundant tokens considerably increase computation costs. To make the training and inference of text-video retrieval models more efficient, we propose to use clustering algorithms to find the most representative token embeddings. This process significantly reduces the number of tokens while maintaining the most valuable information of original tokens. After clustering, we only reserve the center tokens and remove other non-center tokens. The reserved tokens contain most of the information about the video and it is sufficient for text-video retrieval. We describe our multi-segment token clustering in the next section.</p><formula xml:id="formula_5">= { 1 , 2 , . . . , | | }, where | | is</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-segment Token Clustering</head><p>The overall framework of our video encoder can be found in <ref type="figure">Figure 2</ref>. We perform a multi-segment clustering strategy on visual tokens from a certain temporal segment. This is based on the assumption that neighbor frames are more likely to be the same; then tokens of these similar neighbor frames are more possible to be redundant. Our multi-segment token clustering method empowers the model to achieve segment-level semantic alignment of text and video representations. Previously, CLIP and CLIP4clip adopt the average of frame features or the late fusion of frame features as the video representation. However, the former loses the temporal information, and the latter is a post-processing step and loses the detail temporal variations at the early stage of the transformer. By clustering across multiple frames within a segment at an early or middle stage, image patches from different temporal positions can interact with each other via the self-attention mechanism.</p><p>Specifically, a video sequence</p><formula xml:id="formula_6">{ 1 , 2 , . . . , | | } is divided into segments { 1 , 2 , . . . , }.</formula><p>Each segment contains | | frames and | | tokens. Then we perform token clustering on these | | tokens segment-wise, namely, clustering for each segment independently. Then the centers of all clusters from one segment, i.e., center tokens, are selected and other non-center tokens are simply dropped. These center tokens are concatenated and arranged according to their original relative spatial-temporal position. Center tokens from the upper-left position and early frames are at the beginning of the new token sequence. Center tokens from the bottom-right position and late frames are at the rear-end of the new token sequence.</p><p>Multi-segment token clustering algorithm makes our vision model achieve segment-level temporal modeling and be able to capture the detailed temporal variation of video frames. This allows our methods to achieve segment-level alignment of the text and the video consisted of segments { 1 , 2 , . . . , }:</p><formula xml:id="formula_7">( , ) = 1 ?? =1 ?( ) ( ).<label>(5)</label></formula><p>The multi-segment token clustering method has at least two advantages: (1) reducing computation costs by cutting down the number of tokens; (2) achieving segment-level semantic alignment of text and video representations via attention among tokens from different frames within the same temporal segment. As shown in <ref type="figure">Figure 2</ref>, assuming we perform token clustering right after the -th transformer block and the number of clusters is (ignore [CLASS] token after pooling), this means the length of the input sequence length of the following (12 ? ) transformer blocks become . Generally, | | &gt;&gt; , obviously, computational costs are largely reduced. It is worth noting that the clustering module can be inserted at any place of ViT and the clustering procedure can be performed for any times. Clustering at an early stage reduces more computation costs. Next, we discuss two clustering methods used in the multi-segment token clustering algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">k-medoids++.</head><p>In this section, we introduce the first instance of the token clustering method: k-medoids++, a variety of commonly known k-means algorithm. Given a set of tokens { 1 , ..., } ? R , where is the transformer width and = | | is the number of tokens in a temporal segment, the goal of kmeans is to partition the observations into (? ) sets, i.e., C = {C 1 , C 2 , . . . , C }, so as to minimize the within-cluster sum of squares: arg min C =1 ? C ? ? ? 2 2 , where is the mean of points in C , i.e., centroid. Normal k-means contains 4 steps: ? arg max ; 10 end also not suitable for retrieval, as we would obtain inconsistent retrieval results when querying multiple times. Therefore, we need a deterministic centroids initialization method. In this work, we adopt the KKZ initialization method <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b47">48]</ref>. The algorithm is shown in Algorithm 1. It first chooses the point with the maximum ? 2 -norm as the first centroid, then chooses the point with the maximum distance to the existing centroids as the next centroids. The algorithm is simple but effective <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b47">48]</ref>. It makes k-means deterministic and accelerates its convergence speed. In our token clustering process, we use medoids rather than centroids. Namely, we choose the nearest point to centroids as the seed point in steps 2&amp;3 in the normal k-means methods. This is because the semantic of mean pooling representation of tokens within a cluster may shift away from the exact token embeddings. Combining k-medoids and KKZ initialization, we get k-medoids++ -the name philosophy follows k-mean++ <ref type="bibr" target="#b0">[1]</ref>. The complexity of k-medoids++ is O ( ), where is the iteration upper bound and O ( ) for computing the distance between two points in R .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.2</head><p>Spectral clustering. K-means is suitable for spherical data clusters. However, our data points are in a high dimension space R ( = 512 in most cases), and the data distribution is unknown. The shape of data clusters may not be spherical. To resolve this underlying problem, we further introduce spectral clustering into the token clustering process. Spectral clustering is a graph partition method that aims to maximize the weights of connections within groups and minimize the weights of connections between groups. It first needs to construct the graph = ( , ), where is the vertex set and each vertex represents a data point , is the edge set and each edge denotes the (weighted) connection between two vertices. In this work, we use the normalized spectral clustering algorithm described in <ref type="bibr" target="#b35">[36]</ref>. The algorithm contains 5 steps: /* sign(?) return the sign of input, ?, denote the -th column of */ 3 = =1 sign( ?, )( ?, ) 2 ; 4 end 5 for ? 1 to do 6 ? = sign( ) ; 7 end 5. Consider each row of as a new data point, apply k-means to these data points.</p><p>The above algorithm first performs dimension reduction and then data clustering. In this work, we use SVD to solve the eigenvectors of as = .</p><p>A sign correct algorithm is further introduced to resolve the sign ambiguity in SVD as the direction of points after dimension reduction also matters for some distance metrics, e.g., ? 2 distance. The main idea of this sign correct algorithm is to make the direction of singular vectors aligns with the majority direction of data points <ref type="bibr" target="#b4">[5]</ref>, i.e., the sign of the sum of the inner product of singular vectors and data points should be positive. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS 4.1 Experimental Details</head><p>Datatest. We validate our model on four datasets: MSR-VTT <ref type="bibr" target="#b59">[60]</ref>, MSVD <ref type="bibr" target="#b6">[7]</ref>, LSMDC <ref type="bibr" target="#b41">[42]</ref>, and ActivityNet <ref type="bibr" target="#b11">[12]</ref>. To save computational costs, the shorter side of videos are resized to 224 and the frame per second (fps) is set to 3. (a) MSR-VTT contains 10 000 videos with a length ranges from 10~32 seconds and 200 000 captions. We use two types of data splits, training-7K and training-9K, to compare with baselines. The training-7K follows the data splits from HowTo100M <ref type="bibr" target="#b34">[35]</ref> and the training-9K follows the data splits from <ref type="bibr" target="#b12">[13]</ref>. The test data in both splits is 'test 1k-A', which contains 1 000 video-text pairs following JSFusion <ref type="bibr" target="#b62">[63]</ref>. If we do not specify, we use training-9K as the default. (b) MSVD contains 1 970 videos with a duration ranges from 1~62 seconds. Train, validation, and test splits contain 1 200, 100, and 670 videos, respectively. Each video has approximately 40 associated sentences in English. (c) LSMDC is comprised of 118 081 videos that ranges from 2~30 seconds. The videos were extracted from 202 movies. The validation set contains 7 408 videos. The 1 000 videos in the test set are from movies independent from the training and validation splits. (d) Ac-tivityNet <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17]</ref> consists of 20 000 YouTube videos, and some of them are minutes long. We follow <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b66">67]</ref>    descriptions of a video to form a paragraph and evaluate the model with video-paragraph retrieval on the val1 split. Learning strategies. We apply warm up and cosine learning rate decay policy <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16]</ref>. If the initial learning rate is and current epoch is ?, for the first slow_epoch steps, the learning rate is ? epoch slow_epoch ; for the rest epochs, the learning rate is 0.5 ? ? (1 + cos( ? epoch?slow_epoch max_epoch?slow_epoch )). Generally, is 1e-5 for ActivityNet and 5e-6 for other datasets; max_epoch is 8 for ActivityNet and 5 for other datasets; slow_epoch = 0.1 ? max_epoch. AdamW <ref type="bibr" target="#b30">[31]</ref> optimizer is adopted with decoupled weight decay value 0.2. Sequence length and batch size. For ActivityNet, the maximal text sequence length is 77 and the frame length is 60. For other datasets, the maximal text sequence length is 32 and the frame length is 12. The total batch size is always 128. Experiments with ViT-B/32 for ActivityNet are done on 8 NVIDIA Tesla V100 GPUs. Experiments with ViT-B/32 for other datasets need at least 2 RTX 3090 GPUs. All experiments are done with mixed precision <ref type="bibr" target="#b32">[33]</ref>.</p><p>Frame sampling. We adopt a sparse sampling strategy following TSN <ref type="bibr" target="#b53">[54]</ref>. During training, video frames are divided into segments, and we randomly sample one frame from each segment. During the evaluation, frames are uniformly sampled from the video. As the above said, = 60 for ActivityNet and = 12 for the other datasets. These frames are further divided into segments during token clustering. Evaluation metric. We use standard retrieval metrics: recall at rank K (R@K, higher is better), median rank (MdR, lower is better), and mean rank (MnR, lower is better) to evaluate the performance. R@K (Recall at K) calculates the percentage of test samples for which the correct result is found in the top-K retrieved points to the query sample. R@1, R@5, and R@10 are reported. Median Rank calculates the median of ground-truth results in the ranking. Mean Rank calculates the average rank of all correct results. Setting of CenterCLIP. We use ( ? , ) to represent the setting. It means we perform token clustering right after the -th transformer block, the number of temporal segments is , and the   number of clusters/centers are constant . Generally, we construct KNN graph with Gaussian similarity function between two points when applying spectral clustering: exp(?? ? ? 2 /(2 2 )). The neighbours of one vertex is 5? {the number of frames in a segment} for ViT-B/32, and plus an additional 5 for ViT-B/16. The variance of the Gaussian function is simply set to 2.0. No normalization is applied for token embeddings before performing clustering. Baselines in the experiments use the same setting as CenterCLIP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on Common Benchmarks</head><p>As shown in <ref type="table" target="#tab_3">Table 1</ref>, <ref type="table" target="#tab_4">Table 2</ref>, <ref type="table" target="#tab_6">Table 3</ref>, and <ref type="table" target="#tab_7">Table 4</ref>. We achieve SOTA performance on all four datasets. Moreover, we also achieve decent memory usage reduction in all cases and obvious speedup of evaluation in some cases. Specifically, for CenterCLIP (ViT-B/32), we achieve a 32% reduction in memory cost and accelerate the model by 6% of the original speed for MSR-VTT, MSVD, and LSMDC in the best situation. For ActivityNet, the reduction in memory cost is 35% and the speedup of evaluation speed is 14% in the best case. These numbers verify the efficiency of our method. For Center-CLIP (ViT-B/16), as the patch size decreases, the number of tokens increases (4 ? as the number of tokens of ViT-B/32). In this work, the clustering complexity is at least linearly related to the number of data points. Therefore, CenterCLIP does not gain speedup for ViT-B/16. However, CenterCLIP also achieves a 32% reduction in memory cost. In future work, we will introduce faster clustering algorithms to speed up the whole model. Compared to the baseline, CenterCLIP achieves significant improvement on recall. When using ViT-B/32, for MSVD, the maximal gain of text?video R@1 is 1.7%; for MSR-VTT (training-9K), the number is 1.2%; for LSMDC, it is 1.8%; for ActivityNet, it achieves 2.1% improvement of text?video R@1. When using ViT-B/16, for MSVD, the numbers are 1.0%, 2.8%, and 0.1% for text?video R@1, 5.7%, 5.2%, and 2.0% for video?text R@1. CenterCLIP gains more improvement of video?text retrieval performance in this case. All these results demonstrate the effectiveness of our clustering strategy. It aligns segment semantics of text and video.</p><p>It is worth noting that spectral clustering and k-medoids++ achieve similar performance in most cases. This is somehow counterintuitive as spectral clustering should be more suitable for clustering high-dimensional points. This is possible because the data shape of clusters of token embeddings in high dimensional space is nearly spherical. Spectral clustering does achieve better performance in terms of some metrics, e.g., better R@5 and R@10 on MSR-VTT and ActivityNet, and produces the best video?text results on MSVD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Diagnostic Experiments</head><p>In this section, we will analyze CenterCLIP thoroughly. All diagnostic experiments are taken with CenterCLIP (ViT-B/32).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">More baselines.</head><p>We provide four more strong baselines: 1) pooling of nearby tokens in a temporal segment, after pooling, we get average tokens for one segment; 2) sparse sampling of tokens in a temporal segment, namely, randomly sample tokens from a temporal segment during training and uniformly sample tokens during validation; 3) temporal shift described in TSM <ref type="bibr" target="#b53">[54]</ref>, here we apply temporal shift to the tokens except [CLASS] embedding; 4) token shift described in <ref type="bibr" target="#b67">[68]</ref>, the method only shift the <ref type="bibr">[</ref>   embedding. The shift is performed twice in each transformer block, right before MHSA and FFN. Results are shown in <ref type="table" target="#tab_9">Table 5</ref>. Shifting all image patches does not work here. sparse sampling produces a little better results than baseline on MSR-VTT and LSMDC. However, CenterCLIP is much better than sparse sampling, this demonstrates the necessity of selecting representative tokens. Tokenshift achieves pretty good performance on short videos, nevertheless, it does not reduce any computational costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.3.2</head><p>The place of performing token clustering. The influence of places of token clustering (k-medoids++) is shown in <ref type="figure" target="#fig_3">Figure 3</ref>. The smaller the , the lower the memory cost. The performance of the whole model will also decrease along with the decreasing or increasing of . A good trade-off between memory cost and performance achieves at = 6, this is also our default setting. We can also take multiple times of clustering. For instance, firstly perform clustering with ( = 6, = 4) and then with ( = 3, =    <ref type="table" target="#tab_11">Table 6</ref>. Such progressive clustering strategy achieves pretty good R@5, R@10, MdR, and memory cost reduction. However, performing multiple times will increase the time complexity and this is not suitable for large amounts of tokens. Thus we generally perform clustering once in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.3.3</head><p>The number of cluster and segment . We perform experiments on LSMDC and ActivityNet. The results including R@1, memory cost, and inference time are shown in <ref type="figure" target="#fig_4">Figure 4</ref>. Along with the increase of , the performance increases, and computation costs also increase. At the same time, a small segment number does not always achieve better performance, e.g., = 1 on LSMDC and = 6 on ActivityNet. A small segment number means more tokens are dropped. This will cause the loss of more information. When = 1 on LSMDC and = 6 on ActivityNet, the number of tokens in a segment is large, i.e., 12 ? 49 and 10 ? 49, this leads to more computational costs of clustering as shown in <ref type="figure" target="#fig_4">Figure 4c</ref> and <ref type="figure" target="#fig_4">Figure 4d</ref>. Thus a moderate segment number is usually adopted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.3.4</head><p>The number of input frames . We change the number of input video frames and take experiments with CenterCLIP ( 6 ? 15, 49) on ActivityNet. The results are shown in <ref type="table" target="#tab_13">Table 7</ref>. The large the number of input frames , the more computation costs, and a small number of frames will lead to worse performance. Similar ablations about the input of frames on short video datasets like    MSR-VTT can be found in CLIP4clip <ref type="bibr" target="#b31">[32]</ref>. When the number of segments is fixed, a large number of input frames will also increase computation costs of the clustering process as the number of tokens in one temporal segment increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.5">Normalization of token embeddings.</head><p>People may be curious about the influence of embedding normalization when performing token clustering. We showed results with and without ?2 normalization in <ref type="table" target="#tab_14">Table 8</ref>. The difference is trivial. Indeed, different choices of normalization are equal to different choices of distance metrics in clustering. Besides distance metrics, some other factors can also influence the results of clustering, i.e., how to construct graphs in spectral clustering. However, this is not the focus of our work, and we do not make further explorations in this direction. we are stuck with mixed precision and use = 5e-6 for short video datasets. For ActivityNet, we found a large learning rate with more training epochs brings a better result. This is shown in <ref type="table" target="#tab_15">Table 9</ref>. It is possibly because of the large number of different video frames in long videos in ActivityNet and the sparse sampling strategy we used during training. The model needs more training epochs to learn good representations of videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.7">Visualization of image patches of center tokens after clustering.</head><p>We further display visualization results of center tokens after multisegment clustering with different numbers of video frames within a temporal segment. The results are shown in <ref type="figure">Figure 5</ref>. It is clear that the clustering algorithm reserves the most representative tokens, for example, in the second and third row of <ref type="figure">Figure 5</ref>, tokens of the foreground animal are selected and only part of the tokens of the similar background remains. This verifies our beginning motivation that using a few typical tokens is already enough for learning discriminative features for video representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this work, we propose a multi-segment clustering algorithm to reduce the number of redundant tokens of continuous video frames, and achieve segment-level alignment of video and text representations for text-video retrieval task. Our method, named CenterCLIP as we only reserve center tokens of token clusters and drop non-center tokens, is based on the knowledge of largescale image-text pairs pre-trained model -CLIP. We take extensive experiments on four common text-video multi-modal datasets: MSR-VTT, MSVD, LSMDC, and ActivityNet. CenterCLIP achieves stateof-the-art performance on all these four datasets and surpass the old SOTA by a large margin. At the same time, CenterCLIP realizes a decent reduction in memory costs and speedup of inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work is supported by National Key R&amp;D Program of China under Grant No. 2020AAA0108800. Thanks Naiyuan Liu for his helpful discussions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>the number of frames in . After visual tokenization, if each frame produces tokens, the number of visual tokens is | | (do not consider [CLASS] token). It shows that the number of visual tokens is linear to the number of tokens per frame ( ) and the video length. Given an input frame with a size of 224 ? 224, = 49 for the ViT-B/32 model and = 196 for the ViT-B/16 model. With a larger , the number of visual tokens becomes much larger.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Here we describe the special case of this algorithm for symmetric matrix in Algorithm 2. The complexity of spectral clustering is O ( 2 + 3 ), where O ( 3 ) for step 1-4 and O ( 2 ) for step 5. For more details about spectral clustering refer to the tutorial [52].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>(a) T2V R@1 on LSMDC (b) Memory cost on RTX 3090 Influence of places of token clustering.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Influence of cluster number and segment . 8). The results are shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>4. 3 . 6 Figure 5 :</head><label>365</label><figDesc>Learning rate and training epochs. The original CLIP4clip uses a learning rate of 1e-7. When setting = 1e-7 on MSR-VTT (training-7K), we get 39.7 T?V R@1 with mixed precision [33] and 41.7 T?V R@1 without mixed precision on the split 'test 1k-A'. The corresponding result of CLIP4clip baseline is 42.1 T?V R@1. When increasing the learning rate to 5e-6, we get 42.4 T?V R@1 with mixed precision on 'test 1k-A'. As the mixed precision training saves a lot of GPU memory and accelerates the training procedure,The four-legged creature, half-horse, half-eagle, rises above the trees. # frames in a segment = 3, K=49 # frames in a segment = 2, K=49 # frames in a segment = 6, K=49 Visualization of centers after token clustering with different number of frames in a temporal segment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>1{ = } ; 1{?} equals to 1 if and only if the inner condition is true; 4. Repeat step 2 and 3 until convergence.One disadvantage of the normal k-means is that clustering results are sensitive to the centroids initialization. Bad initialization may lead to the collapse of the clustering. Random initialization is Algorithm 1: KKZ initialization for k-means<ref type="bibr" target="#b22">[23]</ref> Input: tokens { 1 , ..., } ? R , cluster number Output: centroids { 1 , 2 , . . ., } ? R 1 1 ? arg max ? ? 2 ; 2 for ? 2 to do</figDesc><table><row><cell>3</cell><cell cols="2">for ? 1 to do</cell></row><row><cell>4</cell><cell cols="2">for ? 1 to do</cell></row><row><cell></cell><cell cols="2">// calculate distance between</cell><cell>and</cell></row><row><cell>5</cell><cell cols="2">, ? distance( , );</cell></row><row><cell>6</cell><cell>end</cell><cell></cell></row><row><cell>7</cell><cell>? min</cell><cell>, ;</cell></row><row><cell>8</cell><cell>end</cell><cell></cell></row><row><cell>9</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>? R randomly; 2 ; arg min ? ? ? 2 1. Initialize cluster centroids 1 , 2 , . . ., 2. For every , set</cell></row><row><cell></cell><cell></cell><cell></cell><cell>3. For every , set</cell></row></table><note>=1 1{ = }=1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Compute the first eigenvectors 1 , . . . , of which correspond to the first least eigenvalues; 4. Let = [ 1 , . . . , ]; Normalize each row of to have norm of 1, generally, ? 2 norm is used; Algorithm 2: sign flip for SVD [5] Input: ? R ? , truncated singular value decomposition ( , ?, ) of , = [ 1 , . . . , ] ? R ? Output: ? = [ ? 1 , . . . , ? ] with appropriate signs 1 for ? 1 to do</figDesc><table><row><cell cols="2">1. Construct similarity graph. Let be its weighted adjacency</cell></row><row><cell>matrix, be the degree matrix;</cell><cell></cell></row><row><cell>2. Compute normalized Laplacian</cell><cell>= ? 1 2 ( ? ) ? 1 2 ;</cell></row><row><cell>3.</cell><cell></cell></row></table><note>2 = ? =1, ? ;</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Results on MSVD. MeM. is the average GPU memory cost when training on 2 and 8 Tesla V100 GPUs for ViT-B/32 and ViT-B/16, respectively. Speed is the inference time per video during evaluation on a Tesla V100 GPU.</figDesc><table><row><cell>Method</cell><cell cols="2">MeM. Speed GB ms</cell><cell cols="10">Text ? Video R@1? R@5? R@10? MdR? MnR? R@1? R@5? R@10? MdR? MnR? Video ? Text</cell></row><row><cell>FSE [67]</cell><cell>-</cell><cell>-</cell><cell>18.2</cell><cell>44.8</cell><cell>-</cell><cell>7.0</cell><cell>-</cell><cell>16.7</cell><cell>43.1</cell><cell>-</cell><cell>7.0</cell><cell>-</cell></row><row><cell>CE [29]</cell><cell>-</cell><cell>-</cell><cell>18.2</cell><cell>47.7</cell><cell>-</cell><cell>6.0</cell><cell>12.1</cell><cell>17.7</cell><cell>46.6</cell><cell>-</cell><cell>6.0</cell><cell>24.4</cell></row><row><cell>CMGSD [15]</cell><cell>-</cell><cell>-</cell><cell>24.2</cell><cell cols="8">56.3 4.0</cell><cell>-</cell></row><row><cell>SSB [38]</cell><cell>-</cell><cell>-</cell><cell>29.2</cell><cell>61.6</cell><cell>-</cell><cell>3.0</cell><cell>-</cell><cell>28.7</cell><cell>60.8</cell><cell>-</cell><cell>2.0</cell><cell>-</cell></row><row><cell>CLIP zero-shot</cell><cell>-</cell><cell>-</cell><cell>21.7</cell><cell>46.0</cell><cell>59.6</cell><cell>7.0</cell><cell>39.7</cell><cell>17.9</cell><cell>40.8</cell><cell>54.2</cell><cell>8.0</cell><cell>43.3</cell></row><row><cell>CLIP4clip (meanP) [32]</cell><cell>25.0</cell><cell>82.0</cell><cell>40.5</cell><cell>72.4</cell><cell>-</cell><cell>2.0</cell><cell>7.4</cell><cell>42.5</cell><cell>74.1</cell><cell>85.8</cell><cell>2.0</cell><cell>6.6</cell></row><row><cell>CLIP4clip (seqTransf)</cell><cell>-</cell><cell>-</cell><cell>40.5</cell><cell>72.4</cell><cell>-</cell><cell>2.0</cell><cell>7.5</cell><cell>41.4</cell><cell>73.7</cell><cell>85.3</cell><cell>2.0</cell><cell>6.7</cell></row><row><cell>baseline (CLIP4clip (meanP), ViT-B/32)</cell><cell>25.0</cell><cell>82.0</cell><cell>41.8</cell><cell>73.9</cell><cell>84.7</cell><cell>2.0</cell><cell>7.3</cell><cell>42.8</cell><cell>73.8</cell><cell>85.3</cell><cell>2.0</cell><cell>6.9</cell></row><row><cell>CenterCLIP (k-medoids++, 6 ? 15, 49)</cell><cell>16.8</cell><cell>71.3</cell><cell>43.9</cell><cell>75.3</cell><cell>85.2</cell><cell>2.0</cell><cell>7.0</cell><cell>44.2</cell><cell>75.0</cell><cell>86.1</cell><cell>2.0</cell><cell>6.8</cell></row><row><cell>CenterCLIP (k-medoids++, 6 ? 12, 49)</cell><cell>16.2</cell><cell>70.4</cell><cell>43.5</cell><cell>75.0</cell><cell>85.9</cell><cell>2.0</cell><cell>6.9</cell><cell>44.5</cell><cell>75.3</cell><cell>86.0</cell><cell>2.0</cell><cell>6.7</cell></row><row><cell>CenterCLIP (spectral, 6 ? 20, 49)</cell><cell>17.7</cell><cell>162</cell><cell>43.5</cell><cell>75.1</cell><cell>85.4</cell><cell>2.0</cell><cell>6.9</cell><cell>44.1</cell><cell>75.1</cell><cell>86.0</cell><cell>2.0</cell><cell>6.7</cell></row><row><cell>CenterCLIP (spectral, 6 ? 15, 49)</cell><cell>16.8</cell><cell>174</cell><cell>43.9</cell><cell>74.6</cell><cell>85.8</cell><cell>2.0</cell><cell>6.7</cell><cell>44.5</cell><cell>75.7</cell><cell>86.2</cell><cell>2.0</cell><cell>6.5</cell></row><row><cell cols="2">CenterCLIP (k-medoids++, 6 ? 15, 160, ViT-B/16) 23.0</cell><cell>419</cell><cell>46.2</cell><cell>77.0</cell><cell>87.6</cell><cell>2.0</cell><cell>5.7</cell><cell>46.7</cell><cell>77.1</cell><cell>88.0</cell><cell>2.0</cell><cell>5.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Results on ActivityNet. MeM. is the average GPU memory cost when training on 8 and 32 Tesla V100 GPUs. Baseline with ViT-B/16 OOM on 32 Tesla V100 GPUs. Speed is the inference time per video during evaluation on a Tesla V100 GPU.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Results on MSR-VTT. MeM. is the average GPU memory cost when training on 2 and 8 Tesla V100 GPUs for ViT-B/32 and ViT-B/16, respectively. Speed is the inference time per video during evaluation on a Tesla V100 GPU.</figDesc><table><row><cell>Method</cell><cell cols="2">MeM. speed GB ms</cell><cell cols="10">Text ? Video R@1? R@5? R@10? MdR? MnR? R@1? R@5? R@10? MdR? MnR? Video ? Text</cell></row><row><cell>JSFusion [63]</cell><cell>-</cell><cell>-</cell><cell>9.1</cell><cell>21.2</cell><cell>34.1</cell><cell>36.0</cell><cell>-</cell><cell>12.3</cell><cell>28.6</cell><cell>38.9</cell><cell>20.0</cell><cell>-</cell></row><row><cell>CE [29]</cell><cell>-</cell><cell>-</cell><cell>11.2</cell><cell>26.9</cell><cell>34.8</cell><cell>25.3</cell><cell>96.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MMT [13]</cell><cell>-</cell><cell>-</cell><cell>12.9</cell><cell>29.9</cell><cell>40.1</cell><cell>19.3</cell><cell>75.0</cell><cell>12.3</cell><cell>28.6</cell><cell>38.9</cell><cell>20.0</cell><cell>76.0</cell></row><row><cell>Frozen in Time [2]</cell><cell>-</cell><cell>-</cell><cell>15.0</cell><cell>30.8</cell><cell>39.8</cell><cell>20.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TT-CE+ [10]</cell><cell>-</cell><cell>-</cell><cell>17.2</cell><cell>36.5</cell><cell>46.3</cell><cell>13.7</cell><cell>-</cell><cell>17.5</cell><cell>36.0</cell><cell>45.0</cell><cell>14.3</cell><cell>-</cell></row><row><cell>CLIP zero-shot</cell><cell>-</cell><cell>-</cell><cell>15.1</cell><cell>28.3</cell><cell>35.8</cell><cell>31.0</cell><cell>132</cell><cell>7.5</cell><cell>18.4</cell><cell>25.1</cell><cell>58.0</cell><cell>151</cell></row><row><cell>CLIP4clip (meanP) [32]</cell><cell>20.8</cell><cell>24.4</cell><cell>20.7</cell><cell>38.9</cell><cell>47.2</cell><cell>13.0</cell><cell>65.3</cell><cell>20.6</cell><cell>39.4</cell><cell>47.5</cell><cell>13.0</cell><cell>56.7</cell></row><row><cell>CLIP4clip (seqTransf)</cell><cell>-</cell><cell>-</cell><cell>22.6</cell><cell>41.0</cell><cell>49.1</cell><cell>11.0</cell><cell>61.0</cell><cell>20.8</cell><cell>39.0</cell><cell>48.6</cell><cell>12.0</cell><cell>54.2</cell></row><row><cell cols="2">baseline (CLIP4clip (meanP), ViT-B/32) 20.8</cell><cell>24.4</cell><cell>20.1</cell><cell>40.2</cell><cell>48.4</cell><cell>12.0</cell><cell>57.1</cell><cell>21.2</cell><cell>39.3</cell><cell>48.4</cell><cell>12.0</cell><cell>50.8</cell></row><row><cell>CenterCLIP (k-medoids++, 6 ? 6, 49)</cell><cell>16.4</cell><cell>23.9</cell><cell>21.9</cell><cell>41.1</cell><cell>50.7</cell><cell>10.0</cell><cell>55.6</cell><cell>21.1</cell><cell>41.2</cell><cell>50.2</cell><cell>10.0</cell><cell>48.7</cell></row><row><cell>CenterCLIP (k-medoids++, 6 ? 4, 49)</cell><cell>15.0</cell><cell>22.9</cell><cell>21.7</cell><cell>39.8</cell><cell>49.8</cell><cell>11.0</cell><cell>54.8</cell><cell>21.4</cell><cell>40.3</cell><cell>50.8</cell><cell>10.0</cell><cell>48.4</cell></row><row><cell>CenterCLIP (spectral, 6 ? 6, 49)</cell><cell>16.4</cell><cell>40.8</cell><cell>21.6</cell><cell>40.9</cell><cell>49.3</cell><cell>11.0</cell><cell>57.2</cell><cell>20.6</cell><cell>39.5</cell><cell>48.8</cell><cell>12.0</cell><cell>51.4</cell></row><row><cell>CenterCLIP (spectral, 6 ? 4, 49)</cell><cell>15.0</cell><cell>43.6</cell><cell>21.4</cell><cell>39.7</cell><cell>49.4</cell><cell>11.0</cell><cell>55.9</cell><cell>19.5</cell><cell>39.9</cell><cell>48.0</cell><cell>12.0</cell><cell>50.1</cell></row><row><cell cols="2">baseline (CLIP4clip (meanP), ViT-B/16) 25.7</cell><cell>59.6</cell><cell>24.1</cell><cell>45.0</cell><cell>55.1</cell><cell>8</cell><cell>51.1</cell><cell>22.5</cell><cell>42.9</cell><cell>53.5</cell><cell>9</cell><cell>45.1</cell></row><row><cell>CenterCLIP (k-medoids++, 6 ? 4, 160)</cell><cell>17.6</cell><cell>86.5</cell><cell>24.2</cell><cell>46.2</cell><cell>55.9</cell><cell>8</cell><cell>47.3</cell><cell>24.5</cell><cell>46.4</cell><cell>55.8</cell><cell>7</cell><cell>41.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Results on LSMDC. MeM. is the average GPU memory cost when training on 2 and 8 Tesla V100 GPUs for ViT-B/32</figDesc><table /><note>and ViT-B/16, respectively. Speed is the inference time per video during evaluation on a Tesla V100 GPU.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Comparison with strong token selection baselines.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Performing clustering twice on LSMDC.</figDesc><table><row><cell>(a) T2V R@1 on LSMDC</cell><cell>(b) Memory cost on RTX 3090</cell></row><row><cell>(c) T2V R@1 on ActivityNet</cell><cell>(d) Inference time</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>CenterCLIP ( 6 ? 15, 49) Mem. T?V R@1? R@5? R@10? MdR? MnR?</figDesc><table><row><cell>= 75</cell><cell>T?V 19.48 V?T</cell><cell>43.8 44.2</cell><cell>74.8 75.5</cell><cell>85.8 86.8</cell><cell>2.0 2.0</cell><cell>6.7 6.5</cell></row><row><cell>= 60</cell><cell>T?V 16.75 V?T</cell><cell>43.9 44.2</cell><cell>75.3 75.0</cell><cell>85.2 86.1</cell><cell>2.0 2.0</cell><cell>7.0 6.8</cell></row><row><cell>= 45</cell><cell>T?V 14.03 V?T</cell><cell>42.6 43.7</cell><cell>74.5 74.5</cell><cell>84.8 85.4</cell><cell>2.0 2.0</cell><cell>7.0 6.7</cell></row><row><cell>= 30</cell><cell>T?V 11.29 V?T</cell><cell>43.2 43.4</cell><cell>74.5 74.7</cell><cell>85.1 85.5</cell><cell>2.0 2.0</cell><cell>6.9 6.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Influence of input frames on ActivityNet.</figDesc><table><row><cell>Method</cell><cell cols="6">Mem. T?V R@1? R@5? R@10? MdR? MnR?</cell></row><row><cell>CenterCLIP ( 6 ? 4, 49) w. ?2 norm</cell><cell>T?V 16.39 V?T</cell><cell>21.6 22.1</cell><cell>40.5 40.8</cell><cell>50.9 50.5</cell><cell>10.0 10.0</cell><cell>55.8 48.4</cell></row><row><cell>CenterCLIP ( 6 ? 6, 49) w. ?2 norm</cell><cell>T?V 14.95 V?T</cell><cell>21.5 20.2</cell><cell>40.8 39.8</cell><cell>50.0 48.6</cell><cell>10.5 12.0</cell><cell>56.7 50.5</cell></row><row><cell>CenterCLIP ( 6 ? 4, 49) wo. ?2 norm</cell><cell>T?V 16.39 V?T</cell><cell>21.7 21.4</cell><cell>39.8 40.3</cell><cell>49.8 50.8</cell><cell>11.0 10.0</cell><cell>54.8 48.4</cell></row><row><cell>CenterCLIP ( 6 ? 6, 49) wo. ?2 norm</cell><cell>T?V 14.95 V?T</cell><cell>21.9 21.1</cell><cell>41.1 41.2</cell><cell>50.7 50.2</cell><cell>10.0 10.0</cell><cell>55.6 48.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>Performing k-medoids++ clustering with and without ?2 normalization on LSMDC.</figDesc><table><row><cell></cell><cell cols="7">epochs T?V R@1? R@5? R@10? MdR? MnR?</cell></row><row><cell>5e-6</cell><cell>5</cell><cell>T?V V?T</cell><cell>40.6 41.8</cell><cell>72.3 73.1</cell><cell>83.7 84.5</cell><cell>2.0 2.0</cell><cell>7.7 7.0</cell></row><row><cell>1e-5</cell><cell>5</cell><cell>T?V V?T</cell><cell>41.0 42.8</cell><cell>73.4 73.8</cell><cell>84.2 85.2</cell><cell>2.0 2.0</cell><cell>7.3 6.9</cell></row><row><cell>1e-5</cell><cell>8</cell><cell>T?V V?T</cell><cell>41.8 42.8</cell><cell>73.9 73.8</cell><cell>84.7 85.3</cell><cell>2.0 2.0</cell><cell>7.3 6.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 9 :</head><label>9</label><figDesc>Different learning rates and training epochs for CLIP4clip baselines on ActivityNet.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">k-means++: the advantages of careful seeding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergei</forename><surname>Vassilvitskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms</title>
		<editor>Nikhil Bansal, Kirk Pruhs, and Clifford Stein</editor>
		<meeting>the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-01-07" />
			<biblScope unit="page" from="1027" to="1035" />
		</imprint>
	</monogr>
	<note>SIAM</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Frozen in time: A joint video and image encoder for end-to-end retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Bain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?l</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.00650</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Longformer: The longdocument transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russ</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simran</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sydney Von Arx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeannette</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brunskill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07258</idno>
		<title level="m">On the opportunities and risks of foundation models</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Resolving the sign ambiguity in the singular value decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">G</forename><surname>Kolda</surname></persName>
		</author>
		<idno type="DOI">10.1002/cem.1122</idno>
		<ptr target="https://doi.org/10.1002/cem.1122" />
	</analytic>
	<monogr>
		<title level="j">Journal of Chemometrics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="135" to="140" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Choose a Transformer: Fourier or Galerkin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhao</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:arXiv:2105.14995</idno>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Collecting highly parallel data for paraphrase evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William B Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies</title>
		<meeting>the 49th annual meeting of the association for computational linguistics: human language technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="190" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerii</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyou</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreea</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamas</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afroz</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.14794</idno>
		<title level="m">Rethinking attention with performers</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Teachtext: Crossmodal generalized distillation for text-video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioana</forename><surname>Croitoru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simion-Vlad</forename><surname>Bogolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11583" to="11593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-05-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ActivityNet: A Large-Scale Video Benchmark for Human Activity Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Bernard Ghanem Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multimodal transformer for video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Gabeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-08-23" />
			<biblScope unit="page" from="214" to="229" />
		</imprint>
	</monogr>
	<note>Proceedings, Part IV 16</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Yangqing Jia, and Kaiming He. 2017. Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving Video Retrieval by Adaptive Margin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajuan</forename><surname>L?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1359" to="1368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Bag of Tricks for Image Classification with Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tong He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno>abs/1812.01187</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Collecting and Annotating Human Activities in Web Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Multimedia Retrieval</title>
		<meeting>International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page">377</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">WenLan: Bridging vision and language by large-scale multi-modal pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manli</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangzhen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhao</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoxing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyuan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baogui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.06561</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Jaegle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Skanda</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.14795</idno>
		<title level="m">Perceiver io: A general architecture for structured inputs &amp; outputs</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Tgif-qa: Toward spatio-temporal reasoning in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunseok</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">2021. Scaling up visual and visionlanguage representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhsuan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duerig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05918</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Nikolaos Pappas, and Fran?ois Fleuret. 2020. Transformers are rnns: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR</title>
		<imprint>
			<biblScope unit="page" from="5156" to="5165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A new initialization technique for generalized Lloyd iteration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katsavounidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C. Jay</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="144" to="146" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Temporal tessellation: A unified approach for video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dotan</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gil</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="94" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04451</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hierarchical conditional relation networks for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vuong</forename><surname>Thao Minh Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetha</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Truyen</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9972" to="9981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Less is more: Clipbert for video-and-language learning via sparse sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7331" to="7341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00200</idno>
		<title level="m">Hierarchical encoder for video+ language omni-representation pre-training</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.13487</idno>
		<title level="m">Use what you have: Video retrieval using representations from collaborative experts</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<title level="m">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Decoupled Weight Decay Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaishao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08860</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mixed Precision Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulius</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonah</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">F</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Garc?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksii</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">End-to-end learning of visual representations from uncurated instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9879" to="9889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Howto100m: Learning a text-video embedding by watching hundred million narrated video clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2630" to="2640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On Spectral Clustering: Analysis and an algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ADVANCES IN NEURAL INFORMATION PROCESS-ING SYSTEMS</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="849" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Styleclip: Text-driven manipulation of stylegan imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Patashnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongze</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2085" to="2094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandela</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Yao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02824</idno>
		<title level="m">Joao Henriques, and Andrea Vedaldi. 2020. Support-set bottlenecks for video-text representation learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Learning Transferable Visual Models From Natural Language Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benlin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.02034</idno>
		<title level="m">DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The long-short story of movie description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German conference on pattern recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="209" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rouditchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angie</forename><surname>Boggust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Harwath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhiraj</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09199</idno>
		<title level="m">Avlnet: Learning audio-visual language representations from instructional videos</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Efficient content-based sparse attention with routing transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="53" to="68" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael S Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arnab</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.11297</idno>
		<title level="m">Mostafa Dehghani, and Anelia Angelova. 2021. TokenLearner: What Can 8 Learned Tokens Do for Images and Videos? arXiv preprint</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Neural Machine Translation of Rare Words with Subword Units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08-07" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
		<respStmt>
			<orgName>Long Papers. The Association for Computer Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liunian</forename><forename type="middle">Harold</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.06383</idno>
		<title level="m">and Kurt Keutzer. 2021. How Much Can CLIP Benefit Vision-and-Language Tasks? arXiv preprint</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">In search of deterministic methods for initializing K-means and Gaussian mixture clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">G</forename><surname>Dy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intell. Data Anal</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="319" to="338" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7464" to="7473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Visualizing Data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett</editor>
		<meeting><address><addrLine>Long Beach, CA, USA, Isabelle Guyon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrike</forename><surname>Von</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luxburg</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xitong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengduo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.11591</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">2021. Efficient Video Transformers with Spatial-Temporal Token Selection. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Belinda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<title level="m">Linformer: Self-attention with linear complexity</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">T2vlad: global-local sequence alignment for text-video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5079" to="5088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03677</idno>
		<title level="m">Visual transformers: Token-based image representation and processing for computer vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Video question answering via gradually refined attention over appearance and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1645" to="1653" />
		</imprint>
	</monogr>
	<note>Hanwang Zhang, Xiangnan He, and Yueting Zhuang</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gargi</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Yao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Okhonko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.14084</idno>
		<title level="m">Videoclip: Contrastive pre-training for zero-shot video-text understanding</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Msr-vtt: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5288" to="5296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">AdaViT: Adaptive Tokens for Efficient Vision Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Hongxu Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mallya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.07658</idno>
		<imprint>
			<date type="published" when="2021-01" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Video paragraph captioning using hierarchical recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haonan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4584" to="4593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A joint sequence fusion model for video question answering and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV</title>
		<meeting>the European Conference on Computer Vision (ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="471" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">End-to-end concept word detection for video captioning, retrieval, and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyungjin</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwook</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3165" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ling</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.11432</idno>
		<title level="m">Florence: A New Foundation Model for Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Classification is a Strong Baseline for Deep Metric Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Yu</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th British Machine Vision Conference 2019, BMVC 2019</title>
		<meeting><address><addrLine>Cardiff, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Cross-modal and hierarchical modeling of video and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="374" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Token shift transformer for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbin</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="917" to="925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Actbert: Learning global-local video-text representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8746" to="8755" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
