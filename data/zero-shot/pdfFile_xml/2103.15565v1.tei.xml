<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RAN-GNNs: breaking the capacity limits of graph neural networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Diego</forename><surname>Valsesia</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Giulia</forename><surname>Fracastoro</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Enrico</forename><surname>Magli</surname></persName>
						</author>
						<title level="a" type="main">RAN-GNNs: breaking the capacity limits of graph neural networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural networks have become a staple in problems addressing learning and analysis of data defined over graphs. However, several results suggest an inherent difficulty in extracting better performance by increasing the number of layers. Recent works attribute this to a phenomenon peculiar to the extraction of node features in graph-based tasks, i.e., the need to consider multiple neighborhood sizes at the same time and adaptively tune them. In this paper, we investigate the recently proposed randomly wired architectures in the context of graph neural networks. Instead of building deeper networks by stacking many layers, we prove that employing a randomlywired architecture can be a more effective way to increase the capacity of the network and obtain richer representations. We show that such architectures behave like an ensemble of paths, which are able to merge contributions from receptive fields of varied size. Moreover, these receptive fields can also be modulated to be wider or narrower through the trainable weights over the paths. We also provide extensive experimental evidence of the superior performance of randomly wired architectures over multiple tasks and four graph convolution definitions, using recent benchmarking frameworks that addresses the reliability of previous testing methodologies.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Data defined over the nodes of graphs are ubiquitous. Social network profiles <ref type="bibr" target="#b0">[1]</ref>, molecular interactions <ref type="bibr" target="#b1">[2]</ref>, citation networks <ref type="bibr" target="#b2">[3]</ref>, 3D point clouds <ref type="bibr" target="#b3">[4]</ref> are just examples of a wide variety of data types where describing the domain as a graph allows to encode constraints and patterns among the data points. Exploiting the graph structure is crucial in order to extract powerful representations of the data. However, this is not a trivial task and only recently graph neural networks (GNNs) have started showing promising approaches to the problem. GNNs <ref type="bibr" target="#b4">[5]</ref> extend the deep learning toolbox to deal with the irregularity of the graph domain. Much of the work has been focused on defining a graph convolution operation <ref type="bibr" target="#b5">[6]</ref>, i.e., a layer that is well-defined over the graph domain but also retains some of the key properties of convolution such as weight reuse and locality. A wide variety of such graph convolution operators has been defined over the years, mostly based on neighborhood aggregation schemes where the features of a node are transformed by processing the features of its neighbors. Such schemes have been shown to be as powerful as the Weisfeiler-Lehman graph isomorphism test <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, enabling them to simultaneuosly learn data features and graph topology. However, contrary to classic literature on CNNs, few works <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b11">[12]</ref> addressed GNNs architectures and their role in extracting powerful representations. Several works, starting with the early GCN <ref type="bibr" target="#b12">[13]</ref>, noticed an inability to build deep GNNs, often resulting in worse performance than that of methods that disregard the graph domain, when trying to build anything but very shallow networks. This calls for exploring whether advances on CNN architectures can be translated to the GNN space, while understanding the potentially different needs of graph representation learning.</p><p>Li et al. <ref type="bibr" target="#b13">[14]</ref> suggest that GCNs suffer from oversmoothing as several layers are stacked, resulting in the extraction of mostly low-frequency features. This is related to the lack of self-loop information in this specific graph convolution. It is suggested that ResNet-like architectures mitigate the problem as the skip connections supply high frequency contributions. Xu et al. <ref type="bibr" target="#b10">[11]</ref> point out that the size of the receptive field of a node, i.e., which nodes contribute to the features of the node under consideration, plays a crucial role, but it can vary widely depending on the graph and too large receptive fields may actually harm performance. They conclude that for graphbased problems it would be optimal to learn how to adaptively merge contributions from receptive fields of multiple size. For this reason they propose an architecture where each layer has a skip connection to the output so that contributions at multiple depths (hence sizes of receptive fields) can be merged. Nonetheless, the problem of finding methods for effectively increasing the capacity of graph neural networks is still standing, since stacking many layers has been proven to provide limited improvements <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b16">[17]</ref>.</p><p>In this paper, we argue that the recently proposed randomly wired architectures <ref type="bibr" target="#b17">[18]</ref> are ideal for GNNs. In a randomly wired architecture, "layers" are arranged according to a random directed acyclic graph and data are propagated through the paths towards the output. Such architecture is ideal for GNNs because it realizes the intuition of <ref type="bibr" target="#b10">[11]</ref> of being able of merging receptive fields of varied size. Indeed, the randomly wired GNNs (RAN-GNNs) can be seen as an extreme generalization of their jumping network approach where layer outputs can not only jump to the network output but to other layers as well, continuously merging receptive fields. Hence, randomly wired architectures provide a way of effectively scaling up GNNs, mitigating the depth problem and creating richer representations. <ref type="figure">Fig. 1</ref> shows a graphical representation of this concept by highlighting the six layers directly contributing to the output, having different receptive fields induced by the distribution of paths from the input.</p><p>Our novel contributions can be summarized as follows: i) we are the first to analyze randomly wired architectures and show that they are generalizations of ResNets when looked at as ensembles of paths <ref type="bibr" target="#b18">[19]</ref>; ii) we show that path ensembling allows to merge receptive fields of varied size and that it can do so adaptively, i.e., trainable weights on the architecture edges can tune the desired size of the receptive fields to be merged to achieve an optimal configuration for the problem; iii) we introduce improvements to the basic design of randomly wired architectures by optionally embedding a path that sequentially goes through all layers in order to promote larger receptive fields when needed, and by presenting MonteCarlo DropPath, which decorrelates path contributions by randomly dropping architecture edges; iv) we provide extensive experimental evidence, using recently introduced benchmarking frameworks <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b19">[20]</ref> to ensure significance and reproducibility, that randomly wired architectures consistently outperform ResNets, often by large margins, for four of the most popular graph convolution definitions on multiple tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Graph Neural Networks</head><p>A major shortcoming of CNNs is that they are unable to process data defined on irregular domains. In particular, one case that is drawing attention is when the data structure can be described by a graph and the data are defined as vectors on the graph nodes. This setting can be found in many applications, including 3D point clouds <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, computational biology <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b22">[23]</ref>, and social networks <ref type="bibr" target="#b12">[13]</ref>. However, extending CNNs from data with a regular structure, such as images and video, to graph-structured data is not straightforward if one wants to preserve useful properties such as locality and weight reuse.</p><p>GNNs redefine the convolution operation so that the new layer definition can be used on domains described by graphs.</p><p>The most widely adopted graph convolutions in the literature rely on message passing, where a weighted aggregation of the feature vectors in a neighborhood is computed. The GCN <ref type="bibr" target="#b12">[13]</ref> is arguably the simplest definition, applying the same linear transformation to all the node features, followed by neighborhood aggregation and non-linear activation:</p><formula xml:id="formula_0">h (l+1) i = ? ? ? 1 |N i | j?Ni Wh (l) j ? ? .</formula><p>Variants of this definition have been developed, e.g., Graph-Sage <ref type="bibr" target="#b0">[1]</ref> concatenates the feature vector of node i to the feature vectors of its neighbors, so that self-information can also be exploited; GIN <ref type="bibr" target="#b7">[8]</ref> uses a multilayer perceptron instead of a linear transform, replaces average with sum to ensure injectivity and proposes a different way of computing the output by using all the feature vectors produced by the intermediate layers. These definitions are all isotropic because they treat every edge in the same way. It has been observed that better representation capacity can be achieved using anistropic definitions, where every edge can have a different transformation, at the cost of increased computational complexity. The Gated GCN <ref type="bibr" target="#b23">[24]</ref> and GAT <ref type="bibr" target="#b24">[25]</ref> definitions fall in this category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Randomly wired architectures</head><p>In recent work, Xie et al. <ref type="bibr" target="#b17">[18]</ref> explore whether it is possible to avoid handcrafted design of neural network architectures and, at the same time, avoid expensive neural architecture search methods <ref type="bibr" target="#b25">[26]</ref>, by designing random architecture generators. They show that "layers" performing convolution, normalization and non-linear activation can be connected in a random architecture graph. Strong performance is observed on the traditional image classification task by outperforming stateof-the-art architectures. The authors conjecture that random architectures generalize ResNets and similar constructions, but the underlying principles of their excellent performance are unclear, as well as whether the performance translates to tasks other than image recognition or to operations other than convolution on grids.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RANDOMLY WIRED GNNS</head><p>In this section, we first introduce randomly wired graph neural networks (RAN-GNNs) and the notation we are going to use. We then analyze their behavior when viewed as ensembles of paths.</p><p>A randomly wired architecture consists of a directed acyclic graph (DAG) connecting a source architecture node, which is fed with the input data, to a sink architecture node. One should not confuse the architecture DAG with the graph representing the GNN domain: to avoid any source of confusion we will use the terms architecture nodes (edges) and domain nodes (edges), respectively. A domain node is a node of the graph that is fed as input to the GNN. An architecture node is effectively a GNN layer performing the following operations ( <ref type="figure" target="#fig_1">Fig. 2)</ref>: i) aggregation of the inputs from other architecture nodes via a weighted sum as in <ref type="bibr" target="#b17">[18]</ref>:</p><formula xml:id="formula_1">h (i) = j?Ai ? ij h (j) = j?Ai ?(w ij )h (j) , i = 1, ..., L ? 1 (1)</formula><p>being ? a sigmoid function, A i the set of direct predecessors of the architecture node i, and w ij a scalar trainable weight; ii) a non-linear activation; iii) a graph-convolution operation (without output activation); iv) batch normalization.</p><p>The architecture DAG is generated using a random graph generator. In this paper, we will focus on the Erd?s-Renyi model where the adjacency matrix of the DAG is a strictly upper triangular matrix with entries being realizations of a Bernoulli random variable with probability p. If multiple input architecture nodes are randomly generated, they are all wired to a single global input. Multiple output architecture nodes are averaged to obtain a global output. Other random generators may be used, e.g., small-world and scale-free random networks have been studied in <ref type="bibr" target="#b17">[18]</ref>. However, a different generator will display a different behavior concerning the properties we study in Sec. III-A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Randomly wired architectures behave like path ensembles</head><p>It has already been shown that ResNets behave like ensembles of relatively shallow networks, where one can see the ResNet architecture as a collection of paths of varied lengths <ref type="bibr" target="#b18">[19]</ref>. More specifically, in a ResNet with n layers, where all layers have a skip connection except the first one and the last one, there are exactly 2 L?2 paths, whose lengths follow a Binomial distribution (i.e., the number of paths of length l from layer k to the last layer is L?k?1 l?2 ), and the average path length is L 2 + 1 <ref type="bibr" target="#b18">[19]</ref>. In this section, we show that a randomly wired neural network can also be considered as an ensemble of networks with varied depth. However, in this case, the distribution of the path length is different from the one obtained with the ResNet, as shown in the following lemma.</p><p>Lemma III.1. Let us consider a randomly wired network with L architecture nodes, where the architecture DAG is generated according to an Erd?s-Renyi graph generator with probability p. The average number of paths of length l from node k to</p><formula xml:id="formula_2">the sink, where k &lt; L, is E[N (k) l ] = L?k?1 l?2 p l?1 and the average total number of paths from node k to the sink is E[N (k) ] = p(1 + p) L?k?1 .</formula><p>Proof. Let us first consider the number of paths of length l from node k to the sink. We define the path length as the number of nodes in the path. In a randomly wired network with n architecture nodes, we have that the first node of all the paths is node k and the last one is node n (i.e., the sink node). Therefore, the minimum path length is 2. If l ? 2, the number of all possible paths of length l between node k and the sink is n?k?1 l?2 . Since in a path of length l there are l ?1 edges and each edge has probability p of being generated by the Erd?s-Renyi model, each one of the paths of length l has probability p l?1 of being present in the network. Thus, the expected number of paths with length l between node k and the sink is</p><formula xml:id="formula_3">E[N (k) l ] = n?k?1 l?2 p l?1 .</formula><p>If we set k = 1, we obtain the average number of paths of length l from source to sink E[N l ] = n?2 l?2 p l?1 . We can now compute the average total number of paths E[N (k) ] as follows</p><formula xml:id="formula_4">E[N (k) ] = n?k+1 l=2 n ? k ? 1 l ? 2 p l?1 =? l =0 ? l pl +1 = p? l =0 ? l pl = p(1 + p)? = p(1 + p) n?k?1 ,</formula><p>where? = n ? k ? 1,l = l ? 2 and the fourth equality follows from the binomial theorem. If we set k = 1, we obtain the average total number of paths from source to sink E[N p ] = p(1 + p) n?2 .</p><p>We can observe that if p = 1, the randomly wired network converges to the ResNet architecture. This allows to think of randomly wired architectures as generalizations of ResNets as they enable increased flexibility in the number and distribution of paths, instead of enforcing the use of all 2 L?2 paths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Receptive field analysis</head><p>In the case of GNNs, we define the receptive field of a domain node as the neighborhood that affects the output features of that node. As discussed in Sec. I, the work in <ref type="bibr" target="#b10">[11]</ref> highlights that one of the possible causes of the depth problem in GNNs is that the size of the receptive field is not adaptive and may rapidly become excessively large. Inspired by this observation, in this section we analyze the receptive field of a randomly wired graph neural network. We show that the receptive field of the output is a combination of the receptive fields of shallower networks, induced by each of the paths. This allows to effectively merge the contributions from receptive fields of varied size. Moreover, we show that the trainable parameters along the path edges modulate the contributions of various path lengths and enable adaptive receptive fields, that can be tuned by the training procedure.</p><p>We first introduce a definition of the receptive field of a feedforward graph neural network 1 .</p><p>Definition III.1. Given a feedforward graph neural network with L layers, the receptive field of radius L of a domain node is its L-hop neighborhood.</p><p>In a randomly wired architecture, each path induces a corresponding receptive field whose radius depends on the length of the path. Then, the receptive field at the output of the network is obtained by combining the receptive fields of all the paths. In order to analyze the contribution of paths of different lengths to the receptive field of the network, we introduce the concept of distribution of the receptive field radius of the paths. Notice that if we consider a feedforward network with L layers, the distribution of the receptive field radius is a delta centered in L.</p><p>The following lemma allows to analyze the distribution of the receptive field radius in a randomly wired architecture.</p><p>Lemma III.2. The derivative ?y ?x0 of the output y of a randomly wired architecture with respect to the input x 0 is</p><formula xml:id="formula_5">?y ?x 0 = p?P ?y p ?x 0 = p?P {i,j}?E p ? ij ?? p ?x 0 = L l=2 p?P l ? p ?? p ?x 0 ,<label>(2)</label></formula><p>where y p is the output of path p,? p is the output of path p when we consider all the aggregation weights equal to 1, ? p = ?yp ?x0 / ??p ?x0 , P is the set of all paths from source to sink, L is the number of architecture nodes, P l is the set of paths from source to sink of length l and E p is the set of edges of the path p.</p><p>Proof. Direct computation.</p><p>From Lemma III.2, we can observe that the contribution of each path to the gradient is weighted by its corresponding architecture edge weights. Thus, we can define the following distribution ? of the receptive field radius:</p><formula xml:id="formula_6">? l = p?P l ? p = p?P l {i,j}?E p ? ij for l = 2, ..., n,<label>(3)</label></formula><p>where we have assumed that the gradient ??p ?x0 depends only on the path length, as done in <ref type="bibr" target="#b18">[19]</ref>. This is a reasonable assumption if all the architecture nodes perform the same operation. The distribution of the receptive field radius is therefore influenced by the architecture edge weights. <ref type="figure" target="#fig_2">Figure  3</ref> shows an example of how such weights can modify the radius distribution. If we consider ? ij = 1 for all i and j, we obtain that the radius distribution is equal to the path length distribution. In order to provide some insight into the role of parameter p in the distribution of the receptive field radius, we focus on this special case and analyze the distribution of the path lengths in a randomly wired architecture by introducing the following Lemma.</p><p>Lemma III.3. Let us consider a randomly wired network with L architecture nodes, where the architecture DAG is generated according to a Erd?s-Renyi graph generator with probability p. The average length of the paths from node k to the sink is</p><formula xml:id="formula_7">E[l (k) ] ? p 1+p (L ? k ? 1) + 2.</formula><p>Proof. From Lemma 3.1, we can compute the average length of the paths from node k to the sink as follows</p><formula xml:id="formula_8">E[l (k) ] = n?k+1 l=2 lE N k l N (k) ? n?k+1 l=2 lE[N (k) l ] E[N (k) ] = n?k+1 l=2 n?k?1 l?2 p l?1 l p(1 + p) n?k?1 ,<label>(4)</label></formula><p>where we have neglected the higher order terms <ref type="bibr" target="#b26">[27]</ref>. The numerator in (4) can be computed as follows</p><formula xml:id="formula_9">n?k+1 l=2 n ? k ? 1 l ? 2 p l?1 l =? l =0 ? l pl +1 (l + 2) =? l =0 ? l pl +1l + 2? l =0 ? l pl +1 = p 2? l =0 ? l pl ?1l + 2p? l =0 ? l pl = p 2? (1 + p)? ?1 + 2p(1 + p)? = p 2 (n ? k ? 1)(1 + p) n?k?2 + 2p(1 + p) n?k?1 ,</formula><p>where? = n ? k ? 1,l = l ? 2 and the fourth equality is obtained differentiating the binomial theorem with respect to p . Then, we obtain</p><formula xml:id="formula_10">E[l (k) ] = p 1 + p (n ? k ? 1) + 2.</formula><p>If we consider k = 1, i.e. the sink, we obtain E[l] = p 1+p (n ? 2) + 2.</p><p>Therefore, if p = 1 and ? ij = 1 for all i and j the radius distribution is a Binomial distribution centered in L 2 + 1 (as  in ResNets), instead when p &lt; 1 the mean of the distribution is lower. The path length distribution for different p values is shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. This shows that, differently from feedforward networks, the receptive field of ResNets and randomly wired architectures is a combination of receptive fields of varied sizes, where most of the contribution is given by shallow paths, i.e. smaller receptive fields. The parameter p of the randomly wired neural network influences the distribution of the receptive field radius: a lower p value skews the distribution towards shallower paths, instead a higher p value skews the distribution towards longer paths.</p><p>After having considered the special case where ? ij = 1 for all i and j, we now focus on the general case. Since the edge architecture weights are trainable parameters, they can be adapted to optimize the distribution of the receptive field radius. This is one of the strongest advantages provided by randomly wired architectures with respect to ResNets. This is particularly relevant in the context of GNNs, where we may have a non-uniform growth of the receptive field caused by the irregularity of the graph structure <ref type="bibr" target="#b10">[11]</ref>. Notice that the randomly wired architecture can be seen as a generalization of the jumping knowledge networks proposed in <ref type="bibr" target="#b10">[11]</ref>, where all the architecture nodes, not only the last one, merge contributions from previous nodes. We also remark that, even if we modify the ResNet architecture by adding trainable weights to each branch of the residual module, we cannot retrieve the behaviour of the randomly wired architecture. In fact, the latter has intrinsically more granularity than a ResNet: the expected number of architecture edge weights of a randomly wired network is pL(L+1) 2 , instead a weighted ResNet has only 2(L ? 2) weights. Ideally, we would like to weigh each path independently (i.e., directly optimizing the value of ? p in Eq. <ref type="formula" target="#formula_5">(2)</ref>). However, this is unfeasible because the number of parameters would become excessively high and the randomly wired architecture provides an effective tradeoff. Given an architecture node, weighting in a different way each input edge is important because to each edge corresponds a different length distribution of the paths going through such edge, as shown by the following Lemma.</p><p>Lemma III.4. Let us consider a randomly wired network with n architecture nodes, where the architecture DAG is generated according to a Erd?s-Renyi graph generator with probability p. Given an edge {i, j} between the architecture nodes i and j where i &lt; j, the average length of the paths from the source to the sink going through that edge is</p><formula xml:id="formula_11">E[l ij ] ? p 1+p (L ? (j ? i) ? 3) + 4.</formula><p>Proof. From Lemma 3.3, we can compute the average length of the paths going through the edge {i, j} as follows</p><formula xml:id="formula_12">E[l ij ] = E[l n?i+1 + l j ] ? p 1 + p (n ? (j ? i) ? 3) + 4.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Sequential path</head><p>In the previous sections we have shown that a randomly wired architecture behaves like an ensemble of paths merging contribution from receptive fields of varied size, where most of the contribution is provided by shallow paths. As discussed previously, this provides numerous advantages with respect to feedforward networks and ResNets. However, some graphbased tasks may actually benefit from a larger receptive field <ref type="bibr" target="#b13">[14]</ref>, so it is interesting to provide randomly wired architectures with mechanisms to directly promote longer paths. Differently from ResNets, in a randomly wired neural network with L architecture nodes the longest path may be shorter than L, leading to a smaller receptive field. In order to overcome this issue, we propose to modify the generation process of the random architecture by imposing that it should also include the sequential path, i.e., the path traversing all architecture nodes. This design of the architecture skews the initial path length distribution towards longer paths, which has the effect of promoting their usage. Nevertheless, the trainable architecture edge weights will ultimately define the importance of such contribution. <ref type="figure" target="#fig_2">Fig. 3</ref> shows an example of how including the sequential path changes the distribution of the receptive field radius.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. MonteCarlo DropPath regularization</head><p>The randomly wired architecture offers new degrees of freedom to introduce regularization techniques. In particular, one could delete a few architecture edges during training with probability p drop as a way to avoid co-adaptation of architecture nodes. This is reminiscent of DropOut <ref type="bibr" target="#b27">[28]</ref> and DropConnect <ref type="bibr" target="#b28">[29]</ref>, although it is carried out at a higher level of abstraction, i.e., connections between "layers" instead of neurons. It is also reminiscent of techniques used in Neural Architecture Search <ref type="bibr" target="#b29">[30]</ref> and the approach used in ImageNet experiments in <ref type="bibr" target="#b17">[18]</ref>, although implementation details are unclear for the latter.</p><p>We propose to use a MonteCarlo approach where paths are also dropped in testing. Inference is performed multiple times for different realizations of dropped architecture edges and results are averaged. This allows to sample from the full predictive distribution induced by DropPath, as in MonteCarlo DropOut <ref type="bibr" target="#b30">[31]</ref>. The following lemma shows that MonteCarlo DropPath decorrelates the contributions of paths in Eq. (2) even if they share architecture edges, thus allowing finer control over the modulation of the receptive field radius.</p><p>Lemma III.5. Let us consider two distinct paths p 1 and p 2 of a randomly wired network where the edges of the paths can be deleted with probability p drop . Then, even if the two paths share some architecture edges, their contributions to the derivative ?y ?x0 , as defined in Lemma III.2, are decorrelated. Proof. Let us consider two paths p 1 and p 2 with at least one common edge, we can compute the covariance between these two paths as follows:</p><formula xml:id="formula_13">Cov(? p1 , ? p2 ) = E[? p1 ? p2 ] ? E[? p1 ]E[? p2 ] = {i,j}?E p 1 ? ij {i,j}?E p 2 ? ij E {i,j}?I(p1,p2) z 2 ij {i,j}?D(p1,p2) z ij ? {i,j}?E p 1 ? ij {i,j}?E p 2 ? ij E ? ? {i,j}?E p 1 z ij ? ? E ? ? {i,j}?E p 2 z ij ? ? = 0, where I(p 1 , p 2 ) = E p1 ? E p2 , D(p 1 , p 2 ) = (E p1 ? E p2 ) ? (E p1 ? E p2 ), ? p1 = {i,j}?p1 z ij ? ij , ? p2 = {i,j}?p2 z ij ? ij , z ij ? Bernoulli(1 ? p drop )</formula><p>, and we have assumed all ? ij deterministic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>Experimental evaluation of GNNs is a topic that has recently received great attention. The emerging consensus is that benchmarking methods routinely used in past literature are inadequate and lack reproducibility. In particular, <ref type="bibr" target="#b31">[32]</ref> showed that commonly used citation network datasets like CORA, CITESEER, PUBMED are too simple and skew results towards simpler architectures or even promote ignoring the underlying graph. TU datasets are also recognized to be too small <ref type="bibr" target="#b33">[33]</ref> and the high variability across splits does not allow for sound comparisons across methods. In order to evaluate the gains offered by randomly wired architectures across, we adopt recently proposed benchmarking frameworks such as the one in <ref type="bibr" target="#b11">[12]</ref> and Open Graph Benchmarks <ref type="bibr" target="#b19">[20]</ref>.</p><p>First, we use two datasets in <ref type="bibr" target="#b11">[12]</ref> to analyse the performance differences between the baseline ResNet architecture, i.e., a feedforward architecture with skip connections after every layer, and the randomly wired architecture. We omit results on architectures without skip connections as these have already been shown to have poorer performance <ref type="bibr" target="#b11">[12]</ref>. We focus on the ZINC and CIFAR10 datasets. ZINC is one of the most popular real-world molecular datasets, and considers the task of property regression (constrained solubility) for the molecules represented as graphs. CIFAR10 is a well-known dataset for image classification, and, in this context, images are described by graphs of superpixels. For this experiment, we test four of the most commonly used graph convolution definitions: GCN <ref type="bibr" target="#b12">[13]</ref>, GIN [8] 2 , Gated GCN <ref type="bibr" target="#b23">[24]</ref>, and GraphSage <ref type="bibr" target="#b0">[1]</ref>. Notice that we do not attempt to optimize a specific method, nor we are interested in comparing one graph convolution to another. A fair comparison is ensured by running both methods with the same number of trainable parameters and with the same hyperparameters, keeping exactly the same ones used in <ref type="bibr" target="#b11">[12]</ref>. The learning rate of both methods is adaptively decayed between 10 ?3 and 10 ?5 and the stopping criterion is validation loss not improving for 5 epochs after reaching the minimum learning rate. Results are averaged over 4 runs with different weight initialization and different random architecture graphs, drawn with p = 0.6. The random architectures use sequential paths (Sec. III-B), but no DropPath (Sec. III-D) for the ZINC experiment, and DropPath but no sequential paths for CIFAR10.</p><p>The results in <ref type="table" target="#tab_1">Tables I and II</ref> show the performance achieved by randomly wired architectures and their ResNets counterparts for increasing model capacity (number of architecture nodes or layers L). We can notice that randomly wired GNNs have compelling performance in many regards. The superscript reports the standard deviation among runs and the subscript reports the level of significance by measuring how many baseline standard deviations the average value of the random architecture deviates from the average value of the baseline. Results are in bold if they are at least 1? significant. First of all, randomly wired GNNs typically provide lower error or higher accuracy than their ResNet counterparts for the same number of parameters. Moreover, they are more effective at increasing capacity than stacking layers: while they are essentially equivalent to ResNets for very short networks (e.g., for L = 4), they enable larger gains when additional layers are introduced. This is highlighted by <ref type="table" target="#tab_1">Table III</ref>, which shows the relative improvement in mean absolute error or accuracy averaged over all the graph convolution definitions, with respect to the short 4-layer network, where random wiring and ResNets are almost equivalent. This table highlights that deeper ResNets always provide smaller gains with respect to their shallow counterpart than the randomly wired GNNs. This allows us to conclude that randomly wired GNNs are a more effective way of increasing model capacity.</p><p>Moreover, we compare the proposed method against other state-of-the-art techniques to build graph neural networks, including methods that address the oversmoothing problem to build deeper GNNs (DeeperGCN) <ref type="bibr" target="#b34">[34]</ref> or argue that going wide instead of deep is more effective to increase the capacity (SIGN) <ref type="bibr" target="#b35">[35]</ref>. This experiment is done on the ogbg-molpcba dataset from Open Graph Benchmarks <ref type="bibr" target="#b19">[20]</ref> and results are taken from the public leaderboard. We use a randomly wired version of GIN and compare results with two different setups: a vanilla RAN-GIN with the same number of parameters as  GIN, and a larger RAN-GIN using the virtual node trick <ref type="bibr" target="#b36">[36]</ref> and FLAG augmentations <ref type="bibr" target="#b37">[37]</ref>. Both versions additionally use DropPath with p drop = 0.01. The results are reported in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. ABLATION EXPERIMENTS</head><p>In this section, we explore how some of the design choices for randomly wired GNNs can affect model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Architecture Edge probability</head><p>We first investigate the impact of the probability p of drawing an edge in the random architecture.    results for a basic random architecture without DropPath nor embedded sequential path. It appears that an optimal value of p exists that maximizes performance. This could be explained by a tradeoff between size of receptive field and the ability to modulate it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. DropPath</head><p>The impact of DropPath on CIFAR10 is shown in <ref type="table" target="#tab_1">Table VI</ref>. We found the improvement due to DropPath to be increasingly significant for a higher number of architecture nodes, as expected due to the increased number of edges. The value of the drop probability p drop = 0.01 was not extensively crossvalidated. However, <ref type="table" target="#tab_1">Table VII</ref> shows that higher drop rates typically lowered performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Embedded sequential path</head><p>The impact of embedding a sequential path as explained in Sec. III-B is shown in <ref type="table" target="#tab_1">Table VIII</ref>. It can be observed that its effect of promoting receptive fields with larger radius is useful on this task for any number of architecture nodes. We remark that, while we do not report results for the sake of brevity, this is not always the case and some tasks (e.g., CIFAR10) do not benefit from promoting larger receptive fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>We showed how randomly wired architectures can boost the performance of GNNs by merging receptive fields of multiple size. Consistent and statistically significant improvements over a wide range of tasks and graph convolutions highlight how such constructions are more effective at increasing model capacity than building deep GNN by stacking several layers in a linear fashion, even when residual connections are used.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Diego</head><label></label><figDesc>Valsesia and Giulia Fracastoro contributed equally to this work. The authors are with Politecnico di Torino -Department of Electronics and Telecommunications, Italy. Email:name.surname@polito.it.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>An architecture node is equivalent to a GNN layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Distribution of receptive field radius (p = 0.4, ? ij = 1 for unweighted, ? ij = 0.5 for weighted).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Path distribution as function of architecture edge probability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I ZINC</head><label>I</label><figDesc>MEAN ABSOLUTE ERROR. ?0.012 0.445 ?0.022 0.426 ?0.011 RAN-GCN 0.509 ?0.015 0.447 ?0.019 ?0.017 0.461 ?0.022 0.633 ?0.089 ?0.007 0.339 ?0.027 0.284 ?0.014 0.277 ?0.025 ?0.005 0.355 ?0.003 0.351 ?0.009 RAN-GraphSage 0.429 ?0.010 0.368 ?0.015</figDesc><table><row><cell></cell><cell>L = 4</cell><cell>L = 8</cell><cell>L = 16</cell><cell>L = 32</cell></row><row><cell>GCN</cell><cell>0.469 ?0.002 2.9?</cell><cell>0.465 1.5?</cell><cell>0.398 ?0.015 2.1?</cell><cell>0.385 ?0.015 3.7?</cell></row><row><cell cols="3">GIN 0.444 RAN-GIN 0.375 ?0.014 0.4? 0.381 ?0.021 0.398 ?0.004 2.7?</cell><cell>0.426 ?0.020 1.6?</cell><cell>0.540 ?0.155 1.0?</cell></row><row><cell cols="2">GatedGCN 0.368 RAN-GatedGCN 0.364 ?0.007 0.5?</cell><cell>0.310 ?0.010 1.1?</cell><cell>0.218 ?0.017 4.7?</cell><cell>0.215 ?0.025 2.5?</cell></row><row><cell>GraphSage</cell><cell>0.428 ?0.007 0.1?</cell><cell>0.363 1.0?</cell><cell>0.340 ?0.009 5.0?</cell><cell>0.333 ?0.008 2.0?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II</head><label>II</label><figDesc>?0.35 54.85 ?0.20 54.74 ?0.52 54.76 ?0.53 ?0.78 2.94? 66.67 ?0.73 63.99 ?1.45 58.18 ?2.92 RAN-GIN 67.48 ?1.08 67.36 ?0.70 ?0.80 69.16 ?0.66 69.46 ?0.47 RAN-GatedGCN 68.55 ?0.03 68.86 ?1.64 GraphSage 65.02 ?0.47 65.31 ?0.38 66.10 ?1.11 67.68 ?0.37</figDesc><table><row><cell></cell><cell cols="3">CIFAR10 ACCURACY.</cell><cell></cell></row><row><cell></cell><cell>L = 4</cell><cell>L = 8</cell><cell>L = 16</cell><cell>L = 32</cell></row><row><cell cols="2">GCN 54.28 RAN-GCN 55.31 ?0.25 2.9?</cell><cell cols="2">57.81 ?0.08 14.8? 57.29 ?0.44 4.9?</cell><cell>58.49 ?0.21 7.0?</cell></row><row><cell>GIN</cell><cell cols="2">70.66 1.0?</cell><cell>67.25 ?0.74 2.2?</cell><cell>62.73 ?1.57 1.6?</cell></row><row><cell>GatedGCN</cell><cell>69.26 ?0.36 2.0?</cell><cell>68.27 0.7?</cell><cell>72.00 ?0.44 4.3?</cell><cell>73.50 ?0.68 8.6?</cell></row><row><cell>GraphSage</cell><cell>66.14 ?0.21 2.4?</cell><cell>65.58 ?0.46 0.6?</cell><cell>66.12 ?0.11 0.0?</cell><cell>65.33 ?0.34</cell></row><row><cell cols="5">RAN-6.9?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table IV .</head><label>IV</label><figDesc>We can see that RAN-GIN (12 layers) significantly outperforms GIN and a number of other techniques for a comparable number of parameters. Furthermore, RAN-GIN with virtual node and FLAG augmentations reaches state-of-the-art performance on this benchmark, outperforming DeeperGCN and being very close to the recently proposed GINE<ref type="bibr" target="#b38">[38]</ref> 3 .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Table V shows the 3 Notice that we did not test RAN-GINE.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III MEDIAN</head><label>III</label><figDesc>RELATIVE GAIN OVER L = 4.</figDesc><table><row><cell></cell><cell>L = 8 L = 16 L = 32</cell></row><row><cell>ZINC</cell><cell>ResNet ?0.50% +0.37% +3.24%</cell></row><row><cell></cell><cell>Random +5.43% +8.89% +11.36%</cell></row><row><cell>CIFAR10</cell><cell>ResNet ?1.14% ?0.08% ?0.47%</cell></row><row><cell></cell><cell>Random +0.45% +2.62% +4.92%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV OGBG</head><label>IV</label><figDesc>-MOLPCBA AVERAGE PRECISION. ?0.0024 0.2059 ?0.0033 565, 928 GIN 0.2266 ?0.0028 0.2305 ?0.0027 1, 923, 433 ChebNet 0.2306 ?0.0016 0.2372 ?0.0018 1, 475, 003 SIGN 0.2047 ?0.0036 0.2163 ?0.0022 5, 516, 228 RAN-GIN 0.2493 ?0.0076 0.2514 ?0.0093 1, 868, 774 GCN+VN+FLAG 0.2384 ?0.0037 0.2556 ?0.0040 2, 017, 028 GIN+VN+FLAG 0.2834 ?0.0038 0.2912 ?0.0026 3, 374, 533 DeeperGCN+VN+FLAG 0.2842 ?0.0043 0.2952 ?0.0029 5, 550, 208 RAN-GIN+VN+FLAG 0.2879 ?0.0048 0.3041 ?0.0031 5, 572, 026 GINE++VN 0.2917 ?0.0015 0.3065 ?0.0030 6, 147, 029</figDesc><table><row><cell></cell><cell>Test AP</cell><cell>Val AP</cell><cell>No. params</cell></row><row><cell>GCN</cell><cell>0.2020</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE V EDGE</head><label>V</label><figDesc>PROBABILITY, L = 16, RAN-GCN.</figDesc><table><row><cell></cell><cell>p = 0.2</cell><cell>p = 0.4</cell><cell>p = 0.6</cell><cell>p = 0.8</cell></row><row><cell>ZINC</cell><cell>0.440 ?0.025</cell><cell>0.427 ?0.025</cell><cell>0.409 ?0.010</cell><cell>0.415 ?0.012</cell></row><row><cell>CIFAR10</cell><cell>56.53 ?0.61</cell><cell>56.21 ?0.48</cell><cell>57.44 ?0.46</cell><cell>56.06 ?0.48</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VI DROPPATH</head><label>VI</label><figDesc>ON CIFAR10, RAN-GATEDGCN. NO SEQUENTIAL PATH EMBEDDING. None 68.07 ?0.94 70.78 ?0.38 72.75 ?0.37 DropPath 68.86 ?1.64 72.00 ?0.44 73.50 ?0.68</figDesc><table><row><cell>L = 8</cell><cell>L = 16</cell><cell>L = 32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VII DROPPATH</head><label>VII</label><figDesc>ON CIFAR10, RAN-GATEDGCN. NO SEQUENTIAL PATH EMBEDDING. ?0.38 70.90 ?0.46 72.00 ?0.44 71.55 ?0.83 71.09 ?1.79</figDesc><table><row><cell></cell><cell></cell><cell>p drop</cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>0.005</cell><cell>0.01</cell><cell>0.02</cell><cell>0.03</cell></row><row><cell>70.78</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE VIII SEQUENTIAL</head><label>VIII</label><figDesc>PATH EMBEDDING ON ZINC, RAN-GATEDGCN. NO DROPPATH. ?0.027 0.264 ?0.029 0.234 ?0.030 Random+Sequential 0.310 ?0.010 0.218 ?0.017 0.215 ?0.025</figDesc><table><row><cell></cell><cell>L = 8</cell><cell>L = 16</cell><cell>L = 32</cell></row><row><cell>Fully random</cell><cell>0.332</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We use the term "feedforward neural network" to indicate an architecture made of a simple line graph, without skip connections: this is a representation of one path.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">GIN and RAN-GIN compute the output as in<ref type="bibr" target="#b10">[11]</ref>, using all architecture nodes.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dynamic edge-conditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond Euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A reduction of a graph to a canonical form and an algebra arising during this reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Weisfeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lehman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nauchno-Technicheskaya Informatsia</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="12" to="16" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">in International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">C</forename><surname>Delgadillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abualshour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06849</idno>
		<title level="m">Deepgcns: Making GCNs go as deep as CNNs</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Understanding the representation power of graph neural networks in learning graph topology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dehmamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-L</forename><surname>Barab?si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5453" to="5462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Benchmarking graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">DeepGCNs: Can GCNs go as deep as CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9267" to="9276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Graph neural networks exponentially lose expressive power for node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Suzuki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10947</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">On the bottleneck of graph neural networks and its practical implications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yahav</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05205</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Revisiting graph neural networks: All we have is low-pass filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maehara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09550</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Exploring randomly wired neural networks for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1284" to="1293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Residual networks behave like ensembles of relatively shallow networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Wilber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="550" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">146</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning Localized Generative Models for 3D Point Clouds via Graph Convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Valsesia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fracastoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Magli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR) 2019</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Predicting the sequence specificities of dna-and rna-binding proteins by deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alipanahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Delong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Weirauch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature biotechnology</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="831" to="838" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Residual gated graph convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07553</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural architecture search: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">55</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Survival models and data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Elandt-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">L</forename><surname>Johnson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980" />
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">110</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Le</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1058" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02142</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On the choice of graph neural network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vignac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ortiz-Jim?nez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint/>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="8489" to="8493" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A fair comparison of graph neural networks for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Errica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Podda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bacciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Micheli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.09893</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Deepergcn: All you need to train deeper gcns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07739</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Sign: Scalable inception graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11198</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Flag: Adversarial data augmentation for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.09891</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Graph convolutions that can finally model local structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Brossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Frigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dehaene</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.15069</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
