<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Discovering Object Masks with Transformers for Unsupervised Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wouter</forename><surname>Van Gansbeke</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Leuven</settlement>
									<country>ESAT-PSI</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Vandenhende</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Leuven</settlement>
									<country>ESAT-PSI</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><forename type="middle">Van</forename><surname>Gool</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Leuven</settlement>
									<country>ESAT-PSI</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<address>
									<settlement>Zurich</settlement>
									<country>CVL</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Discovering Object Masks with Transformers for Unsupervised Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The task of unsupervised semantic segmentation aims to cluster pixels into semantically meaningful groups. Specifically, pixels assigned to the same cluster should share high-level semantic properties like their object or part category. This paper presents MaskDistill: a novel framework for unsupervised semantic segmentation based on three key ideas. First, we advocate a data-driven strategy to generate object masks that serve as a pixel grouping prior for semantic segmentation. This approach omits handcrafted priors, which are often designed for specific scene compositions and limit the applicability of competing frameworks. Second, MaskDistill clusters the object masks to obtain pseudo-ground-truth for training an initial object segmentation model. Third, we leverage this model to filter out low-quality object masks. This strategy mitigates the noise in our pixel grouping prior and results in a clean collection of masks which we use to train a final segmentation model. By combining these components, we can considerably outperform previous works for unsupervised semantic segmentation on PASCAL (+11% mIoU) and COCO (+4% mask AP 50 ). Interestingly, as opposed to existing approaches, our framework does not latch onto low-level image cues and is not limited to object-centric datasets. The code and models are available. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task of assigning a class label to each pixel in an image -known as semantic segmentation -has been researched extensively <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b58">59]</ref>. Semantic segmentation tools are used in many domains like autonomous driving <ref type="bibr" target="#b19">[20]</ref>, medical imaging <ref type="bibr" target="#b57">[58]</ref>, and agriculture <ref type="bibr" target="#b17">[18]</ref>. Today, researchers tackle the segmentation task via deep convolutional nets <ref type="bibr" target="#b36">[37]</ref> which learn hierarchical image representations from fully-annotated datasets <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b51">52]</ref> where each pixel is associated with a category label. However, collecting such annotations consumes large amounts of time and money <ref type="bibr" target="#b5">[6]</ref>. Therefore, several works explored less labor-intensive forms of annotations to train a segmentation model, e.g., scribbles <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b69">70]</ref>, bounding boxes <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b63">64]</ref>, clicks <ref type="bibr" target="#b5">[6]</ref>, and image-level tags <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b84">85]</ref>. Others studied semi-supervised methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b63">64]</ref> that improve the performance by leveraging additional unlabeled images during training. In this paper, we go a step further and learn a segmentation model in a self-supervised way. Specifically, the goal is to learn a clustering function that assigns semantically related pixels to the same cluster without relying on human labeling.</p><p>To realize this concept, end-to-end methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b62">63]</ref> learned a clustering function by imposing consistency on the cluster assignments of pixels in augmented views of an image. However, these methods tend to latch onto low-level image cues like color or texture (see <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b73">74]</ref>). In particular, the clusters strongly depend on the network's initialization leading to degenerate solutions. Unlike these methods, we do not adopt an end-to-end strategy but follow the works discussed next.</p><p>Another group of works proposed a bottom-up approach for tackling the problem. First, they leverage a low-or mid-level visual prior like edge detection <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b90">91]</ref> or saliency estimation <ref type="bibr" target="#b75">[76]</ref> to find image regions that likely share the same semantics. In a second step, they use the image regions to learn pixel-embeddings that capture semantic information. In particular, the image regions serve as a regularizer which removes the segmentation's dependence on the network initialization. The pixel-embeddings can subsequently be clustered via K-means to obtain an image segmentation. While bottom-up approaches report better results, they suffer from several drawbacks too. Most importantly, their dependence on a handcrafted prior, e.g., edges or saliency, to group pixels limits their usage. For example, saliency estimation only applies to object-centric images. Additionally, several works require annotations to identify the appropriate image regions. For example, Hwang et al. <ref type="bibr" target="#b39">[40]</ref> use boundary annotations from <ref type="bibr" target="#b56">[57]</ref>. This paper presents MaskDistill, a novel framework that addresses the above limitations. Like bottomup methods, MaskDistill first identifies groups of pixels that likely belong to the same object. Since objectness is a high-level construct <ref type="bibr" target="#b47">[48]</ref>, we avoid using a handcrafted prior and instead advocate a data-driven approach. We observe that self-supervised vision transformers <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17]</ref> learn spatially structured image representations. In particular, it's possible to distill highly accurate object masks through the attention layers in vision transformers <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b77">78]</ref>. Different from existing works <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b90">91]</ref> which rely on handcrafted priors, this facilitates the scaling of our framework to more challenging datasets. In particular, handcrafted priors tend to be designed for specific scene compositions. For example, saliency estimation works well for images with few objects (e.g., PASCAL <ref type="bibr" target="#b26">[27]</ref>) but fails for more complex scenes (e.g., COCO <ref type="bibr" target="#b51">[52]</ref>). Our framework does not suffer from this problem (see <ref type="bibr">Section 4)</ref>.</p><p>We cluster the object masks and use the result as pseudo-ground-truth to train an object segmentation model, e.g., Mask R-CNN <ref type="bibr" target="#b35">[36]</ref>. As discussed in Section 3.3, this model predicts object mask candidates together with their confidence scores. We empirically observed that higher confidence scores correlate with object masks of better quality (see <ref type="figure">Figure 5</ref>). Based upon this observation, we construct a cleaner set of object masks by leveraging the model's predictions. In particular, we filter out predictions with low confidence scores for each image. The resulting set of object masks is used as pseudo-ground-truth to train a final semantic segmentation model.</p><p>In summary, our contributions are: (i) we develop a novel bottom-up framework to tackle the task of unsupervised semantic segmentation (Section 3), (ii) we present a data-driven strategy to get a pixel grouping prior for semantic segmentation based on self-supervised transformer models (Section 3.2), (iii) we analyze the use of confident object mask candidates to refine the segmentation results (Section 3.3), and (iv) we obtain state-of-the-art results on the well-known PASCAL <ref type="bibr" target="#b26">[27]</ref> and COCO <ref type="bibr" target="#b51">[52]</ref> datasets under the unsupervised setup (Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Unsupervised Semantic Segmentation. Several works tried to segment stuff categories, e.g., sky, grass, mountain, etc. For example, <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b62">63]</ref> maximized the mutual information between augmented views to learn a segmentation model. Others <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b18">19]</ref> iteratively refined the segmentation model's features via a clustering objective. However, these methods rely on the architectural prior which makes them prone to degenerate solutions, and limits their use to small-scale problems, e.g., segmenting roads and vegetation in satellite imagery. We refer to <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b75">76]</ref> for an analysis. This paper differs from these works in two ways. First, we segment object rather than stuff categories. This setting aligns better with popular segmentation benchmarks, e.g., PASCAL <ref type="bibr" target="#b26">[27]</ref>, COCO <ref type="bibr" target="#b51">[52]</ref>, etc. Furthermore, learning object-centric representations is a key component of machine intelligence with applications in augmented reality <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b30">31]</ref>. Second, unlike the referred works, we do not employ an end-to-end learning strategy which makes our framework less dependent on the architectural prior.</p><p>As mentioned, we focus on segmenting object categories. Earlier works <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b90">91]</ref> that studied this problem applied a two-step strategy. First, a handcrafted prior -e.g., superpixels, boundary maps, or saliency -is used to find groups of pixels that likely belong to the same object or part. Next, a pixel-level representation is learned that is discriminative of these groups. This allows the representations to be clustered via K-means to get an image segmentation. Our framework differs from these works in three ways. First, we do not use a handcrafted prior. Instead, we leverage the attention mechanism from self-supervised vision transformers <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17]</ref> to mine object masks as a pixel grouping prior. This data-driven strategy makes fewer assumptions on the scene compositions which eases scaling of our approach (see Section 3.1). Second, unlike earlier works <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b90">91]</ref>, we do not use additional annotations, e.g., boundary maps, to construct our prior. Third, we directly predict the cluster assignments and avoid using K-means as post-processing. For completeness, we include a recent work <ref type="bibr" target="#b33">[34]</ref> which also advocates a data-driven approach but considers fewer categories.</p><p>Unsupervised Object Detection. The task of unsupervised object detection aims to produce object candidates without using human annotations. This concept is realized via a class-agnostic objectness scoring function <ref type="bibr" target="#b1">[2]</ref> which estimates the probability for an image window to contain an object. Existing methods learned such a function via foreground-background masks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref>, superpixels <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b72">73]</ref> or edge information <ref type="bibr" target="#b91">[92]</ref>. Recent approaches <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b79">80]</ref> have shown promising results on large-scale benchmarks, e.g., COCO <ref type="bibr" target="#b51">[52]</ref> and OpenImages <ref type="bibr" target="#b48">[49]</ref>. In this work, we employ an object mask distillation strategy that is related to LOST <ref type="bibr" target="#b67">[68]</ref>. However, unlike our method, LOST fails to generate multiple object mask candidates per image -which is critical for the task of semantic segmentation. A few methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32]</ref> do produce several masks per image, but these are limited to small-scale problems (e.g., CLEVR <ref type="bibr" target="#b43">[44]</ref>).</p><p>Self-Supervised Representation Learning. These works learn visual representations from unlabeled images by solving pretext tasks. Some examples include predicting transformations <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b88">89]</ref>, predicting optical flow <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b87">88]</ref>, solving jigsaw puzzles <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b60">61]</ref>, predicting noise <ref type="bibr" target="#b6">[7]</ref>, performing clustering <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b85">86]</ref>, image colorization <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b89">90]</ref>, inpainting <ref type="bibr" target="#b65">[66]</ref>, predictive coding <ref type="bibr" target="#b61">[62]</ref> etc. The instance discrimination task <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b82">83]</ref> and its alternatives <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b32">33]</ref> outperform their supervised counterparts when transferring the representations to various downstream tasks, e.g., object detection. In this work, we explore self-supervised learning to capture objectness, allowing us to mine object mask candidates in a data-driven way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Our approach follows a bottom-up scheme to tackle the unsupervised semantic segmentation task. First, we advocate a data-driven approach to mine object masks via self-supervised vision transformers (Section 3.1). Second, we distill multiple object masks per image via an object segmentation model, i.e., Mask R-CNN (Section 3.2). Third, we discuss how to train a final segmentation model using the found object masks (Section 3.3). As a key component, we use only object masks with high confidence scores. This strategy mitigates the noise introduced during the mask distillation step. <ref type="figure">Figure 2</ref> shows an overview of our proposed MaskDistill framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Learning Objectness</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data-driven (Ours)</head><p>Edges <ref type="bibr" target="#b39">[40]</ref> Saliency <ref type="bibr" target="#b75">[76]</ref>  End-to-end approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b62">63]</ref> are unlikely to discover image regions that pertain to high-level object categories <ref type="bibr" target="#b73">[74]</ref>, e.g., birds, cats, buildings, etc. For this reason, we follow prior work <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b75">76]</ref>, and advocate a bottom-up approach to tackle the task of unsupervised semantic segmentation. In particular, it's advantageous to break down an image into its different components first, before going after its semantic segmentation. Existing methods achieve this via a handcrafted low-level (e.g., superpixels or edges) or mid-level (e.g., saliency) pixel grouping prior. However, such priors are suboptimal. A lowlevel prior based on superpixels or edges produces an over-segmentation of the image, which yields image regions with low semantic content (see the top row in <ref type="figure" target="#fig_0">Figure 1</ref>). Differently, a mid-level prior can aggregate parts from different objects (see the middle row in <ref type="figure" target="#fig_0">Figure 1</ref>). To address these drawbacks, we propose to obtain a pixel grouping prior in a data-driven way by relying on self-supervised representation learning. The bottom row in <ref type="figure" target="#fig_0">Figure 1</ref> shows some examples. Unlike handcrafted pixel grouping priors, our approach generates object masks that align  <ref type="figure">Figure 2</ref>: Overview of MaskDistill. We present a simple framework for unsupervised semantic segmentation. (Left) We distill class-agnostic object masks from self-supervised transformers <ref type="bibr" target="#b23">[24]</ref>. For each image, we commit to the most discriminative object. The found masks are subsequently clustered and used as pseudo-ground-truth to train an object segmentation model, i.e., Mask R-CNN <ref type="bibr" target="#b35">[36]</ref> (Section 3.2). (Right) The learned Mask R-CNN model predicts multiple object mask candidates per image with their respective confidence scores. We use the most confident predictions as pseudo-ground-truth to train a segmentation model (Section 3.3).</p><p>with true objects. For example, we correctly identify the entire plane in the 1st column, while other methods fail to do so.</p><p>In this paper, we build on self-supervised vision transformers <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17]</ref> to mine object masks. The reason for this decision is three-fold. First, transformers reason at patch-level <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b77">78]</ref> which allows us to construct an affinity graph expressing the similarity between different image regions. Second, self-supervised vision transformers learn rich spatial representations that capture object information <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b74">75]</ref> which facilitates their use for mining object masks. Moreover, the representations encode detailed information about each image component which can exceed a human-defined taxonomy. Third, self-supervised vision transformers do not rely on human annotations which allows us to take advantage of large unlabeled datasets <ref type="bibr" target="#b29">[30]</ref>. Motivated by these findings, we propose to distill object information from the final self-attention layer in the vision transformer <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Distilling Object Masks Using Self-Attention</head><p>Preliminaries. We reshape an image X ? R H?W ?3 into a sequence of N patches. Each patch is of size S ? S pixels. We refer to the image patches as patch tokens [I]. The patch tokens are further concatenated with a special classification token [CLS] resulting in the input sequence X which consists of N + 1 tokens. We use the features {q(h), k(h)} from the final multihead self-attention (MSA) block to compute object masks, where each head h performs a single self-attention operation. The self-supervised vision transformer is initialized with weights from <ref type="bibr" target="#b11">[12]</ref>. We refer the interested reader to <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b77">78]</ref> for more information on the self-attention mechanism in transformers.</p><p>Construct Affinity Graphs. Following prior work <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b77">78]</ref>, we construct an affinity graph to measure the similarity between image patches. Given the input sequence X , we compute the affinity vector a CLS as the pairwise similarities between the classification token [CLS] and the patch tokens </p><formula xml:id="formula_0">a CLS = 1 |H| h?H q CLS (h) ? k I (h) a CLS ? R 1?N ,<label>(1)</label></formula><formula xml:id="formula_1">A I = 1 |H| h?H k I (h) ? k I (h) A I ? R N ?N .<label>(2)</label></formula><p>Select Discriminative Tokens. Our goal is to select patch tokens that likely correspond to an object part. In particular, we focus on the top-k responses according to the affinities w.r.t. the [CLS] token a CLS . Formally, we define the set of patches P = {j | E(CLS, j) is top-k ? G CLS }, where E(CLS, j) denotes the directed edge from the classification token [CLS] to a patch token [I j ] in graph G CLS . In addition, we define the patch with the largest (i.e., top-1) affinity in a CLS as the source patch s = arg max j a j CLS . This region tends to correspond to the most discriminative image component, e.g., the beak of a bird, the horn of a rhino, etc.</p><p>Construct Initial Masks. We generate a single object mask M s ? {0, 1} 1?N per image X based on its source s and proposals P. The source s should belong to the predicted object mask as it represents the object's most discriminative part. We follow <ref type="bibr" target="#b67">[68]</ref> to diffuse the information from s to the proposals P. In particular, only patches in P that are similar to s are further considered</p><formula xml:id="formula_2">as proposals P = {j | j ? P ? A sj I &gt; 0}. The object mask M s is set to 1 at location j only if i?P A ij I &gt; 0.</formula><p>Consequently, patch j belongs to the same object as s, if the total sum of pairwise similarities between s and P is positive. Finally, the obtained mask is reshaped and upsampled to the original image size (H, W ) using nearest neighbor interpolation, resulting in M ? {0, 1} H?W .</p><p>Distill Mask R-CNN. To produce multiple object mask candidates per image, we train a region proposal network, i.e., Mask R-CNN <ref type="bibr" target="#b35">[36]</ref>. This object segmentation model requires the class c, the bounding box coordinates b, and the foreground-background mask M for each image. Notice that we obtained the object masks and their corresponding bounding box coordinates in the previous step. However, these masks are class-agnostic. In order to assign a class label c to each mask, we apply a clustering algorithm (e.g., K-means <ref type="bibr" target="#b52">[53]</ref>) to the output [CLS] tokens of the masked images. Now, we can train Mask R-CNN via the following objective function:</p><formula xml:id="formula_3">L obj = L class (?, c) + L bbox (b, b) + L mask (M , M ),<label>(3)</label></formula><p>where?,b andM denote the predicted class, bounding box and mask. Importantly, the trained model predicts multiple object mask candidates per image with their associated confidence scores. We leverage these predictions as pseudo-ground-truth to train a segmentation model in the next section.</p><p>Discussion. Like prior work <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b67">68]</ref>, MaskDistill constructs an affinity graph from the query and key features, respectively q(h) and k(h), in the final MSA block to produce an object mask. However, our method differs in two important components. First, we can generate multiple candidates for each image. This is crucial when tackling scene-centric datasets. Second, we use the top-k affinities in G CLS to generate the patch proposals P and their initial object mask M . Notice that this strategy does not make assumptions about the underlying scene composition as in <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b75">76]</ref>, e.g., the object should be salient or enclose a smaller area than the background. We empirically observe that our proposed approach results in better performance (see Section 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training a Segmentation Model from Noisy Object Mask Candidates</head><p>Consider the set of images X = {X 1 , . . . , X |D| } with their corresponding object mask candidates M = {M 1 , . . . , M K } and confidence scores S = {s 1 , . . . , s K } -obtained via the Mask R-CNN model from Section 3.2. 2 Some of the masks will inevitably get assigned to the wrong cluster or won't align with an object or part. Interestingly, we experimentally observe that masks for which the model is very confident (s i ? 1) tend to be correct (see experiment in Section 4.2). Unlike previous methods <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b90">91]</ref>, this allows us to leverage confidence scores to suppress the influence of the noise in our prior. Specifically, we only accept confident predictions from Mask R-CNN via a threshold ? as {M i |s i ? S ? s i &gt; ? }. Finally, we aggregate the masks belonging to the same image to obtain an initial semantic segmentation per image Y = {Y 1 , . . . , Y |D| }. We only keep the most confident mask when two candidates overlap. The constructed masks serve as pseudo-ground-truth to train a semantic segmentation model.</p><p>Finally, we train a semantic segmentation model ? ? : R H?W ?3 ? R H?W ?C parameterized with weights ?. This function terminates in a softmax operation to perform a soft assignment over the clusters C = {1, . . . , C}. To overcome class imbalance while simultaneously obtaining fine-grained segmentation results, we adopt a hard pixel mining strategy based on <ref type="bibr" target="#b83">[84]</ref>. The top-k most difficult pixels T are selected in each batch to train ? ? . In particular, the objective function becomes:</p><formula xml:id="formula_4">L seg = ? 1 |T | ? |C| i?T c?C Y (i, c) log? (i, c),<label>(4)</label></formula><p>where the obtained segmentation mask Y (i, c) is 1 if pixel i belongs to class c and 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Datasets. We conduct experiments on two popular benchmarks: PASCAL <ref type="bibr" target="#b26">[27]</ref> and COCO <ref type="bibr" target="#b51">[52]</ref>.</p><p>We follow prior work <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b75">76]</ref> and report our results on the 21 classes of PASCAL. The train_aug and val splits are used for training and evaluation respectively. We use all 80 object categories on COCO -a considerably challenging setting that is usually not considered for unsupervised semantic segmentation. We follow <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b79">80]</ref> and use the COCO20k subset for training and testing. This subset was introduced in [79] to evaluate object detection methods for scene-centric images. During K-means clustering, we use all COCO images to improve the clustering performance.</p><p>Mask Distillation Setup. We use the ViT-S <ref type="bibr" target="#b23">[24]</ref> vision transformer with a patch size of 16 ? 16 pixels for constructing the affinity graphs. The weights are initialized via self-supervised pre-training on ImageNet <ref type="bibr" target="#b11">[12]</ref>. We select the top-40% most discriminative patch tokens in the graph G CLS after resizing the smallest image side to 640 pixels. In order to assign a category label to each mask, we apply K-means <ref type="bibr" target="#b52">[53]</ref> on the output [CLS] tokens when using masked images as input. The Mask R-CNN model consists of a ResNet-50-C4 backbone. The training setup follows <ref type="bibr" target="#b34">[35]</ref>.</p><p>Segmentation Training Setup. We use a DeepLab-v3 <ref type="bibr" target="#b13">[14]</ref> segmentation model with dilated <ref type="bibr" target="#b86">[87]</ref> ResNet-50 backbone <ref type="bibr" target="#b36">[37]</ref> to facilitate a fair comparison with <ref type="bibr" target="#b75">[76]</ref>. The weights are initialized via self-supervised MoCo <ref type="bibr" target="#b16">[17]</ref> pre-training on ImageNet. We train the segmentation model for 45 epochs using batches of size 16. The weights are updated through SGD with momentum 0.9 and weight decay 10 ?4 . The learning rate is 2 ? 10 ?3 at the start and reduced to 2 ? 10 ?4 after 40 epochs. Further, we use confidence threshold ? = 0.9 to select the most confident masks from our Mask R-CNN model (see Section 3.3). We keep the mask with the largest confidence score when thresholding excludes all predictions in an image from being used. The cross-entropy loss in Eq. 4 uses the top-20% hardest pixels. Following <ref type="bibr" target="#b75">[76]</ref>, we freeze the first two ResNet blocks to increase speed.</p><p>Evaluation Protocols. We benchmark our approach via the evaluation protocols from <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b75">76]</ref>.</p><p>(i) Linear classifier: We train a 1 ? 1 convolutional layer on top of frozen features to predict the semantic classes. If the pixels are disentangled according to their semantic category, we should be able to solve the segmentation task via a low-capacity (linear) classifier. (ii) Clustering: We directly evaluate the quality of our clusters by comparing our predictions against the ground-truth annotations via Hungarian matching <ref type="bibr" target="#b46">[47]</ref>. The semantic segmentation results are evaluated via the mean IoU metric. Component Analysis. <ref type="table" target="#tab_0">Table 1</ref> analyzes the effect of different components of MaskDistill on the val set of PASCAL. We achieve 39.0% mIoU (first row) when clustering the initial object masks via K-means.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Studies</head><p>Recall that the object masks are obtained via the affinity graph G I from a self-supervised vision transformer (Section 3.2). The results are further improved when using predictions from a Mask R-CNN model trained with the initial object masks (from 39.0% to 42.0% mIoU -second row). This shows that our object mask candidates capture high-level object information, which is hard to achieve through handcrafted priors. Finally, we capitalize on the confidence scores predicted by Mask R-CNN, and show additional gains with our training recipe from Section 3.3. In particular, by using only confident object mask candidates from Mask R-CNN, our segmentation results improve from 42.0% to 45.8% mIoU. For completeness, removing the hard pixel mining strategy, results in 45.5% mIoU. We refer to the supplementary for additional ablation results.</p><p>Hyperparameter analysis. We study the influence of the hyperparameters on PASCAL and make the following observations: (i) <ref type="figure">Figure 3</ref> quantifies the impact of changing the number of cluster C during K-means clustering of the initial object masks (Section 3.2). We adopt the overclustering procedure from <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b75">76]</ref> and observe that the mask AP metric increases when we increase the amount of predicted clusters C. This means that the discovered clusters contain pixels of semantically related objects, irrespective of the amount C.</p><p>(ii) <ref type="figure">Figure 4</ref> shows the impact of the top-k selection. In order to mitigate the influence of spurious details (e.g., background clutter), we select the top-k patches in G CLS which correspond to the most discriminative patch tokens (see Section 3.2). To strike a good balance between the accuracy and the amount of proposals |P |, we set k to 40% in our experiments. (iii) <ref type="figure">Figure 5</ref> studies the influence of selecting the most confident object mask candidates with threshold ? , discussed in Section 3.3. We observe that the mIoU score plateaus around 75%. Finally, these results show that our approach is not very sensitive to the used hyperparameters, i.e., number of clusters C, top-k and threshold ? . As a result, we use the same setup in all experiments (see Section 4.1).  <ref type="table" target="#tab_1">Table 2</ref> compares our results against the state-of-theart on the PASCAL val set. MaskDistill consistently outperforms prior work under the linear classifier setup (+0.3 without CRF and +3.3% with CRF <ref type="bibr" target="#b45">[46]</ref>). Similarly, we report better results under the clustering setup (+10.8% mIoU). <ref type="figure" target="#fig_4">Figure 6</ref> visualizes the results for our method. The model can segment semantically meaningful image regions, e.g., dogs, cars, persons, etc. In conclusion, our method learns better dense semantic representations of images than existing approaches. We further analyze our results w.r.t. different groups of works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison to State-of-the-art</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Semantic Segmentation</head><p>(i) Proxy-tasks: MaskDistill outperforms works that solve proxy-tasks, e.g., optical flow <ref type="bibr" target="#b87">[88]</ref> or colorization <ref type="bibr" target="#b89">[90]</ref>, to learn dense representations. Such proxy tasks do not capture object-level information -an essential ingredient for tackling semantic segmentation. Differently, we capture such information explicitly by distilling object masks from self-supervised vision transformers. (ii) Clustering: We outperform end-to-end learning methods based on clustering, i.e., IIC <ref type="bibr" target="#b84">[85]</ref>. IIC is prone to degenerate solutions, as the network can easily latch onto low-level image cues like color. MaskDistill decouples feature learning and clustering to avoid this behavior. (iii) Contrastive learning: These works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b70">71]</ref> learn visual representations via a contrastive loss defined at the image level. This is suboptimal because the semantic segmentation task requires disentangling the representations at the object or part level. MaskDistill achieves this via a two-step approach.</p><p>(iv) Handcrafted grouping priors: Finally, MaskDistill outperforms methods <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b90">91]</ref> that relied on handcrafted priors to group pixels. Such priors fail to generalize to a diverse and complex dataset like PASCAL. Differently, we rely on a data-driven approach to model the pixel grouping prior. Surprisingly, we even outperform methods <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b90">91]</ref> ( ?) that finetuned the complete ASPP decoder.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Semantic Instance Segmentation</head><p>This section evaluates our object mask candidates by comparing them against the instance segmentation ground-truth masks on PASCAL and COCO20k. We perform this analysis for (i) the initial mask candidates which are used as pseudo-ground-truth to train Mask R-CNN, and (ii) the object masks predictions from the learned Mask R-CNN model. <ref type="table">Table 3</ref> compares our results against two other unsupervised object mask generation methods: DINO <ref type="bibr" target="#b11">[12]</ref> and LOST <ref type="bibr" target="#b67">[68]</ref>. We draw the following conclusions. First, our initial object masks outperform prior work. Our proposed mask distillation step makes fewer assumptions about the scene composition, e.g., the enclosed object area is not required to be smaller than the background <ref type="bibr" target="#b67">[68]</ref>. MaskDistill effectively combines the advantages in prior approaches <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b67">68]</ref> to address this issue (see Section 3.2). Second, the object mask candidates obtained via Mask R-CNN consistently outperform our initial object masks. We conclude that our model can better handle the multi-object setting. <ref type="figure" target="#fig_5">Figure 7</ref> shows several examples, where our method can retrieve multiple high-quality object masks per image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Limitations</head><p>We presented a novel framework for unsupervised semantic segmentation. It first distills object masks from a self-supervised vision transformer. Next, it learns a semantic segmentation model by leveraging <ref type="table">Table 3</ref>: Semantic Instance Segmentation Results. We follow the COCO-style AP mk metric. We consider all detections for the multi object setting and only the most confident one for the single object setting (see full details in the supplementary). In the class-agnostic case, the class is disregarded during evaluation. ( ?) indicates that the object mask candidates from Mask R-CNN are evaluated instead of the initial object masks from the transformer <ref type="bibr" target="#b11">[12]</ref> (see Section 3.2). the most confident object mask candidates as a pixel grouping prior. This strategy addresses several limitations present in prior works. First, our method learns a pixel grouping prior in a data-driven way, rather than through handcrafted priors, which eases scaling. Second, the segmentation model does not latch onto low-level image features but learns object-level information. Third, our approach can better handle images with multiple objects. Finally, our extensive experimental evaluation shows that our method significantly outperforms the state-of-the-art.</p><p>Undoubtedly, there are still several limitations to our work. First, it's unclear how the pre-training dataset of the self-supervised vision transformer influences the quality of the object masks. Interestingly, recent research <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b74">75]</ref> shows that we can use both object-and scene-centric datasets to learn spatially structured representations. This observation suggests that it's not crucial to train the transformer on a curated dataset, e.g., ImageNet <ref type="bibr" target="#b21">[22]</ref>. Also, there's a possibility to improve the results by scaling the pre-training dataset and model's sizes.</p><p>Another limitation of our work is that some instances can appear as a single object mask if their feature representations are strongly correlated, e.g., a motorcyclist on a motorbike. We identify several promising research directions that could potentially address this problem:</p><p>-Multi-scale grouping: It could be interesting to study pixel grouping priors which incorporate multi-scale features. In particular, such representations represent complementary information at the different scales <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b76">77]</ref> which could disambiguate between frequently co-occurring objects. -Pre-training strategy: We used DINO <ref type="bibr" target="#b11">[12]</ref> to extract object masks. Future work could study whether better results can be obtained by changing the pre-training method, dataset, or network architecture. For example, alternative pre-training techniques <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b80">81]</ref> could be more discriminative towards objects -or their parts -as they incorporate different inductive biases.</p><p>Broader Impact. The proposed method tackles the task of semantic segmentation without using human annotations. Our evaluation shows that our method obtains promising results on several challenging benchmarks. Therefore, this research could benefit several applications where the semantic segmentation task plays an important role, e.g., medical imaging, autonomous driving, etc.</p><p>It is hard to quantify the exact societal impact at this moment. This effect will also depend on the intentions of the users and inventors. In particular, we point out that the users of our method should be aware of the different biases present in the used datasets or pre-trained models. Since our approach does not rely on carefully annotated data, such biases could potentially yield unwanted results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Materials</head><p>We discuss the implementation details in Section A, additional ablations in Section B, additional results in Section C and specific failure cases in Section D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Implementation Details</head><p>This section provides additional implementation details. The code and pre-trained models will be made available upon acceptance. All Mask R-CNN experiments (see Section 3.2) were run on 4 32GB V100 GPUs. The refinement step (see Section 3.3) was run on 2 11GB 1080Ti GPUs. The total training time is around 20 hours. Our approach is implemented with Pytorch <ref type="bibr" target="#b64">[65]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Mask R-CNN</head><p>We follow He et. al. <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b34">35]</ref> to generate object mask candidates. In particular, we train Mask R-CNN <ref type="bibr" target="#b35">[36]</ref> with a ResNet-50-C4 backbone while using the Detectron2 framework <ref type="bibr" target="#b81">[82]</ref>. The model is initialized via self-supervised pre-training on ImageNet <ref type="bibr" target="#b21">[22]</ref>, i.e., MoCo <ref type="bibr" target="#b16">[17]</ref>. The weights of the first two backbone stages are frozen to speedup training. Furthermore, we pick a random value from the interval [480, 800] to resize the smallest image side during training, while the image scale is 800 during inference. The learning rate is set at 0.02 and reduced with a factor 10 after 20k and 22k iterations. The model is trained for a total duration of 24k iterations and learning rate warmup is applied for the first 100 iterations. We refer to <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref> for additional details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Linear Probing</head><p>During linear probing, we train a 1 ? 1 convolutional layer on top of the frozen features. This layer is trained for 45 epochs with a batch size of 24. We use the SGD optimizer with weight decay 10 ?4 and momentum 0.9 to update the model weights. The initial learning is set to 0.1 and decreased with a factor of 10 after 25 epochs. We didn't observe improvements when training longer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Semantic Segmentation</head><p>We follow the training and evaluation setup by Van Gansbeke et al. <ref type="bibr" target="#b75">[76]</ref>. The model is DeepLab-v3 <ref type="bibr" target="#b13">[14]</ref> with ResNet50 <ref type="bibr" target="#b36">[37]</ref> backbone. The model weights are updated using SGD with momentum 0.9 and weight decay 10 ?4 . The initial learning rate is 2 ? 10 ?3 and reduced to 2 ? 10 ?4 after 40 epochs of training. The total training duration is 45 epochs with a batch size of 16. We also apply the same RandomHorizontalFlip and ScaleNRotate augmentations during training. The original resolution is used for testing. Finally, the Hungarian algorithm <ref type="bibr" target="#b46">[47]</ref> matches the predicted clusters with the ground truth classes as in <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b75">76]</ref>. The mean intersection over union (mIoU) is used as the evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Semantic Instance Segmentation</head><p>For PASCAL <ref type="bibr" target="#b26">[27]</ref>, we evaluate on the official VOC2012 object segmentation set (2913 images). Both the VOC2007 and VOC2012 sets are used during training, following <ref type="bibr" target="#b34">[35]</ref>. For COCO <ref type="bibr" target="#b51">[52]</ref>, we evaluate on COCO20k by following prior work <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b78">79]</ref>. We use the mask average precision (AP) metric from Detectron2 <ref type="bibr" target="#b81">[82]</ref> to evaluate the predictions and we report the average over 5 different runs. We consider two scenarios during evaluation: the multi object and single object setting. In the multi object setting, the model must predict all the ground truth masks. In the single object setup, we only keep the mask with the highest confidence score for each image and select the ground truth object with the largest (bounding box) IoU for evaluation. Again, we apply the Hungarian algorithm <ref type="bibr" target="#b46">[47]</ref> to match the predicted clusters with the ground truth classes. To compare with prior work, we use the publicly available code. In DINO <ref type="bibr" target="#b11">[12]</ref>, we sum the attention heads and set the threshold to 0.75. In LOST <ref type="bibr" target="#b67">[68]</ref>, we take 400 patch proposals. These modifications improve their performances.   <ref type="table" target="#tab_0">Table S1</ref> complements the component analysis in the main paper (see <ref type="table" target="#tab_0">Table 1</ref>). We explore the predictions of the Mask R-CNN model by additionally using its confident bounding box predictions. We set the threshold ? to 0.9 as in the main paper. Unsurprisingly the performance drops when using the bounding box predictions instead of the mask predictions from Mask R-CNN (31.8% vs. 42.0%). Applying GrabCut <ref type="bibr" target="#b66">[67]</ref> to the predicted bounding box improves the results (38.2% vs. 31.8%). However, it still underperforms the initial object masks from the vision transformer (38.2% vs. 39.0%). This supports the claim that our masks capture high-level object information, which is hard to mimic by relying on handcrafted priors as used in GrabCut. Finally, we point out that multiple CRF <ref type="bibr" target="#b45">[46]</ref> iterations produce additional gains (48.9% mIoU vs 45.8%), primarily for detailed structures. However, be aware that in order to set the importance weights of the kernels correctly, a small annotated validation set is ideal. Albeit not a required component of our framework, we conclude that iteratively updating the pseudo-ground-truth with a CRF and the model weights ? improves the segmentation results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Ablations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional Results</head><p>This section discusses additional qualitative and quantitative results on the PASCAL dataset in Section C.1 and on the COCO dataset in Section C.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 PASCAL</head><p>We visualize additional examples from the PASCAL dataset. In particular, <ref type="figure" target="#fig_0">Figure S1</ref> displays the learned clusters from our semantic segmentation model ? ? and <ref type="figure" target="#fig_7">Figure S2</ref> shows the confident object mask candidates. Again, we conclude that our approach discovers objects that are semantically meaningful without the necessity for annotations. <ref type="table" target="#tab_1">Table S2</ref> presents the IoU score per class. We compare with prior SOTA <ref type="bibr" target="#b75">[76]</ref> and observe large improvements for all classes. MaskDistill discovers clusters such as bird, cat and train. Not surprisingly, less discriminative classes, like chair, table or plant, are more difficult to segment. Interestingly, when we apply a linear probe, the features quickly adapt to the semantics of the dataset (i.e., the PASCAL classes). We conclude that the model has learned semantically meaningful pixel-embeddings for different object categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 COCO</head><p>We show additional qualitative results. <ref type="figure">Figure S3</ref> displays examples of the confident object mask candidates. In contrast to PASCAL, COCO contains more complex (i.e., scene-centric) images. While the predictions are not perfect, MaskDistill detects and segments various objects fairly accurate. <ref type="figure">Figure S3</ref>: Instance segmentation results obtained with our confident object mask candidates on COCO20k. <ref type="figure">Figure S4</ref> presents several failure cases. These can be grouped as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Failure Cases</head><p>-Merging objects: In some cases, the predicted mask encompasses multiple objects, e.g., "person" and "racket", "person" and "snowboard" etc.</p><p>-Missing objects or parts: The mask excludes certain objects or parts, e.g., "bike handlebars". Similarly, our model is unable to detect certain background objects, e.g., "tennis spectators". -Out-of-taxonomy: The model generates object mask candidates that do not belong to the human-defined object categories, e.g., no class "clock" in the COCO (things) classes. <ref type="figure">Figure S4</ref>: Failure Cases on COCO20k.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Pixel grouping strategies. Our masks (bottom) capture high-level object information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Train segm. model with confident candidates</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>[I] in the final MSA block. Similarly, the affinity matrix A I measures the pairwise similarities between all pairs of patch tokens [I]. In particular, the element A ij is computed between two tokens of the sequence, i and j, as the dot product of their feature representations, f i and f j where f ? {q(h), k(h)}. Finally, we average the affinities over the attention heads H. The edges E CLS and E I in the graphs, G CLS and G I , are defined by their associated affinity weights, shown in Eq. 1 and 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :Figure 4 :Figure 5 :</head><label>345</label><figDesc>Influence of the number of clusters in K-means. Influence of the top-k selection in GCLS. Influence of the confidence threshold parameter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Semantic segmentation results of our method obtained under the clustering setup on PASCAL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Instance segmentation results obtained with our confident object mask candidates on COCO20k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure S1 :</head><label>S1</label><figDesc>Semantic segmentation results of our method obtained under the clustering setup on PASCAL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure S2 :</head><label>S2</label><figDesc>Instance segmentation results obtained with our confident object mask candidates on PASCAL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Component analysis of MaskDistill. Results on PASCAL for the clustering setup.</figDesc><table><row><cell>Setup</cell><cell cols="2">Section val mIoU</cell></row><row><cell cols="2">Self-sup. vision transformer Sec. 3.2</cell><cell>39.0</cell></row><row><cell>+ Mask R-CNN</cell><cell>Sec. 3.2</cell><cell>42.0</cell></row><row><cell>+ Segmentation model</cell><cell>Sec. 3.3</cell><cell>45.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>SOTA comparison on PASCAL.</figDesc><table><row><cell>(a) Linear classifier.</cell><cell></cell><cell>(b) Clustering.</cell></row><row><cell>Method</cell><cell>LC</cell><cell>Clustering</cell></row><row><cell>Proxy-tasks:</cell><cell></cell><cell></cell></row><row><cell>Co-Occurence [42]</cell><cell>13.5</cell><cell>4.0</cell></row><row><cell>CMP [88]</cell><cell>16.5</cell><cell>4.3</cell></row><row><cell>Colorization [90]</cell><cell>25.5</cell><cell>4.9</cell></row><row><cell>Clustering:</cell><cell></cell><cell></cell></row><row><cell>IIC [43]</cell><cell>28.0</cell><cell>9.8</cell></row><row><cell>Contrastive learning:</cell><cell></cell><cell></cell></row><row><cell>Inst. Discr. [83]</cell><cell>26.8</cell><cell>4.3</cell></row><row><cell>MoCo [35]</cell><cell>45.0</cell><cell>3.7</cell></row><row><cell>InfoMin [72]</cell><cell>45.2</cell><cell>4.4</cell></row><row><cell>SwAV [11]</cell><cell>50.7</cell><cell>4.4</cell></row><row><cell cols="2">Handcrafted grouping priors:</cell><cell></cell></row><row><cell>SegSort [40]  ?</cell><cell>36.2</cell><cell>-</cell></row><row><cell>Hierarch. Group. [91]  ?</cell><cell>48.8</cell><cell>-</cell></row><row><cell>MaskContrast [76]</cell><cell>58.4</cell><cell>35.0</cell></row><row><cell cols="2">MaskContrast [76]+CRF 59.5</cell><cell>-</cell></row><row><cell>MaskDistill</cell><cell>58.7 (+0.3)</cell><cell>45.8 (+10.8)</cell></row><row><cell>MaskDistill+CRF</cell><cell>62.8 (+3.3)</cell><cell>48.9 (+13.9)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table S2 :</head><label>S2</label><figDesc>Semantic Segmentation Results. We evaluate on the PASCAL val set. ( ?) indicates that we use a linear probe (see Section A.2). Method backg. aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mIoU MaskContrast [76] 84.4 68.1 23.7 62.6 35.7 0.0 72.8 63.0 46.8 0.0 0.0 8.5 30.6 28.9 49.4 19.4 5.6 34.8 17.2 55.7 27.3 35.0 MaskDistill 84.4 74.7 27.9 70.9 47.5 0.0 72.8 33.2 72.4 0.0 70.0 29.6 38.1 67.5 58.1 28.1 9.2 65.8 20.4 65.5 27.7 45.8 MaskDistill+CRF 85.4 80.3 28.8 74.7 50.4 0.0 72.5 52.1 75.7 0.0 76.5 28.6 38.7 71.3 63.8 32.0 11.2 67.0 20.5 67.7 28.7 48.9 MaskDistill ? 88.1 80.5 30.9 76.7 58.2 52.2 75.7 70.1 82.7 12.9 73.3 35.4 78.8 72.2 62.1 52.4 27.9 73.0 19.7 70.5 39.0 58.7 MaskDistill ? +CRF 89.8 83.1 34.0 85.9 63.3 45.0 79.1 70.1 86.3 16.9 81.5 38.1 84.0 74.9 69.7 63.0 31.3 78.3 23.3 74.4 46.1 62.8</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table S1 :</head><label>S1</label><figDesc>Component analysis.</figDesc><table><row><cell>Setup</cell><cell>val mIoU</cell></row><row><cell>self.sup. vision transformer</cell><cell>39.0</cell></row><row><cell>+ Mask R-CNN (bbox)</cell><cell>31.8</cell></row><row><cell>+ GrabCut</cell><cell>38.2</cell></row><row><cell>self.sup. vision transformer</cell><cell>39.0</cell></row><row><cell>+ Mask R-CNN (mask)</cell><cell>42.0</cell></row><row><cell>+ Segmentation model</cell><cell>45.8</cell></row><row><cell>+ CRF</cell><cell>48.9</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code: https://github.com/wvangansbeke/MaskDistill Preprint.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">K is typically much larger than the number of images |D| in the dataset as Mask R-CNN returns multiple object mask candidates per image.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment. The authors thankfully acknowledge support by Toyota Motor Europe (TME) via the TRACE project.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Augmented reality meets computer vision: Efficient data generation for urban driving scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Abu Alhaija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Mustikovela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Measuring the objectness of image windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Labelling unlabelled videos from scratch with multi-modal self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Self-labelling via simultaneous clustering and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">What&apos;s the point: Semantic segmentation with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bearman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised learning by predicting noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Monet: Unsupervised scene decomposition and representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kabra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11390</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised pre-training of image features on noncurated data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>2021) 2, 3, 4, 5, 6, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cpmc: Automatic object segmentation using constrained parametric min-cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV) (2021) 2, 3, 4</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Agriculture-vision: A large aerial image database for agricultural pattern analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Brunner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Khachatrian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Karapetyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dozier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Picie: Unsupervised semantic segmentation using invariance and equivariance in clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Mall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2021)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Virtex: Learning visual representations from textual annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR) (2021)</title>
		<imprint>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>ICLR) (2021) 2, 4</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Category-independent object proposals with diverse ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Genesis: Generative scene inference and sampling with object-centric latent representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">P</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>IJCV) (2010) 1, 2, 6</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lefaudeux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.01988</idno>
		<title level="m">Self-supervised pretraining of visual features in the wild</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Scaling and benchmarking self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Westbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Byrne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chavis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hamburger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.07058</idno>
		<title level="m">Ego4d: Around the world in 3,000 hours of egocentric video</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-object representation learning with iterative variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kabra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Unsupervised semantic segmentation by distilling feature correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.08414</idno>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR) (2020) 3, 6</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV) (2017) 2, 4</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR) (2016) 1</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Decoupled deep neural network for semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Adversarial learning for semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Liou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.07934</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Segsort: Segmentation by discriminative sorting of segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR) (2019) 2, 3, 5</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Let there be color! joint end-to-end learning of global and local image priors for automatic image colorization with simultaneous classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ishikawa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>SIGGRAPH</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06811</idno>
		<title level="m">Learning visual groups from co-occurrences in space and time</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Invariant information clustering for unsupervised image classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Clevr: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Simple does it: Weakly supervised instance and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval research logistics quarterly</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deepbox: Learning objectness with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The open images dataset v4</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Colorization as a proxy task for visual understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Scribblesup: Scribble-supervised convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Least squares quantization in pcm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lloyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Cross pixel optical-flow similarity for self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thewlis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference of Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Prime object proposals with randomized prim&apos;s algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">The multimodal brain tumor image segmentation benchmark (brats)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jakab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Farahani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Burren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Porz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Slotboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wiest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Image segmentation using deep learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Minaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kehtarnavaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Boosting self-supervised learning via knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vinjimoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Autoregressive unsupervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ouali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hudelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV) (2020) 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Grabcut -interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Localizing objects with self-supervised transformers and no labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sim?oni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Puy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">V</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roburin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC</title>
		<imprint>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>2021) 3, 4, 5, 6, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Normalized cut loss for weakly-supervised cnn segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Djelouah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schroers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">On regularized losses for weakly-supervised cnn segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Djelouah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">B</forename><surname>Ayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schroers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Contrastive multiview coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV) (2020)</title>
		<imprint>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">What makes for good views for contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Scan: Learning to classify images without labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV) (2020) 1, 2, 3</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Revisiting contrastive methods for unsupervised learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS) (2021) 2, 4</title>
		<imprint>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Unsupervised semantic segmentation by contrasting object mask proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV) (2021) 2, 3, 5</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<title level="m">Mti-net: Multi-scale task interaction networks for multi-task learning. In: European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Toward unsupervised, multi-object discovery in large-scale image collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">V</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV) (2020) 3</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Large-scale unsupervised object discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">V</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sizikova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS) (2021)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Dense contrastive learning for self-supervised visual pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Bridging category-level and instance-level semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06885</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Learning to segment under various forms of weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Clusterfit: Improving generalization of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Self-supervised learning via conditional motion propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Aet vs. aed: Unsupervised representation learning by auto-encoding transformations rather than data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Self-supervised visual representation learning from hierarchical grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS) (2020) 2, 3</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Edge boxes : Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
