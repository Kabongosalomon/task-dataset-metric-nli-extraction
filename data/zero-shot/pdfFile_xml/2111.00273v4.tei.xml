<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-Modality Fusion Transformer for Multispectral Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Qingyun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Aerospace Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Dapeng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Aerospace Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Zhaokui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Aerospace Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cross-Modality Fusion Transformer for Multispectral Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Cross-modality</term>
					<term>feature fusion</term>
					<term>multispectral object detection</term>
					<term>Transformer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multispectral image pairs can provide combined information, making object detection applications more reliable and robust in the open world. To fully exploit the different modalities, a simple yet effective cross-modality feature fusion approach, named Cross-Modality Fusion Transformer (CFT) is presented in this paper. Unlike prior CNNs-based works, our network learns long-range dependencies and integrates global contextual information in the feature extraction stage under the guidance of the Transformer scheme. More importantly, by leveraging self attention of the Transformer, the network can naturally execute intra-modality and inter-modality fusion simultaneously, and robustly capture the latent interactions between RGB and thermal domains. Thereby the performance of multispectral object detection is improved significantly. Extensive experiments and ablation studies on multiple datasets demonstrate that the proposed scheme is effective and obtains state-of-the-art detection performance. Our code and models are available at https://github.com/DocF/multispectral-object-detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In real-world object detection applications, the environment is often open and dynamic, requiring models and algorithms to deal with the challenges caused by openness, such as rain, fog, occlusions, poor lighting, low resolution, etc. It is difficult for an algorithm to use only visible-band sensor data to achieve high accuracy under these conditions. Hence, the multispectral imaging technology is being adopted, given its ability to provide the combined information coming from multispectral cameras e.g., visible and thermal. By fusing the complementarity of different modalities, the perceptibility, reliability, and robustness of the detection algorithms can be improved.</p><p>Recent advances in convolutional neural networks (CNNs), more specifically, the invention of two-stream CNN-based detectors, have increased detection performance in the field of multispectral object detection <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>. In addition, some challenging multispectral datasets, e.g., FLIR <ref type="bibr" target="#b11">[12]</ref>, LLVIP <ref type="bibr" target="#b12">[13]</ref>, VEDAI <ref type="bibr" target="#b13">[14]</ref>, have also driven the advancement of this technology. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the advantages of multispectral images over visible-only or thermal-only images under different conditions. However, the exploitation of multispectral data will raise new challenges: How to integrate the representations to fully take advantage of the inherent complementarity between different modalities? And how to design an effective cross-modality fusion mechanism for maximum performance gain? Compared with the visual image on the left, the thermal image on the right can capture a clearer contours of pedestrians under insufficient illumination conditions. Besides, the thermal image also captures pedestrians obscured by a pillar. The paired images in the second row are captured in daytime. During well-lit daytime, the visual image has more details, such as edges, textures, and colors, than thermal images. With these details, we can easily find the driver hidden in the motor three wheeler. However, the driver is difficult to be found in the thermal image. Zoom in to see details.</p><p>In the prior works, no matter how to design the modal fusion mechanism, they are all mostly based on deep convolutional neural networks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. There is considerable literature that has proven that CNNs can have strong representation learning capabilities within a single intra-modal reasoning <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref>, especially for visual modality. However, it is non-trivial to extend them to cross-modality fusion or modality interaction to fully exploit the inherent complementarity. The convolution operator of CNNs can be described as a non-fully connected graph in which each spatial position in the feature maps is consider as a node. Since the convolution operator has a nonglobal receptive field, the information is only integrated in a local area. In contrast, the self attention operator of Transformer <ref type="bibr" target="#b22">[23]</ref> can be regarded as a fully connected graph, so it can learn long-range dependencies and its receptive field can be global. Therefore, the self attention of Transformer provides a natural mechanism which is more suitable for connecting multimodal signals than the convolution of CNNs. Multimodal Transformers have been applied to various tasks including image segmentation <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>, cross-modal sequence generation <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>, video retrieval <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref> and image/video captioning/classification <ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref>. However, there is no work which designs the Transformer for multispectral object detection in the literature.</p><p>We propose a novel and effective multispectral fusion approach, which is called Cross-Modality Fusion Transformer (CFT), to explore the potential of Transformer in the application of multispectral object detection. Specifically, our CFT modules are embedded in the feature extraction backbone in order to integrate global contextual information from different modalities. To the best of our knowledge, this is the first work adopting the Transformer for multispectral object detection.</p><p>Contributions: (1) We introduce a new and powerful twostream backbone that enhances one modality from another modality under the guidance of the Transformer scheme. <ref type="bibr" target="#b1">(2)</ref> We propose a simple yet effective CFT module, and give theoretical insights into it, showing that the CFT module simultaneously fuses the intra-modality and inter-modality features.</p><p>(3) Considerable experiments show that the current method achieves state-of-the-art performance on three public datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Fusing the features of two modalities is the core problem in multispectral object detection, which can be described as</p><formula xml:id="formula_0">F Fused = F (F R , F T ) = F (? R (I R ) , ? T (I T )) .<label>(1)</label></formula><p>I R and I T represents the input RGB image and the input Thermal image, respectively. F R denotes the RGB feature maps and F T denotes the Thermal feature maps. The feature extraction functions of networks, ? R (.) and ? T (.), are applied to generate the feature maps for the different modal input images. F Fused indicates the fused feature maps, and F (.) is the function of fusion. In the Eq. (1), multi-modality fusion can be divided into two aspects: input variable (i.e., input features), and fusion functions. Following the idea of Eq. (1), the previously published investigations in the literature can also be sorted into two categories, one is focued on the input features ("macro" level), and the other is focued on constructing the fusion functions ("micro" level). Macro level. On this level, researchers aim to solve the problem of where to fuse, that is, which stage of the input features to choose to fuse. Most of them explore the best fusion stage by designing the macro network architectures. The first study of this type <ref type="bibr" target="#b34">[35]</ref> investigates two deep fusion architectures (early fusion and late fusion) and analyzes their performance on multispectral data. To explore the potential of deep models for multispectral pedestrian detection further, Liu et al. <ref type="bibr" target="#b0">[1]</ref> designed another two ConvNet fusion architectures (halfway fusion and score fusion) and demonstrated that the halfway fusion model achieved the best detection synergy. Since then, several subsequent works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b35">36]</ref> illustrated that the halfway fusion overwhelmingly outperforms the other three fusion architectures.</p><p>Mirco level. Besides the macro network architectures, the construction of fusion function is another key point for complementary learning of modality interaction. Naturally, the most straightforward way is to utilize concatenation, elementwise addition, element-wise average/maximum, and elementwise cross product to merge feature maps of RGB and thermal modalities directly. Two variations of novel Gated Fusion Units (GFU) <ref type="bibr" target="#b36">[37]</ref> are proposed in GFD-SSD to investigate the combination of feature maps generated by the two SSD middle layers. Zhang et. al. <ref type="bibr" target="#b5">[6]</ref> proposed a novel cycle fuse-andrefine module to improve the multispectral feature fusion while achieving the complementary/consistency balance of the features. Transformer <ref type="bibr" target="#b22">[23]</ref> is originally a classic Natural Language Processing (NLP) model proposed by the Google team in 2017. However, due to its strong representation ability and concise model, it has been extended to computer vision and multimodal fields in recent years <ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref>. Different from the prior methods, a Transformer-based scheme is proposed to fuse intra-modal and inter-modal information for multispectral object detection in current paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>Overview. To demonstrate the effectiveness of our proposed CFT fusion module, we extend the framework of YOLOv5, to enable multispectral object detection. To be precise, we redesign the YOLOv5 feature extraction network as a twostream backbone, which is similar to GFD-SSD and embedded the CFT modules to facilitate modal fusion and modal interaction, named as Cross-Modality Fusion Backbone (CFB). An illustration of our Cross-Modality Fusion Backbone and Cross-Modality Fusion Transformer is presented in <ref type="figure" target="#fig_2">Fig. 2</ref>.</p><p>Details. Specifically, given the intermediate RGB convolution feature maps F R ? R C?H?W , and thermal convolution feature maps F T ? R C?H?W , the sentences I R ? R HW?C and I T ? R HW?C are obtained by flattening each feature map and permuting the order of the matrices. Second, we concatenate the sentences of each modality and add a learnable positional embedding, which is a trainable parameter of dimension 2HW ? C, to get the input sentences I ? R 2HW?C of the Transformer. The positional embedding enables the model to dif-  ferentiate spatial information between different tokens at train time <ref type="bibr" target="#b22">[23]</ref>. Third, the input sequence I is projected onto three weight matrices to compute a set of queries, keys and values (Q, K and V),</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Modality Fusion Transformer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Modality Fusion Backbone</head><formula xml:id="formula_1">Q = IW Q ,<label>(2)</label></formula><formula xml:id="formula_2">K = IW K ,<label>(3)</label></formula><formula xml:id="formula_3">V = IW V ,<label>(4)</label></formula><p>where W Q ? R C?D Q , W K ? R C?D K and W V ? R C?D V are weight matrices. Moreover, D Q , D K and D V are equal in our Transformer , i.e., D Q = D K = D V = C. Fourth, the self attention layer uses the scaled dot products between Q and K to compute the attention weights and then multiply by the values to infer the refined output Z,</p><formula xml:id="formula_4">Z = Attention(Q, K, V) = softmax QK T ? D K V,<label>(5)</label></formula><formula xml:id="formula_5">where 1 ? D K</formula><p>is a scaling factor for preventing the softmax function from falling into a region with extremely small gradients when the magnitude of dot products grow large. To encapsulate multiple complex relationships from different representation subspaces at different positions, the multi-head attention mechanism is adopted,</p><formula xml:id="formula_6">Z = MultiHead(Q, K, V) = Concat (Z 1 , . . . , Z h ) W O , Z i = Attention QW Q i , KW K i , VW V i .<label>(6)</label></formula><p>The subscript h denotes the number of heads, and W O ? R h?C?C denotes the projected matrix of Concat (Z 1 , . . . , Z h ). Then the Transformer uses a two-layer fully connected feed-forward network with a GELU <ref type="bibr" target="#b37">[38]</ref> activation in between to calculate the output sequences O, which are of the same shape as input se-</p><formula xml:id="formula_7">quences I, O = MLP(Z ) + Z , (7) = FC 2 GELU FC 1 (Z ) + Z<label>(8)</label></formula><p>where Z = Z + I. Finally, exploiting the inverse operation of the first step, the output sentences O are converted into the recalibration results F R and F T and added to the original modality branch as complementary information. Implementations. The parameter amount and computational complexity of a standard Transformer block can be formalized as</p><formula xml:id="formula_8">Params ? O(Transformer) = 4HWC + 8C 2 ,<label>(9)</label></formula><p>FLOPs ? ?(Transformer) = 12HWC 2 + 2(HW) 2 C. <ref type="formula" target="#formula_0">(10)</ref> A CFT module has 8 duplicate Transformer blocks, as shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. In addition, since the dimension of the input sentences I is 2HW ? C, the actual expression of HW in the above Eq. <ref type="formula" target="#formula_8">(9)</ref> and <ref type="formula" target="#formula_0">(10)</ref> is 2HW. Apart from the Parameters and the Floating Point Operations (FLOPs) , the memory access also needs to be considered <ref type="bibr" target="#b38">[39]</ref>, especially when calculating the dot product of queries and keys, an intermediate matrix of 2HW ? 2HW dimensions will be generated. When the input picture size is 640 ? 640, after two downsamplings (H = W = 160), the elements of the matrix QK T exceed 2.4G, which is unacceptable for ordinary computers. For reducing expensive computation, our solution is to use a global average pooling that downsamples the feature maps to a low and fixed resolution of H = W = 8 before passing them to the Transformer block. And the output is upsampled by bilinear interpolation to the original resolution before being added to the original mode branch. In this way, the number of parameters and computational complexity of our multispectral detector are acceptable (cf. the Parameters and FLOPs in <ref type="table" target="#tab_1">Table  2)</ref>.</p><formula xml:id="formula_9">? = softmax QK T ? D K = ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? C 1 C HW C HW+1 C 2HW L 1 ? 1,1 ? 1,2 ? ? ? ? 1,HW ? 1,HW+1 ? 1,HW+2 ? ? ? ? 1,2HW ? 2,1 ? 2,2 ? ? ? ? 2,HW ? 2,HW+1 ? 2,HW+2 ? ? ? ? 2,2HW . . . . . . . . . . . . . . . . . . . . . . . .</formula><p>L HW ? HW,1 ? HW,2 ? ? ? ? HW,HW ? HW,HW+1 ? HW,HW+2 ? ? ? ? HW,2HW L HW+1 ? HW+1,1 ? HW+1,2 ? ? ? ? HW+1,HW ? HW+1,HW+1 ? HW+1,HW+2 ? ? ? ? HW+1,2HW ? HW+2,1 ? HW+2,2 ? ? ? ? HW+2,HW ? HW+2,HW+1 ? HW+2,HW+2 ? ? ? ? HW+2,2HW . . .</p><formula xml:id="formula_10">. . . . . . . . . . . . . . . . . . . . . L 2HW ? 2HW,1 ? 2HW,2 ? ? ? ? 2HW,HW ? 2HW,HW+1 ? 2HW,HW+2 ? ? ? ? 2HW,2HW ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ,<label>(11)</label></formula><p>As mentioned before, YOLOv5 is chosen to be our basic detector. By adding an additional branch for the extraction of thermal features, it is transformed into a two-stream convolutional neural network, which constitutes the baseline. In other words, the difference between the baseline and our transformer-based fusion detection algorithm is only the CFT module.</p><p>Why Transformer? The main idea behind our module is leveraging the self attention mechanism to learn the binary relationship of RGB and thermal modalities, more precisely, leveraging the correlation matrix to weight each position of the input feature maps. It can be formulated as Eq.(11). In the formula, ? i, j represents the correlation between the i-th position and the j-th position on the feature maps. According to the Eq. (11), four matrix blocks can be inferred naturally, when calculating the correlation matrix ?. Two of them are intra-modality correlation matrix blocks (RGB and thermal), and the other two are inter-modality correlation matrix blocks, as illustrated in <ref type="figure">Fig.  3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>oposal</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FC Classification Regions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB intra-modality Correlation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inter-modality Correlation Thermal intra-modality Correlation</head><p>Inter-modality Correlation = <ref type="figure">Figure 3</ref>: Illustration of the correlation matrix ?.</p><p>Hence, with the help of Transformer, we don't need to carefully design the modal fusion module. We just need to simply splice the multi-modal features into a sequence, and then the Transformer can automatically perform simultaneous intramodality and inter-modality information fusion and robustly capture the latent interactions between RGB and hermal domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>All experments are evaluated on three benchmark datasets, i.e, FLIR <ref type="bibr" target="#b11">[12]</ref> , LLVIP <ref type="bibr" target="#b12">[13]</ref> and VEDAI <ref type="bibr" target="#b13">[14]</ref>.</p><p>FLIR. The FLIR ADAS dataset is a challenging multispectral object detection dataset that includes day and night scenes. There are a lot of unaligned image pairs in the original data set, which makes network training difficult. Therefore, an "aligned" version <ref type="bibr" target="#b5">[6]</ref> is recently released that manually removes unaligned visible-thermal image pairs. This new dataset contains 5,142 well-aligned multispectral image pairs, of which 4,129 pairs are used for training and 1,013 pairs are used for testing and cover three object categories: "person", "car" and "bicycle". We conduct experiments on the "aligned" version of the FLIR dataset in current work, and for convenience, the FLIR that appears later all refer to its "aligned" version.</p><p>LLVIP. LLVIP is a very recently released visible-infrared paired pedestrians dataset for low-light vision. This dataset contains 33672 images, or 16836 pairs, most of which were taken in low-light environments, and all of the images are strictly spatio-temporal aligned.</p><p>VEDAI. In addition to the above two ground-view datasets, we also test our method on VEDAI which is a multispectral aerial imagery dataset for vehicle detection. The dataset contains nine vehicle classes for a total of more than 3700 annotated targets in more than 1200 images in two different resolutions (1024 ? 1024 and 512 ? 512). Both RGB and IR modalities are available for each image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Loss Function</head><p>In this section, the loss functions utilized for training our proposed mutlispectral object detectors is introduced. Formally, the overall loss function is a sum of the bounding-box regression loss (L box ), the classification loss(L cls ) and the confidence loss (L conf ),</p><formula xml:id="formula_11">L total = L box + L cls + L conf = L box + L cls + L noobj + L obj ,<label>(12)</label></formula><p>where,</p><formula xml:id="formula_12">L box = S 2 i=0 N j=0 1 ob j i, j ? L GIoU i = S 2 i=0 N j=0 1 ob j i, j ? [1 ? GIoU i ] = S 2 i=0 N j=0 1 ob j i, j ? 1 ? B g i ? B p i B g i ? B p i + B c i \(B g i ? B p i ) B c i ,<label>(13)</label></formula><formula xml:id="formula_13">L cls = S 2 i=0 N j=0 1 ob j i, j ? c? classes p i (c) log (p i (c)) ,<label>(14)</label></formula><formula xml:id="formula_14">L noobj = S 2 i=0 N j=0 1 noob j i, j ? (c i ?? i ) 2 ,<label>(15)</label></formula><formula xml:id="formula_15">L obj = S 2 i=0 N j=0 1 ob j i, j ? (c i ?? i ) 2 .<label>(16)</label></formula><p>As Eq. <ref type="formula" target="#formula_0">(13)</ref> shows, Generalized Intersection over Union (GIoU) loss <ref type="bibr" target="#b39">[40]</ref> is employed as the predicted box regression loss. GIoU loss can be a better choice compared to IoU loss, no matter which IoU-based performance measure is ultimately used. S 2 and N denote the number of image grids during prediction and the number of the predicted boxes in each grid. B g , B p , and B c are the ground truth, the predicted box, and the smallest enclosing box surrounding B g and B p , respectively. The coefficient 1 ob j i, j indicates whether the jth predicted box of the ith grid is a positive sample. The classification loss L cls takes the cross-entropy form, p(c) represents the probability that the real sample is class c, andp(c) represents the probability that the network predicts the sample to be class c. The last confidence loss consists of two components, L noobj and L obj , both of which are squared-error losses. The coefficient 1 noob j i, j in Eq. (15) has the opposite definition to the previous coefficient 1 ob j i, j . Finally, c and? is represents the true value of the confidence and the predicted confidence by the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Training Details</head><p>We use an stochastic gradient descent (SGD) optimizer with an initial learning rate of 1e-2, a momentum of 0.937, and a weight decay of 0.0005. All models are trained on two NVIDIA? TITAN RTX? GPUs for 200 epochs with a batch size of 32. In pursuit of better performance, we adopt the YOLOV5 model pre-trained on the COCO dataset <ref type="bibr" target="#b40">[41]</ref> as weight initialization. As for data augmentation, we use the Mosaic method <ref type="bibr" target="#b41">[42]</ref> which mixes four training images in one image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation Metrics</head><p>All models are evaluated with three object detection metrics introduced with MS-COCO: mean Average Precision (mAP), mAP50 and mAP75. TP means true positive, which is that a predicted box by detectors and the ground truth (GT) meet the intersection over union (IoU) threshold; otherwise, it will be considered as a false positive (FP). False negative (FN) means there is a true target, but the detector doesn't find it. Equation <ref type="bibr" target="#b17">(18)</ref> indicates that AP is the integral of the Precision-Recall Curve (PRC) for each category. mAP50 computes the mean of all the AP values for all categories at IoU=0.50 in the Eq. <ref type="bibr" target="#b16">(17)</ref>. Similarly, mAP75 calculate the mean at IoU=0.75. mAP is the primary challenge metric, which can be formulated as the mean at IoU=0.50:0.05:0.95. Obviously, it is much stricter than the other two metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>In <ref type="table" target="#tab_0">Table 1</ref>, the detection performances on different datasets (FLIR, LLVIP, and VEDAI) are compared. The best records and the improvements are marked in bold and by ?, respectively. The mAP values of the RGB-only and thermal-only YOLOv5 on the FLIR dataset are 31.8% and 39.5% respectively, and the thermal-only detection result surpasses the RGBonly, which also appears in the LLVIP dataset. However, on the VEDAI dataset, the RGB-only YOLOv5 performs slightly better than the thermal-only one (mAP: ?0.1). It is possible that there are a lot of low-light scenes in FLIR and LLVIP, resulting in the loss of effective target areas. Furthermore, comparing the mAP of the thermal-only YOLOv5 and the two-stream baseline, the simple two-stream network cannot fully exploit the inherent complementarity between different modalities. What is more, these coarse approaches may increase the difficulty of network learning and aggravate the imbalance of the modalities, which results in performance degradation. It can be observed in <ref type="table" target="#tab_0">Table 1</ref> that, with our proposed CFT, the performance of the detector has improved on all three datasets. Especially for the VEDAI dataset, the evaluation metric mAP75 is increased by 18.2%, and mAP is elevated by 9.2%.</p><p>To present the general effectiveness of our CFT, we further combine the CFT module with other classical detectors, i.e., YOLOv3 <ref type="bibr" target="#b42">[43]</ref> and Faster R-CNN <ref type="bibr" target="#b17">[18]</ref>, and test them on the FLIR dataset. The results of the experiments are evaluated in <ref type="table" target="#tab_1">Table 2</ref>. Evaluation metrics include both efficiency (i.e., network parameters and GFLOPs) and effectiveness (i.e., mAP50, mAP75, and mAP). As shown in <ref type="table" target="#tab_1">Table 2</ref>, the proposed CFT module improves the performance of multispectral object detection using either the one-stage or two-stage detector by a clear margin. Specifically, the CFT approach achieves a 5.7% gain on mAP50, a 3.5% increment of mAP75, and a 2.8% advancement over mAP (on YOLOV5). When combing with YOLOV3, the performance gains are 4.0%, 1.4%, and 2.2%,  respectively. When adopting Faster R-CNN as the basic detector, our CFT outperforms the baseline by 4.3%, 2.6%, 2.1% in terms of mAP50, mAP75, and mAP, respectively. Additionally, to evaluate the detection results more intuitively, we qualitatively compare the proposed CFT with baselines on FLIR, LLVIP, and VEDAI datasets, in <ref type="figure" target="#fig_4">Fig. 4, Fig. 5</ref> and <ref type="figure" target="#fig_6">Fig. 6</ref>, respectively. Visually, even for densely obscured objects, our CFT method can still detect all objects, while the baselines have multiple false positives (FPs) or false negatives (FNs) , i.e., wrong detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Visual Interpretation</head><p>The correlation matrix ? in Eq. (11), or more precisely the correlation matrix ? of the first CFT in <ref type="figure" target="#fig_2">Fig. 2</ref>, is visualized in <ref type="figure" target="#fig_8">Fig. 7</ref> to help clarify clarify the idea described in section "Why Transformer?" As previously mentioned, the correlation matrix boldsymbolalpha can be broken into four matrix blocks, two of which are inter-modality and two of which are intra-modality. It is once more confirmed by the depiction in <ref type="figure" target="#fig_8">Fig. 7</ref>, where the coordinate range of the RGB intra-modality correlation matrix block is (0, 0) to (63, 63), and the coordinate range of the themral intra-modality correlation matrix block is (64, 64) to (127, 127), and the rest are two inter-modality correlation matrix blocks. The illustration in <ref type="figure" target="#fig_8">Fig. 7</ref> that shows the coordinate range of the RGB intra-modality correlation matrix block as (0, 0) to (63, 63) and the themral intra-modality correlation matrix block as (64, 64) to (127, 127), and the rest as two intermodality correlation matrix blocks, serves as further confirmation. Two inter-modality correlation matrix blocks also exhibit some symmetry, as shown in <ref type="figure" target="#fig_8">Fig. 7</ref>.</p><p>Four examples of feature visualization in both daytime and nighttime scenes are shown in <ref type="figure" target="#fig_9">Fig.8</ref>. Compared to the thermal modality features, the visual modality features in daytime scene have a stronger focus on the target region of interest and fewer distractions. In contrast, the visual images are underexposed in the nighttime sense due to the low light, which makes feature extraction faulty by nature. It is clear from <ref type="figure" target="#fig_9">Fig. 8</ref> that the focus of the thermal modality feature is more accurate than the focus of the visual modality feature at night.</p><p>The fact that there is almost no difference between the input features and the features after adding the CFT features in <ref type="figure" target="#fig_2">Fig. 2</ref> is also an intriguing aspect. The cause of this is that the CFT   features are introduced as a residual <ref type="bibr" target="#b15">[16]</ref> (as seen in <ref type="figure" target="#fig_2">Fig. 2)</ref> to enhance the mono-spectral features rather than directly altering the visual or thermal features.</p><p>It can be formalized as:</p><formula xml:id="formula_16">H(x) = F (x) + x<label>(19)</label></formula><p>H(x) forms an identity mapping, i.e., H(x) = x, when the CFT module function F (x) equals to 0. Therefore, the output value of a ideal CFT feature should be as small as possible. As illustrated in <ref type="figure" target="#fig_10">Fig. 9</ref>, the value range of the CFT feature visualization is (?1.7, ?1.2), whereas the value range of the original feature visualization is (?45, 100). Clearly, these two value ranges differ by an order of magnitude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Comparison with State-of-the-art Methods</head><p>On FLIR. 1 Tabel 3 compares the results of our approach and other methods. It can be observed that our CFT achieves stateof-the-art performance on this dataset. Furthermore, it has an overwhelming performance improvement, with a minimum difference of 5.8% and up to 7.5% on mAP50 between CFT and other multimodal networks . Even compared to the latest GAFF <ref type="bibr" target="#b7">[8]</ref> with ResNet18, our method outperforms 5.8%, 2.6%, and 2.7% on mAP50, mAP75, and mAP respectively.       <ref type="table" target="#tab_4">Table 4</ref> presents the detection performance of CFT and other mono-modality networks (especially YOLOv5 which is the foundation of our algorithm), on the LLVIP dataset. It indicates that a more accurate detection (mAP50:97.5, mAP75:72.9, mAP:63.6) can be carried out by interacting and fusing the complementary features of different modalities based on our CFT module.</p><p>On VEDAI. Tabel 5 reports the vehicle detection performance of our and other methods on the multispectral aerial imagery dataset. Again, we can observe that our CFT is more accurate than the best mono-modality network YOLO-fine in mAP50 (9.3%). For multi-modality networks, the detection performance of our baseline surpasses the previous network (YOLOv3 with mid-level fusion) in the mAP (2.2%), while our CFT method does better. It provides non-negligible gains for all the considered evaluation metrics (mAP50:?9.3, mAP:?10.0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we propose a novel Transformer-based fusion approach, namely Cross-Modality Fusion Transformer (CFT), to learn long-range dependencies and integrate global contextual information, thereby the representation power of twostream CNNs is enhanced in multispectral object detection. More specifically, the CFT modules are densely inserted into the backbone to integrate features, hence the inherent complementarity between different modalities can be fully exploited. Moreover, we show the fusion process of the multi-modality feature maps by the CFT module from both formula and implementation points of view. The proposed method achieves state-of-the-art performances of 78.5, 97.5, and 85.3 mAP50 on FLIR, LLVIP, and VEDAI datasets, respectively. To present the general effectiveness, the proposed CFT module is com- bined with three classical detectors, i.e., YOLOV5, YOLOv3, and Faster R-CNN. The experimental results show that the proposed CFT improves the performance of multispectral object detection with either the one-stage or two-stage detector by a clear margin. Since our approach is simple yet effective, it may be applied to other computer vision fields such as RGB-LiDAR, RGB-D, stereo image SR tasks, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgement</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Visible-infrared paired examples from LLVIP. The paired images in the first row are captured in nighttime traffic scenes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Framework of Cross-Modality Fusion Backbone. The backbone has two parts: a two-stream feature extraction network and three Cross-Modality Fusion Transformer modules. Among them, h i and ? i are the convolution modules of the RGB and thermal branches, F Ri and F T i are the feature maps of their respective modalities, and P i represents the input of the subsequent feature pyramid. The right side shows the design details of the Cross-Modality Fusion Transformer module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative comparison of multispectral object detection in the FLIR ADAS dataset. First column: color images, second column: thermal images. From top row to bottom row: ground truth, detection results of the baseline, detection results of our method. Note that the red inverted triangles indicate FNs. Zoom in for more detail.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative comparison of multispectral object detection in the LLVIP dataset. The arrangement of the figure is the same as inFig. 4. Note that red inverted triangles indicate FNs. Zoom in for more detail.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Qualitative comparison of multispectral object detection in the VEDAI dataset. The arrangement of the figure is the same as inFig. 4. Note that red inverted triangles indicate FNs, and blue inverted triangles show FPs. Zoom in for more detail.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Visualization of the correlation matrix . Since H=W=8 is set in the CFT moudle, the dimension of the correlation matrix is 128 ? 128 (2HW ? 2HW).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Four examples of multimodal feature visualization. Two examples on the left are daytime scenes, and two examples on the right are nighttime scenes. The second column and fifth column, subfigure (b) and (e), show feature maps of visual and thermal modalities, as well as the two-branch input for the first CFT module. The third column and last column, subfigure (c) and (f), are the feature maps after adding the CFT module output, that is, F R3 and F T 3 in Fig. 2. Note the top one means the visual branch, and the bottom one represents the thermal branch in each example. Zoom in for more detail.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Comparison of the original feature visualization and the CFT feature visualization. Zoom in for more detail.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparisons of performances with different datasets in terms of mAP50, mAP75, and mAP.</figDesc><table><row><cell>Dataset</cell><cell>Modality</cell><cell>Method</cell><cell>mAP50</cell><cell>mAP75</cell><cell>mAP</cell></row><row><cell></cell><cell>RGB</cell><cell>YOLOV5</cell><cell>67.8</cell><cell>25.9</cell><cell>31.8</cell></row><row><cell>FLIR</cell><cell>Thermal RGB+T</cell><cell>YOLOV5 + Two Stream</cell><cell>73.9 73.0</cell><cell>35.7 32.0</cell><cell>39.5 37.4</cell></row><row><cell></cell><cell>RGB+T</cell><cell>+ CFT</cell><cell cols="3">78.7 (?5.7) 35.5 (?3.5) 40.2 (?2.8)</cell></row><row><cell></cell><cell>RGB</cell><cell>YOLOV5</cell><cell>90.8</cell><cell>51.9</cell><cell>50.0</cell></row><row><cell>LLVIP</cell><cell>Thermal RGB+T</cell><cell>YOLOV5 + Two Stream</cell><cell>94.6 95.8</cell><cell>72.2 71.4</cell><cell>61.9 62.3</cell></row><row><cell></cell><cell>RGB+T</cell><cell>+ CFT</cell><cell cols="3">97.5 (?1.7) 72.9 (?1.5) 63.6 (?1.3)</cell></row><row><cell></cell><cell>RGB</cell><cell>YOLOV5</cell><cell>74.3</cell><cell>46.9</cell><cell>46.2</cell></row><row><cell>VEDAI</cell><cell>Thermal</cell><cell>YOLOV5</cell><cell>74.0</cell><cell>46.8</cell><cell>46.1</cell></row><row><cell></cell><cell>RGB+T</cell><cell>+ Two Stream</cell><cell>79.7</cell><cell>47.7</cell><cell>46.8</cell></row><row><cell></cell><cell>RGB+T</cell><cell>+ CFT</cell><cell cols="3">85.3 (?5.6) 65.9 (?18.2) 56.0 (?9.2)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>), mAP50, mAP75,</cell></row></table><note>Comparisons of performances on the FLIR dataset in terms of network parameters (Param.), Giga Floating Point Operations (GFLOPs</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison of performances on FILR Dataset</figDesc><table><row><cell>Model</cell><cell>Data</cell><cell cols="4">Backbone mAP50 mAP75 mAP</cell></row><row><cell></cell><cell cols="3">mono-modality networks</cell><cell></cell><cell></cell></row><row><cell>Faster R-CNN</cell><cell>RGB</cell><cell>ResNet50</cell><cell>64.9</cell><cell>21.1</cell><cell>28.9</cell></row><row><cell cols="3">Faster R-CNN Thermal ResNet50</cell><cell>74.4</cell><cell>32.5</cell><cell>37.6</cell></row><row><cell>SSD</cell><cell>RGB</cell><cell>VGG16</cell><cell>52.2</cell><cell>15.8</cell><cell>21.8</cell></row><row><cell>SSD</cell><cell cols="2">Thermal VGG16</cell><cell>65.5</cell><cell>22.4</cell><cell>29.6</cell></row><row><cell>YOLOv3</cell><cell cols="3">RGB Darknet53 58.3</cell><cell>19.8</cell><cell>25.7</cell></row><row><cell>YOLOv3</cell><cell cols="3">Thermal Darknet53 73.6</cell><cell>31.3</cell><cell>36.8</cell></row><row><cell>YOLOv5</cell><cell>RGB</cell><cell>CSPD53</cell><cell>67.8</cell><cell>25.9</cell><cell>31.8</cell></row><row><cell>YOLOv5</cell><cell cols="2">Thermal CSPD53</cell><cell>73.9</cell><cell>35.7</cell><cell>39.5</cell></row><row><cell></cell><cell cols="3">multi-modality networks</cell><cell></cell><cell></cell></row><row><cell>Halfway [6]</cell><cell cols="2">RGB+T VGG16</cell><cell>71.2</cell><cell>-</cell><cell>-</cell></row><row><cell>CFR 3 [6]</cell><cell cols="2">RGB+T VGG16</cell><cell>72.4</cell><cell>-</cell><cell>-</cell></row><row><cell>GAFF [8]</cell><cell cols="2">RGB+T ResNet18</cell><cell>72.9</cell><cell>32.9</cell><cell>37.5</cell></row><row><cell>GAFF [8]</cell><cell cols="2">RGB+T VGG16</cell><cell>72.7</cell><cell>30.9</cell><cell>37.3</cell></row><row><cell cols="3">Baseline(Ours) RGB+T CSPD53</cell><cell>73.0</cell><cell>32.0</cell><cell>37.4</cell></row><row><cell>CFT(Ours)</cell><cell>RGB+T</cell><cell>CFB</cell><cell>78.7</cell><cell>35.5</cell><cell>40.2</cell></row><row><cell>On LLVIP. 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison of performances on LLVIP Dataset</figDesc><table><row><cell>Model</cell><cell cols="4">Data Backbone mAP50 mAP75 mAP</cell></row><row><cell></cell><cell cols="3">mono-modality networks</cell><cell></cell></row><row><cell cols="4">Faster R-CNN RGB ResNet50 91.4</cell><cell>48.0 49.2</cell></row><row><cell cols="4">Faster R-CNN Thermal ResNet50 96.1</cell><cell>68.5 61.1</cell></row><row><cell>SSD</cell><cell>RGB</cell><cell>VGG16</cell><cell>82.6</cell><cell>31.8 39.8</cell></row><row><cell>SSD</cell><cell cols="2">Thermal VGG16</cell><cell>90.2</cell><cell>57.9 53.5</cell></row><row><cell>YOLOv3</cell><cell cols="3">RGB Darknet53 85.9</cell><cell>37.9 43.3</cell></row><row><cell>YOLOv3</cell><cell cols="3">Thermal Darknet53 89.7</cell><cell>53.4 52.8</cell></row><row><cell>YOLOv5</cell><cell cols="3">RGB CSPD 53 90.8</cell><cell>51.9 50.0</cell></row><row><cell>YOLOv5</cell><cell cols="2">Thermal CSPD53</cell><cell>94.6</cell><cell>72.2 61.9</cell></row><row><cell></cell><cell cols="3">multi-modality networks</cell><cell></cell></row><row><cell cols="3">Baseline(Ours) RGB+T CSPD53</cell><cell>95.8</cell><cell>71.4 62.3</cell></row><row><cell>CFT(Ours)</cell><cell>RGB+T</cell><cell>CFB</cell><cell>97.5</cell><cell>72.9 63.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparison of performances on VEDAI Dataset</figDesc><table><row><cell>Model</cell><cell>Data</cell><cell>Backbone</cell><cell cols="2">mAP50 mAP</cell></row><row><cell></cell><cell cols="2">mono-modality networks</cell><cell></cell><cell></cell></row><row><cell>Faster R-CNN</cell><cell>RGB</cell><cell>ResNet50</cell><cell cols="2">63.5 39.4</cell></row><row><cell cols="2">Faster R-CNN Thermal</cell><cell>ResNet50</cell><cell cols="2">72.2 42.6</cell></row><row><cell>SSD</cell><cell>RGB</cell><cell>VGG16</cell><cell>70.9</cell><cell>-</cell></row><row><cell>SSD</cell><cell>Thermal</cell><cell>VGG16</cell><cell>69.8</cell><cell>-</cell></row><row><cell>SSSDET [44]</cell><cell cols="2">RGB shallow network</cell><cell>-</cell><cell>46.0</cell></row><row><cell cols="2">YOLO-fine [45] RGB</cell><cell>Darknet53</cell><cell>76.0</cell><cell>-</cell></row><row><cell cols="2">YOLO-fine [45] Thermal</cell><cell>Darknet53</cell><cell>75.2</cell><cell>-</cell></row><row><cell>YOLOv3</cell><cell>RGB</cell><cell>Darknet53</cell><cell cols="2">70.0 42.0</cell></row><row><cell>YOLOv3</cell><cell>Thermal</cell><cell>Darknet53</cell><cell cols="2">69.3 43.3</cell></row><row><cell>YOLOv5</cell><cell>RGB</cell><cell>CSPD53</cell><cell cols="2">74.3 46.2</cell></row><row><cell>YOLOv5</cell><cell>Thermal</cell><cell>CSPD53</cell><cell cols="2">74.0 46.1</cell></row><row><cell></cell><cell cols="2">multi-modality networks</cell><cell></cell><cell></cell></row><row><cell cols="2">early fusion [46] RGB+T</cell><cell>Darknet53</cell><cell>-</cell><cell>44.0</cell></row><row><cell cols="2">Mid fusion [46] RGB+T</cell><cell>Darknet53</cell><cell>-</cell><cell>44.6</cell></row><row><cell cols="2">Baseline(ours) RGB+T</cell><cell>CSPD53</cell><cell cols="2">79.7 46.8</cell></row><row><cell>CFT(ours)</cell><cell>RGB+T</cell><cell>CFB</cell><cell cols="2">85.3 56.0</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The latest ranking of this dataset can be checked on Papers with Code (https://paperswithcode.com/sota/multispectral-object-detection-on-flir?)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Check the ranking website: https://paperswithcode.com/sota/pedestriandetection-on-llvip?</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multispectral deep neural networks for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference<address><addrLine>York, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-09-19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unified multi-spectral pedestrian detection based on probabilistic fusion networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="143" to="155" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multispectral pedestrian detection via simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference, BMVC 2018</title>
		<meeting>the British Machine Vision Conference, BMVC 2018<address><addrLine>Newcastle, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">225</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Illumination-aware faster R-CNN for robust multispectral pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="161" to="171" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cross-modality interactive attention network for multispectral pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="20" to="29" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multispectral fusion for object detection with cyclic fuse-and-refine blocks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Fromont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lef?vre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Avignon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Image Processing</title>
		<meeting>the IEEE International Conference on Image Processing<address><addrLine>Abu Dhabi, United Arab Emirates</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="276" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multimodal facial biometrics recognition: Dual-stream convolutional neural networks with multi-feature fusion layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C O</forename><surname>Tiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Ro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page">103977</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Guided attentive feature fusion for multispectral pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Fromont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lef?vre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Avignon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Winter Conference on Applications of Computer Vision, WACV 2021</title>
		<meeting>the IEEE Winter Conference on Applications of Computer Vision, WACV 2021<address><addrLine>Waikoloa, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="72" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Multimodal object detection via bayesian fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mertz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<idno type="arXiv">CoRRabs/2104.02904.arXiv:2104.02904</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Yolors: Object detection in multimodal remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dhanaraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karnam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Chachlakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ptucha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Markopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Saber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1497" to="1508" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cross-modality attentive feature fusion for object detection in multispectral remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Qingyun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhaokui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="page">108786</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Free flir thermal dataset for algorithm training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Team</surname></persName>
		</author>
		<ptr target="https://www.flir.com/oem/adas/adas-dataset-form/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Llvip: A visible-infrared paired dataset for low-light vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops, ICCVW 2021</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops, ICCVW 2021<address><addrLine>Montreal, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3489" to="3497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Vehicle detection in aerial imagery : A small target detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Razakarivony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis. Commun. Image Represent</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="187" to="203" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems<address><addrLine>NeurIPS; Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-12-03" />
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06-23" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">SSD: single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference Computer Vision, ECCV 2016</title>
		<meeting>the European Conference Computer Vision, ECCV 2016<address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9905</biblScope>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An efficient feature pyramid network for object detection in remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Qingyun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhaokui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="93058" to="93068" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Lightweight and computationally faster hypermetropic convolutional neural network for small size object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Amudhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sudheer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page">104396</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems<address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cross-modal self-attention network for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019<address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10502" to="10511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Into the wild with audioscope: Unsupervised audio-visual separation of on-screen sounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Remez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting>the International Conference on Learning Representations, ICLR 2021, Virtual Event<address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning to generate diverse dance motions with transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno>CoRR abs/2008.08171</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learn to dance with AIST++: music conditioned 3d dance generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<idno>CoRR abs/2101.08779</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-modal transformer for video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gabeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference Computer Vision</title>
		<meeting>the European Conference Computer Vision<address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="214" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">MDMMT: multidomain multimodal transformer for video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dzabraev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kalashnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Komkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Petiushko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPRW 2021, virtual</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPRW 2021, virtual</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3354" to="3363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems<address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision, ICCV 2019</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision, ICCV 2019<address><addrLine>Seoul, Korea (South)</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-02" />
			<biblScope unit="page" from="7463" to="7472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bert4sessrec: Contentbased video relevance prediction with bidirectional encoder representations from transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia, MM 2019</title>
		<meeting>the ACM International Conference on Multimedia, MM 2019<address><addrLine>Nice, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2597" to="2601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Entangled transformer for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision, ICCV 2019</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision, ICCV 2019<address><addrLine>Seoul, Korea (South)</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-02" />
			<biblScope unit="page" from="8927" to="8936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-modal dense video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Iashin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition<address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="4117" to="4126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multispectral pedestrian detection using deep fusion convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Herman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Symposium on Artificial Neural Networks</title>
		<meeting>the European Symposium on Artificial Neural Networks<address><addrLine>Bruges, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-04-27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Weakly aligned crossmodal learning for multispectral pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision, ICCV 2019</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision, ICCV 2019<address><addrLine>Seoul, Korea (South)</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-02" />
			<biblScope unit="page" from="5126" to="5136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">GFD-SSD: gated fusion double SSD for multispectral pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">H</forename><surname>Izzat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ziaee</surname></persName>
		</author>
		<idno type="arXiv">CoRRabs/1903.06999.arXiv:1903.06999</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Shufflenet V2: practical guidelines for efficient CNN architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference Computer Vision</title>
		<meeting>the European Conference Computer Vision<address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">11218</biblScope>
			<biblScope unit="page" from="122" to="138" />
		</imprint>
	</monogr>
	<note>September 8-14</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition<address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-16" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference Computer Vision</title>
		<meeting>the European Conference Computer Vision<address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-09-06" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Yolov4: Optimal speed and accuracy of object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Liao</surname></persName>
		</author>
		<idno>CoRR abs/2004.10934</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">SSSDET: simple short and shallow network for resource efficient vehicle detection in aerial scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Vipparthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Image Processing</title>
		<meeting>the IEEE International Conference on Image Processing<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-09-22" />
			<biblScope unit="page" from="3098" to="3102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Yolo-fine: One-stage detector of small objects under various backgrounds in remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Courtrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Friguet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lef?vre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baussard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="2501" to="2526" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Vehicle detection from multimodal aerial imagery using YOLOv3 with mid-level fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dhanaraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karnam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Chachlakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ptucha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Markopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Saber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SPIE 11395, Big Data II: Learning, Analytics, and Applications</title>
		<meeting>the SPIE 11395, Big Data II: Learning, Analytics, and Applications</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="22" to="32" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
