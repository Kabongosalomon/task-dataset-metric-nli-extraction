<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adapting Sequence to Sequence models for Text Normalization in Social Media</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ismini</forename><surname>Lourentzou</surname></persName>
							<email>lourent2@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kabir</forename><surname>Manghnani</surname></persName>
							<email>kabirm2@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
							<email>czhai@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Adapting Sequence to Sequence models for Text Normalization in Social Media</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Social media offer an abundant source of valuable raw data, however informal writing can quickly become a bottleneck for many natural language processing (NLP) tasks. Off-theshelf tools are usually trained on formal text and cannot explicitly handle noise found in short online posts. Moreover, the variety of frequently occurring linguistic variations presents several challenges, even for humans who might not be able to comprehend the meaning of such posts, especially when they contain slang and abbreviations. Text Normalization aims to transform online user-generated text to a canonical form. Current text normalization systems rely on string or phonetic similarity and classification models that work on a local fashion. We argue that processing contextual information is crucial for this task and introduce a social media text normalization hybrid word-character attention-based encoder-decoder model that can serve as a pre-processing step for NLP applications to adapt to noisy text in social media. Our character-based component is trained on synthetic adversarial examples that are designed to capture errors commonly found in online user-generated text. Experiments show that our model surpasses neural architectures designed for text normalization and achieves comparable performance with state-of-the-art related work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Most text data in the world today is user-generated and online. Vast quantities of online blogs and forums, social media posts, customer reviews, and other textual sources are necessary input of useful information for algorithms that understand user intent and preferences, predict trends or recommend items for purchase in targeted advertising. However, social media usually deviates from standard language usage, with high percentages of non-standard words, such as abbreviations, phonetic substitutions, hashtags, acronyms, internet slang, emoticons, grammatical and spelling errors.</p><p>Such non-standard words cause problems for both users and text mining applications. Users that are not familiar with domain-specific or peculiar language usage, e.g. acronyms found in Twitter messages, may experience problems in understanding the expressed content. Additionally, due to high out-of-vocabulary word rates, NLP approaches struggle with Copyright c 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. the noisy and informal nature of social media written language. Natural language follows a Zipfian distribution where most words are rare. Learning "long tail" word representations requires enormous amounts of data <ref type="bibr" target="#b2">(Bahdanau et al. 2017)</ref>. Recent work showcases the negative impact of noisy text on several NLP tasks and the improvement that text normalization can bring in part-of-speech tagging <ref type="bibr" target="#b18">(Han, Cook, and Baldwin 2013)</ref>, parsing <ref type="bibr" target="#b47">(Zhang et al. 2013)</ref>, and machine translation <ref type="bibr" target="#b19">(Hassan and Menezes 2013)</ref> tasks. Special pre-processing of informal text is therefore necessary to help users understand content more easily and facilitate NLP algorithms. The task of transforming noisy or informal text to a more standard representation is called Text Normalization.</p><p>Normalizing text is challenging and involves a trade-off between high recall, i.e. maximizing the number of corrections and high precision, i.e. minimize the number of incorrect normalizations. In several cases, the task is framed as mapping an out-of-vocabulary (OOV) non-standard word to an in-vocabulary (IV) standard one that preserves the meaning of the sentence. Additionally, text normalization can include modifications that are beyond the framework described above, for example replacing, removing or adding word tokens or punctuation and capitalize or lowercase text. Word mappings might not be unique, i.e. an OOV word can be transformed to more than one IV word, based on context. Due to the dynamic nature of social media text, many words (e.g. named entities) are considered OOV but do not need normalization or there is no appropriate IV word for them.</p><p>Although text normalization may appear to be similar to the task of spelling error correction, it is actually much more difficult to handle noisy social media text. Spelling correction focuses on word errors that can usually be handled with edit distance metrics. Additionally, grammatical error correction, which incorporates local spelling errors with global grammatical errors, e.g. preposition or verb usage mistakes, deals with replacing or adding omitted words, which are often caused unintentionally by non-native writers. Such errors can be partially identified with syntactic knowledge, e.g. semantic parsing, while it is rather unlikely that text normalization systems will benefit from such linguistic sources <ref type="bibr" target="#b3">(Baldwin et al. 2015)</ref>. Due to the new challenges in text normalization, it generally requires new approaches that go beyond the traditional spelling error correction methods.</p><p>The non-standard forms found in user-generated context arXiv:1904.06100v1 [cs.CL] 12 Apr 2019</p><p>can be mostly summarized into several categories:</p><p>1. Misspellings, e.g. "defenitely" ? "definitely" 2. Phonetic substitution of characters with numbers or letters, e.g. "2morrow" ? "tomorrow", "rt" ? "retweet"</p><p>3. Shortening of words, e.g. "convo" ? "conversation" 4. Acronyms, e.g. "idk" ? "i don't know" that can also include standard words usually used as acronyms, e.g. "goat" ? "greatest of all time" 5. Slang, i.e. metaphoric usage of standard words, e.g. "low key", "woke" or "broccoli" 6. Emphasis given to a certain word, either by capitalization, e.g. "YEAH THIS IS SO COOL" or by vowel elongation e.g. "cooooool" ? "cool" 7. Punctuation deleted or misplaced, e.g. "doesnt" ? "doesn't", "do'nt" ? "don't" or intentionally using punctuation instead of letters, e.g. "f@ck"</p><p>Early text normalization systems consider a pipeline of statistical language models, dependency parsing, string similarity, spell-checking and slang dictionaries <ref type="bibr" target="#b29">(Liu et al. 2011;</ref><ref type="bibr" target="#b16">Han and Baldwin 2011;</ref><ref type="bibr" target="#b18">Han, Cook, and Baldwin 2013)</ref>. However, the high-dimensional action space of language (arbitrary word sequences constructed from a vocabulary) makes unsupervised learning inefficient. Additionally, unsupervised text normalization methods often tune hyperparameters based on annotated (supervised) data, thus are not fully unsupervised. Considering the rapid changes of language in online content, with many emerging words appearing daily, lexicon-based approaches are not able to handle social media text properly. String similarity, such as edit distance, does not work on non-standard words where the number of edits is large, for example abbreviations. In order to achieve better pre-processing performance, we need to develop methods that are specifically designed for the problem at hand.</p><p>Recent work relies on candidate generation and ranking (see section "Related work" for a thorough review), with two major deficiencies: Current approaches have mostly ignored the contextual information present in a sentence that can be potentially very useful. More specifically, in most cases the features extracted or the models developed are limited to a specific context window, e.g. <ref type="bibr" target="#b32">Min and Mott (2015)</ref> work with character-ngrams, while <ref type="bibr" target="#b24">Jin (2015)</ref> relies on features that depend on previous, current and next tokens of a candidate term. This requires additional human effort to decide the appropriate ngram order and design features. More importantly, it restricts the system in a way that prevents longer contextual dependencies to be leveraged (see <ref type="figure">Figure 1</ref> for an example). The second limitation is that correcting complex normalization mappings are harder to tackle and methods that rely on candidate generator functions by definition limit their approach to specific types of errors. For example, it would be difficult for such methods to handle multiple normalization errors at once, e.g. spelling errors on an acronym or a slang term, and combining candidate generator functions results in a combinatorial problem.</p><p>Drawing inspiration from neural machine translation <ref type="bibr" target="#b31">Luong, Pham, and Manning 2015)</ref>, we propose to address both limitations by using end-to-end neural network models, particularly sequence-tosequence (Seq2Seq) models. Specifically, Seq2Seq models can encode the entire sequence data with hidden neurons that would naturally capture any useful context information in a sentence to improve text normalization performance. Such models can handle complex normalizations without the need of language-specific tools, given enough training data on any language (i.e. pairs of unnormalized and normalized sentences). We propose and compare several variants of Seq2Seq models for solving the problem of text normalization. The first is a straightforward application of the basic word-level Seq2Seq models to text normalization. However, such an approach faces a major challenge of high percentage of OOV tokens. In contrast, character-level Seq2Seq models do not operate on a limited vocabulary but are much slower to train and recent work has shown that characterbased sequence-to-sequence models are not robust on noisy data <ref type="bibr" target="#b6">(Belinkov and Bisk 2017)</ref>. We propose a novel hybrid end-to-end model that takes into account contextual information as well as addresses the OOV problem in a more robust way. Specifically, our model is based on a recurrent neural encoder-decoder architecture that reads the informal text sequences, transforms them in a continuous-space representation that is passed on the decoder to generate the target normalized sequence. To capture local spelling errors and morphological variations of OOV words, we correct unknown words with a character-level encoder-decoder trained on synthetic adversarial examples that capture common errors in online user-generated text. Our method obtains open vocabulary coverage while maintaining the lower training time of word-based models, when compared with characterlevel sequence-to-sequence architectures.</p><p>Our contribution is two-fold:</p><p>1. We explore variations of encoder-decoder architectures as well as adversarial training for tackling the task of normalizing social media text.</p><p>2. We propose a novel hybrid neural network architecture specifically designed for normalizing OOV tokens. Coupled with adversarial training, our model allows for an open set of corrections while seamlessly incorporates context and long-term dependencies. Through carefully designed experimentation, we show that the proposed hybrid model outperforms both word and character based standard Seq2Seq architectures.</p><p>Our source code and model files are publicly available 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>We briefly discuss related work on text normalization. The normalization problem was originally framed as standardizing numbers, dates and acronyms in formal text <ref type="bibr" target="#b39">(Sproat et al. 2001</ref>) but the definition was later broaden to transform social media informal text into canonical forms that NLP tools were usually trained on <ref type="bibr" target="#b39">(Sproat et al. 2001</ref>). Research <ref type="figure">Figure 1</ref>: Example of source (unnormalized) tweet and target (normalized) pair where context helps in correcting ambiguous terms. The word "exo" needs to be transformed to "extra", while the word "concert" provides the required context to understand that "exo" refers to an extra ticket.</p><p>on this problem adopts several paradigms, from spell checking <ref type="bibr" target="#b9">(Choudhury et al. 2007</ref>), machine translation <ref type="bibr" target="#b1">(Aw et al. 2006;</ref><ref type="bibr" target="#b28">Ling et al. 2013</ref>) and speech recognition (Kobus and Yvon 2008). Early unsupervised methods include probabilistic models <ref type="bibr" target="#b14">(Cook and Stevenson 2009)</ref>, string edit distance metrics <ref type="bibr" target="#b13">(Contractor, Faruquie, and Subramaniam 2010)</ref>, construction of normalization dictionaries <ref type="bibr" target="#b17">(Han, Cook, and Baldwin 2012;</ref><ref type="bibr" target="#b15">Gouws, Hovy, and Metzler 2011)</ref> or extracting training data from search results with carefully designed queries <ref type="bibr" target="#b29">(Liu et al. 2011)</ref>. Another line of work is lexiconbased methods and classification models. Han and Baldwin (2011) train a classifier that detects non-standard words and then generate candidates based on morphological and phonemic similarity metrics plus a normalization lexicon. Other approaches include word association graphs (Sonmez and Ozgur 2014), random walks <ref type="bibr" target="#b19">(Hassan and Menezes 2013)</ref>, statistical <ref type="bibr">(Beaufort et al. 2010;</ref><ref type="bibr" target="#b47">Zhang et al. 2013)</ref> and log-linear models <ref type="bibr" target="#b45">(Yang and Eisenstein 2013)</ref> and ranking candidates with language models <ref type="bibr" target="#b18">(Han, Cook, and Baldwin 2013)</ref>. These methods are quite limited as they rely primarily on string and phonetic similarity for identifying lexical variations.</p><p>Recently, there is a growing trend on applying Deep Learning in a variety of areas, such as Computer Vision or NLP. Such models offer flexibility, can learn representations and tasks jointly and have produced state-of-the-art for several applications, e.g. object recognition, sentiment analysis or machine translation. The representational power of neural models can potentially allow learning of complicated text transformations, automatically handle language drift and work with heterogeneous large streams of user-generated text. We briefly describe state-of-the-art models and models that either leverage deep learning or contain a component that is trained with neural networks. Chrupa?a (2014) leverages unlabeled data by incorporating character embeddings as features in a model that learns to perform edit operations. <ref type="bibr" target="#b40">Sridhar (2015)</ref> used distributed representations of words trained on a large Twitter dataset to extract normalization lexicons based on contextual similarity. Similarly, Ansari, Zafar, and Karim (2017) leverage word embeddings, as well as string and phonetic similarity to match OOV words to IV words (1:1 mapping), however their method does not take into consideration contextual information and thus cannot properly handle cases where multiple canonical forms of a non-standard word are available. In contrast to this line of work, we do not rely on pretrained embeddings; we learn both word and character representations, in addition to the text normalization model in an end-to-end fashion. <ref type="bibr" target="#b3">Baldwin et al. (2015)</ref> present a text normalization task on English tweets as part of the 2015 ACL-IJCNLP Workshop on Noisy User-generated Text (W-NUT). Two categories were introduced based on whether external resources and public tools were used (unconstrained systems) or not (constrained systems). Deep learning methods and lexiconaugmented conditional random fields (CRFs) achieved the best results, while the performance of unconstrained systems was inferior, suggesting that the underlying model produces a bigger impact on the final performance, compared with the usage of additional data or resources. The models of all participating teams are described in <ref type="bibr" target="#b3">(Baldwin et al. 2015)</ref>. We should note that no team explored deep sequence-tosequence models or adversarial training.</p><p>Jin <ref type="formula">(2015)</ref> achieved the best performance in the W-NUT task, with a method that generates candidates based on the training data. A binary random forest classifier is trained to predict whether a candidate is the correct canonical form for a token found in a tweet instance. Their feature set used during training includes string similarity, POS and statistics such as support and confidence. The final canonical form selected is the one that achieves the highest confidence score. van der Goot and van Noord (2017) extends this work by leveraging additional external resources of clean and informal text data. <ref type="bibr" target="#b32">Min and Mott (2015)</ref> combine a lexicon extracted from the training data with a recurrent neural model that performs edit operations trained on character trigrams. Leeman-Munk, Lester, and Cox (2015) use a neural network classifier that predicts whether a word needs normalization and a secondary model that takes as input a word and outputs its canonical form. Framing the task of text normalization as classification in addition to relying on candidate generator functions limits the types of transformations that can be tackled, as candidate generation relies heavily on human engineering effort and existing methods for creating candidates cannot handle multiple complex normalization errors at once.</p><p>The aforementioned methods do not take into account the full context in which a token appears. It is quite reasonable then for one to wonder whether Seq2Seq models would be appropriate for the task and in which ways the input or architecture should be adjusted in order to reach comparable performance given the limited training data available. To this end, we explore encoder-decoder models and study how crucial context is for the normalization of user-generated text. Finally, we design a novel hybrid Seq2Seq model that is trained on synthetic adversarial examples of noisy social media text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sequence-to-Sequence Learning</head><p>Encoder-decoder architectures <ref type="bibr" target="#b41">(Sutskever, Vinyals, and Le 2014;</ref><ref type="bibr" target="#b9">Cho et al. 2014)</ref> have been applied in a wide variety of natural language tasks, such as machine translation <ref type="bibr" target="#b44">(Wu et al. 2016)</ref>, dialogue generation <ref type="bibr" target="#b43">(Vinyals and Le 2015)</ref>, summarization <ref type="bibr" target="#b33">(Nallapati et al. 2016)</ref>, question answering <ref type="bibr" target="#b46">(Yin et al. 2015)</ref>. Several extensions of the Seq2Seq models have been proposed with mechanisms such as attention , copying <ref type="bibr" target="#b36">(See, Liu, and Manning 2017)</ref> and coverage <ref type="bibr" target="#b42">(Tu et al. 2016)</ref>.</p><p>In most cases only the most frequent words are kept, creating a fixed-sized vocabulary, with OOV words mapped to a common UNK token. Consequently, the performance is affected by the limited vocabulary. Recent work propose methods to mitigate this problem, by treating text as a sequence of characters <ref type="bibr" target="#b26">(Lee, Cho, and Hofmann 2017)</ref>, inventing new word segmentation methods <ref type="bibr" target="#b37">(Sennrich, Haddow, and Birch 2015;</ref><ref type="bibr" target="#b8">Bojanowski et al. 2017)</ref> or hybrid word-level models with an additional character-level model to handle problematic cases <ref type="bibr" target="#b30">(Luong and Manning 2016;</ref><ref type="bibr" target="#b22">Ji et al. 2017)</ref>. While character-based models outperform models based on subword units, their extremely high computational cost and inability to handle long-distance dependencies makes them unappealing in practice. Moreover, as hybrid models only use the secondary character model for problematic cases, such as unknown words, they rely on large training datasets, making them inappropriate for domains with limited annotated data and frequent word variations. Our work lies on the hybrid models category but builds upon the properties of text normalization to adjust the character-based model training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text Normalization</head><p>Our architecture consists of two encoder-decoder models, primarily a word-based Seq2Seq model, while for transforming words not found in the word-level model's vocabulary, we either backtrack to a secondary character-based Seq2Seq model when its confidence is high or copy the source token ( <ref type="figure" target="#fig_0">Figure 3</ref>). For completeness, we briefly describe encoder-decoder neural models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word-level sequence-to-sequence model</head><p>Given an unnormalized text represented as an input sequence of words x = [x 1 , . . . , x T ] with length T , we consider generating another output sequence of words y = [y 1 , . . . , y L ] with length L that has the same meaning as x. The task is defined as a sequence-to-sequence learning problem which aims to learn the mapping from one sequence to another. Specifically, the architecture is built based on the encoder-decoder framework <ref type="bibr" target="#b41">Sutskever, Vinyals, and Le 2014)</ref>, both of which are parameterized by attention-based recurrent neural networks (RNN).</p><p>The encoder module reads the input sequence x and transforms it to a corresponding context-specific sequence of hidden states h = [h 1 , . . . , h T ]. In bi-directional models, two encoders are used; one reading the text in forward mode and another one reading text backwards. The final hidden state at time t is the concatenation of the two encoder mod-</p><formula xml:id="formula_0">ules h t = [g f (x t , h t?1 ); g b (x t , h t+1 )]</formula><p>where g f and g b denote the forward and backward encoder units, respectively. Similarly, the decoder defines a sequence of hidden states s j = g s (s j?1 , y j?1 , c j ) that is conditioned on the previous word y j?1 and decoder state s j?1 , as well as the context vector c j , computed as a weighted sum of encoder hidden states based on the attention mechanism : <ref type="bibr" target="#b31">Luong, Pham, and Manning (2015)</ref>. Then, each target word is predicted by a Softmax classifier y j ? p(y j |y &lt;j , x) = Sof tmax(?(s j )), where ? is an affine transformation function that maps the decoder state to a vocabulary-sized vector.</p><formula xml:id="formula_1">c j = |T | i=k ? jk h k where ? jk = Softmax(f (s j?1 , h k )) and f (s j?1 , h k ) = s T j?1 W h k is the general content-based function described in</formula><p>Given training data D, Seq2Seq model is trained by maximizing the log-likelihood:</p><formula xml:id="formula_2">L(?) = ? ( x, y)?D |L| j=1 log p ? (y j |y &lt;j , x)</formula><p>Note that during training a wrong prediction will cause an accumulation of errors in the subsequent time steps. Thus, when computing the conditional probability p ? (y j |y &lt;j , x), Scheduled Sampling <ref type="bibr" target="#b7">(Bengio et al. 2015)</ref> is often used, a method that alternates between using the model prediction on the previous time step? j?1 and the target previous word y j?1 in order to alleviate the presence of compounding errors.</p><p>The word-based Seq2Seq model can capture semantic meaning at a word level and long-term contextual dependencies that help in disambiguation of multiple correction candidates. <ref type="figure">Figure 1</ref> presents an example of source and target pair of tweets for which context helps in appropriately normalizing the content.</p><p>Handling unknown words with a secondary character-based encoder-decoder model</p><p>The model operating on words has a limited vocabulary for both source and target. Words that are beyond this vocabulary are represented with a special UNK symbol. For text normalization, where slight variations occur often due to misspellings, keyboard typing errors and intentionally emphasizing terms by elongation of vowels (e.g. "coooool" or "yaaaay"), many of the words are unseen during training, resulting in loss of information. Three possible solutions can be used to tackle this problem: a) copying source words, b) rely on models fully trained on character-based information and c) design hybrid models that work both on word and character level.</p><p>A naive strategy would be to just copy the source word when it is outside the scope of the vocabulary (see <ref type="figure">Figure 2</ref>), however many unseen non-standard words will be left intact and thus the coverage of our models will decrease. Another way to handle vocabulary coverage is to pre-process the data and learn a subword representation that allows to generalize to new words. Byte pair encoding (BPE) <ref type="bibr" target="#b37">(Sennrich, Haddow, and Birch 2015)</ref> learns the segmentation of text into subwords, e.g. "showed" could be split into "show" and "ed" while "accepting" would be split "accept" and "ing". Such pre-processing is model-agnostic, i.e. can be used irrespectively of the chosen Seq2Seq model. However, BPE relies on <ref type="figure">Figure 2</ref>: Example of an unseen unnormalized token where copying the source word is insufficient the cooccurence and order of characters, which in our case is highly noisy.</p><p>Character models overcome the bottleneck of restricted vocabularies and do not require any pre-processing or tokenization but are computationally expensive and also suffer from data sparsity. <ref type="bibr" target="#b12">Chung, Cho, and Bengio (2016)</ref> provide a detailed analysis regarding the challenges of characterlevel models. <ref type="bibr" target="#b6">Belinkov and Bisk (2017)</ref> recently showed that character-based models fail to translate noisy text that humans can handle easily. They mention that such models are rarely trained to explicitly handle typos and noise, commonly found in natural language.</p><p>Hybrid models on the other hand, rely primarily on a word-based representation where the meaning is naturally preserved and backtrack to a secondary character-level model to deal with problematic text. Because the character model is trained only in some cases, it requires a large pool of such problematic aligned text. Due to limited training data available for text normalization and the long tail of rare non-standard words, hybrid architectures that train character-level models only for words outside the vocabulary would be insufficient. Thus, for in-vocabulary words we rely solely on the word-level model, while when a word is OOV, we backtrack to a character encoder-decoder that is trained on word pairs rather than longer token sequences, i.e. each pair of source and target words in our training set is processed separately.</p><p>Adversarial training for increased robustness to noisy user-generated text To improve our model's robustness to noisy text, we incorporate an adversarial training procedure to our characterbased secondary model. We augment our data by creating synthetic adversarial examples of words, i.e. unnormalized and canonical forms. More specifically, for all source-target pairs of tweets, we keep words that remain unchanged. This process creates our source and target vocabularies for the character model. We later on inject multiple types of noise during training, by editing the source part of each word. More specifically, we introduce 6 types of errors that are typically found in user-generated text, by randomly: del: Deleting a character from a word swap: Swapping the placement of two characters lastchar: Elongating the last character k times when the word ends with {u, y, s, r, a, o, i}, where k ? {1, . . . , 6} punct: Deleting e.g. "I'm" ? "Im" or misplacing apostrophes, e.g. "don't" ? "do'nt" keyboard: Replacing characters based on their distance on the keyboard, e.g. "hello" ? "jello"  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>In this section we present our experimental setup for assessing the performance of the text normalization model described above. We want to test: a) whether a naive replacement of words with their non-standard form would be sufficient for the text normalization task, b) which Seq2Se2 model is the most effective, c) whether BPE and character level models are appropriate for normalizing OOV social media tokens, d) how crucial is context and long-term dependencies for correctly normalizing noisy text and e) whether adversarial training improves robustness of our hybrid architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>We use the LexNorm dataset from the 2015 ACL-IJCNLP Workshop on Noisy User-generated Text (W-NUT) <ref type="bibr" target="#b3">(Baldwin et al. 2015)</ref>. The dataset contains 4,917 tweets with 373 unique non-standard word types, split into 60:40 training/testing ratio. There are 488 non-standard word types that are unseen during training, i.e. not found in the training data. <ref type="table" target="#tab_1">Table 1</ref> lists some statistics of the dataset described in <ref type="bibr" target="#b3">(Baldwin et al. 2015)</ref>. Note that apart from mapping a source word to a target word (1 : 1 mapping), there are also words that are mapped to more than one target tokens (1 : N mapping), e.g. "omw" ? "on my way". To reduce vocabulary size, words are lowercased, while mentions were tagged and anonymized with a mention token. The same anonymization was applied for URLs url and hashtags hash . At test time, we de-anonymize by looking them up in their source sentences. Additionally, we keep a common vocabulary between source and target text. Each sequence is additionally pre-processed by adding a start s and end \s symbol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline models</head><p>We compare our model (HS2S) with a diverse set of baselines, including two naive dictionary-based approaches: we begin by constructing a lexicon from the training data and correcting only unique mappings (Dict1) or additionally choose randomly when multiple canonical forms are available (Dict2). We also compare with a two-staged strategy that first corrects unique mappings based on the dictionary and secondly utilizes a word-level Seq2Seq model trained to correct only multiple mappings (S2SMult).</p><p>Addditionaly, we include a default attention-based wordlevel encoder-decoder (S2S) as our baseline for comparison. For this model, OOV words are solely copied directly from the source sequence, thus "unseen normalizations" are not handled. This baseline should indicate whether targeting unseen normalizations is critical for the model performance.</p><p>Since character or sub-word level representations can alleviate the problem of limited vocabularies, we also experiment with a character-based model (S2SChar) and a model that is trained on subwords with BPE tokenization 2 (S2SBPE).</p><p>Finally, we include a model trained on target sequences preprocessed with a special symbol to indicate that the word should be left intact (S2SSelf), e.g. if the source sequence is "see u soon" and the target sequence is "see you soon", we replace the target sequence with "@self you @self". During prediction, when we generate the normalized target of a source sentence, we replace this special symbol by copying from the source. Ultimately, we seek to find which sequenceto-sequence model is the most effective for the text normalization task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Details</head><p>We keep a shared vocabulary between source and target and also tie the decoder embeddings <ref type="bibr" target="#b35">(Press and Wolf 2017)</ref>. We optimized all models with Adam (Kingma and Ba 2014) and the gradient is rescaled when the norm exceeds {5,10} <ref type="bibr" target="#b34">(Pascanu, Mikolov, and Bengio 2013)</ref>. Batch size is set to {32,500}. All of our models are bi-directional and use attention. To compare performance, we tune each model separately with random search. The best hyper-parameters are summarized in <ref type="table" target="#tab_3">Table 2</ref>. To tune the hyper-parameters we used a 10% random split of the training data and performed random search on the hyper-parameter space. Once the best combination was found, we retrained our system using the full training data set.</p><p>Our adversarial training procedure is guided by an additional hyper-parameter, noise ratio, that tunes the number of adversarial instances used. Our best performing model has a noise ratio of 0.1, i.e., more than 10% of instances used are generated with adversarial training (see <ref type="table" target="#tab_9">Table 6</ref>). In general, we observed that training on large amounts of adversarial instances, e.g. 50% additional instances, results in decreased performance, e.g. when noise ratio is increased to 0.5, the F1 score is decreased to 82.67% <ref type="table" target="#tab_9">(Table 6</ref>: incorporating large quantities of adversarial instances decreases performance). Furthermore, to reduce the amount of false positives we allow the character-level secondary model to correct only words for confident predictions. If the confidence of the character-level model is low, our architecture copies from the source.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison of Sequence to Sequence models</head><p>We compare our novel hybrid model with word-level baseline Seq2Seq models <ref type="table" target="#tab_5">(Table 3)</ref>, as well as character-based and BPE-tokenized Seq2Seq models <ref type="table" target="#tab_6">(Table 4</ref>). In <ref type="table" target="#tab_5">Table 3</ref> we see that our dictionary based baselines result in lower performance. Dict2 shows that handling ambiguous cases   inappropriately results in a dramatic drop in precision. It is therefore necessary for a text normalization model to be able to correctly normalize text in the occurrence of multiple mappings. S2SMulti is a baseline method that firstly normalizes terms that have a unique mapping, based on source and target tokens found in the training data, and later on utilizes a word-level Seq2Seq model that is trained to correct one-to-many mappings (an unnormalized token that can be transformed into several standard words, e.g. "ur" ? {"you are", "your"}). We can see that this method has a better performance, which validates our hypothesis that a naive word replacement would not suffice, however the end-to-end Seq2Seq model (S2S) performs better than S2SMulti, i.e. Seq2Seq models can handle both unique and multiple mappings seamlessly without any additional feature engineering effort apart from tokenization. Our character-level S2SChar model's performance is slightly above the dictionary baseline Dict1 which suggests that characters do not contain enough semantics to appropriately disambiguate between terms. Our results align with relevant literature <ref type="bibr" target="#b6">(Belinkov and Bisk 2017</ref>) that emphasizes on the noise sensitivity of character-based Seq2Seq models. We should also note that character-level Seq2Seq models take longer to train. Our best performing word Seq2Seq model took 22 minutes to train while our top scored character model took 3 hours, for the same number of epochs. The secondary character model of our hybrid architecture, which is trained on pairs of words, took 42 minutes for the same number of epochs.</p><p>Despite extensively tuning the hyper-parameters and experimenting with two subword tokenization tools, we were unable to train a good performing model on subword units (S2SBPE) successfully. S2SBPE has surprisingly very low performance, the poorest of all models. As BPE relies on co-occurrence of characters to extract frequent subword patterns, one would expect that it would not be able to capture useful information due to the high percentage of noise. This emphasizes the importance of developing models robust to informal text, that can learn from noisy input "on the fly".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Error analysis</head><p>We perform an extensive error analysis. First, we check the model output, particularly in which cases our model fails. <ref type="table" target="#tab_8">Table 5</ref> presents the most frequent normalizations that our model performed correctly and the most frequent cases that were missed, as well as the frequency of the source terms. Note that we also keep track of how many times a term remains unchanged, specifically for cases where that term has multiple mappings available <ref type="table" target="#tab_8">(Table 5</ref>, information found in parentheses). We can see that our model can handle multiple mappings when those are adequately represented in the training data. Most of the incorrect normalizations appear less than 50 times in our data (very infrequent) or are ambiguously normalized in some examples and left intact in other examples, e.g. "rt" and "2" remain unchanged a few times, and these are the cases that the model missed. These types of errors can be handled by adding more training data. Furthermore, we should note that our model can also be adjusted to work with distantly supervised data or with ensemble methods.</p><p>Comparing with different representations that can handle unknown words, our character level model performs edits that might result in removing words from the text or are close to the gold-truth canonical form but not entirely correct (see <ref type="table">Table 7</ref>). In contrast to our hybrid model that can preserve contextual and word-level semantic information, our BPE model's poor performance results in editing text that needs no correction.</p><p>Futhermore, we perform error analysis on the secondary character-level model that is trained on synthetic adversarial examples of word pairs. In total, our model correctly normalizes 16.22% of unseen source words. In <ref type="table" target="#tab_11">Table 8</ref> we present OOV terms that the model can correctly normalize. Most frequent errors that are correctly normalized are elongating the last characters, deleting last character [g] in gerund, swapping or replacing characters and typos. There are also several typos that the model was not able to correct, such as missing whitespaces or editing words unnecessarily.</p><p>How contextual information affects performance One of our main questions is how crucial contextual information is for text normalization. We test the importance of context by performing two experiments. For our first experiment, we perform the following preprocessing of the training data: we constrain the target-side of each example by replacing each word that remains unchanged (no normalization needed) with a special @self symbol. With this representation all tokens that need normalization are preserved and the model would learn which words remain the same. We call this model S2SSelf. We notice that this representation lowers performance due to loss of contextual information on the target side (see <ref type="table" target="#tab_5">Table 3</ref>). In <ref type="table" target="#tab_12">Table 9</ref> we present examples of our hybrid model's predictions and compare with the predictions of S2SSelf. In most cases we observe that our model relies on context to normalize short tokens, while S2SSelf fails to correct such terms.</p><p>Moreover, we create ngram representations of our data by splitting the tweets: for example a unigram model is trained on word pairs solely and ignores context when normalizing, as it edits each word separately and similarly a bigram model is trained on phrases that contain two words. We continue with higher-order ngrams, train Seq2Seq models on such ngram-based split of text and analyze the importance of contextual information. By gradually varying the context window, while keeping the rest of the hyper-parameters stable, we can analyze how it affects the performance. In <ref type="figure" target="#fig_1">Figure 4</ref> we can see that recall remains fairly unchanged, while precision increases as the context window grows larger, i.e. train on higher-order ngrams and thus incorporating more context. Overall, we can observe that F1 measure gets better   come on familia don't mess this up please HS2S/S2Multi: come on familia don't mess this up please S2SSelf:</p><p>cmon familia don't mess this up please S2Char:</p><p>comon familia don't mess this up please S2SBPE:</p><p>cmon familia don't just ess this up Source ... i'm not gon diss you on the internet cause ... Target:</p><p>... i'm not gonna disrespect you on the internet because ... HS2S/S2Multi: ... i'm not gonna disrespect you on the internet because ... S2Char:</p><p>... i'm not gonna thiss you on the internet because ... S2SBPE:</p><p>... i'm not gonna be you on the internet because ... <ref type="table">Table 7</ref>: Examples where our model surpasses architectures that rely on lower-level representation of text.</p><p>with additional contextual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with related work</head><p>Finally, we present our comparison with related work on Table 10. We see that all previous Deep Learning approaches are close to 82% F1 score. Due to the nature of our hybrid model, we were able to achieve the best performance so far among neural models in related work. In general, we observe comparable performance with state-of-the-art methods that are constrained on utilizing additional resources 3 . We compare the incorrect normalizations that our Seq2Seq model and MoNoise -the best performing method -produce. Both systems appear to have similar results in terms of most frequent incorrect normalizations <ref type="table" target="#tab_1">(Table 13</ref>). In many cases our hybrid Seq2Seq model leaves intact terms that are ambiguous in terms of whether they should be normalized or not, while (van der Goot and van Noord 2017) normalize words more often <ref type="table" target="#tab_1">(Table 14)</ref>, in some cases incorrectly. Interestingly, there are examples that, despite the lack of correct target annotation, our model normalizes tokens correctly (see <ref type="table" target="#tab_1">Table 12</ref>). More specifically, there are several tokens that were not normalized but in an ideal scenario should be, e.g. many punctuation errors that were not normalized or abbreviated tokens that were not converted into their standard form. As a result, despite correctly transforming text, our system gets penalized for such cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions and future work</head><p>Text normalization is an important preprocessing part that helps users understand online content and increases performance of off-the-shelve pretrained NLP tools. However, due to the inherent constraints of existing feature engineering methods used, existing work cannot capture longer contextual information and is limited to handling specific types of normalization corrections. Neural Seq2Seq models can naturally correct complex normalization errors by learning edits on large pools of text data. Additionally, improving robustness of Seq2Seq models on real-word noisy text data is a crucial problem that remains fairly unexplored. To this end we have introduced a novel hybrid neural model for social media text normalization that utilizes a word-based encoderdecoder architecture for IV tokens and a character-level  think that took everything off my mind for the night HS2S: (80%) think that took everything off ma mind for the night S2SSelf: (50%) think that took everything off ma mind for the tha night Source:</p><p>death penalty would b d verdict @general marley murder will b d case ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target:</head><p>death penalty would be the verdict @general marley murder will be the case ... HS2S: (88.8%) death penalty would be the verdict @general marley murder will b the case ... S2SSelf: (0%) death penalty would b d verdict @general marley murder will b d case ...  sequence-to-sequence model to handle problematic OOV cases. Our character-based component is trained on adversarial examples of word pairs. Experimental results show that our hybrid architecture improves robustness to noisy user-generated text and shows superior performance, when compared with open vocabulary models. Without relying on any external sources of additional data, we built a system that improves the performance of neural models of text normalization and produces results comparable with other models found in the recent related literature. Our system can be deployed as a preprocessor for various NLP applications and Source: @ifumi0819 i see , u can comeee Target:</p><p>@ifumi0819 i see , you can come HS2S:</p><p>@ifumi0819 i see , you can comeee Our RF: @ifumi0819 i see , you can comes Source:</p><p>startin to get into this type of musik @vinnyvitale Target:</p><p>starting to get into this type of music @vinnyvitale HS2S:</p><p>starting to get into this type of musik @vinnyvitale Our RF: startin to get into this type of musik @vinnyvitale Source:</p><p>#youarebeautiful allly this hashtag should be for you im ugly Target:</p><p>#youarebeautiful allly this hashtag should be for you i'm ugly HS2S:</p><p>#youarebeautiful allly this hashtag should be for you i'm ugly Our RF: #youarebeautiful alli this hashtag should be for you i'm ugly <ref type="table" target="#tab_1">Table 11</ref>: Examples of corrent and incorrect normalizations of our model (HS2S) and Jin (2015) (our implementation) Source:</p><p>rt @foxtramedia tony parker sportscenter convo ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target:</head><p>rt @foxtramedia tony parker sportscenter convo ... Prediction: rt @foxtramedia : tony parker sportscenter conversation ... Source:</p><p>... but looking back defo do now, haha ! Target:</p><p>... but looking back defo do now, haha ! Prediction: ... but looking back definitely do now, haha ! <ref type="table" target="#tab_1">Table 12</ref>: Examples where our model performs correct normalization but during the annotation process, some tokens remained unnormalized in the target sequence, which results in lower performance in evaluation.  off-the-shelve tools to improve their performance on social media text.</p><p>We plan to apply the approach to more languages and compare our adversarial training to other methods, e.g. perturbations applied directly to the embedding space instead of the input. While normalizing informal text, it is worth to consider whether the meaning of a noisy version remains the same, for example the extended usage of vowels ("yaaaaaaay") indicates emphasis while capitalization represents raising the tone. We leave the analysis of the trade-off between retaining such information and normalizing noisy text as future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Our Hybrid Sequence-to-Sequence (HS2S) architecture that consists of two nested encoder-decoder architectures, one trained on word-level information and a character-based trained on synthetically generated adversarial examples. The primary model (word encoder-decoder) is trained on sequences of words. When an unknown symbol is encountered, such as token "fidst" (red box) in our example, we leverage a secondary character-level Seq2Seq model (character encoder-decoder) that is trained on a large pool of synthetic adversarial training examples of words to correctly normalize, e.g. token "first" (green box)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Varying the ngram-wise split of sequences to check how context affects performance of text normalization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>LexNorm statistics from<ref type="bibr" target="#b3">(Baldwin et al. 2015)</ref> and vocabulary statistics after preprocessing elong: Extending vowel usage k times, where k is a random number</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Best performing hyper-parameter settings of our proposed text normalization models</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Comparison of our S2S models with word-level baselines.</figDesc><table><row><cell cols="3">Model name Precision Recall</cell><cell>F1</cell><cell>Method highlights</cell></row><row><cell>HS2S</cell><cell>90.66</cell><cell cols="3">78.14 83.94 Hybrid word-char Seq2Seq</cell></row><row><cell>S2SChar</cell><cell>67.14</cell><cell cols="2">70.50 68.78</cell><cell>Character-level Seq2Seq</cell></row><row><cell>S2SBPE</cell><cell>20.00</cell><cell cols="2">52.04 28.90</cell><cell>Word Seq2Seq + BPE</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparison of our HS2S model with additional baselines.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Most frequent correct (left table) and incorrect (right table) normalizations of our word-level Seq2Seq model. We present how many times a source tweet was (in)correctly normalized (Count column) as well as how many times that term appears in the source-side of the examples (Source). For cases where a token can be normalized to itself, we include how many times that term appears unchanged (information in parentheses)</figDesc><table><row><cell>Noise Ratio</cell><cell>Total examples</cell><cell>Noise-injected examples</cell><cell cols="2">Precision Recall</cell><cell>F1</cell></row><row><cell>0.1</cell><cell>34,875</cell><cell>5,739</cell><cell>90.66</cell><cell cols="2">78.14 83.94</cell></row><row><cell>0.2</cell><cell>37,659</cell><cell>8,523</cell><cell>89.92</cell><cell cols="2">78.25 83.68</cell></row><row><cell>0.3</cell><cell>40,489</cell><cell>11,353</cell><cell>88.61</cell><cell cols="2">78.11 83.03</cell></row><row><cell>0.4</cell><cell>43,191</cell><cell>14,055</cell><cell>89.25</cell><cell cols="2">78.25 83.39</cell></row><row><cell>0.5</cell><cell>45,921</cell><cell>16,155</cell><cell>87.33</cell><cell cols="2">78.47 82.67</cell></row><row><cell>0.6</cell><cell>48,625</cell><cell>19,489</cell><cell>86.21</cell><cell cols="2">79.05 82.47</cell></row><row><cell>0.7</cell><cell>51,380</cell><cell>22,244</cell><cell>84.89</cell><cell cols="2">78.83 81.75</cell></row><row><cell>0.8</cell><cell>54,034</cell><cell>24,898</cell><cell>84.37</cell><cell cols="2">79.05 81.62</cell></row><row><cell>0.9</cell><cell>56,829</cell><cell>27,693</cell><cell>83.94</cell><cell cols="2">78.98 81.38</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Varying the amount of adversarial examples</figDesc><table><row><cell>Source:</cell><cell>cmon familia dont mess this up please</cell></row><row><cell>Target:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>OOV words that our secondary character model has normalized correctly (blue) or incorrectly (red)</figDesc><table><row><cell>Source:</cell><cell>think tht took everything off ma mind for tha night</cell></row><row><cell>Target:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Comparing HS2S with S2SSelf shows context is crucial for correct normalization, especially for short tokens.</figDesc><table><row><cell>Model</cell><cell cols="2">Precision Recall</cell><cell>F1</cell></row><row><cell>Hybrid Seq2Seq (HS2S)</cell><cell>90.66</cell><cell cols="2">78.14 83.94</cell></row><row><cell>Random Forest (Jin 2015)</cell><cell>90.61</cell><cell cols="2">78.65 84.21</cell></row><row><cell>Lexicon +LSTM (Min and Mott 2015)</cell><cell>91.36</cell><cell cols="2">73.98 81.75</cell></row><row><cell>ANN (Leeman-Munk, Lester, and Cox 2015)</cell><cell>90.12</cell><cell cols="2">74.37 81.49</cell></row><row><cell>MoNoise* (van der Goot and van Noord 2017)</cell><cell>93.53</cell><cell cols="2">80.26 86.39</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>Comparison of our hybrid Seq2Seq model with related work on Text Normalization. *In contrast with the rest of the presented related work, Monoise leverages additional textual resources.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 13 :</head><label>13</label><figDesc>Examples that both our hybrid (HS2S) model and MoNoise (van der Goot and van Noord 2017) incorrectly normalized.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/Isminoula/TextNormSeq2Seq</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We experimented with the original subword implementation (https://github.com/rsennrich/subword-nmt) as well as a pretrained version (Heinzerling and Strube 2018) (https://github.com/ bheinzerling/bpemb) that produced better results.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">MoNoise (van der Goot and van Noord 2017) leverages large collections of Twitter and Wikipedia data.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. The authors would like to thank the anonymous reviewers for their helpful comments. This material is based upon work supported by the National Science Foundation under Grant No. 1801652.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Improving text normalization by optimizing nearest neighbor matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Ansari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Zafar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karim</surname></persName>
		</author>
		<idno>arXiv1712.09518</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A phrase-based statistical model for sms text normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the COLING/ACL on Main conference poster sessions</title>
		<meeting>the COLING/ACL on Main conference poster sessions</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bosc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00286</idno>
		<idno>arXiv:1409.0473</idno>
	</analytic>
	<monogr>
		<title level="m">Learning to compute word embeddings on the fly</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Shared tasks of the 2015 workshop on noisy user-generated text: Twitter lexical normalization and named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Noisy User-generated Text</title>
		<meeting>the Workshop on Noisy User-generated Text</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Beaufort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roekhaut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-A</forename><surname>Cougnon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fairon</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A hybrid rule/model-based finite-state framework for normalizing sms messages</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Synthetic and natural noise both break neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.02173</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Saraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Basu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Doc</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Investigation and modeling of the structure of texting language. ument Analysis and Recognition (IJDAR</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Normalizing tweets with edit scripts and recurrent neural embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chrupa?a</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd</title>
		<meeting>the 52nd</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Short Papers</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A character-level decoder without explicit segmentation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised cleansing of noisy text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Contractor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Faruquie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Subramaniam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics: Posters. Association for Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics: Posters. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An unsupervised model for text message normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stevenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on computational approaches to linguistic creativity</title>
		<meeting>the workshop on computational approaches to linguistic creativity</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised mining of lexical variants from noisy text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First workshop on Unsupervised Learning in NLP</title>
		<meeting>the First workshop on Unsupervised Learning in NLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Lexical normalisation of short text messages: Makn sens a# twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automatically constructing a normalisation dictionary for microblogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning. Association for Computational Linguistics</title>
		<meeting>the 2012 joint conference on empirical methods in natural language processing and computational natural language learning. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Lexical normalization for social media text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<date type="published" when="2013" />
			<publisher>TIST</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Social text normalization using contextual graph random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Menezes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">BPEmb: Tokenization-free Pre-trained</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Heinzerling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Strube</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Subword Embeddings in 275</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C C</forename><surname>Choukri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Declerck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Goggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hasida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Isahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maegaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mariani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Piperidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tokunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note>chair). European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A nested attention neural hybrid model for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th</title>
		<meeting>the 55th</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ncsu-sas-ning: Candidate generation and feature engineering for supervised lexical normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Noisy User-generated Text</title>
		<meeting>the Workshop on Noisy User-generated Text</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Normalizing sms: are two metaphors better than one?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kobus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yvon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Computational Linguistics</title>
		<meeting>the 22nd International Conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Adam: A method for stochastic optimization</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fully characterlevel neural machine translation without explicit segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ncsu sas sam: deep encoding and reconstruction for normalization of noisy text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leeman-Munk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Noisy User-generated Text</title>
		<meeting>the Workshop on Noisy User-generated Text</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Paraphrasing 4 microblog normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Trancoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Insertion, deletion, or substitution?: normalizing text messages without pre-categorization nor supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Achieving open vocabulary neural machine translation with hybrid wordcharacter models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04025</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Ncsu sas wookhee: a deep contextual long-short term memory model for text normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Noisy Usergenerated Text</title>
		<meeting>the Workshop on Noisy Usergenerated Text</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Abstractive text summarization using sequence-to-sequence rnns and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Using the output embedding to improve language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Get to the point: Summarization with pointer-generator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04368</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.07909</idno>
		<title level="m">Neural machine translation of rare words with subword units</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A graph-based approach for contextual text normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sonmez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ozgur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Normalization of non-standard words. Computer speech &amp; language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sproat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Richards</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised text normalization using distributed representations of words and phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K R</forename><surname>Sridhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing</title>
		<meeting>the 1st Workshop on Vector Space Modeling for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Monoise: modeling noise using a modular normalization system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Der Goot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Noord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.04811</idno>
		<idno>arXiv:1710.03476</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Modeling coverage for neural machine translation</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05869</idno>
		<title level="m">A neural conversational model</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<title level="m">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A log-linear model for unsupervised text normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.01337</idno>
		<title level="m">Neural generative question answering</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Adaptive parser-centric text normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kimelfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
