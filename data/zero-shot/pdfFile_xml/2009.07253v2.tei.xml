<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Autoregressive Knowledge Distillation through Imitation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Lin</surname></persName>
							<email>1alexanderlin01@g.harvard.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Wohlwend</surname></persName>
							<email>2jwohlwend@csail.mit.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howard</forename><surname>Chen</surname></persName>
							<email>3howardchen@cs.princeton.edu4tao@asapp.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">ASAPP, Inc. New York</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">ASAPP, Inc. New York</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">ASAPP, Inc. New York</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">ASAPP, Inc. New York</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Autoregressive Knowledge Distillation through Imitation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The performance of autoregressive models on natural language generation tasks has dramatically improved due to the adoption of deep, self-attentive architectures. However, these gains have come at the cost of hindering inference speed, making state-of-the-art models cumbersome to deploy in real-world, timesensitive settings. We develop a compression technique for autoregressive models that is driven by an imitation learning perspective on knowledge distillation. The algorithm is designed to address the exposure bias problem. On prototypical language generation tasks such as translation and summarization, our method consistently outperforms other distillation algorithms, such as sequence-level knowledge distillation. Student models trained with our method attain 1.4 to 4.8 BLEU/ROUGE points higher than those trained from scratch, while increasing inference speed by up to 14 times in comparison to the teacher model. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Autoregressive models are ubiquitous in natural language processing. Due to the sequential nature of text generation, they are often the tool of choice for tackling sequence-to-sequence problems such as translation <ref type="bibr" target="#b45">(Sutskever et al., 2014)</ref>, summarization <ref type="bibr">(Rush et al., 2015)</ref>, and dialogue <ref type="bibr" target="#b11">(Eric and Manning, 2017)</ref>. Furthermore, they form the backbone of several successful generative pre-training architectures <ref type="bibr" target="#b18">(Howard and Ruder, 2018;</ref><ref type="bibr" target="#b32">Peters et al., 2018;</ref><ref type="bibr" target="#b34">Radford et al., 2019;</ref><ref type="bibr" target="#b7">Dai et al., 2019)</ref>.</p><p>Two recent trends have made autoregressive models cumbersome to deploy in real-world, natural language generation (NLG) applications. First, state-of-the-art models have grown larger and larger, amounting to hundreds of millions and even billions of parameters <ref type="bibr" target="#b10">(Dong et al., 2019;</ref><ref type="bibr" target="#b27">Liu and Lapata, 2019;</ref><ref type="bibr" target="#b35">Raffel et al., 2019)</ref>. The increase in size and depth dramatically slows down inference speed. Second, the architecture of choice for autoregressive models seems to have shifted from the recurrent neural network (RNN) <ref type="bibr" target="#b28">Luong et al., 2015)</ref> to the Transformer <ref type="bibr" target="#b47">(Vaswani et al., 2017)</ref>. Though the Transformer's self-attention mechanism improves performance, it also increases the computational complexity of the step-by-step generation algorithms that are used at test time. Thus, both of these trends have contributed to significantly increasing inference time costs, especially on CPUs and low-resource devices, hindering their use in production systems.</p><p>Knowledge distillation (KD) <ref type="bibr" target="#b3">(Bucilu? et al., 2006;</ref><ref type="bibr" target="#b16">Hinton et al., 2015)</ref> is one popular method for model compression. It transfers the information learned by a large, pretrained teacher to a smaller, untrained student. In comparison to other methods such as weight pruning and quantization, KD allows the compressed model's architecture to significantly differ from that of the original teacher. This feature enables models trained with KD to achieve high performance while meeting particular inference requirements (e.g. memory, speed, etc.).</p><p>Sequence-level knowledge distillation (SeqKD), proposed by <ref type="bibr" target="#b21">Kim and Rush (2016)</ref>, is the dominant technique for autoregressive KD in the current NLG literature, especially for machine translation <ref type="bibr" target="#b14">(Gu et al., 2017;</ref>. This method trains a student model using a modified dataset generated by the teacher model and the standard negative log-likelihood objective. While SeqKD is simple and efficient, we argue that it does not take advantage of the teacher's full potential.</p><p>Training the student model with a static dataset leads to the exposure bias problem. During training, the student model learns to predict the next token given previous tokens provided by the data.</p><p>However, at inference time, the student generates the entire sequence from scratch by repeatedly using its own outputs as context for subsequent steps. This training-inference inconsistency causes a decrease in generation quality. Alternatively, we propose that the student can leverage the teacher in a dynamic fashion during the learning process.</p><p>We devise a new compression algorithm for autoregressive models called imitation-based knowledge distillation (ImitKD). It is inspired by an imitation learning (IL) perspective on the autoregressive distillation problem. Our algorithm trains a student model within an IL framework by treating the teacher as an oracle, and allows the student to explore its own generation during training. The teacher corrects the student's generation at every time step, thereby guiding the student in learning how to generate.</p><p>Experimental results in translation and summarization show that ImitKD is especially suitable for compressing deep Transformer models that achieve high performance into shallow RNNs that generate up to 14 times faster at inference time. Our method consistently outperforms other distillation algorithms (such as word-level KD and sequencelevel KD), and yields student models that beat models trained without a teacher by 1.4 to 4.8 points on generation metrics such as BLEU and ROUGE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Autoregressive Distillation</head><p>First, we formalize the task of autoregressive distillation. An autoregressive model ? specifies a joint distribution over a T -dimensional target sequence y = {y 1 , . . . , y T } ? Y by decomposing it into a product of univariate conditionals:</p><formula xml:id="formula_0">?(y) = T t=1 ?(y t | y &lt;t ),<label>(1)</label></formula><p>where y &lt;t denotes {y 1 , . . . , y t?1 } for t &gt; 1 and ? for t = 1. The joint distribution over y may itself be conditional on some related source feature x ? X (e.g. translation, summarization) or not (e.g. language modeling). Since the former case can generalize the latter by letting X = ?, we will specify the presence of x in the rest of the paper. In autoregressive distillation, the goal is to learn a student model ? that performs well at sequence generation by minimizing its loss with respect to a pre-trained teacher model ? * . In many cases, the training objective can be expressed as</p><formula xml:id="formula_1">L(?) = E y|x?D T t=1 ? * (y &lt;t , x; ?) , (2)</formula><p>where ? * (?; ?) is the next-token loss function measuring the discrepancy between the teacher and student models given some prior context {y &lt;t , x}.</p><p>Here, D denotes a distribution (or dataset) of source-target pairs x ? y. Due to the combinatorial nature of sequence generation, an autoregressive distillation method must maximize its learning efficiency by carefully D, i.e. how it explores the exponentially-sized space. We motivate this choice with the field of imitation learning, an active research area of reinforcement learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Distillation as Imitation Learning</head><p>Autoregressive text generation can be interpreted as a T -step Markov decision process (MDP). In particular, the autoregressive model ? we wish to learn can be treated as a policy learner that maps a state to a distribution over actions. In our case, a state is a partial sequence y &lt;t for t &lt; T , an action is the next token y t , and the action space is the vocabulary. Given a state (partial sequence) and a chosen action (next token), the transition function is deterministic and simply concatenates them to form a new state (partial sequence).</p><p>The policy learner must be trained using some form of supervision. One option is to use rewardbased reinforcement learning, which requires defining the numerical quality of a state. However, for the autoregressive distillation problem, an arguably better choice is imitation learning (IL), which optimizes the policy by learning from demonstrations. In IL settings, an oracle policy ? * that is known to achieve high performance is provided during training. As a result, we can recast the overall goal as minimizing the divergence of the policy ? from the oracle ? * . For example, it may be difficult to objectively define what it means for an aspiring translator to perform well at the local token-by-token level. Yet, if we were given access to an expert translator, we could simply say the learner is performing well if they translate in the same way as the expert.</p><p>The IL framework is well-suited for autoregressive distillation, since the student and teacher models naturally fill the respective roles of the learner ? and the oracle ? * . Thus, we can easily apply theoretical results and practical methods from the IL literature to the autoregressive distillation problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">SeqKD as Behavioral Cloning</head><p>One distinguishing feature between different imitation learning methods pertains to how to define the state distribution D in the training objective (Equation 2). Indeed, this is also one of the key design questions of autoregressive distillation. For instance, one simple and effective IL method is behavioral cloning <ref type="bibr" target="#b38">(Ross and Bagnell, 2010)</ref>, which obtains D by running the oracle ? * on the MDP.</p><p>The popular sequence-level knowledge distillation (SeqKD) algorithm of <ref type="bibr" target="#b21">Kim and Rush (2016)</ref> can be interpreted as behavioral cloning. For each source feature x in the original training data, the teacher/oracle generates its (approximate) mode y * = arg max y ? * (y | x), typically using beam search. This new set of x ? y * pairs forms a teacher-generated dataset D * that serves as the state distribution for training the student. In addition, the negative log-likelihood of the teacher's tokens y * = {y * 1 , ? ? ? , y * T } is used as the loss ? * (?; ?). The overall training objective L SeqKD (?) is</p><formula xml:id="formula_2">E y * |x?D * T t=1 ? log ?(y * t | y * &lt;t , x) .<label>(3)</label></formula><p>The key advantage of SeqKD (as well as behavioral cloning) lies in its simplicity -we only need some samples from the teacher/oracle to work with. In comparison to vanilla supervised learning (which minimizes the negative log-likelihood of human-generated text), SeqKD has no additional training overhead other than the creation of D * .</p><p>However, the simplicity of the algorithm also limits its potential. <ref type="bibr" target="#b38">Ross and Bagnell (2010)</ref> argued that training a policy ? via behavioral cloning incurs regret with respect to the oracle ? * that is a quadratic function of the time horizon T . Intuitively, behavioral cloning suffers from the exposure bias problem. During training, the student model learns to perform good actions for the teacher/oracle's state distribution D * , but is never exposed to its own states. Thus, during testing (when the student must walk an MDP of selfgenerated states), the step-by-step errors compound over time, resulting in suboptimal generations.</p><p>We argue that in autoregressive distillation, the teacher/oracle can do more than produce a static dataset. It is a dynamic entity capable of interacting with the student throughout training. By querying the teacher with its own states, the student has the opportunity to ameliorate exposure bias and learn how to generate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Imitation-Based Distillation Algorithm</head><p>In this section, we present our IL-based algorithm for autoregressive distillation. We begin by describing the key design principles and why we expect them to work well. Then, we elaborate on the algorithm's implementation in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Design Principles and Rationale</head><p>One key principle of our algorithm is that the student model must be trained on its own state distribution so that it will perform better at generation. In practice, we achieve this by sampling training examples fromD, a mixture of an initial distribution D (e.g. a static training set) and the distribution D ? of generations from the student ?. We use D to alleviate the cold-start problem, in which an untrained ? generates poorly at the start of training.</p><p>This idea builds upon the empirical and theoretical foundation of dataset aggregation (DAgger), one of the most popular imitation learning methods that improve upon behavioral cloning. DAgger <ref type="bibr" target="#b39">(Ross et al., 2011)</ref> successively populates its training set by adding new data generated from the oracle-learner mixture. It then re-trains the policy learner on the aggregated dataset at each iteration. Under some assumptions (such as the loss function being strongly convex in ?), <ref type="bibr" target="#b39">Ross et al. (2011)</ref> proved that DAgger yields a policy ? that has linear regret in T with respect to ? * . This is a significant improvement over the behavior cloning result and can be attributed to fixing exposure bias. We expect a similar strategy of mixing oracle and learner distributions to work well for non-convex neural networks, as shown in other applications <ref type="bibr" target="#b52">(Zhang and Cho, 2016;</ref><ref type="bibr" target="#b44">Sun et al., 2017)</ref>.</p><p>Another key principle of our algorithm is that the teacher model should play the role of the oracle and correct the student's generations at each time step. In order for such a training strategy to be successful, the teacher must be able to provide better actions than the student for the student's own states. To test this hypothesis, we experiment with a deep Transformer-based translation model completing the partial translations of a shallow RNN. As shown in <ref type="table" target="#tab_0">Table 1</ref>, the Transformer completions achieve much higher BLEU score than the RNN's full generations. This validates our assumption that a strong teacher model can indeed play the role of the oracle and guide the student to better states. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The ImitKD Algorithm</head><p>Our imitation-based knowledge distillation algorithm (ImitKD) is given in Algorithm 1. The central training objective is</p><formula xml:id="formula_3">L ImitKD (?) = E y|x?D T t=1 ? * (y &lt;t , x; ?) ,<label>(4)</label></formula><p>whereD is the data mixture defined by sampling from the initial dataset D and generating with the student (lines 8-11). The probability ? i ? [0, 1] (line 8) controls how often an example comes from D. The loss function ? * can be realized as the negative log-likelihood of the oracle's optimal next token/action,</p><formula xml:id="formula_4">? * opt (y &lt;t , x; ?) = ? log ?(v * | y &lt;t , x),<label>(5)</label></formula><p>where v * = arg max v?V ? * (v | y &lt;t , x). Alternatively, ? * can be the cross-entropy loss between the full distributions,</p><formula xml:id="formula_5">? * full (y &lt;t , x; ?) (6) = ? v?V ? * (v | y &lt;t , x) ? log ?(v | y &lt;t , x).</formula><p>Next, we describe some practical implementations in order to make Algorithm 1 suitable for compressing deep learning systems. One limitation of DAgger is that the training data keeps growing, making each iteration successively more expensive.</p><p>As an alternative to aggregation, we perform data replacement within each training batch.</p><p>As shown in Algorithm 1, we treat each mini-batchD i as a new iteration of the dataset and perform a single step of stochastic gradient descent on L ImitKD (Equation 4) with respect to the parameters of the previous model ? i to yield ? i+1 . Thus, the number of iterations I becomes the number of mini-batches used to train the student model.</p><p>Our practical algorithmic changes are inspired by theory. The dataset aggregation algorithm <ref type="bibr" target="#b39">(Ross et al., 2011)</ref> achieves its regret bounds because it reduces to the Follow-the-Leader algorithm for Algorithm 1 Imitation-Based Distillation 1: Let D be initial dataset. 2: Initialize ? 1 at random. 3: for i = 1, . . . , I do 4:</p><p>Initialize new datasetD i = ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>repeat B times 6:</p><p>Sample an example e = y | x ? D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>Sample uniformly u ? [0, 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>if u &gt; ? i then 9:</p><p>Generate? from ? i given x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10:</head><p>Replace example with e =? | x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p>end if 12:</p><p>Append example e toD i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>13:</head><p>Compute L ImitKD (? i ) onD i with ? * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>14:</head><p>Let ? i+1 = ? i ? ? i ? ?L ImitKD /?? i . 15: end for 16: return Best policy ? on validation set. online learning <ref type="bibr" target="#b20">(Kakade et al., 2009)</ref>. Our training paradigm can be similarly interpreted as an online gradient descent algorithm, which has comparable guarantees for strongly convex losses <ref type="bibr" target="#b15">(Hazan et al., 2007)</ref> and even certain non-strongly convex losses <ref type="bibr" target="#b12">(Garber, 2019)</ref>. Variants of this paradigm have also been employed in other deep learning work <ref type="bibr" target="#b2">(Bengio et al., 2015;</ref><ref type="bibr" target="#b44">Sun et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Data Mixture Selection and Annealing</head><p>Dataset replacement requires an initial dataset that can be potentially replaced at each step. A natural candidate for this initial dataset is the original supervised training data (denoted as D ), which can be interpreted as a collection of samples from a human oracle. Alternatively, we can use the SeqKD dataset D * , which has generations from the teacher.</p><p>If we take samples from D or D * and replace some of them with student-generated samples, we effectively create a teacher-student dataset mixture. Unlike DAgger, this mixture occurs at the sequence level instead of the token/state level. An advantage of sequence-level mixtures is that they do not require generating with the teacher during each training iteration, which can be quite expensive if the teacher is a large neural network. Instead, the teacher only needs to compute the batched loss, which is comparatively much cheaper. The exact mixing schedule ? 1 , . . . , ? I is a customizable feature of Algorithm 1. Empirically, we have found an exponential decay to work well, i.e. ? i = r i/I , where r ? [0, 1] is the final mixing rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Speeding Up Training</head><p>Generating sequences? on the fly at every iteration (line 9) can be a major computation bottleneck during training. We speed up this step by generating a pool of B ? M examples in parallel only once every M iterations, where B is the batch size and M is a hyperparameter. One caveat of this modification is that at iteration i, the loss function may no longer be computed on examples generated by the most recent set of model parameters, but rather parameters from up to M iterations prior. Nonetheless, we have found that setting M to a small integer (e.g. 2-8) can speed up training time without impacting final model performance.</p><p>We use greedy decoding or top-K sampling with small K to produce samples? (line 9) in our algorithm. These two strategies are efficient to run, operate similarly to the generation employed at inference time, and have empirically worked well in our experiments. Of course, the generation strategy can be customized for different tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>The distillation problem for autoregressive models was first tackled by <ref type="bibr" target="#b21">Kim and Rush (2016)</ref>, who introduced sequence-level knowledge distillation for neural machine translation. Subsequent works have used seqKD for non-autoregressive translation models <ref type="bibr" target="#b14">(Gu et al., 2017;</ref>, lowresource settings <ref type="bibr" target="#b5">(Chen et al., 2017)</ref>, and ensemble distillation with multiple teachers <ref type="bibr" target="#b23">(Kuncoro et al., 2016;</ref>.  proposed a behavioral cloning method for distilling autoregressive translation models into non-autoregresssive translation models. In contrast, our method aims to address the learning challenges in autoregressive distillation, such as exposure bias.</p><p>Various methods other than standard supervised learning have been explored for training generative models of language. <ref type="bibr">MIXER (Ranzato et al., 2015)</ref> and Beam Search Optimization <ref type="bibr" target="#b50">(Wiseman and Rush, 2016)</ref> also perform generation during training, but use sequence-level metrics (e.g. BLEU score) as training supervision. Simlarly, SEARNN <ref type="bibr" target="#b24">(Leblond et al., 2017)</ref> trains RNNs to iteratively generate sequences with beam search to compute the local loss of a single action during the decoding process. Scheduled sampling <ref type="bibr" target="#b2">(Bengio et al., 2015)</ref> and its extensions <ref type="bibr" target="#b13">(Goyal et al., 2017;</ref><ref type="bibr" target="#b53">Zhang et al., 2019)</ref> alleviate exposure bias by replacing some words in the true context with the model's prediction. However, without a dynamic queryable oracle, these methods face the challenge of properly defining the training signal when the generated sequence no longer exists in the static training data. For example, directly reusing the tokens in the static dataset as the target next token leads to an inconsistent training procedure <ref type="bibr" target="#b19">(Husz?r, 2015)</ref>. In contrast to these methods, distillation can fully leverage the teacher oracle, allowing us to design a simple and efficient imitation learning algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>We test our autoregressive distillation method and all baselines on three language generation tasks -IWSLT 2014 German ? English translation, WMT 2016 English ? German translation, and CNN/DailyMail abstractive news summarization.</p><p>Datasets The IWSLT 2014 De?En dataset consists of approximately 170K sequence pairs. Following standard practice <ref type="bibr" target="#b0">(Bahdanau et al., 2016;</ref><ref type="bibr" target="#b8">Deng et al., 2018;</ref>, we randomly sample 4% of this dataset as the validation set and let the remaining be the training set. The test set is the concatenation of the dev2010, tst2010, tst2011, and tst2012 files. We use a shared vocabulary of 14K lowercased BPE tokens <ref type="bibr" target="#b43">(Sennrich et al., 2015)</ref>.</p><p>The WMT 2016 En?De dataset has 4.5 million training pairs. We use the same preprocessing of the prior work <ref type="bibr" target="#b30">(Ott et al., 2018)</ref>, newstest2013 as the validation set and newstest2014 as the test set. The vocabulary consists of 32K cased BPE tokens.</p><p>The CNN/DailyMail summarization dataset has 287K, 13K and 12K pairs in the training, validation and test sets, respectively. Following prior work <ref type="bibr" target="#b42">(See et al., 2017)</ref>, we truncate documents to 400 tokens and summaries to 100 tokens in the training set. During evaluation, we generate up to 128 tokens. We use a pre-trained BERT <ref type="bibr" target="#b9">(Devlin et al., 2018)</ref> tokenizer with a vocabulary of 30K lowercased tokens <ref type="bibr" target="#b27">(Liu and Lapata, 2019)</ref>.</p><p>Models Transformers often attain state-of-theart performance on common language generation tasks. On the other hand, RNNs (without selfattention) generate much faster at inference time. Thus, from a practitioner's standpoint, it may be most desirable to compress a high-performing Transformer into a lightweight RNN. For all tasks, we use the state-of-the-art Transformer architecture <ref type="bibr" target="#b47">(Vaswani et al., 2017)</ref> as the teacher model. The teacher models are trained using vanilla super-  <ref type="table">Table 2</ref>: Summary of training variants. Base variants use the negative log-likelihood (NLL) of the optimal next token -which is taken from the data for Vanilla, found using beam search for SeqKD, and queried from the teacher for ImitKD (i.e. ? * opt ). All "+ Full" variants are trained with the full teacher-student cross entropy. vised learning. For WMT, we directly use the pretrained Transformer model provided by the Fairseq library <ref type="bibr" target="#b30">(Ott et al., 2018</ref><ref type="bibr" target="#b29">(Ott et al., , 2019</ref>.</p><p>In all tasks, we use a recurrent neural network, specifically SRU <ref type="bibr" target="#b25">(Lei et al., 2017)</ref>, as the student model. For completeness, we also train Transformer, GRU , and LSTM <ref type="bibr" target="#b17">(Hochreiter and Schmidhuber, 1997)</ref> based student models on the IWSLT translation task, illustrating the effectiveness of our distillation method for various neural architectures. All RNN-based models follow the seq2seq, encoder-decoder architecture <ref type="bibr" target="#b45">(Sutskever et al., 2014)</ref> and employ a single scaled dot-product attention between the encoder and decoder <ref type="bibr" target="#b28">Luong et al., 2015)</ref>.</p><p>All models are trained using the Adam optimizer (Kingma and Ba, 2014) with an inversesquare-root learning rate scheduler and learning rate warmup <ref type="bibr" target="#b47">(Vaswani et al., 2017)</ref>. Our experiments were conducted using Flamb?, a PyTorch-based model training and evaluation library <ref type="bibr" target="#b51">(Wohlwend et al., 2019)</ref>. More implementation details such as hyperparameter settings are provided in Appendix A.</p><p>Variants For the student models, we compare a wide range of training variants, including baselines such as vanilla supervised learning (which directly uses the original training set) and sequence-level knowledge distillation (SeqKD). All SeqKD variants form the teacher-generated dataset using beam search with beam size K = 5. For our imitationbased method, we experiment with annealing from the original training set (ImitKD) or the teachergenerated SeqKD dataset (ImitKD * ). We also ex-periment with different token-level losses; base variants are trained with the optimal next token while "+ Full" variants are trained with the full cross entropy. <ref type="table">Table 2</ref> summarizes all variants and highlights their differences. Note that the Vanilla + Full baseline -referred to as "WordKD" by <ref type="bibr" target="#b21">Kim and Rush (2016)</ref> -has appeared in other distillation works (e.g. <ref type="bibr" target="#b41">Sanh et al., 2019)</ref>.</p><p>Evaluation We use BLEU score <ref type="bibr" target="#b31">(Papineni et al., 2002)</ref> for translation and report ROUGE-1, ROUGE-2 and ROUGE-L scores <ref type="bibr" target="#b26">(Lin, 2004)</ref> for summarization. For all models, the training checkpoint with the highest BLEU/ROUGE-1 score on the validation set is used for test set evaluation. We also report the perplexity metric for all tasks. <ref type="table" target="#tab_3">Table 3</ref> compares all distillation methods on the IWSLT dataset. The teacher model is an 8-layer Transformer. We use a 3-layer SRU, a 2-layer SRU and a 2-layer Transformer as student models. For all three student models, our ImitKD method outperforms all baselines in terms of BLEU score with beam size 1 (Bleu 1 ), BLEU score with beam size 5 (Bleu 5 ) and perplexity (PPL). The improvement on Bleu score ranges from 1.4 to 4.8 points compared to the Vanilla training method. The 3-layer SRU model trained with ImitKD + Full even slightly exceeds the performance of the teacher model. Furthermore, our method consistently outperforms SeqKD by up to 1.4 BLEU, highlighting the benefit of training the student model with its own state distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IWSLT De?En Translation</head><p>To further demonstrate the effectiveness of ImitKD across different model types, we report validation set Bleu 1 for various 2-layer neural architectures in <ref type="table" target="#tab_4">Table 4</ref>. Our ImitKD method outperforms the baselines in all cases, with the gains being especially large for recurrent architectures. <ref type="table" target="#tab_6">Table 5</ref> presents our results for the WMT dataset. The teacher is a 6-layer Transformer and the student is a 4layer SRU. Here, we see that ImitKD performs closer to SeqKD. These results reveal that direct behavioral cloning (SeqKD) can be quite effective when the amount of oracle demonstrations is sufficiently high, e.g. several millions of examples. Nonetheless, ImitKD and ImitKD* can improve on SeqKD by training the student with its own states. Among all variants, ImitKD + Full performs   the best while avoiding the overhead of creating a teacher-modified dataset. Furthermore, we see that ImitKD is especially effective in low-data regimes. As shown in the bottom block of <ref type="table" target="#tab_6">Table 5</ref>, ImitKD methods achieve much stronger results over baselines when we reduce the WMT training data to the same size as IWSLT. <ref type="table" target="#tab_7">Table 6</ref>, we present the CNN/DailyMail results for a 6-layer Transformer teacher and a 2-layer SRU student. Once again, the best student is ImitKD + Full, which achieves ROUGE scores that are within 1 point of the teacher's. ImitKD variants outperform the baselines on all ROUGE metrics, showcasing the utility of our method on a different NLG task.   Size and Speed Analysis In <ref type="table">Table 7</ref>, we analyze how our distillation technique can reduce computational costs, using the IWSLT <ref type="table" target="#tab_3">(Table 3)</ref>, WMT <ref type="table" target="#tab_6">(Table 5)</ref>, and CNN/DailyMail <ref type="table" target="#tab_7">(Table 6</ref>) teacher/student pairs as case studies. By training small student models with ImitKD, we can substantially decrease model size and increase inference speed, while minimizing performance loss. Shallow, recurrent architectures are especially attractive, because they can generate 4-14 times faster than deep Transformer teachers, and 2-3 times faster than Transformer students of similar size. <ref type="figure" target="#fig_0">Figure 1</ref> breaks down BLEU score vs. decoding length for IWSLT models trained with different algorithms (Vanilla, SeqKD, ImitKD). We show results for the three types of RNNs and the Transformer of <ref type="table" target="#tab_4">Table 4</ref>. All models have two layers.  <ref type="table">Table 7</ref>: Model types, along with statistics on size (in number of parameters) and CPU inference time per decoding (in milliseconds) for beam search with beam size K ? {1, 5}. Teacher models are marked with ? . The "% Perform" column records the ratio of the best student's performance to the teacher's performance on BLEU score for translation tasks and ROUGE-1 for the summarization task. As expected, we observe that the generation quality (in terms of BLEU score) degrades as the decoding length increases. This phenomenon can be explained by the global error compounding with each additional decision step <ref type="bibr" target="#b39">(Ross et al., 2011)</ref> and has been reported in previous works <ref type="bibr" target="#b53">Zhang et al., 2019)</ref>. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, models trained with the vanilla objective, especially RNN-based models, suffer the most from this problem. SeqKD improves the performance across all sequence lengths, but still experiences some BLEU score degradation for longer sequences. ImitKD further improves the BLEU score across all bins, and more importantly, the improvement is most significant for longer sequences. This analysis suggests that ImitKD explicitly addresses the exposure bias problem for training student models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WMT En?De Translation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN/DailyMail Summarization In</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance Analysis at Different Lengths</head><p>Additive Effect of Fine-Tuning <ref type="bibr" target="#b21">Kim and Rush (2016)</ref> propose a fine-tuning method for autoregressive distillation called SeqInter. This method can further improve pretrained student models by exposing them to the sequence in the teacher beam's  that is closest to the target in terms of sentence-level BLEU. In <ref type="table" target="#tab_10">Table 8</ref>, we show the results of applying SeqInter to each of the IWSLT models that were trained from scratch in <ref type="table" target="#tab_4">Table 4</ref>. While SeqInter enables Vanilla models to "close the gap" on SeqKD models, ImitKD models clearly maintain their superior performance even after fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this work, we developed a new knowledge distillation technique inspired by imitation learning for compressing large and cumbersome autoregressive models into smaller and faster counterparts. We demonstrated the empirical success of our method over popular baselines on several natural language generation tasks. We are excited about several possible avenues for future work. One branch of ideas involves incorporating more advanced IL algorithms beyond DAgger, such as LOLS <ref type="bibr" target="#b4">(Chang et al., 2015)</ref>, to further improve the distillation process. Another possibility is to design imitation-based fine-tuning analogs to the SeqInter method. Finally, although our experiments in this paper focused on sequenceto-sequence settings, we are interested in exploring the use of ImitKD for compressing large language models aimed at transfer learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 DAgger Algorithm</head><p>The dataset aggregation (DAgger) algorithm <ref type="bibr" target="#b39">(Ross et al., 2011)</ref> minimizes the following objective:</p><formula xml:id="formula_6">L Imit (?) = E s 1 ,...,s T ?D T t=1 ? * (s t ; ?) , (7)</formula><p>where D is a distribution (or dataset) of T -step state trajectories and ? * (s, ?) is the action-discrepancy loss between the oracle ? * and the policy learner ? in state s. The full DAgger algorithm is given in Algorithm 2.</p><p>Algorithm 2 Dataset Aggregation 1: Let D = ? be initial dataset. 2: Initialize ? 1 at random. 3: for i = 1, . . . , I do</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Let mixture policy? i = ? i ? * + (1 ? ? i )? i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Initialize new dataset D i = ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>repeat B times 7:</p><p>Run MDP on? i , sample {s 1 , . . . , s T }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>Append new states {s 1 , . . . , s T } to D i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>Aggregate D = D ? D i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10:</head><p>Train ? i+1 on D to min L Imit with ? * . 11: end for 12: return Best policy ? on validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Implementation Details</head><p>In all experiments, all RNN-based models with hidden dimension N consist of a bidirectional encoder with hidden dimension N/2 and a left-to-right decoder with hidden dimension N .</p><p>For BLEU score evaluation, we use the NLTK library. 2 For ROUGE score evaluation, we use the py-rouge library. 3 IWSLT The IWSLT 2014 German ? English dataset is taken directly from the source website. <ref type="bibr">4</ref> We train an 8-layer Transformer teacher model with model dimension 256, feedforward dimension 1024, and four attention heads as the teacher model. The 2-layer student SRU model has a hidden dimension 512, and the 3-layer model has hidden dimension 1024 and projection dimension 256. The student Transformer model has model dimension 256, feedforward dimention 768 and 4 attention heads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preliminary Study For</head><p>All models have word embedding dimension 256 and exhibit weight tying between the decoder embeddings and the output layer <ref type="bibr" target="#b33">(Press and Wolf, 2016)</ref>. We train models for 80K steps with batch size 128 using the Adam optimizer with base learning rate 0.1. We use an inverse-square-root learning rate scheduler <ref type="bibr" target="#b47">(Vaswani et al., 2017)</ref> with 10K warmup steps for the teacher and 5K warmup steps for all students. Validation set metrics are recorded every 1K steps. For all ImitKD variants, we set the final mixing rate r = 0.005 (i.e. very close to 0), and use top-K sampling with K = 5 as the generation algorithm during training. We use M = 4 as the batch parallelization parameter.</p><p>In <ref type="table" target="#tab_4">Table 4</ref>, the 2-layer SRU and the 2-layer Transformer follow the same architecture as those in <ref type="table" target="#tab_3">Table 3</ref>. To standardize architecture across RNNs, the GRU and the LSTM have the same embedding dimension (i.e. 256) and hidden dimension (i.e. 512) as the SRU.</p><p>WMT The WMT 2016 dataset is taken from the Fairseq library. <ref type="bibr">5</ref> We use a pre-trained Transformer-large model from the Fairseq library <ref type="bibr" target="#b30">(Ott et al., 2018</ref><ref type="bibr" target="#b29">(Ott et al., , 2019</ref>) as our teacher model. It has embedding dimension 1024, model dimension 1024, and feedforward dimension 4096. The student is a 4-layer SRU with hidden size 1024, projection size 256, and embedding size 256. The student is trained for 15 epochs with batch size 512, base learning rate 0.1, and 4K warmup steps. We record validation metrics every 1/4 of the epoch. The encoder embeddings, decoder embeddings, and decoder output layer share the same weight parameters. We tune the final mixing rate r ? {0.5, 0.1, 0.005} for our ImitKD variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN/Dailymail</head><p>The CNN/DailyMail dataset is taken from Professor Kyunghyun Cho's website, a commonly used source for this dataset. 6 The teacher model is a 6-layer Transformer-base model with embedding dimension 512, model dimension 512, and feedforward dimension 2048. The student is a 2-layer SRU with embedding dimension 256, hidden size 1024, and projection size 256. We use a batch size of 128. For both models, the learning rate follows an inverse-square root schedule with warmup of 2K steps. Validation set metrics are recorded every 2K steps. The teacher has a base learning rate of 0.03, while the student has a base learning rate of 0.1. The teacher benefits from larger effective batch sizes by accumulating gradients every eight steps. On the other hand, the student does not seem to benefit from gradient accumulation and therefore takes a gradient step after processing each batch. All ImitKD variants use final mixing rate r = 0.1 and greedy decoding during training. We use M = 4 as the batch parallelization parameter.</p><p>Size and Speed Analysis CPU generation times for all models were measured on a 2019 MacBook Pro with a 2.6GHz 6-core Intel Core i7 processor. Time estimates reported in <ref type="table">Table 7</ref> were averaged over examples in the test set of the corresponding dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance Analysis at Different Lengths</head><p>For each IWSLT variant, we ran greedy decoding (i.e. beam search decoding with beam size K = 1) on the test set. Then, we sorted the decoded sequences by length into the following bins: [0, 20], <ref type="bibr">[21,</ref><ref type="bibr">40]</ref>, <ref type="bibr">[41,</ref><ref type="bibr">60]</ref>, <ref type="bibr">[61,</ref><ref type="bibr">80]</ref>, <ref type="bibr">[81,</ref><ref type="bibr">100]</ref>, <ref type="bibr">[101,</ref><ref type="bibr">120]</ref>. Each point in <ref type="figure" target="#fig_0">Figure 1</ref> is the Bleu score of all sequences within one of these bins for the corresponding IWSLT variant.</p><p>Additive Effect of Fine-Tuning For the finetuning experiments, we generated SeqInter data with a beam size of K = 5 and NLTK's sentencelevel BLEU implementation. We used the Adam optimizer with a base learning rate of 0.01 and an inverse-square root scheduler with 2K warmup steps. All models were fine-tuned for 20K iterations. Models were validated every 1K iterations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Bleu score versus sequence decoding length across different models and training variants. Each point on the graph represents the Bleu score of all sequences whose length is within a bin of width 20.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Greedy decoding BLEU scores on the IWSLT validation set for the preliminary test.</figDesc><table><row><cell>Decoding Method</cell><cell>Bleu ?</cell></row><row><cell>Transformer only</cell><cell>33.8</cell></row><row><cell>RNN only</cell><cell>28.6</cell></row><row><cell>RNN first, Transformer completes</cell><cell>31.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results on IWSLT test dataset.</figDesc><table><row><cell cols="4">Variant SRU GRU LSTM Transf.</cell></row><row><cell>Vanilla</cell><cell>28.6 28.6</cell><cell>27.7</cell><cell>32.4</cell></row><row><cell cols="2">SeqKD 31.4 31.2</cell><cell>30.5</cell><cell>33.3</cell></row><row><cell cols="2">ImitKD 32.7 32.7</cell><cell>32.4</cell><cell>33.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>BLEU scores of different student architectures on the IWSLT validation set. We use a beam size of 1.</figDesc><table><row><cell>The teacher attains a validation BLEU of 33.8.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell cols="3">: Results on WMT dataset. ImitKD* is trained</cell></row><row><cell cols="2">on a student/teacher dataset mixture.</cell><cell>indicates that</cell></row><row><cell cols="2">the model is trained with 25? less data.</cell></row><row><cell>Variant</cell><cell cols="2">PPL ? R1 ? R2 ? RL ?</cell></row><row><cell>Teacher</cell><cell cols="2">12.5 39.0 17.6 35.7</cell></row><row><cell>Vanilla</cell><cell cols="2">14.7 36.1 15.6 32.8</cell></row><row><cell>SeqKD</cell><cell cols="2">52.9 36.4 16.1 33.1</cell></row><row><cell>ImitKD</cell><cell cols="2">17.2 37.3 16.4 34.1</cell></row><row><cell>ImitKD*</cell><cell cols="2">37.1 37.7 16.7 34.5</cell></row><row><cell>Vanilla + Full</cell><cell cols="2">13.6 36.2 16.0 32.9</cell></row><row><cell>SeqKD + Full</cell><cell cols="2">20.2 37.4 16.5 34.0</cell></row><row><cell>ImitKD + Full</cell><cell cols="2">14.0 38.4 17.1 34.9</cell></row><row><cell>ImitKD* + Full</cell><cell cols="2">17.9 38.1 17.1 34.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Results on CNN/DailyMail dataset. All models generate using beam search K = 5 decoding.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell>: IWSLT validation set BLEU scores of</cell></row><row><cell>SeqInter fine-tuning applied to the different student ar-</cell></row><row><cell>chitectures of Table 4. We use a beam size of 1. The</cell></row><row><cell>teacher (without fine-tuning) attains a validation BLEU</cell></row><row><cell>of 33.8.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 1</head><label>1</label><figDesc>, we train both an 8-layer Transformer and a 2-layer RNN (specifically SRU) on the IWSLT dataset using standard supervised learning. The architectural and training details are the same as those outlined in the IWSLT experiments. At test time, both the Transformer and the RNN perform greedy decoding. On average, ground-truth translations in the IWSLT test set have 24.5 tokens. The "RNN first, Transformer completes" mixed decoding strategy generates 12 tokens (i.e. half on average) with the RNN and the rest with the Transformer. We measure generation quality using Bleu score.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our code can be found at https://github.com/ asappresearch/imitkd.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.nltk.org/ modules/nltk/translate/ bleu score.html 3 https://github.com/Diego999/py-rouge</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://sites.google.com/site/ iwsltevaluation2014/data-provided 5 https://github.com/pytorch/fairseq/tree/ master/examples/translation</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://cs.nyu.edu/ ? kcho/DMQA/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the ASAPP NLP team -especially Yi Yang, Nicholas Matthews, Joshua Shapiro, Hugh Perkins, Amit Ganatra, Lili Yu, Xinyuan Zhang, and Yoav Artzi -as well as the EMNLP reviewers for their helpful feedback on the paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">An actor-critic algorithm for sequence prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philemon</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.07086</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1171" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Bucilu?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="535" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to search better than your teacher</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alekh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2058" to="2066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A teacher-student framework for zeroresource neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.00753</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02860</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Latent alignment and variational attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9712" to="9724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.03197</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A copyaugmented sequence-to-sequence architecture gives good performance on task-oriented dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihail</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.04024</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Logarithmic regret for online gradient descent beyond strong convexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Garber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 22nd International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="295" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06970</idno>
		<title level="m">Differentiable scheduled sampling for credit assignment</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.02281</idno>
		<title level="m">Nonautoregressive neural machine translation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Logarithmic regret algorithms for online convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satyen</forename><surname>Kale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="169" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.06146</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">How (not) to train your generative model: Scheduled sampling, likelihood</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Husz?r</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05101</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">adversary? arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On the complexity of linear prediction: Risk bounds, margin bounds, and regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Sham M Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambuj</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tewari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="793" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.07947</idno>
		<title level="m">Sequencelevel knowledge distillation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Distilling an ensemble of greedy dependency parsers into one mst parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07561</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Leblond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Osokin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04499</idno>
		<title level="m">Searnn: Training rnns with global-local losses</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Artzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.02755</idno>
		<title level="m">Simple recurrent units for highly parallelizable recurrence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Text summarization with pretrained encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08345</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04025</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01038</idno>
		<title level="m">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Scaling neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00187</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Using the output embedding to improve language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.05859</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Better language models and their implications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miles</forename><surname>Brundage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://openai.com/blog/better-language-models" />
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelio</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06732</idno>
		<title level="m">Sequence level training with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fastspeech: Fast, robust and controllable text to speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangjun</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3165" to="3174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Efficient reductions for imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drew</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="661" to="668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A reduction of imitation learning and structured prediction to no-regret online learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drew</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth international conference on artificial intelligence and statistics</title>
		<meeting>the fourteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="627" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.00685</idno>
		<title level="m">Sumit Chopra, and Jason Weston. 2015. A neural attention model for abstractive sentence summarization</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01108</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04368</idno>
		<title level="m">Get to the point: Summarization with pointer-generator networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.07909</idno>
		<title level="m">Neural machine translation of rare words with subword units</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deeply aggrevated: Differentiable imitation learning for sequential prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Venkatraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron</forename><surname>Boots</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J Andrew</forename><surname>Bagnell</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="3309" to="3318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Multilingual neural machine translation with knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10461</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Improving neural language modeling via adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyue</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.03805</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Imitation learning for nonautoregressive neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingzhen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02041</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Sequence-to-sequence learning as beam-search optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02960</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Flamb?: A customizable framework for machine learning experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Wohlwend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Itzcovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="181" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Query-efficient imitation learning for endto-end autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiakai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Bridging the gap between training and inference for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02448</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Understanding knowledge distillation in nonautoregressive machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02727</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
