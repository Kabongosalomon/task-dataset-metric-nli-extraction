<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ERNIE-VILG: UNIFIED GENERATIVE PRE-TRAINING FOR BIDIRECTIONAL VISION-LANGUAGE GENERATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
							<email>zhanghan17@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichong</forename><surname>Yin</surname></persName>
							<email>yinweichong@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yewei</forename><surname>Fang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lanxin</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqiang</forename><surname>Duan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihua</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tian</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ERNIE-VILG: UNIFIED GENERATIVE PRE-TRAINING FOR BIDIRECTIONAL VISION-LANGUAGE GENERATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conventional methods for the image-text generation tasks mainly tackle the naturally bidirectional generation tasks separately, focusing on designing task-specific frameworks to improve the quality and fidelity of the generated samples. Recently, Vision-Language Pre-training models have greatly improved the performance of the image-to-text generation tasks, but large-scale pre-training models for text-to-image synthesis task are still under-developed. In this paper, we propose ERNIE-ViLG, a unified generative pre-training framework for bidirectional image-text generation with transformer model. Based on the image quantization models, we formulate both image generation and text generation as autoregressive generative tasks conditioned on the text/image input. The bidirectional image-text generative modeling eases the semantic alignments across vision and language. For the text-to-image generation process, we further propose an end-to-end training method to jointly learn the visual sequence generator and the image reconstructor. To explore the landscape of large-scale pre-training for bidirectional text-image generation, we train a 10-billion parameter ERNIE-ViLG model on a large-scale dataset of 145 million (Chinese) image-text pairs which achieves state-of-theart performance for both text-to-image and image-to-text tasks, obtaining an FID of 7.9 on MS-COCO for text-to-image synthesis and best results on COCO-CN and AIC-ICC for image captioning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>"What I cannot create, I do not understand."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Richard Feynman</head><p>Cross-modal generation, aiming at mapping or translating one modality to another, requires the model to fully "understand" the source modality and to faithfully "create" (generate) the target modality with high semantic consistency with the source <ref type="bibr" target="#b0">[1]</ref>. As concerned with a big part of multimodal machine learning researches, the cross-modal generation task has attracted dramatic attentions, for audio-linguistic <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, visual-linguistic <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>, audio-visual <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> modalities. In this paper, we focus on the visual-linguistic generation tasks which mainly include image captioning (describing the visual content of an image in natural language) and text-to-image synthesis (creating an image that keeps semantic consistent with a textual description). As the intersection of computer vision and natural language processing, the Vision-Language generation tasks attract much interest from both NLP and CV communities and have achieved great progresses over the past several years.</p><p>Naturally, the text-image generation tasks are bidirectional and a reasonable model should have the capability of both image captioning and text-to-image synthesis. However, due to the distinct generative architectures for language generation and image generation, methods tackle the two tasks have developed separately. The conventional methods for image captioning mainly adopt the encoder-decoder architecture <ref type="bibr" target="#b3">[4]</ref> where the text is generated in a sequence generation manner. The image is first encoded into one or multiple vectors and then a decoder (e.g., LSTM) is utilized arXiv:2112.15283v1 [cs.CV] 31 Dec 2021 to generate language tokens based on the encoder results. Recently, the transformer-based Vision-Language Pre-trained models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12]</ref> have significantly improved the performance of image captioning benefiting from pre-training on large-scale image-text aligned datasets. On the other hand, the dominant text-to-image synthesis methods are the Generative Adversarial Networks (GANs) <ref type="bibr" target="#b12">[13]</ref> variants where ConvNets are utilized as the basic architecture of the generator. While various methods have been proposed to improve the quality and resolution of generated images, such as StackGAN <ref type="bibr" target="#b13">[14]</ref>, XMC-GAN <ref type="bibr" target="#b7">[8]</ref>, it remains difficult to generate images of complex scenes with multiple objects.</p><p>Inspired by the autoregressive models for the images <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>, recent works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17]</ref> have proposed to formulate the text-to-image synthesis as a sequence-to-sequence problem where the image tokens are learned through discrete VAE <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b8">9]</ref>. They model the text tokens and image tokens in the same transformer and have shown great improvement for the image quality and the capability to construct complex scenes. And the text-to-image generation process usually adopts a two-stage pipeline consisting of a generator (to generate the image tokens) and a reconstuctor (to build the image from the generated image tokens), which are trained separately.</p><p>More recently, researchers have attempted to unify the bidirectional image-text generation tasks in a single model. Cho et al. <ref type="bibr" target="#b18">[19]</ref> attempt to enhance the existing Vision-Language Pre-training models with the capability of text-to-image generation, utilizing image discrete tokens generated by K-means clustering of pre-trained image features and generating the image sequence in a non-autoregressive manner. Based on this approach, Huang et al. <ref type="bibr" target="#b19">[20]</ref> formulate both the tasks as sequence generation tasks and further propose a sequence-level training task to improve the performance. Although they unify the two tasks in one single transformer, they employ the non-autoregressive sampling to generate images which enables a fast inference speed but leads to suboptimal performance compared to the autoregressive generative models due to the removal of explicitly modeling of dependencies among the target tokens.</p><p>In the paper, we propose ERNIE-ViLG, a unified pre-training method for the bidirectional image-text generation with transformer model. Based on image quantization techniques <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b20">21]</ref>, both the image-to-text and text-to-image generation are tackled in autoregressive manners within the same transformer. And we further propose an end-toend pre-training method for text-to-image synthesis which makes the traditionally separate two-stage generator and reconstructor training jointly. To explore the landscape of large-scale pre-training for bidirectional text-image generation, we pre-train a 10-billion parameter model on a large-scale dataset of 145 million high-quality Chinese image-text pairs. It has shown superior performance for both text-to-image synthesis and image captioning tasks. Concretely, it achieves the best FID on MS-COCO for text-to-image synthesis and obtains state-of-the-art performance on two Chinese image captioning datasets. Moreover, we evaluate ERNIE-ViLG on a challenging generative Visual Question Answering (VQA) task, and the excellent performance shows that our bidirectional generative model has captured the semantic alignments across vision-language modalities and can also be transferred to tackle complex generative tasks besides image-text generation.</p><p>Overall, our contributions include:</p><p>? We propose ERNIE-ViLG, a unified generative pre-training method for bidirectional image-text generation tasks, where both the image and text generation are formulated as autoregressive generative tasks. And we propose the first end-to-end training method for text-to-image synthesis based on image discrete representation, which enhances both the generator and reconstructor and outperforms the traditional two-stage approach.</p><p>? We train a 10-billion parameter ERNIE-ViLG model and obtain superior performance for both text-to-image and image-to-text generation tasks, setting new SOTA results for text-to-image synthesis on MS-COCO and obtaining SOTA results for image captioning on two popular Chinese datasets.</p><p>? Superior performance on the generative VQA task shows that our bidirectional generative model captures the complex semantic alignments between the vision and the language modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Vision-Language Pre-training</head><p>Vision-Language Pre-training (VLP) models <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref> have greatly improved the performance of various Vision-Language tasks, such as VQA and cross-modal retrieval. Utilizing transformer architecture (two-stream or single-stream) to fuse the visual modality and the linguistic modality, they propose various pre-training tasks, from the conventional masked language modeling (MLM), masked region prediction (MRP), image-text matching (ITM) to word-region alignment <ref type="bibr" target="#b22">[23]</ref>, scene graph prediction <ref type="bibr" target="#b23">[24]</ref>, multimodal conditional text generation <ref type="bibr" target="#b27">[28]</ref>, Prefix-LM <ref type="bibr" target="#b28">[29]</ref>, etc. The pre-training datasets are also a big concern of VLP researches where early works utilize million-scale noisy image-text datasets such as Conceptual Captions <ref type="bibr" target="#b29">[30]</ref> and human labeled datasets such as COCO Captions <ref type="bibr" target="#b30">[31]</ref> and recent works scale up the pre-training with hundreds of millions and nearly billion-scale noisy image-text dataset <ref type="bibr" target="#b28">[29]</ref>.</p><p>While most of VLP methods focus on pre-training for the vision-language understanding tasks, some works have noticed the importance of pre-training for improving the performance of cross-modal generation tasks, mainly image captioning task. Zhou et al. <ref type="bibr" target="#b6">[7]</ref> propose the first unified model to improve the performance of both understanding and generation tasks, where the bidirectional and sequence-to-sequence pre-training tasks utilize different attention masks. Wang et al. <ref type="bibr" target="#b28">[29]</ref> propose prefix-LM pre-training task, similar to image captioning, solely exploiting language modeling objective. Hu et al. <ref type="bibr" target="#b31">[32]</ref> study the scaling behavior of pre-training from both the data and model perspective for image captioning.</p><p>While the image-to-text generation task has been advanced a lot due to the pre-training on large-scale image-text datasets, the benefit of pre-training for text-to-image synthesis task has not been fully explored. Cho et al. <ref type="bibr" target="#b18">[19]</ref> propose to enhance the VLP models with image generation capability based on discretized visual representation and fine-tuned for text-to-image synthesis. Wu et al. <ref type="bibr" target="#b32">[33]</ref> attempt to construct a unified encoder-decoder pre-trained framework aiming at both image and video synthesis tasks.</p><p>ERNIE-ViLG focuses on the pre-training for both the image-to-text and text-to-image generation tasks and simplifying the pre-training objectives to the autoregressive sequence-to-sequence generation tasks for both image and text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Vision-Language Generation</head><p>Image-to-Text Generation The typical methods for image captioning adopt the encoder-decoder architecture formulating the image-to-text problem as text sequence generation task with the context of the image. Vinyals et al. <ref type="bibr" target="#b3">[4]</ref> propose a simple LSTM-based architecture where the image is encoded into a single vector and used as the initial hidden state of a single-layer LSTM. Utilizing the attention-mechanism, various methods <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b34">35]</ref> have been proposed to improve the correlation between the visual image and the generated text sequence where the image is encoded into multiple features and used via cross-modal attention to guide the generation process. Recently, researchers have explored the potential of transformer-based model for image captioning <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b6">7]</ref>. Zhou et al. <ref type="bibr" target="#b6">[7]</ref> propose to use a unified transformer architecture to fuse the visual and textual modalities, for both encoding and decoding. Following this, Li et al. <ref type="bibr" target="#b11">[12]</ref> propose to include objects tags which are extracted from the image using an object detector into the input for augmenting the semantic alignment between the image and text.</p><p>Text-to-Image Synthesis Since the arise of generating images using Generative Adversarial Networks (GANs) <ref type="bibr" target="#b12">[13]</ref>, the GANs based methods have been most popular ones for text-to-image synthesis. Reed et al. <ref type="bibr" target="#b4">[5]</ref> first extend the conditional GANs to generate images from language descriptions. After that, Zhang et al. <ref type="bibr" target="#b13">[14]</ref> propose StackGAN to generate high-resolution images in a multi-stage manner. Zhu et al. <ref type="bibr" target="#b36">[37]</ref> propose to use dynamic memory networks to refine image contents. Zhang et al. <ref type="bibr" target="#b7">[8]</ref> propose to incorporate contrastive learning to improve the fidelity of the generated images to the textual input. Recently, inspired by the autoregressive generative models for pixel-by-pixel image generation <ref type="bibr" target="#b15">[16]</ref>, the transformer-based methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17]</ref> have shown promising results for text-to-image synthesis. Based on the discrete image tokens learned by various discrete VAE <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b20">21]</ref>, they try to model the image tokens and text tokens in a single transformer framework, following a unidirectional manner for both the text input and the image target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>In this section, we will introduce our unified generative pre-training framework for bidirectional Vision-Language generation and the end-to-end method for text-to-image synthesis training. Also, we present the details of the collection of the large-scale image-text dataset and the distributed training strategies for pre-training the 10-billion parameter model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A Unified Generative Framework for Bidirectional Image-text Generation</head><p>As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, ERNIE-ViLG adopts a unified framework for the bidirectional image-text generation. The image is represented as a sequence of discrete representation by vector quantization variational autoencoder (VQVAE) <ref type="bibr" target="#b17">[18]</ref>. The image discrete sequence is used as the input(output) of a parameter-sharing transformer for autoregressive image-to-text (text-to-image) generation. Specifically, for the image-to-text generation, the transformer takes the image discrete sequence as input to generate the corresponding textual sequence. Conversely, for the text-to-image synthesis, the text is inputted to the transformer for generating the corresponding visual discrete sequence, and then the image discrete sequence is used for reconstructing the image. Besides the traditional two-stage pipeline approach, we further propose an end-to-end training method of jointly training the modules for discrete representation sequence generation and image reconstruction to strengthen the ability of text-to-image synthesis. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Image Discrete Representation</head><p>Discrete representation has achieved significant improvements for image generation tasks recently, because it can be better adapted to autoregressive modeling. VQVAE <ref type="bibr" target="#b17">[18]</ref> proposes to represent images as latent discrete sequences using vector quantization. It has stronger semantic representation ability than pixel clustering. VQVAE encodes the original image x ? R H?W ?3 by encoder E and quantizes it to z with codebook C. z is the discrete sequence with h ? w indices, and used to reconstruct x via decoder G and C. The loss function during training is shown in Equation 1-2.</p><formula xml:id="formula_0">L V QV AE = x ?x 2 + sg[z emb ] ? E(x) 2 2 + sg[E(x)] ? z emb 2 2 (1) x = G(z emb ) = G(C(z)) = G(q(E(x)))<label>(2)</label></formula><p>The overall loss function contains two parts. The first term of L V QV AE is reconstruction loss. The last two terms aim to make E(x) (the encoder outputs) and z emb (the embeddings after the lookup of discrete sequence from codebook) close to each other. sg[?] denotes the stop-gradient operation, and q[?] denotes the vector quantization. x andx are the original and reconstructed images. The discrete sequence z servers as the input (or output) of the bidirectional generative model, and its length n is h ? w.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Bidirectional Generative Model</head><p>For the generative model, we use a multi-layer transformer encoder for both image-to-text and text-to-image generation tasks. Different from the decode-only <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17]</ref> or separate encoder-decoder <ref type="bibr" target="#b32">[33]</ref> architectures, the encoder and decoder of ERNIE-ViLG share parameters and use specific self-attention masks to control the contexts like UniLM <ref type="bibr" target="#b37">[38]</ref> and ERNIE-GEN <ref type="bibr" target="#b38">[39]</ref>, where the source tokens are allowed to attend all the source tokens and the target tokens are allowed to attend the source tokens and the target tokens lie left to them. The bidirectional generation pre-training tasks are modeled exact in the same model where we consider sharing of model space helps establish better semantic alignments across vision and language modalities.</p><p>The visual tokens {z 1 , . . . , z n } discretized from the image using VQVAE encoder and the textual tokens {t 1 , . . . , t m } tokenized from the text using WordPiece tokenizer are concatenated and fed into the transformer model. Therefore, during training, the model takes the input stream of [t 1 , . . . , t m , z 1 , . . . , z n ] for text-to-image generation task to predict the image tokens autoregressively and [z 1 , . . . , z n , t 1 , . . . , t m ] for image-to-text generation task. We use the following multi-task loss for learning the image-text bidirectional generation tasks.</p><formula xml:id="formula_1">L = L txt2img + L img2txt (3) L txt2img = n k=1 ? log P (z k |t 1 , . . . , t m , z 1 , . . . , z k?1 )<label>(4)</label></formula><formula xml:id="formula_2">L img2txt = m k=1 ? log P (t k |z 1 , . . . z n , t 1 , . . . , t k?1 )<label>(5)</label></formula><p>Note that the length n of visual sequence z is often large (usually larger than 1024) to reduce the information loss of the image, which causes a relatively high computation cost and memory consumption for the transformer model during training and inference. As shown in <ref type="figure" target="#fig_2">Figure 2</ref>, we utilize a similar sparse attention mechanism as in <ref type="bibr" target="#b8">[9]</ref>. Concretely, we adopt row attention (i mod 4 != 2 ), column attention (i mod 4 = 2) and convolutional attention (the last layer) for the i-th transformer layer. We implement the row attention and column attention as block-wise attention while training. Several primary experiments verify that this sparse attention implementation can roughly increase the speed by 25% and save 50% of GPU memory during training while keeping the convergence of loss consistent with dense attention. And the improvement for predicting is much more significant. While the sparse attentions in <ref type="bibr" target="#b8">[9]</ref> are designed for unidirectional modeling, we adapt them to the bidirectional modeling manner of the visual input tokens for the image-to-text generation as shown in <ref type="figure" target="#fig_2">Figure 2(</ref>   For text-to-image generation, text tokens are visible to each other and the image attention pattern is as same as DALL-E. But for image-to-text generation, the attention pattern of image tokens in the same row, column or convolutional kernel is bidirectional rather than unidirectional. This is a hypothetical version of our transformer with a maximum text length of 6 tokens, image length of 16 tokens (4?4), and convolutional kernel size of 3?3. The actual kernel size of our model is 11?11 following DALL-E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Text-to-Image Synthesis</head><p>Text-to-image synthesis based on image discrete sequence is usually tackled in a two-stage pipeline: discrete representation sequence generation and image reconstruction. As the red path in <ref type="figure" target="#fig_0">Figure 1</ref>, the generated discrete sequence is looked up in the codebook to obtain a 3D tensor z emb ? R h?w?d . Then z emb is sent to the reconstructed decoder to be restored into an image. The generator and the reconstructor are trained independently.</p><p>Besides the traditional two-stage pipeline mode, in the framework of ERNIE-ViLG, the text-to-image synthesis can also use our newly proposed end-to-end training method as the green path in <ref type="figure" target="#fig_0">Figure 1</ref>. Hidden embeddings of image tokens outputted by the last transformer layer are mapped to z emb nonlinearly. Gradients could be propagated backward from the reconstructor to the generator since the non-derivable ID mapping operation is avoided. Therefore, the end-to-end training can be carried out.</p><p>This method is designed to:</p><p>? provide more contextual features for the reconstructor. Compared with context-independent embedding in codebooks, hidden embedding is encoded by a deep model and contains more image semantics. It also has perception of the textual information through the attention interaction.</p><p>? enhance the generator with the reconstruction task. The hidden embedding receives both abstract and original supervised signals from generation and reconstruction. It helps the generator learn better about image representation.</p><p>Our experiments show that the end-to-end training can improve both generator and reconstructor compared with the two-stage pipeline. Detailed experimental results are presented in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Image-to-Text Generation</head><p>For the image-to-text generation task, the image is first discretized into visual tokens using the encoder and the codebook of the VQVAE. The image tokens are then fed into the Transformer to generate the text tokens autoregressively. In our current implementation, the quantization modules are pre-trained and fixed during the image-to-text generation training task while they could also be updated jointly with the generative model, and we will explore it for the future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Large-scale Pre-training of ERNIE-ViLG</head><p>To explore the landscape of large-scale pre-training for bidirectional text-image generation, we train a 10-billion parameter ERNIE-ViLG model, a 48-layer transformer encoder for both image-to-text and text-to-image generation. The core issues of large-scale pre-training include collection of training data and distributed parallel training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Large-scale Image-Text Dataset Collection</head><p>To pre-train a generative model with general capabilities of bidirectional text-image generation, from basic entities to complex scenes, we build a large-scale image-text dataset consisting of over 145 million high-quality Chinese image-text pairs. The sources of our dataset are listed as follows:</p><p>? Chinese Webpages. We crawl 800 million raw Chinese alt-text descriptions paired with images from various Chinese webpages, conduct several steps of filtering and totally harvest 70 million text-image pairs. The filtering rules mainly include: (1) Text-length: the number of words in alt-text is less than 15.</p><p>(2) Text-content: the alt-text must contain at least one noun and contain no special characters. (3) Image-text similarity: the similarity score of between the alt-text and image (calculated by an in-house text-image matching model with the score range from 0.0 to 1.0) is greater than 0.5.</p><p>? Image Search Engine. We collect roughly 60 million query texts and corresponding user-clicked images from our internal image search engine. There is often a strong correlation between the query and user-clicked images.</p><p>? Public image-text Dataset. We collect a total of 15 million text-image pairs from two public datasets, CC <ref type="bibr" target="#b29">[30]</ref> and CC12M <ref type="bibr" target="#b39">[40]</ref>. The captions in these datasets are translated to Chinese through Baidu Translate API.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Distributed Training Strategies for Large-Scale Generative Models</head><p>The 10-billion parameter ERNIE-ViLG is implemented based on PaddlePaddle platform <ref type="bibr" target="#b40">[41]</ref>. Serious challenges need to be addressed to train such a large-scale model, such as limited device memory and computation efficiency. Also, it is rather demanding to fit a 10B generative model into a single GPU, and it becomes more challenging considering our bidirectional model's structure which doubles the memory consumption of activations and gradients.</p><p>As shown in <ref type="figure" target="#fig_3">Figure 3</ref>, we adopt Group Sharded data parallelism techniques <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref> to eliminate memory redundancies by partitioning the optimizer states, gradients, and parameters across multiple devices. In addition, activation recomputation <ref type="bibr" target="#b43">[44]</ref> and mixed-precision <ref type="bibr" target="#b44">[45]</ref> are applied to reduce the GPU memory footprint and increase throughput. Moreover, Optimizer-offload <ref type="bibr" target="#b45">[46]</ref> is introduced to swap the sharded optimizer states and master parameters to the CPU which largely reduces the GPU memory footprint. Furthermore, we combine Optimizer-offload with Gradient Accumulation to bring down the communication frequency between CPU and GPU, delivering a higher computation efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We pre-train a 10-billion parameter ERNIE-ViLG model and verify its generation ability. We conduct experiments on the conventional bidirectional image-text tasks: text-to-image synthesis and image captioning. Besides, in order to further verify the cross-modal understanding ability of our model, we transfer ERNIE-ViLG for the challenging generative VQA task to evaluate the alignments our model has captured across vision and language modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Settings</head><p>We use VQGAN <ref type="bibr" target="#b20">[21]</ref>, an enhanced variant of VQVAE, as our "image tokenizer". By adding adversarial training and perceptual loss, VQGAN enables clearer and more realistic image restoration from discrete representations. We adopt the model of VQGAN with f = 8 and vocab size = 8192, where f denotes the reduction factor in the side-length. We pre-process the original image to 256 * 256 through center crop, and thus the length of the visual discrete token sequence n is 1024 (h ? w, h = w = 32).</p><p>For the generator, we use a multi-layer transformer encoder for both image-to-text and text-to-image generation tasks, which consists of 48 transformer layers with 4096 hidden units and 64 attention heads and totally has 10 billion parameters. Considering the instability of the training of GAN-based models, we choose the two-stage text-to-image synthesis mode when training this model and use the decoder of VQGAN as the image reconstructor directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Text-to-Image Synthesis</head><p>To generate images given the text, we follow the same sampling strategy as in <ref type="bibr" target="#b8">[9]</ref> and the generated images are re-ranked with an in-house contrastive learning-based image-text matching model. We carry out the automatic evaluations of ERNIE-ViLG on a commonly used text-to-image synthesis dataset in both zero-shot and fine-tuning settings. Moreover, we conduct a human evaluation campaign to directly assess the quality of the images generated. <ref type="table">Table 1</ref>: Comparison of FID with previous text-to-image synthesis models on MS-COCO. Results of other models are directly taken from their papers. "Pre-trained" means the model is obtained by vision-language pre-training, "Supervised" illustrates the model is trained in a fully-supervised manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model FID ?</head><p>Supervised (fine-tuned) AttnGAN <ref type="bibr" target="#b46">[47]</ref> 35.5 DM-GAN <ref type="bibr" target="#b36">[37]</ref> 32.6 DF-GAN <ref type="bibr" target="#b47">[48]</ref> 21.4 XMC-GAN <ref type="bibr" target="#b7">[8]</ref> 9.3 X-LXMERT <ref type="bibr" target="#b18">[19]</ref> 37.4 Huang et al. <ref type="bibr" target="#b19">[20]</ref> 29.9 N?WA <ref type="bibr" target="#b32">[33]</ref> 12.9 ERNIE-ViLG 7.9 Zero-shot DALL-E <ref type="bibr" target="#b8">[9]</ref> 27.5 CogView <ref type="bibr" target="#b16">[17]</ref> 27.1 ERNIE-ViLG 14.7</p><p>Automatic Evaluation For automatic evaluation, we compare ERNIE-ViLG with other strong methods on MS-COCO <ref type="bibr" target="#b48">[49]</ref>. MS-COCO is a publicly available benchmark for text-to-image synthesis, which is challenging for containing many complex scenes that involve common objects. Following previous works, we randomly sample 30,000 images from the validation set and translate their corresponding captions to Chinese through Baidu Translate API. For the generated images, we rerank the samples and select best of 60 samples for the zero-shot experiments for a fair comparison with <ref type="bibr" target="#b16">[17]</ref> and best of 10 samples when fine-tuning. Fr?chet Inception Distance (FID) <ref type="bibr" target="#b49">[50]</ref> is adopted for image quality assessment 1 .</p><p>The results illustrated in <ref type="table">Table 1</ref> compare our model with previous works. In the zero-shot setting, ERNIE-ViLG surpasses DALL-E which is a 12-billion parameter model by a large margin, with a significant FID improvement of 12.8, even comparable to the fully-supervised models. It confirms that ERNIE-ViLG acquires a general semantic alignment between image and text, even for complex open-domain scenes. After fine-tuning on the domain-specific dataset, ERNIE-ViLG achieves the state-of-art result on MS-COCO, improving the FID metric by 5.0 and 1.4 over the best transformer-based method and the best GAN-base method respectively. Human Evaluation To obtain a direct assessment of the quality of images our model generated in zero-shot setting, we build a diverse dataset for human evaluation, consisting of 500 texts for various circumstances. The text sentences are collected from a variety of aspects to fully explore the general capability of ERNIE-ViLG, such as detailed attributes for objects, combining multiple objects in a reasonable manner, etc. See more details about this dataset in Appendix A. For the evaluations, three evaluators are asked to assess each image from three perspectives (image clarity, texture of the image, relevance between the text and the image) with a quality score range from 1 to 5. We rerank and select the best of 60 generated samples for each text and compare against CogView 2 . The average evaluation results are listed in <ref type="table" target="#tab_1">Table 2</ref>. ERNIE-ViLG achieves higher quality scores than CogView <ref type="bibr" target="#b16">[17]</ref> from all three perspectives which shows that our model acquires better zero-shot text-to-image generation capability.    Qualitative Results ERNIE-ViLG has acquired the generation capability for various scenes, from basic objects to complex combinations of objects. Some examples are shown in <ref type="figure" target="#fig_4">Figure 4</ref>. As can be seen in the examples, ERNIE-ViLG can not only draw entities mentioned in the given text description, but also combine them together with the background in a reasonable way. Surprisingly, we also find two special skills ERNIE-ViLG develops. Firstly, ERNIE-ViLG can generate images of different styles by simply adding text prompts without fine-tuning like CogView does ( <ref type="figure" target="#fig_5">Figure 5)</ref>. Secondly, our model can generate realistic images given Chinese ancient poetry which shows promising understanding of brief and abstractive descriptions. Real concepts in the poetry are well-organized and artistic conception is welldescribed ( <ref type="figure">Figure 6</ref>).  <ref type="figure">Figure 6</ref>: Generated images given Chinese ancient poetry.</p><formula xml:id="formula_3">? ? ? ? ? A t d u s k ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Image Captioning</head><p>To fully assess the capability of ERNIE-ViLG for image captioning, we carry out automatic evaluations for a fair comparison with previous methods and also evaluate the quality of generated captions through human judgement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Automatic Evaluation</head><p>We conduct experiments on two commonly used Chinese image captioning datasets, AIC-ICC <ref type="bibr" target="#b50">[51]</ref> and COCO-CN <ref type="bibr" target="#b51">[52]</ref>. For AIC-ICC, we fine-tune ERNIE-ViLG on the training split and evaluate on the validation split, following the same practice as <ref type="bibr" target="#b52">[53]</ref>. And for the experiments on COCO-CN, we use the standard train/val/test split and report results on the test split. We adopt the widely used metrics, BLEU@4, METEOR, ROUGE-L and CIDERr, for all the evaluations.</p><p>As illustrated in <ref type="table" target="#tab_5">Table 3</ref> and <ref type="table" target="#tab_6">Table 4</ref>, ERNIE-ViLG achieves the best results on both datasets. Specifically, compared to the pre-training-based method <ref type="bibr" target="#b52">[53]</ref>, we obtain a significant improvement of 2.1 BLEU@4 on AIC-ICC.  Human Evaluation We also carry out a human evaluation campaign to assess the quality of captions generated by ERNIE-ViLG. Given the image and generated predictions, the evaluator is asked to evaluate the caption quality from three perspectives: fluency (whether the caption is fluent or not), relevance (whether the caption is related to the given image or not) and richness (whether the caption adequately describe the whole content of the image or not) with a score from 0 to 2 (higher is better). We randomly select 200 images from COCO-CN test set and make the predictions from the zero-shot ERNIE-ViLG model and the fine-tuned model.</p><p>The evaluation results are shown in <ref type="table" target="#tab_7">Table 5</ref>. The fine-tuned ERNIE-ViLG model obtains an average score of 1.62. We also find that zero-shot ERNIE-ViLG model obtains a high fluency score, close to that of fine-tuned model, but the gap of the relevance score and the richness score between them are significant. We consider that it is due to that the web-crawled pre-training dataset tends to be noisy and most captions are less descriptive while the captions in human-labeled captioning datasets (e.g., COCO-CN) are often descriptive which capture all the details of the image. Some examples are illustrated in <ref type="figure" target="#fig_6">Figure 7</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Generative Visual Question Answering</head><p>In the literature, the Visual Question Answering task is often converted to a multi-label classification problem over a predefined candidate answer set. However, it is hard to define a closed set of candidate answers in real-world application, which makes this simplification remains a gap between research and practical application. In this paper, we study the open-ended generative Visual Question Answering task, where the model is required to directly generate the answer, given the image and question. An ideal generative model for this task needs to build semantic connections between the image and the question based on visual and linguistic understanding, and generate fluent textual answer. We conduct  experiments on a public FMIQA <ref type="bibr" target="#b53">[54]</ref> dataset, which has freestyle and diversified question-answer annotations. For the evaluation, we conduct both Turing Test <ref type="bibr" target="#b53">[54]</ref> (a human evaluator is asked to judge whether the answer is generated by a machine or not) and answer quality evaluation (score from 0 to 2, higher is better) by human's judgement.</p><p>We randomly select 200 samples from FMIQA validation split for the evaluations, as there is no official release of the test set. We fine-tune ERNIE-ViLG on the train split and use the fine-tuned model to make predictions for the samples. We also make the evaluation for the human-labeled answers to eliminate the bias of different evaluators compared to <ref type="bibr" target="#b53">[54]</ref>. The evaluation results can be seen in <ref type="table" target="#tab_9">Table 6</ref>. ERNIE-ViLG achieves a Turing Text passing rate of 78.5%, which significantly surpasses that of mQA <ref type="bibr" target="#b53">[54]</ref> and an answer score of 1.495 which clearly verify that our model has captured the semantic alignments across vision and language modalities. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>To evaluate the benefits our proposed end-to-end text-to-image synthesis method brings, we conduct experiments with a lite version of transformer as the image discrete representation generator, which has about 300 million parameters (24 transformer layers with 1024 hidden units and 16 attention heads). We utilize dVAE <ref type="bibr" target="#b8">[9]</ref>, rather than VQGAN, to discretize the image to visual sequence and build the image from the visual tokens, considering the instability of the training process of GAN. For the reconstructor, we use the same network architecture as dVAE. We develop a multi-task learning process which assigns equal weights to the generation loss and the reconstruction loss. The models are trained on a merged dataset of CC and CC12M and the evaluation is carried out on the MS-COCO validation set in zero-shot setting.</p><p>We compare the results of our end-to-end training method with the two-stage pipeline baseline as shown in <ref type="table" target="#tab_10">Table 7</ref>. For the two-stage pipeline, we train a text-to-image generator and use the decoder of dVAE directly as the reconstructor. The Two-stage G(R) refers to the separately trained generator(reconstructor), and the end-to-end G(R) refers to the end-to-end trained generator(reconstructor). Our end-to-end method achieves a significant FID improvement of 1.5 compared to the two-stage pipeline. We find that combining the end-to-end trained generator (end-to-end G) and dVAE decoder (two-stage R) also brings a FID improvement of 0.9 compared to that of two-stage pipeline, but falls behind compared to the end-to-end methods. This indicates our proposed end-to-end method can improve both the performance of the generator (two-stage G &amp; two-stage R vs end-to-end G &amp; two-stage R) and the reconstructor (end-to-end G &amp; two-stage R vs end-to-end G &amp; end-to-end R).</p><p>We also input visual sequence of real images discretized by dVAE(gold image sequence) to the two reconstructors for comparison. Experimental results (the last two lines in <ref type="table" target="#tab_10">Table 7)</ref> show that the end-to-end trained reconstructor has more obvious advantage in the reconstruction from real image discrete representation. We consider that end-to-end training will be more effective on ERNIE-ViLG with 10 billion parameters, for the image discrete representation generated by more capable generator is much closer to the real distribution, and hidden embeddings of larger model provides more useful features for the reconstructor. Due to the instability of the training of both GAN and large-scale generative model, we haven't used end-to-end training for our 10-billion parameter model based on VQGAN. We will address the instability issue for future work and improve the 10-billion parameter ERNIE-ViLG through end-to-end training. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The unified model architecture of ERNIE-ViLG for bidirectional image-text generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( b )</head><label>b</label><figDesc>Sparse attention for image-to-text generation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Sparse attention for bidirectional text-image generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Hybrid parallelism of data parallelism and sharding in PaddlePaddle. We organize the cluster in two dimensions: the data is sharded in column dimension while the model states (optimizer states, gradients, and parameters) are sharded among row dimensions. All communication needed by sharding is confined to the same machine.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Example images ERNIE-ViLG generated in zero-shot setting with texts from open domain. Figure 4(a)-Figure 4(b) show the generated images of simple objects. Figure 4(c)-Figure 4(e) exhibit generated images of complex scenes with multiple objects. The example of creating image of non-existing objects is displayed in Figure 4(f).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Images of different styles generated by ERNIE-ViLG. "None" indicates not adding any prompts about image style.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Generated captions on COCO-CN. Although the zero-shot generated captions are various, it is sufficient to describe the image from the semantic perspective.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>b).</figDesc><table><row><cell>CLS + txt_tokens + SEP</cell><cell>CLS + txt_tokens + SEP</cell><cell>CLS + txt_tokens + SEP</cell></row><row><cell>IMG + img_tokens</cell><cell>IMG + img_tokens</cell><cell>IMG + img_tokens</cell></row><row><cell>row mask</cell><cell>col mask</cell><cell>conv mask</cell></row><row><cell cols="3">(a) Sparse attention for text-to-image generation</cell></row><row><cell>IMG + img_tokens</cell><cell>IMG + img_tokens</cell><cell>IMG + img_tokens</cell></row><row><cell>CLS + txt_tokens + SEP</cell><cell>CLS + txt_tokens + SEP</cell><cell>CLS + txt_tokens + SEP</cell></row><row><cell>row mask</cell><cell>col mask</cell><cell>conv mask</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Average scores (1~5) of human evaluation.</figDesc><table><row><cell>Model</cell><cell cols="3">Image Clarity Texture Quality Relevance to the Text</cell></row><row><cell>CogView</cell><cell>3.867</cell><cell>2.623</cell><cell>2.203</cell></row><row><cell>ERNIE-ViLG</cell><cell>4.221</cell><cell>2.723</cell><cell>2.641</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Evaluation results for image captioning on COCO-CN test split. The results are all calculated at character-level where the results of 'seq-learn' are re-evaluated with their provided predictions.</figDesc><table><row><cell>Model</cell><cell cols="4">BLEU@4 METEOR ROUGE-L CIDERr</cell></row><row><cell>seq-learn [52]</cell><cell>48.4</cell><cell>29.5</cell><cell>59.2</cell><cell>128.4</cell></row><row><cell>ERNIE-ViLG</cell><cell>50.0</cell><cell>31.6</cell><cell>60.3</cell><cell>138.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Evaluation results for image captioning on AIC-ICC val split.</figDesc><table><row><cell>Model</cell><cell cols="4">BLEU@4 METEOR ROUGE-L CIDERr</cell></row><row><cell>BriVL [53]</cell><cell>66.1</cell><cell>41.1</cell><cell>71.9</cell><cell>220.7</cell></row><row><cell>ERNIE-ViLG</cell><cell>68.2</cell><cell>41.7</cell><cell>72.5</cell><cell>231.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Human evaluation of the generated captions on the sampling images from COCO-CN test split.</figDesc><table><row><cell>Model</cell><cell cols="4">fluency relevance richness average score</cell></row><row><cell>ERNIE-ViLG (zero-shot)</cell><cell>1.82</cell><cell>1.09</cell><cell>0.76</cell><cell>1.22</cell></row><row><cell>ERNIE-ViLG</cell><cell>1.96</cell><cell>1.47</cell><cell>1.43</cell><cell>1.62</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Human evaluation on FMIQA. * means the evaluations on manual captions for our samples.</figDesc><table><row><cell>Model</cell><cell>Turing Test passing rate(%)</cell><cell>Quality Evaluation avg.score</cell></row><row><cell>Human annotations from [54]</cell><cell>94.8</cell><cell>1.918</cell></row><row><cell>mQA [54]</cell><cell>64.7</cell><cell>1.454</cell></row><row><cell>Human annotations *</cell><cell>97.5</cell><cell>1.870</cell></row><row><cell>ERNIE-ViLG</cell><cell>78.5</cell><cell>1.495</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Comparison of the two-stage pipeline and end-to-end training.</figDesc><table><row><cell>Generator</cell><cell cols="2">Reconstructor FID ?</cell></row><row><cell>Two-stage G</cell><cell>Two-stage R</cell><cell>41.4</cell></row><row><cell>End-to-end G</cell><cell>Two-stage R</cell><cell>40.5</cell></row><row><cell>End-to-end G</cell><cell>End-to-end R</cell><cell>39.9</cell></row><row><cell cols="2">Gold image sequence Two-stage R</cell><cell>21.7</cell></row><row><cell cols="2">Gold image sequence End-to-end R</cell><cell>18.6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Same as previous works, we adopt evaluation code from https://github.com/MinfengZhu/DM-GAN 2 Given texts, images generated by CogView are manually crawled from its official website.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose ERNIE-ViLG to unify the bidirectional image-text generation tasks in one single generative model and present an end-to-end training method for the text-to-image synthesis. Pre-trained on a large-scale image-text dataset, ERNIE-ViLG captures the capabilities of bidirectional Vision-Language Generation and achieves superior performance on various cross-modal generation tasks including text-to-image synthesis, image captioning and generative Visual Question Answering. Overall, our model advances the unified pre-training for both the image-to-text and text-to-image generation tasks further.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A Dataset for human evaluation of text-to-image synthesis The dataset for human evaluation of text-to-image synthesis has 500 texts that are collected from 9 different aspects. Details of this dataset is shown in <ref type="table">Table 8</ref>. 102 texts are randomly selected from MS-COCO's validation set, while the others are manually designed.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multimodal machine learning: A survey and taxonomy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Baltru?aitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="423" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdel-Rahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal processing magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
	</analytic>
	<monogr>
		<title level="m">Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio</title>
		<meeting><address><addrLine>Alex Graves</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Oriol Vinyals</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generative adversarial text to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1060" to="1069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unified vision-language pre-training for image captioning and vqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13041" to="13049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cross-modal contrastive learning for text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yu Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="833" to="842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12092</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep cross-modal audio-visual generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lele</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudhanshu</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyao</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the on Thematic Workshops of ACM Multimedia</title>
		<meeting>the on Thematic Workshops of ACM Multimedia</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="349" to="357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Audio-to-image cross-modal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacek</forename><surname>Maciej?elaszczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma?dziuk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.13354</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Object-semantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="121" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5907" to="5915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05328</idno>
		<title level="m">Conditional image generation with pixelcnn decoders</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1691" to="1703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.13290</idno>
		<title level="m">Mastering text-to-image generation via transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00937</idno>
		<title level="m">Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">X-lxmert: Paint, caption and answer questions with multi-modal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaemin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.11278</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unifying multimodal transformer for bi-directional image and text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yupan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1138" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Taming transformers for high-resolution image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12873" to="12883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.02265</idno>
		<title level="m">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11740</idno>
		<title level="m">Uniter: Learning universal image-text representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Ernie-vil: Knowledge enhanced vision-language representations through scene graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiji</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichong</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16934</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Large-scale adversarial training for vision-and-language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06195</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Unimo: Towards unified-modal understanding and generation via cross-modal contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guocheng</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15409</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Vinvl: Revisiting visual representations in vision-language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Unifying vision-and-language tasks via text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaemin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.02779</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Simvlm: Simple visual language model pretraining with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.10904</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2556" to="2565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Microsoft coco captions: Data collection and evaluation server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Scaling up vision-language pre-training for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yumao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.12233</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.12417</idno>
		<title level="m">Yuejian Fang, Daxin Jiang, and Nan Duan. N\&quot; uwa: Visual synthesis pre-training for neural visual world creation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Attention on attention for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Yong</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4634" to="4643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simao</forename><surname>Herdade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armin</forename><surname>Kappeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kofi</forename><surname>Boakye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Soares</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05963</idno>
		<title level="m">Image captioning: Transforming objects into words</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dm-gan: Dynamic memory generative adversarial networks for text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minfeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingbo</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5802" to="5810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Unified Language Model Pre-training for Natural Language Understanding and Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">ERNIE-GEN: An Enhanced Multi-Flow Pre-training and Fine-tuning Framework for Natural Language Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Dongling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yukun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Haifeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3558" to="3568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Paddlepaddle: An open-source deep learning platform from industrial practice. Frontiers of Data and Domputing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ma</forename><surname>Yanjun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Dianhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Wu Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Haifeng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="105" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><forename type="middle">Y</forename><surname>Zero</surname></persName>
		</author>
		<title level="m">Memory optimizations toward training trillion parameter models. International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">End-to-end adaptive distributed training on paddlepaddle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ao</forename><surname>Yulong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Zhihua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Dianhai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.02752</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Training deep nets with sublinear memory cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.06174</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Mixed precision training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alben</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.03740</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aminabadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zero-Offload</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.06840</idno>
		<title level="m">Democratizing billion-scale model training</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Attngan: Fine-grained text to image generation with attentional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1316" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songsong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Yuan</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingkun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Df-Gan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.05865</idno>
		<title level="m">Deep fusion generative adversarial networks for text-to-image synthesis</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (5)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Ai challenger : A large-scale dataset for going deeper in image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonggang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.06475</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Coco-cn for cross-lingual image tagging, captioning, and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xirong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyu</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxiong</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieping</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2347" to="2360" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manli</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangzhen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhao</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoxing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyuan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baogui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongzheng</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueqian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anwen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinming</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruichen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yida</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">Yang</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuhao</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.06561</idno>
		<title level="m">Wenlan: Bridging vision and language by large-scale multi-modal pre-training</title>
		<editor>Zhicheng Dou, Qin Jin, Yanyan Lan, Wayne Xin Zhao, Ruihua Song, and Ji-Rong Wen</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Are you talking to a machine? dataset and methods for multilingual image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyuan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.05612</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
