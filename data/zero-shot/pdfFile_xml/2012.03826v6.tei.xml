<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HEBO: Pushing The Limits of Sample-Efficient Hyperparameter Optimisation Honorary position</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021">2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">I</forename><surname>Cowen-Rivers</surname></persName>
							<email>alexander.cowen.rivers@huawei.com?</email>
							<affiliation key="aff0">
								<orgName type="institution">Technische Universit?t Darmstadt</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Cambridge University College London *</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlong</forename><surname>Lyu</surname></persName>
							<email>lvwenlong2@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Cambridge University College London *</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rasul</forename><surname>Tutunov</surname></persName>
							<email>rasul.tutunov@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Cambridge University College London *</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Wang</surname></persName>
							<email>wangzhi55@huawei.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Grosnit</surname></persName>
							<email>antoine.grosnit@huawei.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">Rhys</forename><surname>Griffiths</surname></persName>
							<email>ryan.rhys.griffiths1@huawei.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><forename type="middle">Max</forename><surname>Maravel</surname></persName>
							<email>alexandre.maravel@huawei.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Jianye</surname></persName>
							<email>haojianye@huawei.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Peters</surname></persName>
							<email>peters@ias.tu-darmstadt.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Technische Universit?t Darmstadt</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitham</forename><surname>Bou-Ammar</surname></persName>
							<email>haitham.ammar@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Cambridge University College London *</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Corresponding</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Cambridge University College London *</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HEBO: Pushing The Limits of Sample-Efficient Hyperparameter Optimisation Honorary position</title>
					</analytic>
					<monogr>
						<title level="j" type="main">Journal of Artificial Intelligence Research</title>
						<imprint>
							<biblScope unit="volume">70</biblScope>
							<biblScope unit="page" from="1" to="15"/>
							<date type="published" when="2021">2021</date>
						</imprint>
					</monogr>
					<note type="submission">Submitted 10/2020; published 1/2021</note>
					<note>* Equal contribution ? Huawei Noah&apos;s Ark Lab Cowen-Rivers et al.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work we rigorously analyse assumptions inherent to black-box optimisation hyper-parameter tuning tasks. Our results on the Bayesmark benchmark indicate that heteroscedasticity and non-stationarity pose significant challenges for black-box optimisers. Based on these findings, we propose a Heteroscedastic and Evolutionary Bayesian Optimisation solver (HEBO). HEBO performs non-linear input and output warping, admits exact marginal log-likelihood optimisation and is robust to the values of learned parameters. We demonstrate HEBO's empirical efficacy on the NeurIPS 2020 Black-Box Optimisation challenge, where HEBO placed first. Upon further analysis, we observe that HEBO significantly outperforms existing black-box optimisers on 108 machine learning hyperparameter tuning tasks comprising the Bayesmark benchmark. Our findings indicate that the majority of hyper-parameter tuning tasks exhibit heteroscedasticity and non-stationarity, multi-objective acquisition ensembles with Pareto front solutions improve queried configurations, and robust acquisition maximisers afford empirical advantages relative to their non-robust counterparts. We hope these findings may serve as guiding principles for practitioners of Bayesian optimisation. All code is made available at https://github.com/huawei-noah/HEBO.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Although achieving significant success across numerous applications <ref type="bibr" target="#b8">(Bobadilla et al., 2013;</ref><ref type="bibr" target="#b39">Litjens et al., 2017;</ref><ref type="bibr" target="#b15">Fatima &amp; Pasha, 2017;</ref><ref type="bibr" target="#b25">Kandasamy et al., 2018;</ref><ref type="bibr">Cowen-Rivers et al., 2020)</ref>, the performance of machine learning models chiefly depends on the correct setting of hyper-parameters. As models grow larger and more complex, efficient and autonomous hyper-parameter tuning algorithms become crucial determinants of performance. A variety of methods from black-box and multi-fidelity optimisation <ref type="bibr" target="#b24">(Kandasamy et al., 2017;</ref><ref type="bibr" target="#b52">Sen et al., 2018)</ref> have been adopted for hyperparameter tuning with varying degrees of success. Techniques such as Bayesian optimisation (BO), for example, enable sample efficiency (in terms of black-box evaluations) at the expense of high computational demands, while "unguided" bandit-based approaches can fail to converge <ref type="bibr" target="#b14">(Falkner et al., 2018)</ref>. Identifying such failure modes, the authors in <ref type="bibr" target="#b14">(Falkner et al., 2018)</ref> built on <ref type="bibr" target="#b37">(Li et al., 2017)</ref> and proposed a combination of bandits and BO that achieves the best of both worlds; fast convergence and computational scalability. More recently in the context of the 2020 NeurIPS competition on Black-Box Optimisation, many BO variants have been convincingly demonstrated to be superior to random search for the task of hyper-parameter tuning <ref type="bibr" target="#b61">(Turner et al., 2021)</ref>. Though impressive, such successes of BO and alternative black-box optimisers, belie a set of restrictive modelling and acquisition function assumptions. We begin by describing these assumptions.</p><p>Modelling Assumptions: A core determinant of BO performance is the set of data modelling assumptions required to specify an appropriate probabilistic model of the black-box objective (e.g., the choice of validation loss in hyper-parameter tuning tasks). The model should not only provide accurate point estimates, but should also maintain calibrated uncertainty estimates to guide exploration of the objective. Amongst many possible surrogates <ref type="bibr" target="#b58">(Springenberg et al., 2016;</ref><ref type="bibr" target="#b23">Hutter et al., 2011)</ref>, Gaussian processes <ref type="bibr" target="#b62">(Williams &amp; Rasmussen, 1996)</ref> (GPs) are the default choice due to their flexibility and sample efficiency. Growing interest in applications of Bayesian optimisation has catalysed engineering feats that enhance scalability and training efficiency of GP surrogates by exploiting graphical processing units <ref type="bibr" target="#b32">(Knudde et al., 2017;</ref><ref type="bibr" target="#b4">Balandat et al., 2020)</ref>.</p><p>Similar to any other framework, the correct specification of a GP model is dictated by the data modelling assumptions imposed by the user. For instance, a homoscedastic GP suffers from misspecification when required to model data with heteroscedastic noise whilst stationary GPs fail to track non-stationary targets. The aforementioned shortcomings are not unnatural across a range of real-world problems <ref type="bibr" target="#b27">(Kersting et al., 2007;</ref><ref type="bibr" target="#b18">Griffiths et al., 2021a</ref><ref type="bibr" target="#b19">Griffiths et al., , 2021b</ref> and hyper-parameter tuning of machine learning algorithms is no exception, as illustrated in our hypothesis tests of Section 3.2. Hence, even if one succeeds in improving computational efficiency, frequently-made assumptions such as homoscedasticity and stationarity can easily inhibit the performance of any BO-based hyper-parameter tuning algorithm. Despite the importance of these assumptions in practice, GPs that presume homoscedasticity and stationarity still constitute the most common choice of surrogate.</p><p>Acquisition Function &amp; Optimiser Assumptions: Modelling choices such as those described above are not unique to the GP fitting procedure but rather transcend to other steps in the BO algorithm. Precisely, given a model that adheres to some (or all) assumptions mentioned above, the second step involves maximising an acquisition function to query novel input locations that are then evaluated. Hence, practitioners introduce additional constraints relating to the category of optimisation variables and the choice of acquisition function. When it comes to variable categories, mainstream implementations <ref type="bibr" target="#b32">(Knudde et al., 2017;</ref><ref type="bibr" target="#b4">Balandat et al., 2020)</ref> assume continuous domains and employ first and second-order optimisers such as LBFGS <ref type="bibr" target="#b40">(Liu &amp; Nocedal, 1989</ref>) and ADAM <ref type="bibr" target="#b28">(Kingma &amp; Ba, 2015)</ref> to propose query locations. Real-valued configurations cover but a subset of possible machine learning hyper-parameters rendering discrete variable categories out of scope, an example being the hidden layer size in deep networks. Moreover, from the point of view of acquisition functions, libraries tend to presuppose that one unique acquisition performs best in a given task, while research has shown that benefits that can arise from a combined solution <ref type="bibr" target="#b54">(Shahriari et al., 2014</ref><ref type="bibr" target="#b53">(Shahriari et al., , 2016</ref><ref type="bibr" target="#b42">Lyu et al., 2018)</ref> as we demonstrate in Section 5.</p><p>Contributions: Having identified important modelling choices in BO, our goal in this paper is to provide empirical insight into the impact of modelling choice on empirical performance. As a case study, we consider best practices for hyper-parameter tuning. We wish for our findings to be applicable across a broad range of tasks and datasets, be attentive to the effect of random initialisation on algorithmic performance, and naturally, be reproducible. As such, we prefer to build on established benchmark packages, especially those that facilitate fast and scalable evaluations with multi-seeding protocols. To that end, we undertake our evaluation in 2140 experiments from 108 real-world problems from the UCI repository <ref type="bibr">(Dua &amp; Graff, 2017)</ref>, which was also the testbed of choice for the NeurIPS 2020 Black-Box Optimisation challenge <ref type="bibr" target="#b61">(Turner et al., 2021)</ref>. Our findings point towards the following conclusions:</p><p>1. Hyper-parameter tuning tasks exhibit significant levels of heteroscedasticity and nonstationarity.</p><p>2. Input and output warping mitigate the effects of heteroscedasticity and non-stationarity giving rise to better performing tuning algorithms with higher mean and median performance across all 108 black-box functions under examination.</p><p>3. Individual acquisition functions tend to conflict in their solution (i.e., an optimum for one acquisition function can be a sub-optimal point for another and vice versa). Using a multi-objective formulation significantly improves performance;.</p><p>To verify our principal conclusions, we conduct additional ablation studies on our proposed solution method, Heteroscedastic and Evolutionary Bayesian Optimisation (HEBO) which attempts to address the shortcomings identified in our analysis and placed first in the 2020 NeurIPS Black-Box Optimisation Challenge. We obtain a ranked order of importance for significant components of HEBO, finding that output warping, multi-objective acquisitions and input warping lead to the most significant improvements followed by robust acquisition function formulations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Standard Design Choices in BO</head><p>As discussed earlier, the problem of hyper-parameter tuning can be framed as an instance of black-box optimisation:</p><p>arg max</p><formula xml:id="formula_0">x?X f (x),<label>(1)</label></formula><p>with x denoting a configuration choice, X a (potentially) mixed design space, and f (x) a validation accuracy we wish to maximise. In this paper, we focus on BO as a solution concept for black-box problems of the form depicted in Equation 1. BO considers a sequential decision approach to the global optimisation of a black-box function f : X ? R over a bounded input domain X . At each decision round, i, the algorithm selects a collection of q inputs x 1:q ). The goal is to rapidly approach the maximum x = arg max x?X f (x). Since both f (?) and x are unknown, solvers need to trade off exploitation and exploration during this search process.</p><p>To achieve this goal, BO algorithms operate in two steps. In the first, a Bayesian model is learned, while in the second an acquisition function determining new query locations is maximised. Next, we survey frequently-made assumptions in mainstream BO implementations and contemplate their implications for performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Modelling Assumptions</head><p>When black-boxes are real-valued, Gaussian processes <ref type="bibr" target="#b49">(Rasmussen &amp; Williams, 2006)</ref> are effective surrogates due to their flexibility and ability to maintain calibrated uncertainty estimates. In established implementations of BO, designers place GP priors on latent functions, f (?), which are fully specified through a mean function, m(x), and a covariance function or kernel k ? (x, x ) with ? ? R p representing kernel hyper-parameters. The model specification is completed by defining a likelihood. Here, practitioners typically assume that observations y l adhere to a Gaussian noise model such that y l = f (x l ) + l where l ? N (0, ? 2 noise ). This, in turn, generates a Gaussian likelihood of the form y l |x l ? N (f l , ? 2 noise ) where we use f l to denote f (x l ) with f (x) ? GP(m(x), k ? (x, x )). Additionally, a further design choice commonly made by practitioners is that the GP kernel is stationary, depending only on the norm between x and x , ||x ? x ||. From this exposition, we conclude two important modelling assumptions stated as data stationarity and homoscedasticity of the noise distribution. Where homoscedasticity implies a constant noise term ? 2 noise . Heteroscedasticity is usually harder to model as implies ? 2 noise is a function of the input: i.e., depending on the data, the noise changes around the mean. Of course, it is clear that there are significant differences between homoscedastic functions and heteroscedastic functions, and later we show indeed heteroscedastic functions require a different approach to optimise over than the typical homoscedastic (synthetic) functions usually researched in Bayesian Optimisation. If the true latent process does not adhere to these assumptions, the resultant model will be a poor approximation to the black-box. Realising the potential empirical implications of these modelling choices, we identify the first two questions addressed by this paper:</p><p>Q.I. Are hyper-parameter tuning tasks stationary? Q.II. Are hyper-parameter tuning tasks homoscedastic?</p><p>In Section 3.2, we show that even amongst the simplest hyper-parameter tuning tasks, the null hypothesis may be rejected in the case of statistical hypothesis tests for heteroscedasticity and non-stationarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Acquisition Function &amp; Optimisation Assumptions</head><p>Acquisition functions trade off exploration and exploitation by utilising statistics from the posterior p ? (f (?)|D) with D denoting the data (hyper-parameter configurations as inputs and validation accuracy as outputs) collected so far. Under a GP surrogate with Gaussiancorrupted observations y = f (x ) + where ? N (0, ? 2 ), and given a data set D = {x, y}, the joint distribution of D and an arbitrary set of input points x 1:q is given by</p><formula xml:id="formula_1">y f (x 1:q ) ? ? N m(x) m(x 1:q ) , K ? + ? 2 I k ? (x 1:q ) k T ? (x 1:q ) k ? (x 1:q , x 1:q )</formula><p>, where K ? = K ? (x, x) and k ? (x 1:q ) = k ? (x, x 1:q ). From this joint distribution one can derive though marginalisation <ref type="bibr" target="#b49">(Rasmussen &amp; Williams, 2006)</ref> the posterior predictive p(f (x 1:q )|D) = N (? ? (x 1:q ), ? ? (x 1:q )) with:</p><p>? ? (x 1:q ) = m(x 1:q ) + k ? (x 1:q ) (K ? + ? 2 I) ?1 (y ? m(x)) ? ? (x 1:q ) = K ? (x 1:q , x 1:q ) ? k ? (x 1:q ) (K ? + ? 2 I) ?1 k ? (x 1:q ).</p><p>As such we note that p(f (x 1:q )|D) = N (? ? (x 1:q ), ? ? (x 1:q )). In this paper, we focus on three widely-used myopic acquisition functions which in a reparameterised form can be written as <ref type="bibr" target="#b63">(Wilson et al., 2017)</ref>:</p><p>Expected Improvement (EI):</p><formula xml:id="formula_2">? ? EI (x 1:q |D) = E post. max j?1:q {ReLU(f (x j ) ? f (x + ))} ,</formula><p>where the subscript 'post.' is the predictive posterior of a GP <ref type="bibr" target="#b49">(Rasmussen &amp; Williams, 2006</ref>), x j is the j th vector of x 1:q , and x + is the best performing input in the data so far.</p><p>Probability of Improvement (PI):</p><formula xml:id="formula_3">? ? PI (x 1:q |D) = E post. max j?1:q {1 1{f(x j ) ? f (x + )}} ,</formula><p>where 1 1{?} is the left-continuous Heaviside step function.</p><p>Upper Confidence Bound (UCB):</p><formula xml:id="formula_4">? ? UCB (x j ) = E post. max j?1:q ? ? (x j ) + ?? /2|? ? (x j )| ,</formula><p>where ? ? (x j ) is the posterior mean of the predictive distribution and ? ? (</p><formula xml:id="formula_5">x j ) = f (x j )?? ? (x j ).</formula><p>When it comes to practicality, generic BO implementations make additional assumptions during the acquisition maximisation step. First, it is assumed that one of the aforementioned acquisitions works best for a specific task, and that the GP model is an accurate approximation to the black-box. However, when it comes to real-world applications, both of these assumptions are difficult to validate; the best-performing acquisition is challenging to identify upfront and GP models may easily be misspecified. With this in mind, we identify a third question that we wish to address:</p><p>Q.III. Can acquisition function solutions conflict in hyper-parameter tuning tasks?</p><p>In the following section, we affirm that acquisitions can conflict even on the simplest of hyper-parameter tuning tasks. Moreover, we show that a robust formulation to tackle misspecification of acquisition maximisation can improve overall performance (see Section 4.2.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Modelling Assumption Analysis</head><p>Before discussing the improvements afforded to BO via our solution method, we detail analyses conducted to answer questions (Q.I., Q.II., and Q.III.) posed in the previous section. Our analyses indicate:</p><p>A.I.: Even simple hyper-parameter tuning tasks exhibit significant heteroscedasticity.</p><p>A.II.: Even simple hyper-parameter tasks exhibit significant non-stationarity. A.III.: Acquisition functions conflict in their optima, occasionally leading to opposing solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Setting:</head><p>We create a wide range of hyper-parameter tasks (108) across a variety of classification and regression problems. We use nine models, (e.g. multilayer perceptrons, support vector machines) and six datasets (two regression and four classification) from the UCI repository, and two metrics per dataset (such as negative log-likelihood or mean squared error). Each model possesses tuneable hyper-parameters, e.g. the number of hidden units of a neural network. The goal is to fit these hyper-parameters so as to maximise/minimise one of the specified metrics. Values of the black-box objective are stochastic with noise contributions originating from the train-test splits used to compute the losses. Experimentation was facilitated by the Bayesmark 1 package. Full hyper-parameter search spaces are defined in <ref type="table" target="#tab_3">Table 2 and Table 3</ref>. 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistical Hypothesis Testing for Heteroscedasticity and Non-Stationarity:</head><p>We describe here the statistical hypothesis tests we use to answer Q.II.. GP regression typically considers a conditional normal distribution of the observations y|? ? N (f (?), ? 2 (?)) and in most cases ?(?) 2 is assumed to be constant, in which case the GP is termed homoscedastic.</p><p>To assess whether the homoscedasticity assumption holds for the tasks under examination, we make use of Levene's test and the Fligner-Killeen test. To give the reader intuition as to how we apply these tests, Levene's test asseses whether the variance is equal in two groups of data, assuming the data is normally distributed. I.e for a given task, given multiple evaluations of the black-box of two distinct hyperparameter sets, do the share the same variance (Homoscedasticity), or do their variances differ (Heteroscedasticity). Secondly, the Fligner-Killeen test is similarly a test for Homoscedasticity, however it is particularly useful when the data is non-normal. We refer the reader to the Appendix A for additional information regarding the tests. To run these tests on a given task, we evaluate k = 50 distinct sets of hyperparameters {x i } 1?i?k for n = 10 times and obtain scores {Y ij } 1?i?k,1?j?n , where Y ij is the j th score observed when evaluating the i th configuration. For i = 1, . . . , k, let ? 2 i denote the observed variance of y|x i , then both Levene's test and the Fligner-Killeen test share the same null hypothesis of homoscedasticity:</p><formula xml:id="formula_6">H 0 : ? 2 1 = ? ? ? = ? 2 k .</formula><p>In all 108 tests, we see a p-value significantly lower than 0.05 in 72 tasks using Levene's test, and in 73 tasks using Fligner-Killeen test. Such results (shown in detail in Appendix C) imply that at least 66% of the experimental tasks exhibit heteroscedastic behaviour. <ref type="table">Table 1</ref>: Hypothesis Testing for 108 tasks with respect to GP fit. In the table below we show, out of all 108 tasks, whether the GP fit (marginal log-likelihoods) was improved (Better) when either the Output transform or Input warping was added into the surrogate model, or was worse. Furthermore, we include significantly testing using the one sided t-test and detail how many tasks the GP fit was significantly better or worse with these additional modelling components. We find that output transformations which tackle heteroscedasticity significantly improve GP modelling capabilities in general (improve marginal log-likelihoods). Similarly, input transformations which tackle non-stationarity significantly improve GP modelling capabilities in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Better</head><p>Sig. To assess the impact of the extent of non-stationarity on BO performance, we conduct probabilistic regression experiments to gauge the predictive performance of a stationary GP on the hyper-parameter tuning tasks with and without input warping transformations which correct for non-stationarity. We first run a one-sided t-test for each of the 108 tasks where the null hypothesis is that the application of the input warping yields no difference in the log probability metric. In <ref type="table">Table 1</ref> significance tests show that in 106/108 tasks, the log probability metric is more favourable when input warping is applied. In 79/108 tasks, the gain is significant at the 95% level of confidence (p-value &lt; 0.025). It is clear that tackling Non-stationarity improves GP fit as shown in <ref type="table">Table 1</ref> and improves BO performance, as . The y-axis shows the acquisition value, and x-axis a given configuration of hyperparameters. Clearly, in these examples, not only different acquisitions lead to different optima, but it can be seen that such solutions might conflict (minimum value for one acquisition function is a maximum value for another acquisition function).</p><p>shown by the algorithm BO Base w Non-stationarity in <ref type="figure" target="#fig_3">Figure 4b</ref>. We thus conclude that non-stationarity is an important consideration for BO performance due to the observed effect on the log probability metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Answers A.II.: Simple Hyper-parameter Tuning Tasks are Heteroscedastic</head><p>We perform an analogous hypothesis test as in Section 3.1, assessing a vanilla GP's performance with and without output transformations <ref type="bibr">(Box-Cox/ Yeo-Johnson)</ref>. We run a two-sided paired t-test for each of the 108 tasks where the null hypothesis is that the application of the output transform yields no difference in the log probability metric. In <ref type="table">Table 1</ref> significance tests show that in 70/108 tasks, the log probability metric is more favourable when output transformations are applied. In 58/108 tasks, the gain is significant at the 95% level of confidence (p-value &lt; 0.025). It is clear that tackling Heteroscedasticity improves GP fit as shown in <ref type="table">Table 1</ref> and improves BO performance, as shown by the algorithm BO Base w Heteroscedasticity in <ref type="figure" target="#fig_3">Figure 4b</ref>. We thus conclude that heteroscedasticity is an important consideration for BO performance due to it isimpact on the log probability metric. Furthermore, to gauge the level of heteroscedasticity in the underlying data, we use the Fligner-Killeen <ref type="bibr" target="#b16">(Fligner &amp; Killeen, 1976)</ref> and <ref type="bibr" target="#b36">Levene (Levene, 1960)</ref> tests. For both tests, the null hypothesis is that the underlying black-box function noise process is homoscedastic. In all 108 tests, we see a p-value significantly lower than 0.05 in 72 tasks using Levene's test, and in 73 tasks using Fligner-Killeen. Such results (shown in detail in Appendix C) imply that at least 66% of the experimental tasks exhibit heteroscedastic behaviour.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Answer A.III.: No Clear Winner</head><p>It has previously been observed that acquisition functions can conflict in their optima <ref type="bibr" target="#b54">(Shahriari et al., 2014)</ref>. To provide further support for the answer to Q.III., we collect 128 samples from each task by evaluating various hyper-parameter configurations across metrics. We then assemble a data set D = {hyper-param i , y i } 32 i=1 , where hyper-param i is a vector with dimensionality dependent on the number of hyper-parameters in a given model, and y i is an evaluation metric, (e.g., mean squared error) We subsequently fit a GP surrogate model and consider each of the three acquisition functions from Section 2.2. Given the difficulty involved in the graphical depiction of an acquisition function conflict in more than two dimensions, we examine a simple, two-dimensional illustrative example. From <ref type="figure" target="#fig_1">Figure 1</ref>, it is apparent that even in the simplest 2D case, many examples of conflicting acquisitions exist. Thus, in higher dimensions this behaviour will also occur.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Optimising Bayesian Optimisation</head><p>In this section we describe the component design choices that may mitigate for heteroscedastic and non-stationary aspects of commonly-encountered BO problems. Input and output transformations as well as multi-objective acquisition functions have been introduced in isolation previously, whilst acquisition function robustness is unique to this work. The overall design choices produce the method which we name Heteroscedastic and Evolutionary Bayesian Optimisation (HEBO).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Tackling Heteroscedasticity and Non-Stationarity</head><p>To parsimoniously handle heteroscedasticity and non-stationarity, we leverage ideas from the warped GP literature <ref type="bibr" target="#b55">(Snelson et al., 2004)</ref> where output transformations facilitate the modelling of complex noise processes. We observe that the well-known Box-Cox <ref type="bibr" target="#b10">(Box &amp; Cox, 1964)</ref> and <ref type="bibr">Yeo-Jonhson (Yeo &amp; Johnson, 2000)</ref> output transformations in conjunction with the Kumaraswamy <ref type="bibr" target="#b34">(Kumaraswamy, 1980)</ref> input transformation, offer a balance between simplicity of implementation and empirical performance. In our ablation study (Section 5), we demonstrate that the addition of these two modelling components alone yields large performance gains. Note, we refit the parameters for the output transformation before we refit the GP after receiving a new samples.</p><p>Output Transformation for Heteroscedasticity: We consider the Box-Cox transformation most frequently used as a corrective mapping for non-Gaussian data. The transform depends on a tuneable parameter ? and applies the following map to each of the labels: T ? (y l ) = y ? l ? 1 /? for ? = 0 and T ? (y l ) = log y l if ? = 0, where in our case y l denotes the validation accuracy of the l th hyper-parameter configuration. ? must be fit based on the observed data such that the distribution of the transformed labels closely resembles a Gaussian distribution. This is achieved by minimising the negative Box-Cox likelihood function:</p><formula xml:id="formula_7">log n l=1 (T ? (y l ) ? T ? (y)) 2 n n 2 + n l=1 log [T ? (y l )] (1??) ,</formula><p>where n is the number of datapoints and T ? (y) is the sample mean of the transformed labels. Box-Cox transforms only consider strictly positive (or strictly negative) labels y l .</p><p>When labels take on arbitrary values, we use the Yeo-Johnson transform in place of the Box-Cox transform. The Yeo-Johnson transform is defined as follows:</p><formula xml:id="formula_8">Y.J. ? (y l ) = ? ? ? ? ? ? ? ? ? (y l +1) ? ?1 ? , if ? = 0, y l ? 0 log(y l + 1), if ? = 0, y l ? 0 (1?y l ) 2?? ?1 ??2 if ? = 2, y l &lt; 0 ? log(1 ? y l ) if ? = 2, y l &lt; 0.</formula><p>In an analogous fashion to the Box-Cox transform, the Yeo-Johnson's parameter is fit based on the observed data through solving the following 1-dimensional optimisation problem:</p><formula xml:id="formula_9">max ? ? n 2 log n j=1 (Y.J. ? (y l ) ? Y.J. ? (y)) 2 n ? 1 + (? ? 1) n i=1 [sign(y l ) log(|y l | + 1)] ,</formula><p>with Y.J. ? (y) the sample average computed after applying the Yeo-Johnson transformation.</p><p>Input Transformations for Non-Stationarity: As a general solution concept for correcting for non-stationarity, we consider input warping see <ref type="bibr" target="#b56">(Snoek et al., 2012)</ref>. Input warping performs a (usually non-linear and learnable) transformation to the input variables (x l ). It was proven in <ref type="bibr" target="#b56">(Snoek et al., 2012)</ref> that Input warping also helps tackle non-stationary functions. We rely on the Kumaraswamy input warping transform as used in <ref type="bibr" target="#b56">(Snoek et al., 2012)</ref>, which operates as follows for each input dimension:</p><formula xml:id="formula_10">[Kumaraswamy ? (x l )] k = 1 ? 1 ? [x l ] a k k b k ?k ? [1 : d],</formula><p>where d is the dimensionality of the decision variable (i.e. the number of free hyper-parameters), a k and b k are tuneable warping parameters for each of the dimensions, and ? is a vector concatenating all free parameters, i.e., ? = [a 1:d , b 1:d ] T . ? is fit based on the observed data. Similar to <ref type="bibr" target="#b4">(Balandat et al., 2020)</ref>, we optimise ? under the marginal likelihood objective used to fit the GP surrogate.</p><p>All Modelling Improvements Together: Combining the above corrective measures for heteroscedasticity and non-stationarity leads us to an improved GP surrogate with more flexible modelling capabilities. The implementation of such a model is relatively simple and involves maximising a new marginal likelihood which may be written as:</p><formula xml:id="formula_11">max ?,? ? 1 2 T ? (y) T (K ? ? + ? noise I) ?1 T ? (y) ? 1 2 |K ? ? + ? 2 noise I| ? const,</formula><p>where ? are GP hyper-parameters, ? indicates the use of non-stationary transformations, and ? denotes the solution to a Box-Cox likelihood objective. It is worth noting that we use Box-Cox as a running example but as mentioned previously we interchange Box-Cox with Yeo-Johnson transforms based on the properties of the label y l . We use K ? ? ? R n?n to represent a matrix such that each entry depends on both ? and ?, where k ? ? (x, x ) = k ? (Kumaraswamy ? (x), Kumaraswamy ? (x )).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Tackling Acquisition Conflict &amp; Robustness</head><p>Having proposed modifications to the surrogate model component of the Bayesian optimisation scheme, we now turn our attention to the acquisition maximisation step. In particular, we focus on two considerations, the first related to the assumption of a perfect GP surrogate, and the second centred on conflicting acquisitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">A Robust Acquisition Objective</head><p>As mentioned in Section 2.2, the acquisition maximisation step assumes that an adequate surrogate model is readily available. During early rounds of training especially, where data is scarce, such a property is often violated, leading to potentially severe model misspecification. One way to tackle such model misspecification is to adopt a robust formulation <ref type="bibr" target="#b30">(Kirschner et al., 2020;</ref><ref type="bibr" target="#b31">Klein et al., 2017)</ref> which attempts to identify the best-performing query location under the worst-case GP model, i.e., solving max x min ? ? ? (x|D). Though such a formulation admits a solution x that is robust to worst-case misspecification in ?, having a max min acquisition is problematic for several reasons. From a conceptual perspective max min formulations are known to lead to very conservative solutions if not correctly constrained or regularised since the optimiser possesses the power to impair the GP fit while updating ? 3 . From the perspective of implementation, one encounters two further issues. First, no global convergence guarantees are known for the non-convex, non-concave case <ref type="bibr" target="#b38">(Lin et al., 2020)</ref>, and second, ensuring gradients can propagate through the computation graph restricts surrogates and acquisition functions to be within the same programming framework.</p><p>To avoid worst-case solutions and engender independence between acquisition functions and surrogate models, given a set of parameters from a trained GP ?, we leverage ideas from domain randomisation <ref type="bibr" target="#b59">(Tobin et al., 2017)</ref> and consider an expected formulation instead over these parameters: max x ? ? rob. (x|D) ? max x E ?N (0,? 2 I) ? ?+ (x|D) . Importantly, this problem seeks to find new query locations that perform well on average over a distribution of surrogate models in favour of assuming a perfect surrogate. Despite on an intractable nature of ? ? rob. (?|D), in HEBO we show (the rigorous representation of this result is presented in Appendix B) that it can be approximated with any arbitrary precision and high confidence with ? ? (x|D) = ? ? (x|D) + N (0, ? 2 n ) by properly choosing parameters ? and ? n : Theorem: ( Informal ) Let us consider the stochastic version of the acquisition function utilised in HEBO and given by ? ? (x|D) = ? ? (x|D) + N (0, ? 2 n ) and Let ? ? rob. (x|D) ? E ?N (0,? 2 I) ? ?+ (x|D) be the robust form of the standard acquisition function given as expectation over random perturbation of parameter ?. Then, with proper choice of parameters ? n and ? , HEBO acquisition function ? ? (x|D) accurately approximates the robust acquisition function ? ? rob. (x|D) with high probability 4 and for any ?, x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Multi-Objective Acquisition functions</head><p>As a final component of our general framework, we propose the use of multi-objective acquisitions seeking a Pareto-front solution. This formulation facilitates the process of "hedging" between different acquisitions such that no single acquisition dominates the solution <ref type="bibr" target="#b42">(Lyu et al., 2018)</ref>. Formally, we solve</p><formula xml:id="formula_12">max x ? ? EI (x|D), ? ? PI (x|D), ? ? UCB (x|D) ,<label>(2)</label></formula><p>where ? ? type (x|D) is a robust acquisition of type ? {EI, PI, UCB} as introduced in the previous section. We also note that our formulation is designed to admit the use of a robust objective value of ? ? (x|D) = ? ? (x|D) + ? n with ? n being a sample from N (0, ? n ) at each iteration of the evolutionary solver.</p><p>Although solving the problem in Equation 2 is a formidable challenge, we note the existence of many mature multi-objective optimisation algorithms. These range from firstorder <ref type="bibr" target="#b29">(Kingma &amp; Welling, 2014)</ref> to zero-order <ref type="bibr" target="#b41">(Loshchilov &amp; Hutter, 2016;</ref><ref type="bibr" target="#b17">Gabillon et al., 2020)</ref> and evolutionary methods <ref type="bibr" target="#b20">(Hansen, 2016;</ref><ref type="bibr">Deb et al., 2002)</ref>. Due to the discrete nature of hyper-parameters in machine learning tasks, we advocate the use of evolutionary solvers that naturally handle categorical and integer-valued variables. In our experiments, we employ the non-dominated sorting genetic algorithm II (NSGA-II) which allows for mixed variable crossover and mutation to optimise real-valued and integer-valued inputs <ref type="bibr">(Deb et al., 2002)</ref>. We use the implementation of NSGA-II found in the Pymoo <ref type="bibr" target="#b7">(Blank &amp; Deb, 2020)</ref> library. Alternatively, one may use the GP Hedge acquisition as used in Dragonfly <ref type="bibr" target="#b26">(Kandasamy et al., 2020)</ref> in <ref type="bibr" target="#b22">(Hoffman et al., 2011b)</ref> or in SkOpt to select between acquisitions. We however, observed this formulation to perform poorly when compared against individual acquisitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments and Results</head><p>In this section, we continue our empirical evaluation and validate gains (if any) that arise from the improvements proposed in Section 4. The experimental setup remains as described in Section 3. To assess performance, we use the normalised task score 5 . We run experiments on either 16 iterations with a batch of 8 query points per iteration or 100 iterations with 1 query point. Each experiment is repeated for 20 random seeds. We baseline against a wide range of solvers that either rely on BO-strategies or follow zero-order techniques such as differential evolution or particle swarms. These include SkOpt <ref type="bibr">(Pedregosa et al., 2011) 6</ref> pySOT 7 a parallel global optimisation package <ref type="bibr">(Eriksson et al., 2019a)</ref>, HyperOpt <ref type="bibr" target="#b5">(Bergstra et al., 2015)</ref> 8 , OpenTuner 9 a package for ensembling methods <ref type="bibr" target="#b2">(Ansel et al., 2014)</ref>, NeverGrad <ref type="bibr" target="#b48">(Rapin &amp; Teytaud, 2018</ref>) 10 a gradient-free optimisation toolbox (where we use the One Plus One optimiser with the associated label NeverGrad (1+1)), Wine MLP-SGD Normalised Score <ref type="figure">Figure 2</ref>: HEBO compared against all baselines for 16 iterations and a batch size of 8 query points per iteration. Each experiment is repeated with 20 random seeds. We average each seed over both metrics in all tasks and display a subset of 36 summary plots for the 108 black-box functions. HEBO achieves the highest normalised mean score in 68.5% of the 108 black-box functions. Full results for the 108 tasks are presented in Appendix D in tabular format.</p><p>BOHB <ref type="bibr">(Falkner et al., 2018) 11</ref> and Dragonfly <ref type="bibr" target="#b26">(Kandasamy et al., 2020)</ref> 12 . Additionally, we carried our modelling improvements to TuRBO 13 <ref type="bibr">(Eriksson et al., 2019b)</ref>, augmenting the standard GP with mitigation strategies from Section 4 producing a new baseline that we entitle TuRBO+. Finally, we introduce Heteroscedastic Evolutionary Bayesian Optimisation (HEBO), in which we construct an optimiser with the improvements introduced in Section 4.</p><p>Implementation Details for BOHB: BOHB is a scalable hyper-parameter tuning algorithm introduced in <ref type="bibr" target="#b14">(Falkner et al., 2018)</ref> mixing bandits and BO approaches to achieve both competitive anytime and final performance. Contrary to the other solvers considered in this paper, BOHB is specifically designed to tackle multi-fidelity optimisation and uses the Hyperband <ref type="bibr" target="#b37">(Li et al., 2017)</ref> routine to define the fidelity levels under which points are asynchronously evaluated. The selection of points follows a BO strategy based on the Tree Parzen Estimator (TPE) method. Given a data set D of observed data points and a threshold ? ? R, the TPE models p(x|y), using kernel density estimates of (x) = p(y &lt; ?|x, D)</p><formula xml:id="formula_13">g(x) = p(y ? ?|x, D).</formula><p>In the TPE algorithm, maximising the expected improvement criterion</p><formula xml:id="formula_14">? EI (x) = max(0, ? ? p(y|x))p(y|x)dy is equivalent to maximising the ratio r(x) = (x) g(x)</formula><p>which is carried out to select a single new candidate point at a time.</p><p>In the absence of a multi-fidelity setup in our experiments, we run a modified version of the BOHB algorithm implemented in the HpBandSter package. We leave the TPE method for modelling unchanged but ignore the fidelity level assignment from Hyperband. Moreover, as our experimental setup involves batch acquisitions, we tested two alternatives to the standard BOHB acquisition procedure to support synchronous suggestion of multiple points. In the first approach, we run q independent maximisation processes of r(x) from random starting points and recover a single candidate from each process to form the q-batch suggestion. In the second approach, we obtain one point as a result of a single maximisation of r(x) and we sample q ? 1 random points to complete the q-batch suggestion. As the latter method yields better overall performance, the results reported under the BOHB-BB label are obtained using the second approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Black-Box Functions</head><p>As discussed in Section 3, we evaluate black-box optimisation solvers on a large set of tasks from the Bayesmark package. Each task involves optimising the hyper-parameters of a machine learning algorithm to minimise the cross validation loss incurred when this model is applied in a regression (reg) or a classification (clf) setting for a given data set. Thus, a task is characterised by a model, a data set and a loss function (metric) quantifying the quality of the regression or classification performance. In total, 108 distinct tasks can be defined from the valid combinations of the nine models specified in <ref type="table">Table 2</ref>, the following six real-world UCI datasets (Dua &amp; Graff, 2017), Boston (reg), Breast Cancer (clf), Diabetes (reg), Digits (clf), Iris (clf) and Wine (clf); the following two regression metrics, negative mean-squared error (MSE), negative mean absolute error (MAE), and two classification metrics, negative log-likelihood (NLL) and negative accuracy (ACC). The results reported in <ref type="figure" target="#fig_2">Figures 3 and 4</ref> have been obtained by applying each black-box optimisation method using 16 iterations of 8-batch acquisition steps on all of the 108 tasks. In order to provide a reliable evaluation of the different solvers, we repeated each run with 20 random seeds and considered the normalised score given by:</p><formula xml:id="formula_15">Normalised Score = 100 ? L ? L * L rand ? L * (3)</formula><p>where L is the best-achieved cross validation loss at the end of the 16 acquisition steps, L * is the estimated optimal loss for the task and L rand is the mean loss (across multiple runs) obtained using random search with the same number of acquisition steps. The normalisation procedure permits aggregation of the scores across tasks despite the different cross-validation loss functions used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Black-Box Optimisation Input Variables</head><p>We provide in <ref type="table">Table 2</ref> and <ref type="table" target="#tab_3">Table 3</ref> the list of the hyper-parameters controlling the behaviour of each model along with their optimisation domains, which can differ depending on whether the model is used for a classification or a regression task. The search domain may include a mix of continuous and integer-valued variables (e.g. the MLP-SGD hyper-parameter set includes an integer-valued hidden layer size, and a continuous-valued initial learning rate that can take on values between 10 ?5 and 10 ?1 ). The dimensionality of the input space, i.e. the number of hyper-parameters to tune, ranges from 2 to 9. We specify in the final column of the tables whether the search domain is modified through a standard transformation ( log or logit) in order to facilitate optimisation. <ref type="table" target="#tab_5">Table 4</ref> synthesises the performance achieved on the 108 tasks by the black-box optimisation solvers considered in our experiments. We note that the distribution of the scores attained by HEBO has the largest mean and the smallest standard deviation, indicating that HEBO significantly outperforms competitor algorithms. <ref type="figure" target="#fig_2">Figure 3</ref> demonstrates gains from adopting the general HEBO framework. We note that due to optimising over numerous regression and classification metrics, we show that irrelevant of the validation score HEBO performs better than other optimisers. In <ref type="figure" target="#fig_3">Figure 4</ref>, we compare HEBO against baselines and report up to an 8% performance gain relative to a random search strategy. It is also worth noting that TuRBO+ tends to underperform 14 , achieving ca. 4% improvement relative to random search. We believe such a result is related to the interplay between our approach's capabilities to address heteroscedasticity and non-stationarity as well as the size of the trust regions; an interesting avenue that we plan to explore in future work, as well as experimenting with deeper neural networks as well as other architectures such as convolutional/ recurrent neural networks. Overall, HEBO achieves the highest normalised mean scores on 74 of the 108 datasets. Complete results on all tasks may be found in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2 and</head><p>Comparison to Asynchronous BO Algorithms: We perform a comparison to blackbox optimisers, such as Dragonfly and BOHB, which operate in the asynchronous setting. We run each method for 100 iterations of data collection with a single query location per iteration. We label the asynchronous algorithms without their multi-fidelity components with an addition BB for black-box optimiser (Dragonfly-BB and BOHB-BB) to assess black-box optimisation performance only. The results of <ref type="figure" target="#fig_3">Figure 4a</ref> show that in the asynchronous setting, both Dragonfly-BB and BOHB-BB under-perform relative to other black-box optimisers, with HEBO performing best. However, this result is not surprising as asynchronous methods trade off sample efficiency with speed. Nevertheless, this experiment reveals a large gap in suggestion power between SOTA asynchronous and synchronous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Results</head><p>To better understand the relative importance of each component of the HEBO algorithm, we conduct an ablation study by first removing each component of HEBO and testing the remaining components and second, by starting with basic BO and sequentially adding and testing each component of HEBO. The components comprise the consideration of heteroscedasticity, non-stationarity and robustness, as well as the use of a multiobjective acquisition function. We report average normalised scores in <ref type="figure" target="#fig_3">Figure 4b</ref>. The precedence order observed is: heteroscedasticity, multi-objective acquisition functions, non-stationarity and robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related Work</head><p>We introduce work on the following topics relating to modelling, acquisition and optimisers in Bayesian optimisation:</p><p>Heteroscedasticity with output transforms: Among various approaches to handling heteroscedasticity <ref type="bibr" target="#b27">(Kersting et al., 2007;</ref><ref type="bibr" target="#b35">L?zaro-Gredilla &amp; Titsias, 2011;</ref><ref type="bibr" target="#b33">Kuindersma et al., 2013;</ref><ref type="bibr" target="#b11">Calandra, 2017;</ref><ref type="bibr" target="#b18">Griffiths et al., 2021a)</ref>, transforming the output variables is a straightforward option giving rise to warped Gaussian processes <ref type="bibr" target="#b55">(Snelson et al., 2004)</ref>. More recently, output transformations have been extended to compositions of elementary functions <ref type="bibr" target="#b51">(Rios &amp; Tobar, 2019)</ref> and normalising flows <ref type="bibr" target="#b50">(Rezende &amp; Mohamed, 2015;</ref><ref type="bibr" target="#b43">Maronas et al., 2020)</ref>. Output transformations have not featured prominently in the Bayesian optimisation literature, perhaps due to the commonly-held opinion that warped GPs require more data relative to standard GPs in order to function as effective surrogates <ref type="bibr">(Nguyen &amp;</ref>   2020). Rather than introduce additional hyper-parameters to the GP, we enable efficient output warping through methods that only require pre-training. Recent work <ref type="bibr" target="#b13">(Eriksson &amp; Poloczek, 2021)</ref> has also investigated Gaussian copula transforms which may prove to be particularly effective in situations where there are outliers.</p><p>Non-stationarity with input warping: Many surrogate models with input warping exist for optimising non-stationary black-box objectives <ref type="bibr" target="#b57">(Snoek et al., 2014;</ref><ref type="bibr" target="#b12">Calandra et al., 2016;</ref><ref type="bibr" target="#b45">Oh et al., 2018</ref>) and have enjoyed particular success in hyper-parameter tuning where the natural scale of parameters is often logarithmic. Traditionally, a Beta cumulative distribution function is used. In this paper, we adopt the Kumaraswamy warping which is another instance of the generalised Beta class of distributions which we have observed to achieve superior performance <ref type="bibr" target="#b57">(Snoek et al., 2014)</ref> 15 ; confirming results reported in <ref type="bibr" target="#b4">(Balandat et al., 2020)</ref>.</p><p>Multi-objective acquisition ensembles: Multi-objective acquisition ensembles were first proposed in <ref type="bibr" target="#b42">(Lyu et al., 2018)</ref> and are closely related to portfolios of acquisition functions <ref type="bibr" target="#b21">(Hoffman et al., 2011a;</ref><ref type="bibr" target="#b54">Shahriari et al., 2014;</ref><ref type="bibr" target="#b4">Balandat et al., 2020)</ref>. In this form, the optimisation problem involves at least two conflicting and expensive black-box objectives and as such, solutions are located along the Pareto-efficient frontier. The multi-objective acquisition ensemble employs these ideas to find a Pareto-efficient solution amongst multiple acquisition functions. Although we utilised the multi-objective acquisition ensemble, we note that our framework is solver agnostic in so far as any multi-objective optimiser <ref type="bibr" target="#b0">(Abdolshah et al., 2019</ref>) may be applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robustness of Acquisitions:</head><p>Methods achieving robustness with respect to either surrogates <ref type="bibr" target="#b46">(Park et al., 2020)</ref> or the optimisation process <ref type="bibr" target="#b9">(Bogunovic et al., 2018;</ref><ref type="bibr" target="#b6">Bertsimas et al., 2010)</ref> have been previously proposed. Most relevant to our setting, is the approach of <ref type="bibr" target="#b9">(Bogunovic et al., 2018)</ref> that introduces robustness to BO by solving a max min objective to determine optimal input perturbations. Their method, however, relies on gradient ascentdescent-type algorithms that require real-valued variables and are not guaranteed to converge in the general non-convex, non-concave setting <ref type="bibr" target="#b38">(Lin et al., 2020)</ref>. On the other hand, our solution possesses two advantages: 1) simplicity of implementation as we merely require random perturbations of acquisition functions to guarantee robustness, and 2) support for mixed variable solutions through the use of evolutionary solvers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion &amp; Future Work</head><p>In this paper, we presented an in-depth empirical study of Bayesian optimisation for hyperparameter tuning tasks. We demonstrated that even the simplest among machine learning problems can exhibit heteroscedasticity and non-stationarity. We also reflected on the affects of misspecified models and conflicting acquisition functions. We augmented BO algorithms with various enhancements and revealed that with a revised set of assumptions BO can in fact act as a competitive baseline in hyper-parameter tuning. We highlight the large discrepancy between suggestion power of synchronous and asynchronous methods. We hope for future work to focus on integrating the best of asynchronous and synchronous methods for optimal performance. We hope this paper's findings can guide the community when employing black-box and Bayesian optimisation in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Addition Detail Of Hypothesis Tests</head><p>Levene's Test Levene's test statistic is defined as</p><formula xml:id="formula_16">W = N ? k k ? 1 ? k i=1 n(Z i? ?Z ?? ) 2 k i=1 n j=1 (Z ij ?Z i? ) 2 , where N = k ? n, Z ij = |Y ij ? 1 n n j=1 Y ij |,Z i? = 1 n n j=1 Z ij andZ ?? = 1 k k i=1Z i? , for all i = 1, . . . , k, j = 1, . . . , n. Levene's test rejects the null hypothesis of homoscedasticity H 0 if W &gt; F ?,k?1,N ?k ,</formula><p>where F ?,k?1,N ?k is the upper critical value at a significance level ? of the F distribution with k ? 1 and N ? k degrees of freedom. The Fligner-Killeen test is an alternative to Levene's test that is particularly robust to outliers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fligner-Killeen Test: Computation of the Fligner-Killeen test involves ranking all the absolute values</head><formula xml:id="formula_17">{|Y ij ?? i |} 1?i?k,1?j?n , where? i is the median of {Y ij } 1?j?n . Increasing scores a N,r = ? ?1 1+ r N +1 2</formula><p>are associated with each rank r = 1, . . . , N , where N = kn and ?(?) is the cumulative distribution function for a standard normal random variable. We denote the rank score associated with Y ij as r ij . The Fligner-Killeen test statistic is given by</p><formula xml:id="formula_18">? 2 o = k i=1 n ? i ?? 2 V 2 , where? i = 1 n n j=1 a N,r i j ,? = 1 N N r=1 a N,r and V 2 = 1 N ?1 N r=1 (a N,r ??) 2 .</formula><p>As ? 0 has an asymptotic X 2 distribution with (k ? 1) degrees of freedom, the test rejects the null hypothesis of homoscedasticity H 0 if</p><formula xml:id="formula_19">? 0 &gt; X 2 ?,k?1 where X 2 ?,k?1 is the upper critical value at a significance level ? of the X 2 distribution with k ? 1 degrees of freedom.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Details of Robust Acquisition</head><p>Though appealing, our formulation still assumes access to the GP hyper-parameters which complicates the implementation by restricting models and optimisers to the same programming paradigm. Ideally, we would wish to illicit robustness through only the GP predictive mean and predictive variance. Fortunately, we are able to show that upon a simple acquisition perturbation it becomes possible to approximate ? rob (?) above. As such, we demonstrate that robust acquisition formulations are achievable using only the GP predictive mean and variance.</p><p>Theorem: Let us consider the stochastic version of the acquisition function utilised in HEBO and given by ? ? (x|D) = ? ? (x|D) + ?? n with ? ? N (0, 1) and standard deviation parameter ? n &gt; 0 16 . Let ? ? rob. (x|D) ? E ?N (0,? 2 I) ? ?+ (x|D) be the robust form of the standard acquisition function given as expectation over random perturbation of parameter ?. Then, by properly choosing parameters ? n and ? with high probability 17 , HEBO acquisition function ? ? (x|D) accurately approximates the robust acquisition function ? ? rob. (x|D) for any ?, x. More formally, for any ? ? (0, 1) and ? ? (0, 1), there are parameters ? n = ? n (?, ?) and ? = ? (?, ?) such that:</p><formula xml:id="formula_20">? ? (x|D) ? ? ? rob. (x|D) ? ?, ??, x</formula><p>with probability at least 1 ? ?. <ref type="bibr">16</ref> We note that gradient-based algorithms remain applicable upon addition of the ??n term. In our formulation however, we use an evolutionary method which utilises acquisition function values. Consequently, the path followed by the optimiser will be altered based on ? samples leading to more robust query locations.</p><p>17 Here we use the common approach for proving stochastic expressions with high probability (see <ref type="bibr" target="#b60">(Tripuraneni et al., 2017)</ref>, <ref type="bibr" target="#b65">(Zhu &amp; Li, 2016)</ref>). Specifically, we show that for any confidence parameter ? ? (0, 1) the stochastic expression under consideration is valid with probability at least 1 ? ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.0.1 Proof of the Robustness Bound</head><p>Let ? ? (0, 1) be the desired probability threshold, and ? ? (0, 1) be a desired accuracy parameter. Consider the GP with mean function m(x) and covariance function k</p><formula xml:id="formula_21">? (x, x ) such that ?x, x ? X , ? ? R p : |k ? (x, x)| ? M 0 , |k ? (x, x )| ? M 1 ,<label>(4)</label></formula><formula xml:id="formula_22">||? ? k ? (x, x )|| 2 ? M 2 , |m(x)| ? M 4 .</formula><p>Moreover, assume that observations y ? D are bounded, i.e. |y| ? C and let ? ? (x|D) = ? ? (x|D) + ?? n with ? a standard normal random variable. Then, we are going to show that there are values c 1 = c 1 (?, ?) and c 2 = c 2 (?, ?), such that choosing ? n ? c 1 and ? ? c 2 :</p><formula xml:id="formula_23">? ? (x|D) ? E ?N (0,? 2 I) ? ?+ (x|D) ? ?.</formula><p>with probability at least 1 ? ?. Note, the robust form of the acquisition function given as ? ? rob. (x|D) ? E ?N (0,? 2 I) ? ?+ (x|D) constitutes an intractable integral. Therefore, in order to be optimised during the course of Bayesian optimisation, the intractable integral must be replaced by an accurate approximation. Without loss of generality we choose the UCB acquisition function ? ? (x|D) = ? ? UCB (x|D) and to avoid technical complications relating to multivariate calculus we consider a batch size q = 1. In this case, the UCB acquisition function can be written as ? ? </p><formula xml:id="formula_24">UCB (x|D) = ? ? (x|D) + ?? 2 ? ? (x|D),</formula><formula xml:id="formula_25">? ? (x|D) = 1 N N j=1 ? ?+ j (x|D)</formula><p>where j are i.i.d samples drawn from N (0, ? 2 I). Then, adding and subtracting? ? (x|D) gives:</p><formula xml:id="formula_26">? ? (x|D) ? E ?N (0,? 2 I) ? ?+ (x|D) ? ? ? (x|D) ?? ? (x|D) + ? ? (x|D) ? E ?N (0,? 2 I) ? ?+ (x|D) .</formula><p>Using the definition of? ? (x|D) in the above result yields:</p><formula xml:id="formula_27">? ? (x|D) ? E ?N (0,? 2 I) ? ?+ (x|D) ? (5) 1 N N j=1 ? ? (x|D) ? ? ?+ j (x|D) + 1 N N j=1 ? ?+ j (x|D) ? E ?N (0,? 2 I) ? ?+ (x|D)</formula><p>Let us now study separately each term in the above result. Applying the Chebyshev inequality for the second term in the above expression, we have that with probability at least</p><formula xml:id="formula_28">p 1 = 1 ? 8[E [? 2 ?+ (x|D)]+ ?? 2 E [? 2 ?+ (x|D)]] N ? 2 : 1 N N j=1 ? ?+ j (x|D) ? E ?N (0,? 2 I) ? ?+ (x|D) ? ? 2 .<label>(6)</label></formula><p>In order to ensure that p 1 = 1 ? ? 2 , the sample number j should be taken:</p><formula xml:id="formula_29">N = ? ? ? ? 16 E ? 2 ?+ (x|D) + ?? 2 E ? 2 ?+ (x|D) ?? 2 ? ? ? ? .</formula><p>We will later simplify this expression using the bounds in (4). For now, we restrict our focus on the second term in (5). To bound it, we will establish a bound on |? ? (x|D) ? ? ?+ j (x|D). For a small random perturbation j we have (with probability 1):</p><formula xml:id="formula_30">? ?+ j (x|D) = ? ? (x|D) + T j ? ? ? ? (x|D) + o(|| j ||) = ? ? (x|D) + T j ? ? ? ? (x|D) + ?? 2 ? ? (x|D) + o(|| j || 2 ). Let us define h ? (x|D) = ? ? ? ? (x|D) + ?? 2 ? ? (x|D) ,</formula><p>then, using the Cauchy-Schwarz inequality we have:</p><formula xml:id="formula_31">? ?+ j (x|D) ? ? ? (x|D) ? || j || 2 ||[h ? (x|D)|| 2 + o(1)]</formula><p>Since j ? N (0, 1), then with probability at least 1 ? ? 4N :</p><formula xml:id="formula_32">|| j || 2 ? 4? ? p + 2? log 4N ?</formula><p>Let us assume (and later we will prove the existence of such a bound) that ||h ? (x|D)|| 2 ? A 1 . Then, with probability at least 1 ? ? 4N :</p><formula xml:id="formula_33">? ?+ j (x|D) ? ? ? (x|D) ? 4? ? p + 2? log 4N ? [A 1 + o(1)]</formula><p>On the other hand, for ? ? (x|D) = ? ? (x|D) + ?? ? with probability at least 1 ? ? 4N we have:</p><formula xml:id="formula_34">? ? (x|D) ? ? ? (x|D) ? ? ?1 1 ? ? 8N ? n .</formula><p>where ?(?) is the cumulative distribution function for a standard Gaussian random variable.</p><p>Hence, by choosing ? = min</p><formula xml:id="formula_35">? ? ? 1, ? ?1 1? ? 8N ?n 4 ? p+2 log 4N ? [A 1 +o(1)] ? ? ? with probability at least 1 ? ? 2N</formula><p>we have that both ? ? (x|D) and ? ?+ j (x|D) belong to the interval centred at ? ? (x|D) of</p><formula xml:id="formula_36">size ? ?1 1 ? ? 8N ? n . Therefore, with probability at least 1 ? ? 2N : ? ? (x|D) ? ? ?+ j (x|D) ? 2? ?1 1 ? ? 8N ? n Hence, by choosing ? n = ? 4? ?1 1? ? 8N</formula><p>we arrive at:</p><formula xml:id="formula_37">? ? (x|D) ? ? ?+ j (x|D) ? ? 2</formula><p>and, therefore, for the first term in (5) with probability at least 1 ? ? 2 we have:</p><formula xml:id="formula_38">1 N N j=1 ? ? (x|D) ? ? ?+ j (x|D) ? ? 2</formula><p>Combining this result with <ref type="formula" target="#formula_28">(6)</ref> gives, that with probability at least 1 ? ?:</p><formula xml:id="formula_39">? ? (x|D) ? E ?N (0,? 2 I) ? ?+ (x|D) ? ?, ??, x,</formula><p>upon defining:</p><formula xml:id="formula_40">? n = ? 4? ?1 1 ? ? 8N , ? = min ? ? ? ? ? ? ? 1, ? 8 2 ? p + log 4N ? [A 1 + o(1)] ? ? ? ? ? ? ? ,<label>(7)</label></formula><p>with</p><formula xml:id="formula_41">N = 16 E ? 2 ?+ (x|D) + ?? 2 E ? 2 ?+ (x|D) ?? 2 .</formula><p>Our last step is to prove the existence of a constant A 1 such that ||h ? (x)|| 2 ? A 1 and also to simplify these expressions by deriving bounds on E [? ?+ (x|D)] and E ? 2 ?+ (x|D) . This will be provided as a separate Claim:</p><p>Claim: Let the bounds in (4) hold, then there are positive constants A 1 , A 2 and A 3 , such that</p><formula xml:id="formula_42">||h ? (x)|| 2 ? A 1 , E [? ?+ (x|D)] ? A 2 , E ? 2 ?+ (x|D) ? A 3 .<label>(8)</label></formula><p>Proof. We start with the bound on ||h ? (x)|| 2 . Let us denote for simplicity a ? =</p><formula xml:id="formula_43">[k ? (x, x i )] x i ?D , B ? = [k ? (x, x )] x?D,x ?D + I ?1 , y = [y(x)] x?D , m D = [m(x)] x?D , m = m(x), and k ? = k ? (x, x), then ? ? (x|D) = a T ? B ? [y ? m D ] + m, ? 2 ? (x|D) = a T ? B ? a ? + k ?</formula><p>Let us also denote the size of D as N , then we have:</p><formula xml:id="formula_44">? ? ? ? (x|D) = N i=1 N j=1 ? ? [[y j ? m j ][a ? ] i [B ? ] ij ] = N i=1 N j=1 [[y j ? m j ][B ? ] ij ? ? [a ? ] i ] + N i=1 N j=1 [[y j ? m j ][a ? ] i ? ? [[B ? ] ij ]] .</formula><p>Consider each term in this expression separately:</p><formula xml:id="formula_45">N i=1 N j=1 [y j ? m j ][B ? ] ij ? ? [a ? ] i 2 = N i=1 [B ? [y ? m D ]] i ? ? [a ? ] i 2 ? N i=1 ||B ? (i, :)|| 2 ||y ? m D || 2 ||? ? [a ? ] i || 2 .</formula><p>Using |y| ? C and |m(</p><formula xml:id="formula_46">x)| ? M 4 , we have ||y?m D || 2 ? (C +M 4 ) ? N and ||? ? [a(?)] i || 2 ? M 2 and as such: N i=1 N j=1 [y j ? m j ][B ? ] ij ? ? [a ? ] i 2 ? (C + M 4 ) ? N N i=1 ||B(?)(i, :)|| 2 ||? ? [a ? ] i || 2 ? (9) (C + M 4 ) ? N ||B ? || F N i=1 ||? ? [a ? ] i || 2 ? (C + M 4 ) ? N rank(B ? ||B ? || 2 N i=1 ||? ? [a ? ] i || 2 ? (C + M 4 )N ? 2 n N i=1 ||? ? [a(?)] i || 2 = (C + M 4 )N 2 M 2 ? 2 n</formula><p>Now, let us consider the second term in the expression for the posterior mean:</p><formula xml:id="formula_47">N i=1 N j=1 [y j ? m j ][a ? ] i ? ? [[B ? ] ij ] = N i=1 [a ? ] i ? ? j=1 [y j ? m j ]? ? [[B ? ] ij ] ? ? .</formula><p>Notice, that the gradient expression above is presented in the form of a vector:</p><formula xml:id="formula_48">? ? [[B ? ] ij ] = ? ? ? ? ?? 1 [K ? + ? n I] ?1 ij , . . . ? ??p [K ? + ? n I] ?1 ij , ? ? ? where we use the notation K ? = [k ? (x i , x j )] N,N i=1,j=1 . For the r th component we have ? ?? r [K ? + ? n I] ?1 ij = ? [K ? + ? n I] ?1 ? ?? r [K ? + ? n I] [K ? + ? n I] ?1 ij<label>(10)</label></formula><p>Now we can study the gradient of the second term in the posterior mean expression,</p><formula xml:id="formula_49">N i=1 [a ? ] i ? ? j=1 [y j ? m j ]? ? [[B ? ] ij ] ? ? 2 ? N i=1 |[a ? ] i | ? ? N j=1 ||y ? m D || 2 ||? ? [[B ? ] ij ]|| 2 ? ? ? (C + M 4 ) ? N M 1 N i=1 N j=1 p r=1 ? ?? r [K ? + ? n I] ?1 ij .</formula><p>Using result (10) in the above expression we now have</p><formula xml:id="formula_50">N i=1 [a ? ] i ? ? j=1 [y j ? m j ]? ? [[B ? ] ij ] ? ? 2 ? (C + M 4 ) ? N M 1 N i=1 N j=1 p r=1 ? ?? r [K ? + ? n I] ?1 ij ? (C + M 4 ) ? N M 1 p r=1 N i=1 N j=1 ? ?? r [K ? + ? n I] ?1 ij ? (C + M 4 )N ? N M 1 ? p r=1 [K ? + ? n I] ?1 ? ?? r [K ? + ? n I] [K ? + ? n I] ?1 F , where we used that N i=1 N j=1 |C ij | ? N ||C|| F for any arbitrary matrix C ? R N ?N . Because ? ??r [K ? + ? n I] = ? ??r K ? N i=1 [a ? ] i j=1 [y j ? m j ]? ? [[B ? ] ij ] 2 (C + M 4 )N ? N M 1 ? p r=1 [K ? + ? n I] ?1 ? ?? r K ? [K ? + ? n I] ?1 F ? ? N p r=1 [K ? + ? n I] ?1 ? ?? r K ? [K ? + ? n I] ?1 2</formula><p>and using the properties of the matrix 2-norm || ? || 2</p><formula xml:id="formula_51">[K ? + ? n I] ?1 2 ? 1 ? 2 n Hence, N i=1 [a ? ] i j=1 [y j ? m j ]? ? [[B ? ] ij ] 2 (C + M 4 )N 2 M 1 ? (11) p r=1 [K ? + ? n I] ?1 2 ? ?? r K ? 2 [K ? + ? n I] ?1 2 ? 1 ? 4 n p r=1 ? ?? r K ? 2 .</formula><p>Let us study the last term in the expression above. Using c 2 1 + . . . + c 2 R ? |c 1 | + . . . + |c R | for any set of real numbers c 1 , . . . , c R ? R we have</p><formula xml:id="formula_52">? ?? r K ? 2 = ? ? ? ? ??r k ? (x 1 , x 1 ), . . . ? ??r k ? (x 1 , x N ) . . . . . . . . . ? ??r k ? (x N , x 1 ), . . . ? ??r k ? (x N , x N ) l ? ? ? 2 ? ? ? ? ? ??r k ? (x 1 , x 1 ), . . . ? ??r k ? (x 1 , x N ) . . . . . . . . . ? ??r k ? (x N , x 1 ), . . . ? ??r k ? (x N , x N ) ? ? ? F = N i=1 N j=1 ? ?? r k ? (x i , x j ) 2 ? N i=1 N j=1 ? ?? r k ? (x i , x j ) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Substituting this expression in (11) gives us</head><formula xml:id="formula_53">N i=1 [a ? ] i j=1 [y j ? m j ]? ? [[B ? ] ij ] 2 (C + M 4 )N 2 M 1 ? 1 ? 4 n d r=1 N i=1 N j=1 ? ?? r k ? (x i , x j ) ? (12) ? p ? 4 n N i=1 N j=1 ||? ? k ? (x i , x j )|| 2 ? N 2 ? pM 2 ? 4 n .</formula><p>Hence, combining results <ref type="formula">(9)</ref> and <ref type="formula" target="#formula_0">(12)</ref> we have</p><formula xml:id="formula_54">||? ? ? ? (x|D)|| 2 ? (C + M 4 )N 2 M 2 ? 2 n 1 + N 2 M 1 ? p ? 2 n .<label>(13)</label></formula><p>Now, let us focus on the gradient of the posterior standard deviation,</p><formula xml:id="formula_55">? ? ? ? (x|D) = ? ? k ? (x, x) ? a T ? [K ? + ? 2 n I] ?1 a ? = 1 2? ? (x|D) ? ? k ? (x, x) ? a T ? [K ? + ? 2 n I] ?1 a ? = 1 2? ? (x) ? ? k ? (x, x) ? ? ? a T ? [K ? + ? 2 n I] ?1 a ? .</formula><p>Let us study the second gradient expression. Using our notation we have</p><formula xml:id="formula_56">a T ? [K ? + ? 2 n I] ?1 a ? = a T ? B ? a ? = N i=1 N j=1 [a ? ] i [a ? ] j [B ? ] ij .</formula><p>Hence, for the gradient,</p><formula xml:id="formula_57">? ? a T ? B ? a ? = N i=1 N j=1 ? ? [a ? ] i [a ? ] j [B ? ] ij = N i=1 N j=1 ? ? [[a ? ] i ] [a ? ] j [B ? ] ij + N i=1 N j=1 ? ? [[a ? ] j ] [a ? ] i [B ? ] ij + N i=1 N j=1 ? ? [B ? ] ij [a ? ] i [a ? ] j .</formula><p>and for the norm of the above expression we have</p><formula xml:id="formula_58">? ? a T ? B ? a ? 2 = N i=1 N j=1 ? ? [a ? ] i [a ? ] j [B ? ] ij = N i=1 N j=1 ? ? [B ? ] ij 2 |[a ? ] i [a ? ] j | + N i=1 N j=1 ||? ? || 2 [[a ? ] i ] [a ? ] j [B ? ] ij + N i=1 N j=1 ||? ? [[a ? ] j ]|| 2 [a ? ] i [B ? ] ij .</formula><p>Let us now bound each term in this expression:</p><p>1. The first term:</p><formula xml:id="formula_59">N i=1 N j=1 ? ? [B ? ] ij 2 |[a ? ] i [a ? ] j | ? N i=1 N j=1 ? ? [B ? ] ij 2 ||a ? || 2 ||a ? || 2 ? M 2 1 N i=1 N j=1 ? ? [B ? ] ij 2 Using the previous bound for ? ? [B ? ] ij 2 we have: N i=1 N j=1 ? ? [B ? ] ij 2 |[a ? ] i [a ? ] j | ? M 2 1 N i=1 N j=1 p r=1 ? ?? r K ? + ? 2 n I ?1 ij = N M 2 1 p r=1 [K ? + ? n I] ?1 ? ?? r K ? [K ? + ? n I] ?1 F ? N 3 2 M 2 1 p r=1 [K ? + ? n I] ?1 ? ?? r K ? [K ? + ? n I] ?1 2 Since [K ? + ? n I] ?1 2 ? 1 ? 2 n we have: N i=1 N j=1 ? ? [B ? ] ij 2 |[a ? ] i [a ? ] j | ? N ? N M 2 1 ? 4 n p r=1 N i=1 N j=1 ? ?? r k ? (x i , x j ) . Using p r=1 N i=1 N j=1 ? ??r k ? (x i , x j ) = ? p N i=1 N j=1 ||? ? k ? (x i , x j )|| 2 ? N 2 ? pM 2 , we have: N i=1 N j=1 ? ? [B ? ] ij 2 |[a ? ] i [a ? ] j | ? N 7 2 ? pM 2 1 M 2 ? 4 n 2.</formula><p>The second and the third terms are identical with respect to the bounding strategy,</p><formula xml:id="formula_60">N i=1 N j=1 ||? ? [[a ? ] i ] || 2 [a ? ] j [B ? ] ij = N i=1 |B ? (i, :)a ? | ||? ? [[a ? ] i ]|| 2 ? N i=1 ||B ? (i, :)|| 2 ||a ? || 2 ||? ? [[a ? ] i ]|| 2 ? ||B ? || F ||a ? || 2 N i=1 ||? ? [[a ? ] i ]|| 2 , since ||B ? || F ? rank(B ? )||B ? || 2 ? ? N ? 2 n . Hence, N i=1 N j=1 ||? ? [[a ? ] i ] || 2 [a ? ] j [B ? ] ij ? N ? N M 1 M 2 ? 2 n .</formula><p>Combining these results and using</p><formula xml:id="formula_61">||? ? k ? (x, x)|| ? M 2 , |? ? (x|D)| ? k ? (x, x) ? M 0 , we have ||? ? [? ? (x|D)]|| 2 ? N ? N M 1 M 2 2? 2 n M 0 N 2 ? pM 1 ? 2 n + 2<label>(14)</label></formula><p>Hence, combining <ref type="formula" target="#formula_0">(13)</ref> and <ref type="formula" target="#formula_0">(14)</ref> we have</p><formula xml:id="formula_62">||h ? (x|D)|| 2 ? ||? ? ? ? (x|D)|| 2 + ?? 2 ||? ? [? ? (x|D)]|| 2 ? (C + M 4 )N 2 M 2 ? 2 n 1 + N 2 M 1 ? p ? 2 n + ?? 2 N ? N M 1 M 2 2? 2 n M 0 N 2 ? pM 1 ? 2 n + 2 A 1 .</formula><p>Now, we are ready to bound the other two terms in the claim:</p><formula xml:id="formula_63">? 2 ?+ (x|D) ? 2 a T ?+ B ?+ [y ? m D ] 2 + 2|m| 2 ? 2 (C + M 4 ) 2 M 2 1 ? 4 n + 2M 2 4 Therefore, for E ? 2 ?+ (x|D) we have E ? 2 ?+ (x|D) ? 2 (C + M 4 ) 2 M 2 1 ? 4 n + 2M 2 4 A 2 .</formula><p>Finally, for the posterior mean</p><formula xml:id="formula_64">? 2 ?+ (x|D) ? k ? (x, x) + a T ?+ B ?+ a ?+ ? M 1 + M 2 1 ? 2 n Therefore, for E ? 2 ?+ (x|D) we have E ? 2 ?+ (x|D) ? M 1 + M 2 1 ? 2 n A 3 .</formula><p>This finishes the proof of the claim.</p><p>Equipped with these results, we can further simplify the expressions <ref type="formula" target="#formula_40">(7)</ref>:</p><formula xml:id="formula_65">? n = ? 4? ?1 1 ? ? 8N , ? = min ? ? ? ? ? ? ? 1, ? 8 2 ? p + log 4N ? [A 1 + o(1)] ? ? ? ? ? ? ? , with N = ? ? ? ? 16 A 2 + ?? 2 A 3 ?? 2 ? ? ? ? .</formula><p>This finishes the proof of the lemma.</p><p>As such, we may now implement robust formulations of acquisition functions using only the GP predictive mean and variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. Statistical Hypothesis Tests for Heteroscedasticity</head><p>In this section we present the full results for the statistical hypothesis testing using Levene's test and the Fligner-Killeen test in <ref type="table" target="#tab_6">Table 5, Table 6, Table 7</ref>, <ref type="table" target="#tab_9">Table 8</ref>, <ref type="table" target="#tab_10">Table 9 and Table 10</ref> for the Boston, breast cancer, diabetes, digits, iris and wine datasets respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D. Task-Level Results Breakdown</head><p>In this section we present the full task-level breakdown of the results with each metric, data set and model combination for each black-box optimiser summarised with the mean and variance achieved across 20 seeds. We show a summary plot in <ref type="table" target="#tab_12">Table 11</ref>.</p><p>We now present sequentially the full results for each of the 6 datasets: Boston (Section D.1), Breast cancer (Section D.2), Diabetes (Section D.3), Digits (Section D.4), Iris (Section D.5), Wine (Section D.6). For each optimiser we give the mean and variance of the performance metric across all 18 tasks (2 metrics x 9 models) for a given data set. <ref type="table">Table 2</ref>: Search spaces for hyper-parameter tuning on classification tasks. We specify the variable type of each hyper-parameter (with R for real-valued and Z for integer-valued) as well as the search domain. We specify log ?U (resp. logit ? U) to indicate that a log (resp. logit) transformation is applied to the optimisation domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Parameter Type Domain                                                                                                         </p><formula xml:id="formula_66">kNN n_neighbors Z U(1, 25) p Z U(1, 4) Support Vector Machine C R log ?U(1, 10 3 ) gamma R log ?U(10 ?4 , 10 ?3 ) tol R log ?U(10 ?5 , 10 ?1 ) Decision Tree max_depth Z U(1, 15) min_samples_split R logit ? U(0.01, 0.99) min_samples_leaf R logit ? U(0.01, 0.49) min_weight_fraction_leaf R logit ? U(0.01, 0.49) max_features R logit ? U(0.01, 0.99) min_impurity_decrease R U(0, 0.5) Random Forest max_depth Z U(1, 15) max_features R logit ? U(0.01, 0.99) min_samples_split R logit ? U(0.01, 0.99) min_samples_leaf R logit ? U(0.01, 0.49) min_weight_fraction_leaf R logit ? U(0.01, 0.49) min_impurity_decrease R U(0, 0.5) MLP-Adam hidden_layer_sizes Z U(50, 200) alpha R log ?U(10 ?5 , 10 1 ) batch_size Z U(10, 250) learning_rate_init R log ?U(10 ?5 , 10 ?1 ) tol R log ?U(10 ?5 , 10 ?1 ) validation_fraction R logit ? U(0.1, 0.9) beta_1 R logit ? U(0.5, 0.99) beta_2 R logit ? U(0.9, 1 ? 10 ?6 ) epsilon R log ?U(10 ?9 , 10 ?6 ) MLP-SGD hidden_layer_sizes Z U(50, 200) alpha R log ?U(10 ?5 , 10 1 ) batch_size Z U(10, 250) learning_rate_init R log ?U(10 ?5 , 10 ?1 ) power_t R logit ? U(0.1, 0.9) tol R log ?U(10 ?5 , 10 ?1 ) momentum R logit ? U(0.001, 0.999) validation_fraction R logit ? U(0.1, 0.9) AdaBoost n_estimators Z U(10, 100) learning_rate R log ?U(10 ?4 , 10 1 ) Lasso C R log ?U(10 ?2 , 10 2 ) intercept_scaling R log ?U(10 ?2 , 10 2 ) Linear C R log ?U(10 ?2 , 10 2 ) intercept_scaling R log ?U(10 ?2 , 10 2 )</formula><formula xml:id="formula_67">) Lasso alpha R log ?U(10 ?2 , 10 2 ) fit_intercept Z U(0, 1) normalize Z U(0, 1) max_iter Z log ?U(10, 5000) tol R log ?U(10 ?5 , 10 ?1 ) positive Z U(0, 1) Linear alpha R log ?U(10 ?2 , 10 2 ) fit_intercept Z U(0, 1) normalize Z U(0, 1) max_iter Z log ?U(10, 5000) tol R log ?U(10 ?4 , 10 ?1 )<label>Algorithm</label></formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>and observes values of the black-box function y</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Examples depicting conflicting acquisitions across data sets (Wine, Boston Housing, and Iris) and models (AdaBoost, Multilayer perceptron, K-Nearest neighbours, and support vector machines)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Analysis of the results on 108 tuning tasks. (Left) Normalised score comparison demonstrating that HEBO (i.e., BO with improvements from Section 4) outperforms competitor algorithms. We observe a 5% relative improvement to SOTA optimisers such as TuRBO. (Right) HEBO yields an 8% improvement compared to random search.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>(a) We compare HEBO against several popular hyper-parameter tuning approaches including BOHB-BB and Dragonfly-BB, running all methods for 100 iterations with a batch size of 1 (i.e. one set of hyper-parameters queried per iteration). BOHB-BB and Dragonfly-BB feature asynchronous queries, suggesting a batch of one set of hyper-parameters at each iteration. We remove the multi-fidelity components from BOHB and Dragonfly to assess Black-Box optimisation alone, hence the additional BB appended to their label. (b) Ablation study where X denotes a general component of HEBO. HEBO w/o X takes one component X out at a time and BO Base w X adds one component X in at a time. We show TuRBO as a baseline and refer to HEBO with all significant components removed as BO Base. The ablation demonstrates that the corrections for each misspecified modelling assumption yield a tangible gain in empirical performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>where ? ? (x|D) and ? ? (x|D) are the posterior mean and posterior standard deviation respectively. Consider a Monte-Carlo estimation of ? ? rob. (x|D) ? E ?N (0,? 2 I) ? ?+ (x|D) :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Models and search spaces for hyper-parameter tuning on regression tasks. Models having the same search spaces for classification and regression tasks are omitted (cf.Table 2).</figDesc><table><row><cell>Model</cell><cell>Parameter</cell><cell cols="2">Type Domain</cell></row><row><cell cols="2">AdaBoost n_estimators</cell><cell>Z</cell><cell>U(10, 100)</cell></row><row><cell></cell><cell>learning_rate</cell><cell>R</cell><cell>log ?U(10 ?4 , 10 1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Mean Std Median 40 th Centile 30 th Centile 20 th Centile 5 th Centile</figDesc><table><row><cell>HEBO</cell><cell cols="3">100.12 8.70 100.01</cell><cell>100.00</cell><cell>99.88</cell><cell>98.64</cell><cell>85.71</cell></row><row><cell>PySOt</cell><cell>98.18</cell><cell>9.03</cell><cell>100.00</cell><cell>99.81</cell><cell>98.60</cell><cell>95.36</cell><cell>80.00</cell></row><row><cell>TuRBO</cell><cell cols="2">97.95 10.80</cell><cell>100.00</cell><cell>99.88</cell><cell>98.75</cell><cell>95.26</cell><cell>78.63</cell></row><row><cell>HyperOpt</cell><cell>96.37</cell><cell>8.79</cell><cell>99.31</cell><cell>98.16</cell><cell>95.94</cell><cell>92.38</cell><cell>78.52</cell></row><row><cell>SkOpt</cell><cell cols="2">96.18 11.51</cell><cell>99.78</cell><cell>98.66</cell><cell>96.73</cell><cell>91.62</cell><cell>74.77</cell></row><row><cell>TuRBO+</cell><cell cols="2">95.29 10.93</cell><cell>98.97</cell><cell>97.60</cell><cell>95.27</cell><cell>90.92</cell><cell>74.77</cell></row><row><cell>OpenTuner</cell><cell cols="2">94.32 14.18</cell><cell>98.44</cell><cell>96.93</cell><cell>93.84</cell><cell>89.97</cell><cell>68.96</cell></row><row><cell>Nevergrad (1+1)</cell><cell cols="2">93.20 17.52</cell><cell>99.65</cell><cell>97.84</cell><cell>94.57</cell><cell>88.28</cell><cell>55.34</cell></row><row><cell>BOHB</cell><cell cols="2">92.03 11.16</cell><cell>96.02</cell><cell>93.55</cell><cell>90.14</cell><cell>85.71</cell><cell>67.82</cell></row><row><cell>Random-Search</cell><cell cols="2">92.00 11.71</cell><cell>96.18</cell><cell>93.55</cell><cell>90.05</cell><cell>85.16</cell><cell>69.55</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Mean and n-th percentile normalised scores over 108 black-box functions, each repeated with 20 random seeds. We observe significant mean improvements from HEBO compared to all competitor algorithms.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Heteroscedasticity tests on tasks involving Boston data set.</figDesc><table><row><cell>Data set Model</cell><cell cols="2">Metric Fligner Statistic</cell><cell cols="2">p-value Levene Statistic</cell><cell>p-value</cell></row><row><cell>Boston DT</cell><cell>mae</cell><cell>73.51</cell><cell>0.01327</cell><cell cols="2">1.752 1.900e-03</cell></row><row><cell cols="2">MLP-adam mae</cell><cell cols="2">336.3 1.737e-44</cell><cell cols="2">14.4 3.611e-65</cell></row><row><cell cols="2">MLP-SGD mae</cell><cell cols="2">272.6 8.694e-33</cell><cell cols="2">6.561 1.480e-29</cell></row><row><cell>RF</cell><cell>mae</cell><cell>28.79</cell><cell>0.9906</cell><cell>0.6768</cell><cell>0.9537</cell></row><row><cell>SVM</cell><cell>mae</cell><cell>48.08</cell><cell>0.5106</cell><cell>0.9612</cell><cell>0.5508</cell></row><row><cell>ada</cell><cell>mae</cell><cell cols="2">218.7 2.692e-23</cell><cell cols="2">13.59 5.542e-62</cell></row><row><cell>kNN</cell><cell>mae</cell><cell>33.15</cell><cell>0.9597</cell><cell>0.619</cell><cell>0.98</cell></row><row><cell>lasso</cell><cell>mae</cell><cell>30.4</cell><cell>0.983</cell><cell>0.6091</cell><cell>0.983</cell></row><row><cell>linear</cell><cell>mae</cell><cell>16.17</cell><cell>1</cell><cell>0.251</cell><cell>1</cell></row><row><cell>DT</cell><cell>mse</cell><cell>60.75</cell><cell>0.1211</cell><cell>1.33</cell><cell>0.07387</cell></row><row><cell cols="2">MLP-adam mse</cell><cell cols="2">387 4.504e-54</cell><cell cols="2">15.32 1.147e-68</cell></row><row><cell cols="2">MLP-SGD mse</cell><cell cols="2">353.2 1.185e-47</cell><cell cols="2">8.239 3.548e-38</cell></row><row><cell>RF</cell><cell>mse</cell><cell>35.59</cell><cell>0.9242</cell><cell>0.8985</cell><cell>0.6692</cell></row><row><cell>SVM</cell><cell>mse</cell><cell>25.01</cell><cell>0.9983</cell><cell>0.4491</cell><cell>0.9996</cell></row><row><cell>ada</cell><cell>mse</cell><cell cols="2">249.1 1.398e-28</cell><cell cols="2">14.4 3.682e-65</cell></row><row><cell>kNN</cell><cell>mse</cell><cell>27.75</cell><cell>0.9938</cell><cell>0.8247</cell><cell>0.7951</cell></row><row><cell>lasso</cell><cell>mse</cell><cell>31.38</cell><cell>0.9764</cell><cell>0.5397</cell><cell>0.9955</cell></row><row><cell>linear</cell><cell>mse</cell><cell>16.67</cell><cell>1</cell><cell>0.1726</cell><cell>1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Heteroscedasticity tests on tasks involving Breast cancer (BC) data set.</figDesc><table><row><cell cols="2">Data set Model</cell><cell cols="2">Metric Fligner Statistic</cell><cell cols="2">p-value Levene Statistic</cell><cell>p-value</cell></row><row><cell>BC</cell><cell>DT</cell><cell>acc</cell><cell cols="2">97.79 4.302e-05</cell><cell cols="2">4.62 6.650e-19</cell></row><row><cell></cell><cell cols="2">MLP-adam acc</cell><cell cols="2">133 1.113e-09</cell><cell cols="2">2.939 1.923e-09</cell></row><row><cell></cell><cell cols="2">MLP-SGD acc</cell><cell cols="2">116.8 1.854e-07</cell><cell cols="2">2.469 6.495e-07</cell></row><row><cell></cell><cell>RF</cell><cell>acc</cell><cell cols="2">154.9 6.469e-13</cell><cell cols="2">6.661 4.353e-30</cell></row><row><cell></cell><cell>SVM</cell><cell>acc</cell><cell>20.7</cell><cell>0.9999</cell><cell>0.3995</cell><cell>0.9999</cell></row><row><cell></cell><cell>ada</cell><cell>acc</cell><cell cols="2">272.5 9.178e-33</cell><cell cols="2">13.57 6.582e-62</cell></row><row><cell></cell><cell>kNN</cell><cell>acc</cell><cell>33.16</cell><cell>0.9596</cell><cell>0.5519</cell><cell>0.9941</cell></row><row><cell></cell><cell>lasso</cell><cell>acc</cell><cell>20.78</cell><cell>0.9999</cell><cell>0.4291</cell><cell>0.9998</cell></row><row><cell></cell><cell>linear</cell><cell>acc</cell><cell>21.15</cell><cell>0.9998</cell><cell>0.4545</cell><cell>0.9995</cell></row><row><cell></cell><cell>DT</cell><cell>nll</cell><cell cols="2">260.5 1.280e-30</cell><cell cols="2">9.52 2.294e-44</cell></row><row><cell></cell><cell cols="2">MLP-adam nll</cell><cell cols="2">166.6 1.008e-14</cell><cell cols="2">3.643 2.247e-13</cell></row><row><cell></cell><cell cols="2">MLP-SGD nll</cell><cell cols="2">141.2 7.115e-11</cell><cell cols="2">2.669 5.661e-08</cell></row><row><cell></cell><cell>RF</cell><cell>nll</cell><cell cols="2">185.8 8.495e-18</cell><cell cols="2">7.553 1.013e-34</cell></row><row><cell></cell><cell>SVM</cell><cell>nll</cell><cell cols="2">76.98 6.526e-03</cell><cell cols="2">1.707 2.970e-03</cell></row><row><cell></cell><cell>ada</cell><cell>nll</cell><cell cols="2">142 5.458e-11</cell><cell cols="2">4.283 5.274e-17</cell></row><row><cell></cell><cell>kNN</cell><cell>nll</cell><cell cols="2">125.7 1.155e-08</cell><cell cols="2">4.337 2.635e-17</cell></row><row><cell></cell><cell>lasso</cell><cell>nll</cell><cell>71.41</cell><cell>0.02</cell><cell>1.011</cell><cell>0.4565</cell></row><row><cell></cell><cell>linear</cell><cell>nll</cell><cell>18.55</cell><cell>1</cell><cell>0.2714</cell><cell>1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Heteroscedasticity tests on tasks involving diabetes data set.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell cols="2">Metric Fligner Statistic</cell><cell cols="2">p-value Levene Statistic</cell><cell>p-value</cell></row><row><cell cols="2">Diabetes DT</cell><cell>mae</cell><cell>56.52</cell><cell>0.2146</cell><cell>1.131</cell><cell>0.2601</cell></row><row><cell></cell><cell cols="2">MLP-adam mae</cell><cell>74.64</cell><cell>0.01059</cell><cell cols="2">2.573 1.847e-07</cell></row><row><cell></cell><cell cols="2">MLP-SGD mae</cell><cell cols="2">191.3 1.062e-18</cell><cell cols="2">17.87 8.498e-78</cell></row><row><cell></cell><cell>RF</cell><cell>mae</cell><cell cols="2">79.38 3.898e-03</cell><cell>1.558</cell><cell>0.01174</cell></row><row><cell></cell><cell>SVM</cell><cell>mae</cell><cell>2.436</cell><cell>1</cell><cell>1.810e-04</cell><cell>1</cell></row><row><cell></cell><cell>ada</cell><cell>mae</cell><cell cols="2">179.8 7.883e-17</cell><cell cols="2">7.542 1.154e-34</cell></row><row><cell></cell><cell>kNN</cell><cell>mae</cell><cell>67.48</cell><cell>0.04106</cell><cell cols="2">2.101 4.747e-05</cell></row><row><cell></cell><cell>lasso</cell><cell>mae</cell><cell cols="2">176.2 2.950e-16</cell><cell cols="2">4.75 1.225e-19</cell></row><row><cell></cell><cell>linear</cell><cell>mae</cell><cell cols="2">206 3.792e-21</cell><cell cols="2">5.714 5.490e-25</cell></row><row><cell></cell><cell>DT</cell><cell>mse</cell><cell>44.52</cell><cell>0.6551</cell><cell>0.8264</cell><cell>0.7925</cell></row><row><cell></cell><cell cols="2">MLP-adam mse</cell><cell cols="2">100.4 2.109e-05</cell><cell cols="2">3.582 4.951e-13</cell></row><row><cell></cell><cell cols="2">MLP-SGD mse</cell><cell cols="2">202.9 1.257e-20</cell><cell cols="2">14.31 7.960e-65</cell></row><row><cell></cell><cell>RF</cell><cell>mse</cell><cell>37.1</cell><cell>0.8938</cell><cell>0.8063</cell><cell>0.8224</cell></row><row><cell></cell><cell>SVM</cell><cell>mse</cell><cell>4.004</cell><cell>1</cell><cell>4.740e-04</cell><cell>1</cell></row><row><cell></cell><cell>ada</cell><cell>mse</cell><cell cols="2">189 2.510e-18</cell><cell cols="2">7.348 1.138e-33</cell></row><row><cell></cell><cell>kNN</cell><cell>mse</cell><cell cols="2">88.62 4.545e-04</cell><cell cols="2">2.964 1.407e-09</cell></row><row><cell></cell><cell>lasso</cell><cell>mse</cell><cell cols="2">257.6 4.341e-30</cell><cell cols="2">10.86 1.637e-50</cell></row><row><cell></cell><cell>linear</cell><cell>mse</cell><cell cols="2">278.2 8.540e-34</cell><cell cols="2">10.01 1.216e-46</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Heteroscedasticity tests on tasks involving digits data set.</figDesc><table><row><cell cols="2">Data set Model</cell><cell cols="2">Metric Fligner Statistic</cell><cell cols="2">p-value Levene Statistic</cell><cell>p-value</cell></row><row><cell>Digits</cell><cell>DT</cell><cell>acc</cell><cell cols="2">205 5.670e-21</cell><cell cols="2">14.29 9.219e-65</cell></row><row><cell></cell><cell cols="2">MLP-adam acc</cell><cell cols="2">256.7 6.239e-30</cell><cell cols="2">7.342 1.219e-33</cell></row><row><cell></cell><cell cols="2">MLP-SGD acc</cell><cell cols="2">210 8.188e-22</cell><cell cols="2">6.53 2.167e-29</cell></row><row><cell></cell><cell>RF</cell><cell>acc</cell><cell cols="2">184.3 1.458e-17</cell><cell cols="2">15.61 9.379e-70</cell></row><row><cell></cell><cell>SVM</cell><cell>acc</cell><cell cols="2">91.72 2.093e-04</cell><cell cols="2">2.187 1.790e-05</cell></row><row><cell></cell><cell>ada</cell><cell>acc</cell><cell cols="2">99.34 2.832e-05</cell><cell cols="2">2.305 4.601e-06</cell></row><row><cell></cell><cell>kNN</cell><cell>acc</cell><cell>35</cell><cell>0.9343</cell><cell>0.7042</cell><cell>0.9349</cell></row><row><cell></cell><cell>lasso</cell><cell>acc</cell><cell>22.97</cell><cell>0.9994</cell><cell>0.4292</cell><cell>0.9998</cell></row><row><cell></cell><cell>linear</cell><cell>acc</cell><cell>17.3</cell><cell>1</cell><cell>0.2963</cell><cell>1</cell></row><row><cell></cell><cell>DT</cell><cell>nll</cell><cell cols="2">249.6 1.140e-28</cell><cell cols="2">15.71 3.892e-70</cell></row><row><cell></cell><cell cols="2">MLP-adam nll</cell><cell cols="2">339.8 3.816e-45</cell><cell cols="2">6.882 3.012e-31</cell></row><row><cell></cell><cell cols="2">MLP-SGD nll</cell><cell cols="2">244.8 7.740e-28</cell><cell cols="2">6.104 4.129e-27</cell></row><row><cell></cell><cell>RF</cell><cell>nll</cell><cell cols="2">144 2.791e-11</cell><cell cols="2">7.435 4.059e-34</cell></row><row><cell></cell><cell>SVM</cell><cell>nll</cell><cell>4.373</cell><cell>1</cell><cell>0.06091</cell><cell>1</cell></row><row><cell></cell><cell>ada</cell><cell>nll</cell><cell cols="2">135.1 5.444e-10</cell><cell cols="2">3.294 2.061e-11</cell></row><row><cell></cell><cell>kNN</cell><cell>nll</cell><cell cols="2">108.2 2.326e-06</cell><cell cols="2">3.059 4.211e-10</cell></row><row><cell></cell><cell>lasso</cell><cell>nll</cell><cell cols="2">88.4 4.799e-04</cell><cell cols="2">2.116 3.995e-05</cell></row><row><cell></cell><cell>linear</cell><cell>nll</cell><cell cols="2">103 1.024e-05</cell><cell cols="2">3.328 1.335e-11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Heteroscedasticity tests on tasks involving iris data set.</figDesc><table><row><cell cols="2">Data set Model</cell><cell cols="2">Metric Fligner Statistic</cell><cell cols="2">p-value Levene Statistic</cell><cell>p-value</cell></row><row><cell>Iris</cell><cell>DT</cell><cell>acc</cell><cell cols="2">207.1 2.440e-21</cell><cell cols="2">6.523 2.355e-29</cell></row><row><cell></cell><cell cols="2">MLP-adam acc</cell><cell cols="2">83.81 1.436e-03</cell><cell cols="2">1.838 7.989e-04</cell></row><row><cell></cell><cell cols="2">MLP-SGD acc</cell><cell>68.52</cell><cell>0.03413</cell><cell>1.409</cell><cell>0.04082</cell></row><row><cell></cell><cell>RF</cell><cell>acc</cell><cell cols="2">155.5 5.311e-13</cell><cell cols="2">6.138 2.726e-27</cell></row><row><cell></cell><cell>SVM</cell><cell>acc</cell><cell cols="2">198.4 6.990e-20</cell><cell cols="2">3.345 1.065e-11</cell></row><row><cell></cell><cell>ada</cell><cell>acc</cell><cell cols="2">155.7 4.788e-13</cell><cell cols="2">5.018 3.858e-21</cell></row><row><cell></cell><cell>kNN</cell><cell>acc</cell><cell>55.68</cell><cell>0.2378</cell><cell>1.124</cell><cell>0.2701</cell></row><row><cell></cell><cell>lasso</cell><cell>acc</cell><cell>19.72</cell><cell>0.9999</cell><cell>0.4045</cell><cell>0.9999</cell></row><row><cell></cell><cell>linear</cell><cell>acc</cell><cell cols="2">106.4 3.965e-06</cell><cell cols="2">2.959 1.502e-09</cell></row><row><cell></cell><cell>DT</cell><cell>nll</cell><cell cols="2">322.2 7.375e-42</cell><cell cols="2">6.118 3.506e-27</cell></row><row><cell></cell><cell cols="2">MLP-adam nll</cell><cell cols="2">106.3 4.070e-06</cell><cell cols="2">3.123 1.869e-10</cell></row><row><cell></cell><cell cols="2">MLP-SGD nll</cell><cell cols="2">155.6 4.966e-13</cell><cell cols="2">6.386 1.264e-28</cell></row><row><cell></cell><cell>RF</cell><cell>nll</cell><cell cols="2">321.3 1.066e-41</cell><cell cols="2">8.339 1.136e-38</cell></row><row><cell></cell><cell>SVM</cell><cell>nll</cell><cell cols="2">188.4 3.217e-18</cell><cell cols="2">4.736 1.470e-19</cell></row><row><cell></cell><cell>ada</cell><cell>nll</cell><cell>74.04</cell><cell>0.01194</cell><cell>1.414</cell><cell>0.03938</cell></row><row><cell></cell><cell>kNN</cell><cell>nll</cell><cell cols="2">212.6 2.863e-22</cell><cell cols="2">8.838 4.118e-41</cell></row><row><cell></cell><cell>lasso</cell><cell>nll</cell><cell>45.45</cell><cell>0.6177</cell><cell>0.5045</cell><cell>0.998</cell></row><row><cell></cell><cell>linear</cell><cell>nll</cell><cell>36.64</cell><cell>0.9037</cell><cell>0.733</cell><cell>0.9101</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Heteroscedasticity tests on tasks involving the wine data set.</figDesc><table><row><cell cols="2">Data set Model</cell><cell cols="2">Metric Fligner Statistic</cell><cell cols="2">p-value Levene Statistic</cell><cell>p-value</cell></row><row><cell>Wine</cell><cell>DT</cell><cell>acc</cell><cell cols="2">127.3 6.912e-09</cell><cell cols="2">3.553 7.195e-13</cell></row><row><cell></cell><cell cols="2">MLP-adam acc</cell><cell cols="2">85.37 9.945e-04</cell><cell cols="2">1.874 5.544e-04</cell></row><row><cell></cell><cell cols="2">MLP-SGD acc</cell><cell cols="2">109 1.845e-06</cell><cell cols="2">2.48 5.701e-07</cell></row><row><cell></cell><cell>RF</cell><cell>acc</cell><cell cols="2">128.5 4.717e-09</cell><cell cols="2">5.069 2.014e-21</cell></row><row><cell></cell><cell>SVM</cell><cell>acc</cell><cell>28.73</cell><cell>0.9908</cell><cell>0.5136</cell><cell>0.9975</cell></row><row><cell></cell><cell>ada</cell><cell>acc</cell><cell cols="2">156.6 3.527e-13</cell><cell cols="2">3.968 3.215e-15</cell></row><row><cell></cell><cell>kNN</cell><cell>acc</cell><cell>37.67</cell><cell>0.8807</cell><cell>0.6869</cell><cell>0.9473</cell></row><row><cell></cell><cell>lasso</cell><cell>acc</cell><cell>29.8</cell><cell>0.9862</cell><cell>0.5981</cell><cell>0.9859</cell></row><row><cell></cell><cell>linear</cell><cell>acc</cell><cell>21.28</cell><cell>0.9998</cell><cell>0.3839</cell><cell>1</cell></row><row><cell></cell><cell>DT</cell><cell>nll</cell><cell cols="2">349.2 6.614e-47</cell><cell cols="2">10.46 1.115e-48</cell></row><row><cell></cell><cell cols="2">MLP-adam nll</cell><cell>57.19</cell><cell>0.1971</cell><cell>1.21</cell><cell>0.1646</cell></row><row><cell></cell><cell cols="2">MLP-SGD nll</cell><cell cols="2">110.1 1.362e-06</cell><cell cols="2">2.597 1.380e-07</cell></row><row><cell></cell><cell>RF</cell><cell>nll</cell><cell cols="2">258 3.660e-30</cell><cell cols="2">6.468 4.597e-29</cell></row><row><cell></cell><cell>SVM</cell><cell>nll</cell><cell>57.18</cell><cell>0.1975</cell><cell>1.006</cell><cell>0.4663</cell></row><row><cell></cell><cell>ada</cell><cell>nll</cell><cell cols="2">152.8 1.323e-12</cell><cell cols="2">3.072 3.555e-10</cell></row><row><cell></cell><cell>kNN</cell><cell>nll</cell><cell cols="2">178.2 1.410e-16</cell><cell cols="2">5.446 1.635e-23</cell></row><row><cell></cell><cell>lasso</cell><cell>nll</cell><cell cols="2">83.94 1.394e-03</cell><cell cols="2">1.782 1.416e-03</cell></row><row><cell></cell><cell>linear</cell><cell>nll</cell><cell cols="2">185.8 8.404e-18</cell><cell cols="2">5.01 4.312e-21</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>Number of tasks for which each optimiser performed best.</figDesc><table><row><cell>HEBO</cell><cell>TuRBO</cell><cell>PySOT</cell><cell cols="4">Skopt Nevergrad (1+1) BOHB-BB Opentuner Hyperopt TuRBO+</cell></row><row><cell cols="4">71 (65.7%) 14 (13.0%) 7 (6.5 %) 5 (4.6 %)</cell><cell>4 (3.7 %)</cell><cell>3 (2.8%)</cell><cell>2 (1.9%)</cell><cell>1 (0.9%)</cell><cell>1 (0.9%)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 :</head><label>12</label><figDesc>Boston with MAE loss for tuning DT model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell cols="3">Dataset Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell>Boston DT</cell><cell>MAE</cell><cell>106.658094</cell><cell>3.514569</cell></row><row><cell>PySOT</cell><cell>Boston DT</cell><cell>MAE</cell><cell>104.811444</cell><cell>4.220520</cell></row><row><cell>Skopt</cell><cell>Boston DT</cell><cell>MAE</cell><cell>104.303237</cell><cell>4.311549</cell></row><row><cell>TuRBO</cell><cell>Boston DT</cell><cell>MAE</cell><cell>103.863640</cell><cell>6.184650</cell></row><row><cell cols="2">Nevergrad (1+1) Boston DT</cell><cell>MAE</cell><cell>102.938925</cell><cell>17.667195</cell></row><row><cell>Hyperopt</cell><cell>Boston DT</cell><cell>MAE</cell><cell>99.231898</cell><cell>5.681478</cell></row><row><cell>TuRBO+</cell><cell>Boston DT</cell><cell>MAE</cell><cell>98.464758</cell><cell>35.071496</cell></row><row><cell>Random-search</cell><cell>Boston DT</cell><cell>MAE</cell><cell>95.273667</cell><cell>21.448475</cell></row><row><cell>BOHB-BB</cell><cell>Boston DT</cell><cell>MAE</cell><cell>93.360381</cell><cell>37.490514</cell></row><row><cell>Opentuner</cell><cell>Boston DT</cell><cell>MAE</cell><cell cols="2">86.421660 313.239599</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 13 :</head><label>13</label><figDesc>Boston with MAE loss for tuning MLP-adam model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is TuRBO.</figDesc><table><row><cell>Algorithm</cell><cell>Dataset Model</cell><cell cols="2">Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>TuRBO</cell><cell cols="2">Boston MLP-adam MAE</cell><cell cols="2">104.423509 17.216156</cell></row><row><cell>PySOT</cell><cell cols="2">Boston MLP-adam MAE</cell><cell cols="2">103.369026 15.037764</cell></row><row><cell>HEBO</cell><cell cols="2">Boston MLP-adam MAE</cell><cell cols="2">101.377994 31.981786</cell></row><row><cell>TuRBO+</cell><cell cols="2">Boston MLP-adam MAE</cell><cell cols="2">98.759781 46.065189</cell></row><row><cell cols="3">Nevergrad (1+1) Boston MLP-adam MAE</cell><cell cols="2">96.279956 45.560329</cell></row><row><cell>Hyperopt</cell><cell cols="2">Boston MLP-adam MAE</cell><cell cols="2">95.255250 28.728828</cell></row><row><cell>Random-search</cell><cell cols="2">Boston MLP-adam MAE</cell><cell cols="2">92.866456 23.342156</cell></row><row><cell>BOHB-BB</cell><cell cols="2">Boston MLP-adam MAE</cell><cell cols="2">92.001680 17.523725</cell></row><row><cell>Opentuner</cell><cell cols="2">Boston MLP-adam MAE</cell><cell cols="2">89.595926 47.161525</cell></row><row><cell>Skopt</cell><cell cols="2">Boston MLP-adam MAE</cell><cell cols="2">86.257482 24.245633</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 14 :</head><label>14</label><figDesc>Boston with MAE loss for tuning MLP-SGD model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is PySOT.</figDesc><table><row><cell>Algorithm</cell><cell>Dataset Model</cell><cell cols="2">Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>PySOT</cell><cell cols="2">Boston MLP-SGD MAE</cell><cell>96.325329</cell><cell>34.592455</cell></row><row><cell>HEBO</cell><cell cols="2">Boston MLP-SGD MAE</cell><cell>95.885011</cell><cell>4.520840</cell></row><row><cell>TuRBO</cell><cell cols="2">Boston MLP-SGD MAE</cell><cell>94.936543</cell><cell>21.232934</cell></row><row><cell>Hyperopt</cell><cell cols="2">Boston MLP-SGD MAE</cell><cell>93.605852</cell><cell>4.184636</cell></row><row><cell>Skopt</cell><cell cols="2">Boston MLP-SGD MAE</cell><cell>91.084570</cell><cell>8.855305</cell></row><row><cell>Random-search</cell><cell cols="2">Boston MLP-SGD MAE</cell><cell>89.854633</cell><cell>9.131124</cell></row><row><cell>BOHB-BB</cell><cell cols="2">Boston MLP-SGD MAE</cell><cell>89.622364</cell><cell>20.485508</cell></row><row><cell>TuRBO+</cell><cell cols="2">Boston MLP-SGD MAE</cell><cell>89.288302</cell><cell>18.617165</cell></row><row><cell cols="3">Nevergrad (1+1) Boston MLP-SGD MAE</cell><cell cols="2">87.149541 115.487770</cell></row><row><cell>Opentuner</cell><cell cols="2">Boston MLP-SGD MAE</cell><cell>85.130342</cell><cell>26.218217</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 15 :</head><label>15</label><figDesc>Boston with MAE loss for tuning RF model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell cols="3">Dataset Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell>Boston RF</cell><cell>MAE</cell><cell>101.726637</cell><cell>0.282878</cell></row><row><cell>TuRBO</cell><cell>Boston RF</cell><cell>MAE</cell><cell>100.095822</cell><cell>2.339170</cell></row><row><cell>PySOT</cell><cell>Boston RF</cell><cell>MAE</cell><cell>99.773979</cell><cell>1.147489</cell></row><row><cell>Skopt</cell><cell>Boston RF</cell><cell>MAE</cell><cell>99.352605</cell><cell>0.805112</cell></row><row><cell>Hyperopt</cell><cell>Boston RF</cell><cell>MAE</cell><cell>98.142703</cell><cell>1.649156</cell></row><row><cell cols="2">Nevergrad (1+1) Boston RF</cell><cell>MAE</cell><cell cols="2">96.790676 20.349353</cell></row><row><cell>TuRBO+</cell><cell>Boston RF</cell><cell>MAE</cell><cell>95.817383</cell><cell>7.005061</cell></row><row><cell>Opentuner</cell><cell>Boston RF</cell><cell>MAE</cell><cell cols="2">94.012783 32.962245</cell></row><row><cell>BOHB-BB</cell><cell>Boston RF</cell><cell>MAE</cell><cell>93.869871</cell><cell>6.026597</cell></row><row><cell>Random-search</cell><cell>Boston RF</cell><cell>MAE</cell><cell>93.223774</cell><cell>8.626423</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 16 :</head><label>16</label><figDesc>Boston with MAE loss for tuning SVM model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell cols="3">Dataset Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell>Boston SVM</cell><cell>MAE</cell><cell>102.469764</cell><cell>0.015274</cell></row><row><cell cols="2">Nevergrad (1+1) Boston SVM</cell><cell>MAE</cell><cell>102.236442</cell><cell>0.135306</cell></row><row><cell>TuRBO</cell><cell>Boston SVM</cell><cell>MAE</cell><cell>101.826793</cell><cell>0.169932</cell></row><row><cell>PySOT</cell><cell>Boston SVM</cell><cell>MAE</cell><cell>101.262466</cell><cell>0.082329</cell></row><row><cell>Skopt</cell><cell>Boston SVM</cell><cell>MAE</cell><cell>100.914194</cell><cell>0.281085</cell></row><row><cell>Hyperopt</cell><cell>Boston SVM</cell><cell>MAE</cell><cell>98.943800</cell><cell>9.092074</cell></row><row><cell>Opentuner</cell><cell>Boston SVM</cell><cell>MAE</cell><cell>97.906378</cell><cell>12.428820</cell></row><row><cell>TuRBO+</cell><cell>Boston SVM</cell><cell>MAE</cell><cell cols="2">90.935974 182.645481</cell></row><row><cell>Random-search</cell><cell>Boston SVM</cell><cell>MAE</cell><cell>88.908494</cell><cell>80.396927</cell></row><row><cell>BOHB-BB</cell><cell>Boston SVM</cell><cell>MAE</cell><cell>86.714806</cell><cell>46.398841</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 17 :</head><label>17</label><figDesc>Boston with MAE loss for tuning Ada model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell cols="3">Dataset Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell>Boston Ada</cell><cell>MAE</cell><cell>103.445454</cell><cell>19.376575</cell></row><row><cell>Opentuner</cell><cell>Boston Ada</cell><cell>MAE</cell><cell>101.293346</cell><cell>54.800061</cell></row><row><cell>TuRBO</cell><cell>Boston Ada</cell><cell>MAE</cell><cell>99.774003</cell><cell>84.126539</cell></row><row><cell cols="2">Nevergrad (1+1) Boston Ada</cell><cell>MAE</cell><cell cols="2">97.261376 394.018233</cell></row><row><cell>Skopt</cell><cell>Boston Ada</cell><cell>MAE</cell><cell>97.141270</cell><cell>48.739222</cell></row><row><cell>TuRBO+</cell><cell>Boston Ada</cell><cell>MAE</cell><cell cols="2">97.035052 178.220595</cell></row><row><cell>Hyperopt</cell><cell>Boston Ada</cell><cell>MAE</cell><cell>93.394811</cell><cell>59.032286</cell></row><row><cell>PySOT</cell><cell>Boston Ada</cell><cell>MAE</cell><cell>91.618187</cell><cell>25.548574</cell></row><row><cell>Random-search</cell><cell>Boston Ada</cell><cell>MAE</cell><cell>85.889844</cell><cell>39.688171</cell></row><row><cell>BOHB-BB</cell><cell>Boston Ada</cell><cell>MAE</cell><cell>84.104672</cell><cell>99.526333</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 18 :</head><label>18</label><figDesc>Boston with MAE loss for tuning Knn model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell cols="3">Dataset Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell>Boston Knn</cell><cell>MAE</cell><cell>100.000000</cell><cell>0.000000</cell></row><row><cell>Opentuner</cell><cell>Boston Knn</cell><cell>MAE</cell><cell>100.000000</cell><cell>0.000000</cell></row><row><cell>Skopt</cell><cell>Boston Knn</cell><cell>MAE</cell><cell>100.000000</cell><cell>0.000000</cell></row><row><cell>TuRBO</cell><cell>Boston Knn</cell><cell>MAE</cell><cell>100.000000</cell><cell>0.000000</cell></row><row><cell>TuRBO+</cell><cell>Boston Knn</cell><cell>MAE</cell><cell>99.689888</cell><cell>1.923386</cell></row><row><cell>PySOT</cell><cell>Boston Knn</cell><cell>MAE</cell><cell>99.069665</cell><cell>5.162772</cell></row><row><cell>Hyperopt</cell><cell>Boston Knn</cell><cell>MAE</cell><cell>98.023553</cell><cell>9.829689</cell></row><row><cell cols="2">Nevergrad (1+1) Boston Knn</cell><cell>MAE</cell><cell cols="2">97.379434 28.707287</cell></row><row><cell>BOHB-BB</cell><cell>Boston Knn</cell><cell>MAE</cell><cell cols="2">97.134442 18.411664</cell></row><row><cell>Random-search</cell><cell>Boston Knn</cell><cell>MAE</cell><cell cols="2">96.297696 45.566838</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 19 :</head><label>19</label><figDesc>Boston with MAE loss for tuning Lasso model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell cols="2">Dataset Model Metric Normalised Score Variance</cell></row><row><cell>HEBO</cell><cell>Boston Lasso MAE</cell><cell>100.031658 0.000141</cell></row><row><cell>PySOT</cell><cell>Boston Lasso MAE</cell><cell>100.004491 0.001119</cell></row><row><cell cols="2">Nevergrad (1+1) Boston Lasso MAE</cell><cell>99.989103 0.005199</cell></row><row><cell>Skopt</cell><cell>Boston Lasso MAE</cell><cell>99.979143 0.001668</cell></row><row><cell>TuRBO</cell><cell>Boston Lasso MAE</cell><cell>99.977645 0.002141</cell></row><row><cell>Hyperopt</cell><cell>Boston Lasso MAE</cell><cell>99.967041 0.002263</cell></row><row><cell>BOHB-BB</cell><cell>Boston Lasso MAE</cell><cell>99.926445 0.006747</cell></row><row><cell>TuRBO+</cell><cell>Boston Lasso MAE</cell><cell>99.921337 0.031836</cell></row><row><cell>Random-search</cell><cell>Boston Lasso MAE</cell><cell>99.917670 0.002902</cell></row><row><cell>Opentuner</cell><cell>Boston Lasso MAE</cell><cell>99.306403 0.956136</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 20 :</head><label>20</label><figDesc>Boston with MAE loss for tuning Linear model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell cols="2">Dataset Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell>Boston Linear MAE</cell><cell>99.964657</cell><cell>0.002542</cell></row><row><cell>Hyperopt</cell><cell>Boston Linear MAE</cell><cell>99.922142</cell><cell>0.006819</cell></row><row><cell>TuRBO</cell><cell>Boston Linear MAE</cell><cell>99.695751</cell><cell>1.213154</cell></row><row><cell>PySOT</cell><cell>Boston Linear MAE</cell><cell>99.394823</cell><cell>0.799652</cell></row><row><cell>BOHB-BB</cell><cell>Boston Linear MAE</cell><cell>98.547907</cell><cell>5.197760</cell></row><row><cell>Skopt</cell><cell>Boston Linear MAE</cell><cell>98.327627</cell><cell>6.150494</cell></row><row><cell>Random-search</cell><cell>Boston Linear MAE</cell><cell>97.843101</cell><cell>5.252852</cell></row><row><cell>TuRBO+</cell><cell>Boston Linear MAE</cell><cell>95.874716</cell><cell>153.782738</cell></row><row><cell cols="2">Nevergrad (1+1) Boston Linear MAE</cell><cell cols="2">80.996813 1523.062377</cell></row><row><cell>Opentuner</cell><cell>Boston Linear MAE</cell><cell cols="2">45.221227 2080.382131</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 21 :</head><label>21</label><figDesc>Boston with MSE loss for tuning DT model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is Skopt.</figDesc><table><row><cell>Algorithm</cell><cell cols="3">Dataset Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>Skopt</cell><cell>Boston DT</cell><cell>MSE</cell><cell>105.587343</cell><cell>12.872983</cell></row><row><cell>PySOT</cell><cell>Boston DT</cell><cell>MSE</cell><cell>105.072075</cell><cell>4.900371</cell></row><row><cell>HEBO</cell><cell>Boston DT</cell><cell>MSE</cell><cell>104.838117</cell><cell>6.188087</cell></row><row><cell>TuRBO</cell><cell>Boston DT</cell><cell>MSE</cell><cell>104.030707</cell><cell>10.799242</cell></row><row><cell>Hyperopt</cell><cell>Boston DT</cell><cell>MSE</cell><cell>102.229770</cell><cell>11.149492</cell></row><row><cell>TuRBO+</cell><cell>Boston DT</cell><cell>MSE</cell><cell>100.450728</cell><cell>12.162198</cell></row><row><cell cols="2">Nevergrad (1+1) Boston DT</cell><cell>MSE</cell><cell cols="2">97.629442 407.356319</cell></row><row><cell>Random-search</cell><cell>Boston DT</cell><cell>MSE</cell><cell>95.359894</cell><cell>28.675740</cell></row><row><cell>BOHB-BB</cell><cell>Boston DT</cell><cell>MSE</cell><cell>94.520125</cell><cell>20.675992</cell></row><row><cell>Opentuner</cell><cell>Boston DT</cell><cell>MSE</cell><cell cols="2">87.826986 205.761429</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 22 :</head><label>22</label><figDesc>Boston with MSE loss for tuning MLP-adam model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell>Dataset Model</cell><cell cols="2">Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell cols="2">Boston MLP-adam MSE</cell><cell cols="2">101.227304 15.854379</cell></row><row><cell>TuRBO</cell><cell cols="2">Boston MLP-adam MSE</cell><cell>100.906498</cell><cell>4.709893</cell></row><row><cell>PySOT</cell><cell cols="2">Boston MLP-adam MSE</cell><cell>98.878397</cell><cell>7.952874</cell></row><row><cell>TuRBO+</cell><cell cols="2">Boston MLP-adam MSE</cell><cell cols="2">98.090398 17.299141</cell></row><row><cell cols="3">Nevergrad (1+1) Boston MLP-adam MSE</cell><cell cols="2">97.631386 45.709714</cell></row><row><cell>Hyperopt</cell><cell cols="2">Boston MLP-adam MSE</cell><cell cols="2">95.833644 15.829315</cell></row><row><cell>BOHB-BB</cell><cell cols="2">Boston MLP-adam MSE</cell><cell cols="2">95.267050 21.466780</cell></row><row><cell>Random-search</cell><cell cols="2">Boston MLP-adam MSE</cell><cell cols="2">93.691552 18.033615</cell></row><row><cell>Opentuner</cell><cell cols="2">Boston MLP-adam MSE</cell><cell cols="2">92.489905 52.692896</cell></row><row><cell>Skopt</cell><cell cols="2">Boston MLP-adam MSE</cell><cell cols="2">87.610428 15.511744</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 23 :</head><label>23</label><figDesc>Boston with MSE loss for tuning MLP-SGD model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is TuRBO.</figDesc><table><row><cell>Algorithm</cell><cell>Dataset Model</cell><cell cols="2">Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>TuRBO</cell><cell cols="2">Boston MLP-SGD MSE</cell><cell>104.817056</cell><cell>17.856834</cell></row><row><cell>HEBO</cell><cell cols="2">Boston MLP-SGD MSE</cell><cell>103.285788</cell><cell>4.467401</cell></row><row><cell>Hyperopt</cell><cell cols="2">Boston MLP-SGD MSE</cell><cell>102.049189</cell><cell>6.848005</cell></row><row><cell>PySOT</cell><cell cols="2">Boston MLP-SGD MSE</cell><cell>99.992157</cell><cell>17.117778</cell></row><row><cell>TuRBO+</cell><cell cols="2">Boston MLP-SGD MSE</cell><cell>98.973335</cell><cell>45.162334</cell></row><row><cell>Skopt</cell><cell cols="2">Boston MLP-SGD MSE</cell><cell>98.293397</cell><cell>6.746134</cell></row><row><cell>BOHB-BB</cell><cell cols="2">Boston MLP-SGD MSE</cell><cell>94.731025</cell><cell>17.820451</cell></row><row><cell>Random-search</cell><cell cols="2">Boston MLP-SGD MSE</cell><cell>94.524667</cell><cell>28.931228</cell></row><row><cell cols="3">Nevergrad (1+1) Boston MLP-SGD MSE</cell><cell cols="2">94.267722 207.308935</cell></row><row><cell>Opentuner</cell><cell cols="2">Boston MLP-SGD MSE</cell><cell>88.380591</cell><cell>58.962581</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 24 :</head><label>24</label><figDesc>Boston with MSE loss for tuning RF model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell cols="3">Dataset Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell>Boston RF</cell><cell>MSE</cell><cell>103.461058</cell><cell>0.440697</cell></row><row><cell>PySOT</cell><cell>Boston RF</cell><cell>MSE</cell><cell>101.853905</cell><cell>0.492860</cell></row><row><cell>TuRBO</cell><cell>Boston RF</cell><cell>MSE</cell><cell>101.839078</cell><cell>0.800142</cell></row><row><cell>Skopt</cell><cell>Boston RF</cell><cell>MSE</cell><cell>101.472976</cell><cell>0.606579</cell></row><row><cell cols="2">Nevergrad (1+1) Boston RF</cell><cell>MSE</cell><cell>100.814244</cell><cell>2.514980</cell></row><row><cell>Hyperopt</cell><cell>Boston RF</cell><cell>MSE</cell><cell>100.547643</cell><cell>0.964402</cell></row><row><cell>TuRBO+</cell><cell>Boston RF</cell><cell>MSE</cell><cell>98.915015</cell><cell>3.362617</cell></row><row><cell>Random-search</cell><cell>Boston RF</cell><cell>MSE</cell><cell>98.167459</cell><cell>4.278077</cell></row><row><cell>Opentuner</cell><cell>Boston RF</cell><cell>MSE</cell><cell cols="2">98.049981 11.073665</cell></row><row><cell>BOHB-BB</cell><cell>Boston RF</cell><cell>MSE</cell><cell>97.839608</cell><cell>6.645583</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table 25 :</head><label>25</label><figDesc>Boston with MSE loss for tuning SVM model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell cols="3">Dataset Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell>Boston SVM</cell><cell>MSE</cell><cell>104.852372</cell><cell>0.005667</cell></row><row><cell cols="2">Nevergrad (1+1) Boston SVM</cell><cell>MSE</cell><cell>104.578053</cell><cell>0.288069</cell></row><row><cell>TuRBO</cell><cell>Boston SVM</cell><cell>MSE</cell><cell>103.821417</cell><cell>1.334006</cell></row><row><cell>PySOT</cell><cell>Boston SVM</cell><cell>MSE</cell><cell>103.398778</cell><cell>0.101558</cell></row><row><cell>Hyperopt</cell><cell>Boston SVM</cell><cell>MSE</cell><cell>101.271084</cell><cell>3.903429</cell></row><row><cell>Skopt</cell><cell>Boston SVM</cell><cell>MSE</cell><cell>100.753282</cell><cell>76.344135</cell></row><row><cell>TuRBO+</cell><cell>Boston SVM</cell><cell>MSE</cell><cell>97.946333</cell><cell>50.658268</cell></row><row><cell>Opentuner</cell><cell>Boston SVM</cell><cell>MSE</cell><cell cols="2">91.009871 467.963686</cell></row><row><cell>BOHB-BB</cell><cell>Boston SVM</cell><cell>MSE</cell><cell cols="2">90.350500 127.546283</cell></row><row><cell>Random-search</cell><cell>Boston SVM</cell><cell>MSE</cell><cell cols="2">83.204838 135.308788</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table 26 :</head><label>26</label><figDesc>Boston with MSE loss for tuning Ada model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell cols="3">Dataset Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell>Boston Ada</cell><cell>MSE</cell><cell>96.942494</cell><cell>18.658049</cell></row><row><cell>Opentuner</cell><cell>Boston Ada</cell><cell>MSE</cell><cell>94.274205</cell><cell>34.779775</cell></row><row><cell>TuRBO</cell><cell>Boston Ada</cell><cell>MSE</cell><cell>94.131115</cell><cell>43.476954</cell></row><row><cell cols="2">Nevergrad (1+1) Boston Ada</cell><cell>MSE</cell><cell cols="2">92.333218 117.061847</cell></row><row><cell>TuRBO+</cell><cell>Boston Ada</cell><cell>MSE</cell><cell>91.017786</cell><cell>86.796910</cell></row><row><cell>Hyperopt</cell><cell>Boston Ada</cell><cell>MSE</cell><cell>89.721591</cell><cell>41.637778</cell></row><row><cell>PySOT</cell><cell>Boston Ada</cell><cell>MSE</cell><cell>88.996278</cell><cell>29.924305</cell></row><row><cell>Skopt</cell><cell>Boston Ada</cell><cell>MSE</cell><cell>87.835681</cell><cell>55.855654</cell></row><row><cell>BOHB-BB</cell><cell>Boston Ada</cell><cell>MSE</cell><cell>82.947338</cell><cell>67.514235</cell></row><row><cell>Random-search</cell><cell>Boston Ada</cell><cell>MSE</cell><cell>80.119051</cell><cell>17.165168</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head>Table 27 :</head><label>27</label><figDesc>Boston with MSE loss for tuning Knn model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell cols="3">Dataset Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell>Boston Knn</cell><cell>MSE</cell><cell>100.000000</cell><cell>0.000000</cell></row><row><cell>Opentuner</cell><cell>Boston Knn</cell><cell>MSE</cell><cell>100.000000</cell><cell>0.000000</cell></row><row><cell>PySOT</cell><cell>Boston Knn</cell><cell>MSE</cell><cell>100.000000</cell><cell>0.000000</cell></row><row><cell>TuRBO</cell><cell>Boston Knn</cell><cell>MSE</cell><cell>100.000000</cell><cell>0.000000</cell></row><row><cell>TuRBO+</cell><cell>Boston Knn</cell><cell>MSE</cell><cell>99.704490</cell><cell>1.253051</cell></row><row><cell>Hyperopt</cell><cell>Boston Knn</cell><cell>MSE</cell><cell>99.319467</cell><cell>6.373596</cell></row><row><cell>Random-search</cell><cell>Boston Knn</cell><cell>MSE</cell><cell>99.068697</cell><cell>7.272026</cell></row><row><cell>Skopt</cell><cell>Boston Knn</cell><cell>MSE</cell><cell>99.014267</cell><cell>19.433403</cell></row><row><cell>BOHB-BB</cell><cell>Boston Knn</cell><cell>MSE</cell><cell>96.129465</cell><cell>62.658638</cell></row><row><cell cols="2">Nevergrad (1+1) Boston Knn</cell><cell>MSE</cell><cell cols="2">88.882774 227.331571</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_29"><head>Table 28 :</head><label>28</label><figDesc>Boston with MSE loss for tuning Lasso model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell cols="2">Dataset Model Metric Normalised Score Variance</cell></row><row><cell>HEBO</cell><cell>Boston Lasso MSE</cell><cell>100.037610 0.000010</cell></row><row><cell>PySOT</cell><cell>Boston Lasso MSE</cell><cell>100.008571 0.000269</cell></row><row><cell>Hyperopt</cell><cell>Boston Lasso MSE</cell><cell>100.007350 0.000123</cell></row><row><cell>TuRBO</cell><cell>Boston Lasso MSE</cell><cell>99.982366 0.021257</cell></row><row><cell>Skopt</cell><cell>Boston Lasso MSE</cell><cell>99.806671 0.433679</cell></row><row><cell>BOHB-BB</cell><cell>Boston Lasso MSE</cell><cell>99.679375 0.206443</cell></row><row><cell cols="2">Nevergrad (1+1) Boston Lasso MSE</cell><cell>99.372815 4.020588</cell></row><row><cell>Random-search</cell><cell>Boston Lasso MSE</cell><cell>99.325799 0.769118</cell></row><row><cell>TuRBO+</cell><cell>Boston Lasso MSE</cell><cell>98.971017 2.235443</cell></row><row><cell>Opentuner</cell><cell>Boston Lasso MSE</cell><cell>97.388970 6.008643</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_30"><head>Table 29 :</head><label>29</label><figDesc>Boston with MSE loss for tuning Linear model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell cols="2">Dataset Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell>Boston Linear MSE</cell><cell>100.000001</cell><cell>3.207627e-10</cell></row><row><cell>Hyperopt</cell><cell>Boston Linear MSE</cell><cell>99.999348</cell><cell>1.340314e-06</cell></row><row><cell>PySOT</cell><cell>Boston Linear MSE</cell><cell>99.994196</cell><cell>1.378338e-04</cell></row><row><cell>Skopt</cell><cell>Boston Linear MSE</cell><cell>99.986643</cell><cell>6.872575e-04</cell></row><row><cell cols="2">Nevergrad (1+1) Boston Linear MSE</cell><cell>99.976999</cell><cell>5.017296e-03</cell></row><row><cell>TuRBO</cell><cell>Boston Linear MSE</cell><cell>99.889263</cell><cell>2.451020e-01</cell></row><row><cell>BOHB-BB</cell><cell>Boston Linear MSE</cell><cell>99.837413</cell><cell>2.455636e-01</cell></row><row><cell>Random-search</cell><cell>Boston Linear MSE</cell><cell>99.802681</cell><cell>2.731228e-01</cell></row><row><cell>TuRBO+</cell><cell>Boston Linear MSE</cell><cell cols="2">99.332841 1.085780e+00</cell></row><row><cell>Opentuner</cell><cell>Boston Linear MSE</cell><cell cols="2">98.545654 1.344348e+00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_31"><head>Table 30 :</head><label>30</label><figDesc>Breast cancer dataset with ACC loss for tuning DT model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell>Dataset</cell><cell cols="3">Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell cols="2">Breast cancer dataset DT</cell><cell>ACC</cell><cell>106.694915 52.457703</cell></row><row><cell>PySOT</cell><cell cols="2">Breast cancer dataset DT</cell><cell>ACC</cell><cell>103.220339 32.930646</cell></row><row><cell>TuRBO</cell><cell cols="2">Breast cancer dataset DT</cell><cell>ACC</cell><cell>99.830508 32.930646</cell></row><row><cell cols="3">Nevergrad (1+1) Breast cancer dataset DT</cell><cell>ACC</cell><cell>99.661017 47.052420</cell></row><row><cell>Skopt</cell><cell cols="2">Breast cancer dataset DT</cell><cell>ACC</cell><cell>99.322034 27.033974</cell></row><row><cell>Opentuner</cell><cell cols="2">Breast cancer dataset DT</cell><cell>ACC</cell><cell>97.457627 19.201984</cell></row><row><cell>Hyperopt</cell><cell cols="2">Breast cancer dataset DT</cell><cell>ACC</cell><cell>97.288136 17.357384</cell></row><row><cell>TuRBO+</cell><cell cols="2">Breast cancer dataset DT</cell><cell>ACC</cell><cell>96.610169 26.913017</cell></row><row><cell>Random-search</cell><cell cols="2">Breast cancer dataset DT</cell><cell>ACC</cell><cell>93.813559 29.415322</cell></row><row><cell>BOHB-BB</cell><cell cols="2">Breast cancer dataset DT</cell><cell>ACC</cell><cell>93.305085 22.823145</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_32"><head>Table 31 :</head><label>31</label><figDesc>Breast cancer dataset with ACC loss for tuning MLP-adam model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is Hyperopt.</figDesc><table><row><cell>Algorithm</cell><cell>Dataset</cell><cell>Model</cell><cell cols="2">Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>Hyperopt</cell><cell cols="3">Breast cancer dataset MLP-adam ACC</cell><cell cols="2">99.078947 14.779851</cell></row><row><cell>HEBO</cell><cell cols="3">Breast cancer dataset MLP-adam ACC</cell><cell cols="2">98.684211 20.046654</cell></row><row><cell cols="4">Nevergrad (1+1) Breast cancer dataset MLP-adam ACC</cell><cell cols="2">96.973684 53.415221</cell></row><row><cell>PySOT</cell><cell cols="3">Breast cancer dataset MLP-adam ACC</cell><cell cols="2">96.578947 11.736405</cell></row><row><cell>TuRBO</cell><cell cols="3">Breast cancer dataset MLP-adam ACC</cell><cell cols="2">96.447368 17.695728</cell></row><row><cell>TuRBO+</cell><cell cols="3">Breast cancer dataset MLP-adam ACC</cell><cell cols="2">96.447368 18.424697</cell></row><row><cell>Skopt</cell><cell cols="3">Breast cancer dataset MLP-adam ACC</cell><cell cols="2">95.921053 20.757399</cell></row><row><cell>Opentuner</cell><cell cols="3">Breast cancer dataset MLP-adam ACC</cell><cell cols="2">95.000000 21.067211</cell></row><row><cell>Random-search</cell><cell cols="3">Breast cancer dataset MLP-adam ACC</cell><cell cols="2">94.078947 13.759294</cell></row><row><cell>BOHB-BB</cell><cell cols="3">Breast cancer dataset MLP-adam ACC</cell><cell cols="2">93.684211 11.955095</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_33"><head>Table 32 :</head><label>32</label><figDesc>Breast cancer dataset with ACC loss for tuning MLP-SGD model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell>Dataset</cell><cell>Model</cell><cell cols="2">Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell cols="3">Breast cancer dataset MLP-SGD ACC</cell><cell>100.847458</cell><cell>1.209574</cell></row><row><cell>TuRBO</cell><cell cols="3">Breast cancer dataset MLP-SGD ACC</cell><cell>99.491525</cell><cell>1.391010</cell></row><row><cell>Skopt</cell><cell cols="3">Breast cancer dataset MLP-SGD ACC</cell><cell>99.237288</cell><cell>0.824022</cell></row><row><cell>PySOT</cell><cell cols="3">Breast cancer dataset MLP-SGD ACC</cell><cell>99.067797</cell><cell>1.504407</cell></row><row><cell>Hyperopt</cell><cell cols="3">Breast cancer dataset MLP-SGD ACC</cell><cell>98.771186</cell><cell>1.396680</cell></row><row><cell cols="4">Nevergrad (1+1) Breast cancer dataset MLP-SGD ACC</cell><cell>98.771186</cell><cell>4.647409</cell></row><row><cell>TuRBO+</cell><cell cols="3">Breast cancer dataset MLP-SGD ACC</cell><cell>98.559322</cell><cell>2.502306</cell></row><row><cell>Random-search</cell><cell cols="3">Breast cancer dataset MLP-SGD ACC</cell><cell>96.864407</cell><cell>1.973117</cell></row><row><cell>BOHB-BB</cell><cell cols="3">Breast cancer dataset MLP-SGD ACC</cell><cell>96.355932</cell><cell>3.787478</cell></row><row><cell>Opentuner</cell><cell cols="3">Breast cancer dataset MLP-SGD ACC</cell><cell cols="2">84.703390 190.241386</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_34"><head>Table 33 :</head><label>33</label><figDesc>Breast cancer dataset with ACC loss for tuning RF model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is Skopt.</figDesc><table><row><cell>Algorithm</cell><cell>Dataset</cell><cell cols="3">Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>Skopt</cell><cell cols="2">Breast cancer dataset RF</cell><cell>ACC</cell><cell>98.846154</cell><cell>5.985365</cell></row><row><cell>HEBO</cell><cell cols="2">Breast cancer dataset RF</cell><cell>ACC</cell><cell>98.356643</cell><cell>8.250307</cell></row><row><cell>PySOT</cell><cell cols="2">Breast cancer dataset RF</cell><cell>ACC</cell><cell>97.377622</cell><cell>5.964775</cell></row><row><cell>Hyperopt</cell><cell cols="2">Breast cancer dataset RF</cell><cell>ACC</cell><cell>96.888112</cell><cell>3.988099</cell></row><row><cell>TuRBO</cell><cell cols="2">Breast cancer dataset RF</cell><cell>ACC</cell><cell>96.853147</cell><cell>8.416317</cell></row><row><cell cols="3">Nevergrad (1+1) Breast cancer dataset RF</cell><cell>ACC</cell><cell cols="2">96.083916 13.249908</cell></row><row><cell>TuRBO+</cell><cell cols="2">Breast cancer dataset RF</cell><cell>ACC</cell><cell>95.419580</cell><cell>6.047137</cell></row><row><cell>BOHB-BB</cell><cell cols="2">Breast cancer dataset RF</cell><cell>ACC</cell><cell>94.335664</cell><cell>2.980457</cell></row><row><cell>Opentuner</cell><cell cols="2">Breast cancer dataset RF</cell><cell>ACC</cell><cell>93.636364</cell><cell>3.289313</cell></row><row><cell>Random-search</cell><cell cols="2">Breast cancer dataset RF</cell><cell>ACC</cell><cell>93.321678</cell><cell>2.495296</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_35"><head>Table 34 :</head><label>34</label><figDesc>Breast cancer dataset with ACC loss for tuning SVM model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is TuRBO.</figDesc><table><row><cell>Algorithm</cell><cell>Dataset</cell><cell cols="3">Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>TuRBO</cell><cell cols="2">Breast cancer dataset SVM</cell><cell>ACC</cell><cell>89.285714</cell><cell>40.279270</cell></row><row><cell>Hyperopt</cell><cell cols="2">Breast cancer dataset SVM</cell><cell>ACC</cell><cell>86.428571</cell><cell>53.168636</cell></row><row><cell>TuRBO+</cell><cell cols="2">Breast cancer dataset SVM</cell><cell>ACC</cell><cell>86.428571</cell><cell>96.133190</cell></row><row><cell>PySOT</cell><cell cols="2">Breast cancer dataset SVM</cell><cell>ACC</cell><cell>85.714286</cell><cell>42.964554</cell></row><row><cell>HEBO</cell><cell cols="2">Breast cancer dataset SVM</cell><cell>ACC</cell><cell>84.285714</cell><cell>40.816327</cell></row><row><cell>BOHB-BB</cell><cell cols="2">Breast cancer dataset SVM</cell><cell>ACC</cell><cell cols="2">80.714286 113.319012</cell></row><row><cell>Opentuner</cell><cell cols="2">Breast cancer dataset SVM</cell><cell>ACC</cell><cell cols="2">78.571429 182.599356</cell></row><row><cell cols="3">Nevergrad (1+1) Breast cancer dataset SVM</cell><cell>ACC</cell><cell cols="2">77.142857 481.203007</cell></row><row><cell>Random-search</cell><cell cols="2">Breast cancer dataset SVM</cell><cell>ACC</cell><cell>76.428571</cell><cell>48.872180</cell></row><row><cell>Skopt</cell><cell cols="2">Breast cancer dataset SVM</cell><cell>ACC</cell><cell cols="2">76.428571 349.624060</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_36"><head>Table 35 :</head><label>35</label><figDesc>Breast cancer dataset with ACC loss for tuning Ada model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell>Dataset</cell><cell cols="3">Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell cols="2">Breast cancer dataset Ada</cell><cell>ACC</cell><cell>99.06250</cell><cell>4.214638</cell></row><row><cell>Opentuner</cell><cell cols="2">Breast cancer dataset Ada</cell><cell>ACC</cell><cell>98.59375</cell><cell>6.656044</cell></row><row><cell>TuRBO</cell><cell cols="2">Breast cancer dataset Ada</cell><cell>ACC</cell><cell>98.43750</cell><cell>3.597862</cell></row><row><cell>TuRBO+</cell><cell cols="2">Breast cancer dataset Ada</cell><cell>ACC</cell><cell>97.96875</cell><cell>8.506373</cell></row><row><cell cols="3">Nevergrad (1+1) Breast cancer dataset Ada</cell><cell>ACC</cell><cell cols="2">97.65625 12.207031</cell></row><row><cell>PySOT</cell><cell cols="2">Breast cancer dataset Ada</cell><cell>ACC</cell><cell>97.03125</cell><cell>5.628084</cell></row><row><cell>Hyperopt</cell><cell cols="2">Breast cancer dataset Ada</cell><cell>ACC</cell><cell>96.87500</cell><cell>8.223684</cell></row><row><cell>Random-search</cell><cell cols="2">Breast cancer dataset Ada</cell><cell>ACC</cell><cell>95.31250</cell><cell>6.681743</cell></row><row><cell>BOHB-BB</cell><cell cols="2">Breast cancer dataset Ada</cell><cell>ACC</cell><cell>94.53125</cell><cell>3.983347</cell></row><row><cell>Skopt</cell><cell cols="2">Breast cancer dataset Ada</cell><cell>ACC</cell><cell>93.12500</cell><cell>9.868421</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_37"><head>Table 36 :</head><label>36</label><figDesc>Breast cancer dataset with ACC loss for tuning Knn model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell>Dataset</cell><cell cols="3">Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell cols="2">Breast cancer dataset Knn</cell><cell>ACC</cell><cell>100.000</cell><cell>0.000000</cell></row><row><cell>Opentuner</cell><cell cols="2">Breast cancer dataset Knn</cell><cell>ACC</cell><cell>100.000</cell><cell>0.000000</cell></row><row><cell>Hyperopt</cell><cell cols="2">Breast cancer dataset Knn</cell><cell>ACC</cell><cell cols="2">97.500 26.315789</cell></row><row><cell cols="3">Nevergrad (1+1) Breast cancer dataset Knn</cell><cell>ACC</cell><cell cols="2">95.625 53.865132</cell></row><row><cell>Random-search</cell><cell cols="2">Breast cancer dataset Knn</cell><cell>ACC</cell><cell cols="2">95.625 53.865132</cell></row><row><cell>Skopt</cell><cell cols="2">Breast cancer dataset Knn</cell><cell>ACC</cell><cell cols="2">95.000 55.921053</cell></row><row><cell>BOHB-BB</cell><cell cols="2">Breast cancer dataset Knn</cell><cell>ACC</cell><cell cols="2">95.000 39.473684</cell></row><row><cell>TuRBO+</cell><cell cols="2">Breast cancer dataset Knn</cell><cell>ACC</cell><cell cols="2">92.500 55.921053</cell></row><row><cell>TuRBO</cell><cell cols="2">Breast cancer dataset Knn</cell><cell>ACC</cell><cell cols="2">91.875 37.417763</cell></row><row><cell>PySOT</cell><cell cols="2">Breast cancer dataset Knn</cell><cell>ACC</cell><cell cols="2">91.250 34.539474</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_38"><head>Table 37 :</head><label>37</label><figDesc>Breast cancer dataset with ACC loss for tuning Lasso model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is PySOT.</figDesc><table><row><cell>Algorithm</cell><cell>Dataset</cell><cell cols="2">Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>PySOT</cell><cell cols="2">Breast cancer dataset Lasso ACC</cell><cell>95.000000</cell><cell>32.163743</cell></row><row><cell>TuRBO</cell><cell cols="2">Breast cancer dataset Lasso ACC</cell><cell>93.888889</cell><cell>32.163743</cell></row><row><cell>Opentuner</cell><cell cols="2">Breast cancer dataset Lasso ACC</cell><cell>93.888889</cell><cell>32.163743</cell></row><row><cell>HEBO</cell><cell cols="2">Breast cancer dataset Lasso ACC</cell><cell>93.333333</cell><cell>31.189084</cell></row><row><cell>TuRBO+</cell><cell cols="2">Breast cancer dataset Lasso ACC</cell><cell>92.222222</cell><cell>27.290448</cell></row><row><cell>Hyperopt</cell><cell cols="2">Breast cancer dataset Lasso ACC</cell><cell>90.555556</cell><cell>16.569201</cell></row><row><cell>Random-search</cell><cell cols="2">Breast cancer dataset Lasso ACC</cell><cell>90.555556</cell><cell>16.569201</cell></row><row><cell>BOHB-BB</cell><cell cols="2">Breast cancer dataset Lasso ACC</cell><cell>90.000000</cell><cell>11.695906</cell></row><row><cell cols="3">Nevergrad (1+1) Breast cancer dataset Lasso ACC</cell><cell>88.333333</cell><cell>58.154646</cell></row><row><cell>Skopt</cell><cell cols="2">Breast cancer dataset Lasso ACC</cell><cell cols="2">87.777778 180.636777</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_39"><head>Table 38 :</head><label>38</label><figDesc>Breast cancer dataset with ACC loss for tuning Linear model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell>Dataset</cell><cell cols="2">Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell cols="2">Breast cancer dataset Linear ACC</cell><cell>107.0</cell><cell>95.789474</cell></row><row><cell cols="3">Nevergrad (1+1) Breast cancer dataset Linear ACC</cell><cell cols="2">103.0 390.526316</cell></row><row><cell>TuRBO</cell><cell cols="2">Breast cancer dataset Linear ACC</cell><cell cols="2">99.0 188.421053</cell></row><row><cell>PySOT</cell><cell cols="2">Breast cancer dataset Linear ACC</cell><cell cols="2">98.0 164.210526</cell></row><row><cell>TuRBO+</cell><cell cols="2">Breast cancer dataset Linear ACC</cell><cell cols="2">98.0 164.210526</cell></row><row><cell>Opentuner</cell><cell cols="2">Breast cancer dataset Linear ACC</cell><cell cols="2">98.0 290.526316</cell></row><row><cell>Hyperopt</cell><cell cols="2">Breast cancer dataset Linear ACC</cell><cell cols="2">96.0 109.473684</cell></row><row><cell>Skopt</cell><cell cols="2">Breast cancer dataset Linear ACC</cell><cell cols="2">95.0 205.263158</cell></row><row><cell>Random-search</cell><cell cols="2">Breast cancer dataset Linear ACC</cell><cell cols="2">89.0 146.315789</cell></row><row><cell>BOHB-BB</cell><cell cols="2">Breast cancer dataset Linear ACC</cell><cell>85.0</cell><cell>78.947368</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_40"><head>Table 39 :</head><label>39</label><figDesc>Breast cancer dataset with NLL loss for tuning DT model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell>Dataset</cell><cell cols="3">Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell cols="2">Breast cancer dataset DT</cell><cell>NLL</cell><cell>111.047413</cell><cell>51.627976</cell></row><row><cell>TuRBO+</cell><cell cols="2">Breast cancer dataset DT</cell><cell>NLL</cell><cell cols="2">98.912425 121.134669</cell></row><row><cell>PySOT</cell><cell cols="2">Breast cancer dataset DT</cell><cell>NLL</cell><cell cols="2">98.696805 115.036349</cell></row><row><cell>Hyperopt</cell><cell cols="2">Breast cancer dataset DT</cell><cell>NLL</cell><cell>98.201008</cell><cell>75.868808</cell></row><row><cell>Skopt</cell><cell cols="2">Breast cancer dataset DT</cell><cell>NLL</cell><cell cols="2">97.647135 165.617171</cell></row><row><cell>TuRBO</cell><cell cols="2">Breast cancer dataset DT</cell><cell>NLL</cell><cell cols="2">95.406797 230.498379</cell></row><row><cell>Opentuner</cell><cell cols="2">Breast cancer dataset DT</cell><cell>NLL</cell><cell>91.076386</cell><cell>92.789945</cell></row><row><cell cols="3">Nevergrad (1+1) Breast cancer dataset DT</cell><cell>NLL</cell><cell>90.579282</cell><cell>92.638289</cell></row><row><cell>BOHB-BB</cell><cell cols="2">Breast cancer dataset DT</cell><cell>NLL</cell><cell>90.418909</cell><cell>43.016544</cell></row><row><cell>Random-search</cell><cell cols="2">Breast cancer dataset DT</cell><cell>NLL</cell><cell>87.752435</cell><cell>64.953742</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_41"><head>Table 40 :</head><label>40</label><figDesc>Breast cancer dataset with NLL loss for tuning MLP-adam model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell>Dataset</cell><cell>Model</cell><cell cols="2">Metric Normalised Score Variance</cell></row><row><cell>HEBO</cell><cell cols="3">Breast cancer dataset MLP-adam NLL</cell><cell>100.303210 0.252546</cell></row><row><cell>TuRBO</cell><cell cols="3">Breast cancer dataset MLP-adam NLL</cell><cell>100.260440 0.386150</cell></row><row><cell>PySOT</cell><cell cols="3">Breast cancer dataset MLP-adam NLL</cell><cell>99.965212 0.410841</cell></row><row><cell>Hyperopt</cell><cell cols="3">Breast cancer dataset MLP-adam NLL</cell><cell>99.778153 0.295595</cell></row><row><cell>Opentuner</cell><cell cols="3">Breast cancer dataset MLP-adam NLL</cell><cell>99.614006 0.559840</cell></row><row><cell>Skopt</cell><cell cols="3">Breast cancer dataset MLP-adam NLL</cell><cell>99.422393 0.444736</cell></row><row><cell>TuRBO+</cell><cell cols="3">Breast cancer dataset MLP-adam NLL</cell><cell>99.298544 1.362840</cell></row><row><cell cols="4">Nevergrad (1+1) Breast cancer dataset MLP-adam NLL</cell><cell>98.865542 5.064746</cell></row><row><cell>Random-search</cell><cell cols="3">Breast cancer dataset MLP-adam NLL</cell><cell>98.538089 1.227034</cell></row><row><cell>BOHB-BB</cell><cell cols="3">Breast cancer dataset MLP-adam NLL</cell><cell>98.396155 0.577588</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_42"><head>Table 41 :</head><label>41</label><figDesc>Breast cancer dataset with NLL loss for tuning MLP-SGD model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell>Dataset</cell><cell>Model</cell><cell cols="2">Metric Normalised Score Variance</cell></row><row><cell>HEBO</cell><cell cols="3">Breast cancer dataset MLP-SGD NLL</cell><cell>100.721473 0.061946</cell></row><row><cell>TuRBO</cell><cell cols="3">Breast cancer dataset MLP-SGD NLL</cell><cell>100.162853 1.622656</cell></row><row><cell>PySOT</cell><cell cols="3">Breast cancer dataset MLP-SGD NLL</cell><cell>100.091236 0.153482</cell></row><row><cell>Hyperopt</cell><cell cols="3">Breast cancer dataset MLP-SGD NLL</cell><cell>100.049807 0.198361</cell></row><row><cell>TuRBO+</cell><cell cols="3">Breast cancer dataset MLP-SGD NLL</cell><cell>99.717751 0.569165</cell></row><row><cell>Skopt</cell><cell cols="3">Breast cancer dataset MLP-SGD NLL</cell><cell>99.682763 0.194010</cell></row><row><cell>Random-search</cell><cell cols="3">Breast cancer dataset MLP-SGD NLL</cell><cell>99.084890 0.573484</cell></row><row><cell>BOHB-BB</cell><cell cols="3">Breast cancer dataset MLP-SGD NLL</cell><cell>99.061649 0.934630</cell></row><row><cell cols="4">Nevergrad (1+1) Breast cancer dataset MLP-SGD NLL</cell><cell>98.762368 4.114520</cell></row><row><cell>Opentuner</cell><cell cols="3">Breast cancer dataset MLP-SGD NLL</cell><cell>97.518858 6.031878</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_43"><head>Table 42 :</head><label>42</label><figDesc>Breast cancer dataset with NLL loss for tuning RF model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell>Dataset</cell><cell cols="3">Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell cols="2">Breast cancer dataset RF</cell><cell>NLL</cell><cell>104.284930</cell><cell>1.565493</cell></row><row><cell>PySOT</cell><cell cols="2">Breast cancer dataset RF</cell><cell>NLL</cell><cell>102.796116</cell><cell>4.187314</cell></row><row><cell>Skopt</cell><cell cols="2">Breast cancer dataset RF</cell><cell>NLL</cell><cell>101.998789</cell><cell>4.898058</cell></row><row><cell>Hyperopt</cell><cell cols="2">Breast cancer dataset RF</cell><cell>NLL</cell><cell cols="2">98.198100 10.076521</cell></row><row><cell>TuRBO</cell><cell cols="2">Breast cancer dataset RF</cell><cell>NLL</cell><cell cols="2">97.942753 25.132499</cell></row><row><cell cols="3">Nevergrad (1+1) Breast cancer dataset RF</cell><cell>NLL</cell><cell cols="2">96.820234 31.448689</cell></row><row><cell>TuRBO+</cell><cell cols="2">Breast cancer dataset RF</cell><cell>NLL</cell><cell cols="2">96.029192 12.874949</cell></row><row><cell>Opentuner</cell><cell cols="2">Breast cancer dataset RF</cell><cell>NLL</cell><cell cols="2">92.790267 10.042108</cell></row><row><cell>BOHB-BB</cell><cell cols="2">Breast cancer dataset RF</cell><cell>NLL</cell><cell>92.660352</cell><cell>5.982402</cell></row><row><cell>Random-search</cell><cell cols="2">Breast cancer dataset RF</cell><cell>NLL</cell><cell>92.434443</cell><cell>6.196296</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_44"><head>Table 43 :</head><label>43</label><figDesc>Breast cancer dataset with NLL loss for tuning SVM model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell>Dataset</cell><cell cols="3">Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell cols="2">Breast cancer dataset SVM</cell><cell>NLL</cell><cell>100.999289</cell><cell>0.870906</cell></row><row><cell>TuRBO</cell><cell cols="2">Breast cancer dataset SVM</cell><cell>NLL</cell><cell>100.331139</cell><cell>0.502305</cell></row><row><cell>PySOT</cell><cell cols="2">Breast cancer dataset SVM</cell><cell>NLL</cell><cell>100.038823</cell><cell>0.614796</cell></row><row><cell>Hyperopt</cell><cell cols="2">Breast cancer dataset SVM</cell><cell>NLL</cell><cell>99.238507</cell><cell>1.374858</cell></row><row><cell>Opentuner</cell><cell cols="2">Breast cancer dataset SVM</cell><cell>NLL</cell><cell>99.130932</cell><cell>3.472981</cell></row><row><cell>TuRBO+</cell><cell cols="2">Breast cancer dataset SVM</cell><cell>NLL</cell><cell>98.787566</cell><cell>7.629457</cell></row><row><cell cols="3">Nevergrad (1+1) Breast cancer dataset SVM</cell><cell>NLL</cell><cell cols="2">98.258990 41.913612</cell></row><row><cell>Skopt</cell><cell cols="2">Breast cancer dataset SVM</cell><cell>NLL</cell><cell cols="2">98.153794 16.959059</cell></row><row><cell>BOHB-BB</cell><cell cols="2">Breast cancer dataset SVM</cell><cell>NLL</cell><cell>97.569333</cell><cell>1.733710</cell></row><row><cell>Random-search</cell><cell cols="2">Breast cancer dataset SVM</cell><cell>NLL</cell><cell>97.191463</cell><cell>1.506776</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_45"><head>Table 44 :</head><label>44</label><figDesc>Breast cancer dataset with NLL loss for tuning Ada model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell>Dataset</cell><cell cols="3">Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell cols="2">Breast cancer dataset Ada</cell><cell>NLL</cell><cell>103.298980</cell><cell>1.970935</cell></row><row><cell>TuRBO</cell><cell cols="2">Breast cancer dataset Ada</cell><cell>NLL</cell><cell>100.445760</cell><cell>7.172282</cell></row><row><cell>PySOT</cell><cell cols="2">Breast cancer dataset Ada</cell><cell>NLL</cell><cell>100.402760</cell><cell>4.365384</cell></row><row><cell>Skopt</cell><cell cols="2">Breast cancer dataset Ada</cell><cell>NLL</cell><cell>98.923004</cell><cell>5.054022</cell></row><row><cell cols="3">Nevergrad (1+1) Breast cancer dataset Ada</cell><cell>NLL</cell><cell>98.874005</cell><cell>6.293192</cell></row><row><cell>Hyperopt</cell><cell cols="2">Breast cancer dataset Ada</cell><cell>NLL</cell><cell>98.645011</cell><cell>1.948260</cell></row><row><cell>TuRBO+</cell><cell cols="2">Breast cancer dataset Ada</cell><cell>NLL</cell><cell>98.474514</cell><cell>3.187387</cell></row><row><cell>Random-search</cell><cell cols="2">Breast cancer dataset Ada</cell><cell>NLL</cell><cell>97.922922</cell><cell>2.325453</cell></row><row><cell>BOHB-BB</cell><cell cols="2">Breast cancer dataset Ada</cell><cell>NLL</cell><cell>97.671007</cell><cell>2.051746</cell></row><row><cell>Opentuner</cell><cell cols="2">Breast cancer dataset Ada</cell><cell>NLL</cell><cell cols="2">96.538817 164.478434</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_46"><head>Table 45 :</head><label>45</label><figDesc>Breast cancer dataset with NLL loss for tuning Knn model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell>Dataset</cell><cell cols="3">Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell cols="2">Breast cancer dataset Knn</cell><cell>NLL</cell><cell>100.000000</cell><cell>0.000000</cell></row><row><cell>Opentuner</cell><cell cols="2">Breast cancer dataset Knn</cell><cell>NLL</cell><cell>100.000000</cell><cell>0.000000</cell></row><row><cell>TuRBO</cell><cell cols="2">Breast cancer dataset Knn</cell><cell>NLL</cell><cell>99.835048</cell><cell>0.257772</cell></row><row><cell>Hyperopt</cell><cell cols="2">Breast cancer dataset Knn</cell><cell>NLL</cell><cell>99.815764</cell><cell>0.321563</cell></row><row><cell>Random-search</cell><cell cols="2">Breast cancer dataset Knn</cell><cell>NLL</cell><cell>99.248073</cell><cell>0.729147</cell></row><row><cell>BOHB-BB</cell><cell cols="2">Breast cancer dataset Knn</cell><cell>NLL</cell><cell>99.121241</cell><cell>2.900580</cell></row><row><cell>PySOT</cell><cell cols="2">Breast cancer dataset Knn</cell><cell>NLL</cell><cell>98.175743</cell><cell>5.063927</cell></row><row><cell cols="3">Nevergrad (1+1) Breast cancer dataset Knn</cell><cell>NLL</cell><cell>97.334954</cell><cell>8.601850</cell></row><row><cell>TuRBO+</cell><cell cols="2">Breast cancer dataset Knn</cell><cell>NLL</cell><cell cols="2">95.752544 121.788536</cell></row><row><cell>Skopt</cell><cell cols="2">Breast cancer dataset Knn</cell><cell>NLL</cell><cell cols="2">85.667024 457.645212</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_47"><head>Table 46 :</head><label>46</label><figDesc>Breast cancer dataset with NLL loss for tuning Lasso model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is PySOT.</figDesc><table><row><cell>Algorithm</cell><cell>Dataset</cell><cell cols="2">Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>PySOT</cell><cell cols="2">Breast cancer dataset Lasso NLL</cell><cell>92.444124</cell><cell>41.420464</cell></row><row><cell>Opentuner</cell><cell cols="2">Breast cancer dataset Lasso NLL</cell><cell>92.364922</cell><cell>52.804059</cell></row><row><cell>Hyperopt</cell><cell cols="2">Breast cancer dataset Lasso NLL</cell><cell>90.483697</cell><cell>40.153553</cell></row><row><cell>TuRBO+</cell><cell cols="2">Breast cancer dataset Lasso NLL</cell><cell>89.669648</cell><cell>32.541741</cell></row><row><cell>TuRBO</cell><cell cols="2">Breast cancer dataset Lasso NLL</cell><cell>89.275892</cell><cell>40.406114</cell></row><row><cell>Random-search</cell><cell cols="2">Breast cancer dataset Lasso NLL</cell><cell>89.267387</cell><cell>86.949148</cell></row><row><cell>HEBO</cell><cell cols="2">Breast cancer dataset Lasso NLL</cell><cell>87.648453</cell><cell>9.938537</cell></row><row><cell cols="3">Nevergrad (1+1) Breast cancer dataset Lasso NLL</cell><cell>84.512735</cell><cell>81.507309</cell></row><row><cell>BOHB-BB</cell><cell cols="2">Breast cancer dataset Lasso NLL</cell><cell>81.089709</cell><cell>45.062520</cell></row><row><cell>Skopt</cell><cell cols="2">Breast cancer dataset Lasso NLL</cell><cell cols="2">80.270494 104.357765</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_48"><head>Table 47 :</head><label>47</label><figDesc>Breast cancer dataset with NLL loss for tuning Linear model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell>Dataset</cell><cell cols="2">Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell cols="2">Breast cancer dataset Linear NLL</cell><cell>97.097724</cell><cell>4.317349</cell></row><row><cell>TuRBO</cell><cell cols="2">Breast cancer dataset Linear NLL</cell><cell cols="2">96.073657 20.501975</cell></row><row><cell>PySOT</cell><cell cols="2">Breast cancer dataset Linear NLL</cell><cell>95.497805</cell><cell>3.788872</cell></row><row><cell>Opentuner</cell><cell cols="2">Breast cancer dataset Linear NLL</cell><cell>94.970576</cell><cell>5.694999</cell></row><row><cell>Skopt</cell><cell cols="2">Breast cancer dataset Linear NLL</cell><cell>93.683985</cell><cell>8.175137</cell></row><row><cell>TuRBO+</cell><cell cols="2">Breast cancer dataset Linear NLL</cell><cell cols="2">93.460437 18.265925</cell></row><row><cell>Hyperopt</cell><cell cols="2">Breast cancer dataset Linear NLL</cell><cell>92.065782</cell><cell>7.999446</cell></row><row><cell cols="3">Nevergrad (1+1) Breast cancer dataset Linear NLL</cell><cell cols="2">91.268968 86.801548</cell></row><row><cell>Random-search</cell><cell cols="2">Breast cancer dataset Linear NLL</cell><cell>90.477323</cell><cell>8.865931</cell></row><row><cell>BOHB-BB</cell><cell cols="2">Breast cancer dataset Linear NLL</cell><cell cols="2">89.401005 15.561542</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_49"><head>Table 48 :</head><label>48</label><figDesc>Diabetes with MAE loss for tuning DT model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell>Dataset</cell><cell cols="3">Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell cols="2">Diabetes DT</cell><cell>MAE</cell><cell>96.638714</cell><cell>19.424434</cell></row><row><cell>Skopt</cell><cell cols="2">Diabetes DT</cell><cell>MAE</cell><cell>96.283909</cell><cell>12.675138</cell></row><row><cell>TuRBO</cell><cell cols="2">Diabetes DT</cell><cell>MAE</cell><cell>96.161335</cell><cell>22.916642</cell></row><row><cell>PySOT</cell><cell cols="2">Diabetes DT</cell><cell>MAE</cell><cell>95.330459</cell><cell>14.421246</cell></row><row><cell>Hyperopt</cell><cell cols="2">Diabetes DT</cell><cell>MAE</cell><cell>92.694296</cell><cell>30.442937</cell></row><row><cell>TuRBO+</cell><cell cols="2">Diabetes DT</cell><cell>MAE</cell><cell>92.417202</cell><cell>24.654829</cell></row><row><cell>Random-search</cell><cell cols="2">Diabetes DT</cell><cell>MAE</cell><cell>89.414992</cell><cell>33.396944</cell></row><row><cell>BOHB-BB</cell><cell cols="2">Diabetes DT</cell><cell>MAE</cell><cell>88.578413</cell><cell>21.742304</cell></row><row><cell cols="3">Nevergrad (1+1) Diabetes DT</cell><cell>MAE</cell><cell cols="2">87.192577 491.678538</cell></row><row><cell>Opentuner</cell><cell cols="2">Diabetes DT</cell><cell>MAE</cell><cell cols="2">84.459098 199.389559</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_50"><head>Table 49 :</head><label>49</label><figDesc>Diabetes with MAE loss for tuning MLP-adam model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is PySOT.</figDesc><table><row><cell>Algorithm</cell><cell>Dataset</cell><cell>Model</cell><cell cols="2">Metric Normalised Score Variance</cell></row><row><cell>PySOT</cell><cell cols="3">Diabetes MLP-adam MAE</cell><cell>100.011743 0.039479</cell></row><row><cell>TuRBO</cell><cell cols="3">Diabetes MLP-adam MAE</cell><cell>100.003598 0.132742</cell></row><row><cell>HEBO</cell><cell cols="3">Diabetes MLP-adam MAE</cell><cell>99.905117 0.071792</cell></row><row><cell cols="4">Nevergrad (1+1) Diabetes MLP-adam MAE</cell><cell>99.783143 0.276722</cell></row><row><cell>Hyperopt</cell><cell cols="3">Diabetes MLP-adam MAE</cell><cell>99.747348 0.024272</cell></row><row><cell>Skopt</cell><cell cols="3">Diabetes MLP-adam MAE</cell><cell>99.691162 0.057345</cell></row><row><cell>Opentuner</cell><cell cols="3">Diabetes MLP-adam MAE</cell><cell>99.564832 0.075922</cell></row><row><cell>TuRBO+</cell><cell cols="3">Diabetes MLP-adam MAE</cell><cell>99.518025 0.125512</cell></row><row><cell>BOHB-BB</cell><cell cols="3">Diabetes MLP-adam MAE</cell><cell>99.343161 0.055340</cell></row><row><cell>Random-search</cell><cell cols="3">Diabetes MLP-adam MAE</cell><cell>99.288591 0.104943</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_51"><head>Table 50 :</head><label>50</label><figDesc>Diabetes with MAE loss for tuning MLP-SGD model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell>Dataset</cell><cell>Model</cell><cell cols="2">Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell cols="3">Diabetes MLP-SGD MAE</cell><cell>99.305259</cell><cell>0.571598</cell></row><row><cell>TuRBO</cell><cell cols="3">Diabetes MLP-SGD MAE</cell><cell>99.173074</cell><cell>2.151207</cell></row><row><cell>PySOT</cell><cell cols="3">Diabetes MLP-SGD MAE</cell><cell>98.493992</cell><cell>3.290314</cell></row><row><cell>TuRBO+</cell><cell cols="3">Diabetes MLP-SGD MAE</cell><cell>98.017532</cell><cell>2.468709</cell></row><row><cell>Hyperopt</cell><cell cols="3">Diabetes MLP-SGD MAE</cell><cell>97.991732</cell><cell>1.875066</cell></row><row><cell>Skopt</cell><cell cols="3">Diabetes MLP-SGD MAE</cell><cell>97.323238</cell><cell>1.402552</cell></row><row><cell>BOHB-BB</cell><cell cols="3">Diabetes MLP-SGD MAE</cell><cell>94.355532</cell><cell>9.155887</cell></row><row><cell>Random-search</cell><cell cols="3">Diabetes MLP-SGD MAE</cell><cell>94.196391</cell><cell>42.702734</cell></row><row><cell>Opentuner</cell><cell cols="3">Diabetes MLP-SGD MAE</cell><cell cols="2">93.018054 116.857957</cell></row><row><cell cols="4">Nevergrad (1+1) Diabetes MLP-SGD MAE</cell><cell cols="2">87.052894 249.800579</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_52"><head>Table 51 :</head><label>51</label><figDesc>Diabetes with MAE loss for tuning RF model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is TuRBO.</figDesc><table><row><cell>Algorithm</cell><cell>Dataset</cell><cell cols="3">Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>TuRBO</cell><cell cols="2">Diabetes RF</cell><cell>MAE</cell><cell>100.689329</cell><cell>1.625789</cell></row><row><cell>PySOT</cell><cell cols="2">Diabetes RF</cell><cell>MAE</cell><cell>100.621188</cell><cell>2.067673</cell></row><row><cell>HEBO</cell><cell cols="2">Diabetes RF</cell><cell>MAE</cell><cell>100.420933</cell><cell>1.415993</cell></row><row><cell>Skopt</cell><cell cols="2">Diabetes RF</cell><cell>MAE</cell><cell>99.206462</cell><cell>1.447476</cell></row><row><cell cols="3">Nevergrad (1+1) Diabetes RF</cell><cell>MAE</cell><cell cols="2">98.695634 22.941598</cell></row><row><cell>Hyperopt</cell><cell cols="2">Diabetes RF</cell><cell>MAE</cell><cell>98.321835</cell><cell>1.287212</cell></row><row><cell>TuRBO+</cell><cell cols="2">Diabetes RF</cell><cell>MAE</cell><cell>97.985211</cell><cell>5.657556</cell></row><row><cell>BOHB-BB</cell><cell cols="2">Diabetes RF</cell><cell>MAE</cell><cell>96.181909</cell><cell>6.997435</cell></row><row><cell>Random-search</cell><cell cols="2">Diabetes RF</cell><cell>MAE</cell><cell>95.677899</cell><cell>5.078577</cell></row><row><cell>Opentuner</cell><cell cols="2">Diabetes RF</cell><cell>MAE</cell><cell cols="2">93.512496 46.716147</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_53"><head>Table 52 :</head><label>52</label><figDesc>Diabetes with MAE loss for tuning SVM model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is Nevergrad.</figDesc><table><row><cell>Algorithm</cell><cell>Dataset</cell><cell cols="3">Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell cols="3">Nevergrad (1+1) Diabetes SVM</cell><cell>MAE</cell><cell>114.952221</cell><cell>1.997569e-23</cell></row><row><cell>HEBO</cell><cell cols="2">Diabetes SVM</cell><cell>MAE</cell><cell>114.952206</cell><cell>4.667718e-10</cell></row><row><cell>TuRBO</cell><cell cols="2">Diabetes SVM</cell><cell>MAE</cell><cell>114.828240</cell><cell>9.438118e-03</cell></row><row><cell>PySOT</cell><cell cols="2">Diabetes SVM</cell><cell>MAE</cell><cell>114.410275</cell><cell>1.494253e-01</cell></row><row><cell>Skopt</cell><cell cols="2">Diabetes SVM</cell><cell>MAE</cell><cell>113.977223</cell><cell>2.466076e-01</cell></row><row><cell>Opentuner</cell><cell cols="2">Diabetes SVM</cell><cell>MAE</cell><cell cols="2">113.263069 9.959103e+00</cell></row><row><cell>TuRBO+</cell><cell cols="2">Diabetes SVM</cell><cell>MAE</cell><cell cols="2">106.842965 1.060824e+02</cell></row><row><cell>Hyperopt</cell><cell cols="2">Diabetes SVM</cell><cell>MAE</cell><cell cols="2">104.021953 8.292988e+01</cell></row><row><cell>Random-search</cell><cell cols="2">Diabetes SVM</cell><cell>MAE</cell><cell cols="2">76.950614 2.326428e+02</cell></row><row><cell>BOHB-BB</cell><cell cols="2">Diabetes SVM</cell><cell>MAE</cell><cell cols="2">70.832961 2.115443e+02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_54"><head>Table 53 :</head><label>53</label><figDesc>Diabetes with MAE loss for tuning Ada model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is Opentuner.</figDesc><table><row><cell>Algorithm</cell><cell>Dataset</cell><cell cols="3">Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>Opentuner</cell><cell cols="2">Diabetes Ada</cell><cell>MAE</cell><cell>82.820466 729.771421</cell></row><row><cell>HEBO</cell><cell cols="2">Diabetes Ada</cell><cell>MAE</cell><cell>81.642682 638.761474</cell></row><row><cell>TuRBO</cell><cell cols="2">Diabetes Ada</cell><cell>MAE</cell><cell>80.090059 159.113760</cell></row><row><cell>PySOT</cell><cell cols="2">Diabetes Ada</cell><cell>MAE</cell><cell>79.899689 271.945194</cell></row><row><cell>Hyperopt</cell><cell cols="2">Diabetes Ada</cell><cell>MAE</cell><cell>79.323266 146.085370</cell></row><row><cell>BOHB-BB</cell><cell cols="2">Diabetes Ada</cell><cell>MAE</cell><cell>77.689133 209.828285</cell></row><row><cell>TuRBO+</cell><cell cols="2">Diabetes Ada</cell><cell>MAE</cell><cell>76.467861 135.190360</cell></row><row><cell>Skopt</cell><cell cols="2">Diabetes Ada</cell><cell>MAE</cell><cell>75.430634 172.038737</cell></row><row><cell cols="3">Nevergrad (1+1) Diabetes Ada</cell><cell>MAE</cell><cell>73.932751 141.078137</cell></row><row><cell>Random-search</cell><cell cols="2">Diabetes Ada</cell><cell>MAE</cell><cell>73.572795 183.641961</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_55"><head>Table 54 :</head><label>54</label><figDesc>Diabetes with MAE loss for tuning Knn model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell>Dataset</cell><cell cols="3">Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell cols="2">Diabetes Knn</cell><cell>MAE</cell><cell>100.000000</cell><cell>0.000000</cell></row><row><cell>Opentuner</cell><cell cols="2">Diabetes Knn</cell><cell>MAE</cell><cell>100.000000</cell><cell>0.000000</cell></row><row><cell>PySOT</cell><cell cols="2">Diabetes Knn</cell><cell>MAE</cell><cell>99.918437</cell><cell>0.133050</cell></row><row><cell>TuRBO</cell><cell cols="2">Diabetes Knn</cell><cell>MAE</cell><cell>99.276294</cell><cell>10.475009</cell></row><row><cell>Hyperopt</cell><cell cols="2">Diabetes Knn</cell><cell>MAE</cell><cell>97.929753</cell><cell>28.990634</cell></row><row><cell cols="3">Nevergrad (1+1) Diabetes Knn</cell><cell>MAE</cell><cell>97.515979</cell><cell>58.516868</cell></row><row><cell>Random-search</cell><cell cols="2">Diabetes Knn</cell><cell>MAE</cell><cell>95.595510</cell><cell>46.440649</cell></row><row><cell>BOHB-BB</cell><cell cols="2">Diabetes Knn</cell><cell>MAE</cell><cell>95.543615</cell><cell>56.425722</cell></row><row><cell>Skopt</cell><cell cols="2">Diabetes Knn</cell><cell>MAE</cell><cell>95.429465</cell><cell>66.513444</cell></row><row><cell>TuRBO+</cell><cell cols="2">Diabetes Knn</cell><cell>MAE</cell><cell cols="2">91.604083 134.441613</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_56"><head>Table 55 :</head><label>55</label><figDesc>Diabetes with MAE loss for tuning Lasso model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is Skopt.</figDesc><table><row><cell>Algorithm</cell><cell>Dataset</cell><cell cols="2">Model Metric Normalised Score Variance</cell></row><row><cell>Skopt</cell><cell cols="2">Diabetes Lasso MAE</cell><cell>100.024636 0.001070</cell></row><row><cell>HEBO</cell><cell cols="2">Diabetes Lasso MAE</cell><cell>100.020048 0.000540</cell></row><row><cell>TuRBO</cell><cell cols="2">Diabetes Lasso MAE</cell><cell>100.011646 0.000588</cell></row><row><cell>PySOT</cell><cell cols="2">Diabetes Lasso MAE</cell><cell>100.006646 0.000432</cell></row><row><cell>Hyperopt</cell><cell cols="2">Diabetes Lasso MAE</cell><cell>100.004698 0.000154</cell></row><row><cell>TuRBO+</cell><cell cols="2">Diabetes Lasso MAE</cell><cell>99.997964 0.000615</cell></row><row><cell>BOHB-BB</cell><cell cols="2">Diabetes Lasso MAE</cell><cell>99.995219 0.000595</cell></row><row><cell cols="3">Nevergrad (1+1) Diabetes Lasso MAE</cell><cell>99.993918 0.000879</cell></row><row><cell>Random-search</cell><cell cols="2">Diabetes Lasso MAE</cell><cell>99.993704 0.000037</cell></row><row><cell>Opentuner</cell><cell cols="2">Diabetes Lasso MAE</cell><cell>99.813361 0.285529</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_57"><head>Table 56 :</head><label>56</label><figDesc>Diabetes with MAE loss for tuning Linear model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell>Dataset</cell><cell cols="2">Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell cols="2">Diabetes Linear MAE</cell><cell cols="2">99.999997 7.405442e-11</cell></row><row><cell>TuRBO</cell><cell cols="2">Diabetes Linear MAE</cell><cell cols="2">99.999955 5.421510e-08</cell></row><row><cell>PySOT</cell><cell cols="2">Diabetes Linear MAE</cell><cell cols="2">99.999884 8.825730e-08</cell></row><row><cell cols="3">Nevergrad (1+1) Diabetes Linear MAE</cell><cell cols="2">99.999798 1.823753e-07</cell></row><row><cell>Hyperopt</cell><cell cols="2">Diabetes Linear MAE</cell><cell cols="2">99.999756 1.834449e-07</cell></row><row><cell>Skopt</cell><cell cols="2">Diabetes Linear MAE</cell><cell cols="2">99.999461 3.064474e-07</cell></row><row><cell>BOHB-BB</cell><cell cols="2">Diabetes Linear MAE</cell><cell cols="2">99.999386 4.929089e-07</cell></row><row><cell>Random-search</cell><cell cols="2">Diabetes Linear MAE</cell><cell cols="2">99.999226 7.463938e-07</cell></row><row><cell>TuRBO+</cell><cell cols="2">Diabetes Linear MAE</cell><cell cols="2">99.998033 5.543436e-05</cell></row><row><cell>Opentuner</cell><cell cols="2">Diabetes Linear MAE</cell><cell cols="2">99.990706 1.793190e-04</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_58"><head>Table 57 :</head><label>57</label><figDesc>Diabetes with MSE loss for tuning DT model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is TuRBO.</figDesc><table><row><cell>Algorithm</cell><cell>Dataset</cell><cell cols="3">Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>TuRBO</cell><cell cols="2">Diabetes DT</cell><cell>MSE</cell><cell>99.159752</cell><cell>20.394596</cell></row><row><cell>HEBO</cell><cell cols="2">Diabetes DT</cell><cell>MSE</cell><cell>99.105320</cell><cell>27.539740</cell></row><row><cell>PySOT</cell><cell cols="2">Diabetes DT</cell><cell>MSE</cell><cell>98.463863</cell><cell>12.943768</cell></row><row><cell>TuRBO+</cell><cell cols="2">Diabetes DT</cell><cell>MSE</cell><cell>98.146099</cell><cell>29.207851</cell></row><row><cell>Skopt</cell><cell cols="2">Diabetes DT</cell><cell>MSE</cell><cell>98.011055</cell><cell>9.114515</cell></row><row><cell cols="3">Nevergrad (1+1) Diabetes DT</cell><cell>MSE</cell><cell>95.965807</cell><cell>89.742185</cell></row><row><cell>Hyperopt</cell><cell cols="2">Diabetes DT</cell><cell>MSE</cell><cell>94.106805</cell><cell>17.465775</cell></row><row><cell>BOHB-BB</cell><cell cols="2">Diabetes DT</cell><cell>MSE</cell><cell>91.746558</cell><cell>29.504056</cell></row><row><cell>Random-search</cell><cell cols="2">Diabetes DT</cell><cell>MSE</cell><cell>90.512956</cell><cell>24.763756</cell></row><row><cell>Opentuner</cell><cell cols="2">Diabetes DT</cell><cell>MSE</cell><cell cols="2">85.951624 189.632259</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_59"><head>Table 58 :</head><label>58</label><figDesc>Diabetes with MSE loss for tuning MLP-adam model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is TuRBO.</figDesc><table><row><cell>Algorithm</cell><cell>Dataset</cell><cell>Model</cell><cell cols="2">Metric Normalised Score Variance</cell></row><row><cell>TuRBO</cell><cell cols="3">Diabetes MLP-adam MSE</cell><cell>100.080222 0.011338</cell></row><row><cell>PySOT</cell><cell cols="3">Diabetes MLP-adam MSE</cell><cell>100.021190 0.010076</cell></row><row><cell cols="4">Nevergrad (1+1) Diabetes MLP-adam MSE</cell><cell>100.017846 0.019891</cell></row><row><cell>HEBO</cell><cell cols="3">Diabetes MLP-adam MSE</cell><cell>99.977968 0.004366</cell></row><row><cell>Hyperopt</cell><cell cols="3">Diabetes MLP-adam MSE</cell><cell>99.937035 0.011698</cell></row><row><cell>Opentuner</cell><cell cols="3">Diabetes MLP-adam MSE</cell><cell>99.886863 0.005519</cell></row><row><cell>Skopt</cell><cell cols="3">Diabetes MLP-adam MSE</cell><cell>99.838359 0.008621</cell></row><row><cell>TuRBO+</cell><cell cols="3">Diabetes MLP-adam MSE</cell><cell>99.825080 0.033512</cell></row><row><cell>Random-search</cell><cell cols="3">Diabetes MLP-adam MSE</cell><cell>99.777438 0.009177</cell></row><row><cell>BOHB-BB</cell><cell cols="3">Diabetes MLP-adam MSE</cell><cell>99.742448 0.011606</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_60"><head>Table 59 :</head><label>59</label><figDesc>Diabetes with MSE loss for tuning MLP-SGD model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell>Dataset</cell><cell>Model</cell><cell cols="2">Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell cols="3">Diabetes MLP-SGD MSE</cell><cell>99.502940</cell><cell>0.179089</cell></row><row><cell>TuRBO</cell><cell cols="3">Diabetes MLP-SGD MSE</cell><cell>99.498069</cell><cell>0.391187</cell></row><row><cell>Hyperopt</cell><cell cols="3">Diabetes MLP-SGD MSE</cell><cell>98.948022</cell><cell>0.324618</cell></row><row><cell>TuRBO+</cell><cell cols="3">Diabetes MLP-SGD MSE</cell><cell>98.704370</cell><cell>1.720076</cell></row><row><cell>Skopt</cell><cell cols="3">Diabetes MLP-SGD MSE</cell><cell>98.452683</cell><cell>1.161588</cell></row><row><cell>PySOT</cell><cell cols="3">Diabetes MLP-SGD MSE</cell><cell>98.357894</cell><cell>3.117919</cell></row><row><cell>Random-search</cell><cell cols="3">Diabetes MLP-SGD MSE</cell><cell>97.569960</cell><cell>3.685257</cell></row><row><cell>BOHB-BB</cell><cell cols="3">Diabetes MLP-SGD MSE</cell><cell>95.630189</cell><cell>18.807287</cell></row><row><cell>Opentuner</cell><cell cols="3">Diabetes MLP-SGD MSE</cell><cell>94.462201</cell><cell>34.471258</cell></row><row><cell cols="4">Nevergrad (1+1) Diabetes MLP-SGD MSE</cell><cell cols="2">92.620769 179.226746</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_61"><head>Table 60 :</head><label>60</label><figDesc>Diabetes with MSE loss for tuning RF model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell>Dataset</cell><cell cols="3">Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell cols="2">Diabetes RF</cell><cell>MSE</cell><cell>99.567950</cell><cell>1.137694</cell></row><row><cell>PySOT</cell><cell cols="2">Diabetes RF</cell><cell>MSE</cell><cell>99.197939</cell><cell>1.240447</cell></row><row><cell>TuRBO</cell><cell cols="2">Diabetes RF</cell><cell>MSE</cell><cell>99.154475</cell><cell>1.995345</cell></row><row><cell>Skopt</cell><cell cols="2">Diabetes RF</cell><cell>MSE</cell><cell>98.665889</cell><cell>0.909053</cell></row><row><cell>Hyperopt</cell><cell cols="2">Diabetes RF</cell><cell>MSE</cell><cell>98.271427</cell><cell>1.306007</cell></row><row><cell>TuRBO+</cell><cell cols="2">Diabetes RF</cell><cell>MSE</cell><cell>97.396962</cell><cell>3.233997</cell></row><row><cell cols="3">Nevergrad (1+1) Diabetes RF</cell><cell>MSE</cell><cell cols="2">96.904507 16.420320</cell></row><row><cell>Random-search</cell><cell cols="2">Diabetes RF</cell><cell>MSE</cell><cell>96.478596</cell><cell>4.042486</cell></row><row><cell>BOHB-BB</cell><cell cols="2">Diabetes RF</cell><cell>MSE</cell><cell>96.269017</cell><cell>3.544861</cell></row><row><cell>Opentuner</cell><cell cols="2">Diabetes RF</cell><cell>MSE</cell><cell cols="2">95.846844 22.993955</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_62"><head>Table 61 :</head><label>61</label><figDesc>Diabetes with MSE loss for tuning SVM model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is Nevergrad.</figDesc><table><row><cell>Algorithm</cell><cell>Dataset</cell><cell cols="3">Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell cols="3">Nevergrad (1+1) Diabetes SVM</cell><cell>MSE</cell><cell>102.789567</cell><cell>2.184625e-24</cell></row><row><cell>HEBO</cell><cell cols="2">Diabetes SVM</cell><cell>MSE</cell><cell>102.789540</cell><cell>1.530581e-09</cell></row><row><cell>TuRBO</cell><cell cols="2">Diabetes SVM</cell><cell>MSE</cell><cell>102.673445</cell><cell>8.390893e-03</cell></row><row><cell>PySOT</cell><cell cols="2">Diabetes SVM</cell><cell>MSE</cell><cell>102.020190</cell><cell>4.231205e-01</cell></row><row><cell>Skopt</cell><cell cols="2">Diabetes SVM</cell><cell>MSE</cell><cell>101.868102</cell><cell>1.554345e-01</cell></row><row><cell>Opentuner</cell><cell cols="2">Diabetes SVM</cell><cell>MSE</cell><cell cols="2">100.897226 1.367135e+01</cell></row><row><cell>Hyperopt</cell><cell cols="2">Diabetes SVM</cell><cell>MSE</cell><cell cols="2">95.268658 4.223803e+01</cell></row><row><cell>TuRBO+</cell><cell cols="2">Diabetes SVM</cell><cell>MSE</cell><cell cols="2">94.606843 4.791899e+01</cell></row><row><cell>BOHB-BB</cell><cell cols="2">Diabetes SVM</cell><cell>MSE</cell><cell cols="2">74.069393 1.232001e+02</cell></row><row><cell>Random-search</cell><cell cols="2">Diabetes SVM</cell><cell>MSE</cell><cell cols="2">65.895981 1.918664e+02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_63"><head>Table 62 :</head><label>62</label><figDesc>Diabetes with MSE loss for tuning Ada model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is Nevergrad.</figDesc><table><row><cell>Algorithm</cell><cell>Dataset</cell><cell cols="3">Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell cols="3">Nevergrad (1+1) Diabetes Ada</cell><cell>MSE</cell><cell cols="2">68.161267 269.888332</cell></row><row><cell>Skopt</cell><cell cols="2">Diabetes Ada</cell><cell>MSE</cell><cell cols="2">67.321751 190.562037</cell></row><row><cell>Opentuner</cell><cell cols="2">Diabetes Ada</cell><cell>MSE</cell><cell>65.700020</cell><cell>63.006486</cell></row><row><cell>PySOT</cell><cell cols="2">Diabetes Ada</cell><cell>MSE</cell><cell cols="2">63.875149 105.453227</cell></row><row><cell>Hyperopt</cell><cell cols="2">Diabetes Ada</cell><cell>MSE</cell><cell cols="2">63.247637 100.721372</cell></row><row><cell>HEBO</cell><cell cols="2">Diabetes Ada</cell><cell>MSE</cell><cell>63.117246</cell><cell>55.910472</cell></row><row><cell>Random-search</cell><cell cols="2">Diabetes Ada</cell><cell>MSE</cell><cell>63.112054</cell><cell>51.487703</cell></row><row><cell>TuRBO</cell><cell cols="2">Diabetes Ada</cell><cell>MSE</cell><cell>62.719630</cell><cell>59.718734</cell></row><row><cell>BOHB-BB</cell><cell cols="2">Diabetes Ada</cell><cell>MSE</cell><cell>62.520126</cell><cell>91.727211</cell></row><row><cell>TuRBO+</cell><cell cols="2">Diabetes Ada</cell><cell>MSE</cell><cell>61.558612</cell><cell>66.945920</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_64"><head>Table 63 :</head><label>63</label><figDesc>Diabetes with MSE loss for tuning Knn model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell>Dataset</cell><cell cols="3">Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell cols="2">Diabetes Knn</cell><cell>MSE</cell><cell>100.000000</cell><cell>0.000000</cell></row><row><cell>PySOT</cell><cell cols="2">Diabetes Knn</cell><cell>MSE</cell><cell>100.000000</cell><cell>0.000000</cell></row><row><cell>Opentuner</cell><cell cols="2">Diabetes Knn</cell><cell>MSE</cell><cell>99.019554</cell><cell>19.225491</cell></row><row><cell cols="3">Nevergrad (1+1) Diabetes Knn</cell><cell>MSE</cell><cell>96.078216</cell><cell>64.759548</cell></row><row><cell>Hyperopt</cell><cell cols="2">Diabetes Knn</cell><cell>MSE</cell><cell>95.868028</cell><cell>72.584707</cell></row><row><cell>BOHB-BB</cell><cell cols="2">Diabetes Knn</cell><cell>MSE</cell><cell>94.460734</cell><cell>99.952329</cell></row><row><cell>Skopt</cell><cell cols="2">Diabetes Knn</cell><cell>MSE</cell><cell>93.696947</cell><cell>98.818821</cell></row><row><cell>TuRBO</cell><cell cols="2">Diabetes Knn</cell><cell>MSE</cell><cell cols="2">91.661607 114.292743</cell></row><row><cell>Random-search</cell><cell cols="2">Diabetes Knn</cell><cell>MSE</cell><cell cols="2">89.280338 126.210687</cell></row><row><cell>TuRBO+</cell><cell cols="2">Diabetes Knn</cell><cell>MSE</cell><cell cols="2">88.510080 118.833506</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_65"><head>Table 64 :</head><label>64</label><figDesc>Diabetes with MSE loss for tuning Lasso model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is TuRBO.</figDesc><table><row><cell>Algorithm</cell><cell>Dataset</cell><cell cols="2">Model Metric Normalised Score Variance</cell></row><row><cell>TuRBO</cell><cell cols="2">Diabetes Lasso MSE</cell><cell>100.001427 0.000065</cell></row><row><cell>HEBO</cell><cell cols="2">Diabetes Lasso MSE</cell><cell>99.993203 0.000066</cell></row><row><cell>Skopt</cell><cell cols="2">Diabetes Lasso MSE</cell><cell>99.988906 0.000109</cell></row><row><cell>Hyperopt</cell><cell cols="2">Diabetes Lasso MSE</cell><cell>99.988425 0.000162</cell></row><row><cell>PySOT</cell><cell cols="2">Diabetes Lasso MSE</cell><cell>99.984161 0.000170</cell></row><row><cell>TuRBO+</cell><cell cols="2">Diabetes Lasso MSE</cell><cell>99.982936 0.000182</cell></row><row><cell>BOHB-BB</cell><cell cols="2">Diabetes Lasso MSE</cell><cell>99.969386 0.000650</cell></row><row><cell>Random-search</cell><cell cols="2">Diabetes Lasso MSE</cell><cell>99.968212 0.000182</cell></row><row><cell cols="3">Nevergrad (1+1) Diabetes Lasso MSE</cell><cell>99.889782 0.015621</cell></row><row><cell>Opentuner</cell><cell cols="2">Diabetes Lasso MSE</cell><cell>99.549967 2.274095</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_66"><head>Table 65 :</head><label>65</label><figDesc>Diabetes with MSE loss for tuning Linear model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell>Dataset</cell><cell cols="2">Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell cols="2">Diabetes Linear MSE</cell><cell cols="2">99.999999 5.900912e-12</cell></row><row><cell>TuRBO</cell><cell cols="2">Diabetes Linear MSE</cell><cell cols="2">99.999987 3.453784e-09</cell></row><row><cell>Hyperopt</cell><cell cols="2">Diabetes Linear MSE</cell><cell cols="2">99.999900 1.682948e-07</cell></row><row><cell>Skopt</cell><cell cols="2">Diabetes Linear MSE</cell><cell cols="2">99.999887 5.424762e-08</cell></row><row><cell>PySOT</cell><cell cols="2">Diabetes Linear MSE</cell><cell cols="2">99.999856 1.702504e-07</cell></row><row><cell>Random-search</cell><cell cols="2">Diabetes Linear MSE</cell><cell cols="2">99.999833 4.783733e-08</cell></row><row><cell>BOHB-BB</cell><cell cols="2">Diabetes Linear MSE</cell><cell cols="2">99.999821 1.116438e-07</cell></row><row><cell>TuRBO+</cell><cell cols="2">Diabetes Linear MSE</cell><cell cols="2">99.999698 2.978636e-07</cell></row><row><cell cols="3">Nevergrad (1+1) Diabetes Linear MSE</cell><cell cols="2">99.999633 5.677318e-07</cell></row><row><cell>Opentuner</cell><cell cols="2">Diabetes Linear MSE</cell><cell cols="2">99.989516 7.327061e-04</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_67"><head>Table 66 :</head><label>66</label><figDesc>Digits with ACC loss for tuning DT model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell cols="4">Dataset Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell>Digits</cell><cell>DT</cell><cell>ACC</cell><cell>109.992773</cell><cell>0.333759</cell></row><row><cell>TuRBO</cell><cell>Digits</cell><cell>DT</cell><cell>ACC</cell><cell>109.229157</cell><cell>2.411464</cell></row><row><cell>PySOT</cell><cell>Digits</cell><cell>DT</cell><cell>ACC</cell><cell>106.415805</cell><cell>30.276129</cell></row><row><cell>Skopt</cell><cell>Digits</cell><cell>DT</cell><cell>ACC</cell><cell>102.201342</cell><cell>234.180142</cell></row><row><cell>Hyperopt</cell><cell>Digits</cell><cell>DT</cell><cell>ACC</cell><cell>92.812052</cell><cell>64.914274</cell></row><row><cell>TuRBO+</cell><cell>Digits</cell><cell>DT</cell><cell>ACC</cell><cell>75.463155</cell><cell>434.854745</cell></row><row><cell>Opentuner</cell><cell>Digits</cell><cell>DT</cell><cell>ACC</cell><cell>71.263614</cell><cell>642.656253</cell></row><row><cell>BOHB-BB</cell><cell>Digits</cell><cell>DT</cell><cell>ACC</cell><cell>68.750324</cell><cell>284.398998</cell></row><row><cell>Random-search</cell><cell>Digits</cell><cell>DT</cell><cell>ACC</cell><cell>65.051159</cell><cell>432.111162</cell></row><row><cell cols="2">Nevergrad (1+1) Digits</cell><cell>DT</cell><cell>ACC</cell><cell cols="2">61.284002 1994.275954</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_68"><head>Table 67 :</head><label>67</label><figDesc>Digits with ACC loss for tuning MLP-adam model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is TuRBO.</figDesc><table><row><cell>Algorithm</cell><cell cols="2">Dataset Model</cell><cell cols="2">Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>TuRBO</cell><cell>Digits</cell><cell cols="2">MLP-adam ACC</cell><cell>102.902359</cell><cell>5.829126</cell></row><row><cell>HEBO</cell><cell>Digits</cell><cell cols="2">MLP-adam ACC</cell><cell cols="2">101.875822 12.570464</cell></row><row><cell>TuRBO+</cell><cell>Digits</cell><cell cols="2">MLP-adam ACC</cell><cell cols="2">100.266068 18.108832</cell></row><row><cell>Hyperopt</cell><cell>Digits</cell><cell cols="2">MLP-adam ACC</cell><cell>100.263270</cell><cell>4.310725</cell></row><row><cell>PySOT</cell><cell>Digits</cell><cell cols="2">MLP-adam ACC</cell><cell cols="2">99.686892 13.193611</cell></row><row><cell cols="2">Nevergrad (1+1) Digits</cell><cell cols="2">MLP-adam ACC</cell><cell cols="2">96.867646 36.871957</cell></row><row><cell>BOHB-BB</cell><cell>Digits</cell><cell cols="2">MLP-adam ACC</cell><cell cols="2">95.526339 22.930349</cell></row><row><cell>Skopt</cell><cell>Digits</cell><cell cols="2">MLP-adam ACC</cell><cell cols="2">95.215922 17.997087</cell></row><row><cell>Opentuner</cell><cell>Digits</cell><cell cols="2">MLP-adam ACC</cell><cell cols="2">94.365035 11.136649</cell></row><row><cell>Random-search</cell><cell>Digits</cell><cell cols="2">MLP-adam ACC</cell><cell>94.005964</cell><cell>6.753444</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_69"><head>Table 68 :</head><label>68</label><figDesc>Digits with ACC loss for tuning MLP-SGD model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell cols="2">Dataset Model</cell><cell cols="2">Metric Normalised Score Variance</cell></row><row><cell>HEBO</cell><cell>Digits</cell><cell cols="2">MLP-SGD ACC</cell><cell>101.197199 0.137161</cell></row><row><cell>TuRBO</cell><cell>Digits</cell><cell cols="2">MLP-SGD ACC</cell><cell>101.186343 0.329266</cell></row><row><cell>PySOT</cell><cell>Digits</cell><cell cols="2">MLP-SGD ACC</cell><cell>100.810497 0.305607</cell></row><row><cell>Hyperopt</cell><cell>Digits</cell><cell cols="2">MLP-SGD ACC</cell><cell>100.374369 0.547541</cell></row><row><cell>Skopt</cell><cell>Digits</cell><cell cols="2">MLP-SGD ACC</cell><cell>100.221920 0.490938</cell></row><row><cell>Opentuner</cell><cell>Digits</cell><cell cols="2">MLP-SGD ACC</cell><cell>100.009681 0.631299</cell></row><row><cell cols="2">Nevergrad (1+1) Digits</cell><cell cols="2">MLP-SGD ACC</cell><cell>99.638780 5.425609</cell></row><row><cell>TuRBO+</cell><cell>Digits</cell><cell cols="2">MLP-SGD ACC</cell><cell>99.213829 1.526439</cell></row><row><cell>Random-search</cell><cell>Digits</cell><cell cols="2">MLP-SGD ACC</cell><cell>98.434458 1.856923</cell></row><row><cell>BOHB-BB</cell><cell>Digits</cell><cell cols="2">MLP-SGD ACC</cell><cell>98.135339 0.652457</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_70"><head>Table 69 :</head><label>69</label><figDesc>Digits with ACC loss for tuning RF model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell cols="4">Dataset Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell>Digits</cell><cell>RF</cell><cell>ACC</cell><cell>106.100300</cell><cell>1.232976</cell></row><row><cell>PySOT</cell><cell>Digits</cell><cell>RF</cell><cell>ACC</cell><cell>104.238234</cell><cell>40.163502</cell></row><row><cell>Skopt</cell><cell>Digits</cell><cell>RF</cell><cell>ACC</cell><cell>104.091452</cell><cell>1.113028</cell></row><row><cell>Hyperopt</cell><cell>Digits</cell><cell>RF</cell><cell>ACC</cell><cell>99.371979</cell><cell>7.584547</cell></row><row><cell>TuRBO</cell><cell>Digits</cell><cell>RF</cell><cell>ACC</cell><cell cols="2">90.437346 1074.875086</cell></row><row><cell>TuRBO+</cell><cell>Digits</cell><cell>RF</cell><cell>ACC</cell><cell>86.244306</cell><cell>105.700384</cell></row><row><cell>BOHB-BB</cell><cell>Digits</cell><cell>RF</cell><cell>ACC</cell><cell>81.694197</cell><cell>146.883857</cell></row><row><cell>Opentuner</cell><cell>Digits</cell><cell>RF</cell><cell>ACC</cell><cell>78.227366</cell><cell>197.260382</cell></row><row><cell>Random-search</cell><cell>Digits</cell><cell>RF</cell><cell>ACC</cell><cell>77.402425</cell><cell>236.222327</cell></row><row><cell cols="2">Nevergrad (1+1) Digits</cell><cell>RF</cell><cell>ACC</cell><cell cols="2">62.720269 2400.500537</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_71"><head>Table 70 :</head><label>70</label><figDesc>Digits with ACC loss for tuning SVM model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell cols="4">Dataset Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell>Digits</cell><cell>SVM</cell><cell>ACC</cell><cell>131.797994</cell><cell>0.000000</cell></row><row><cell>Hyperopt</cell><cell>Digits</cell><cell>SVM</cell><cell>ACC</cell><cell>130.140907</cell><cell>54.918774</cell></row><row><cell>Opentuner</cell><cell>Digits</cell><cell>SVM</cell><cell>ACC</cell><cell cols="2">128.483819 104.056624</cell></row><row><cell>TuRBO</cell><cell>Digits</cell><cell>SVM</cell><cell>ACC</cell><cell cols="2">126.826732 147.413550</cell></row><row><cell>Random-search</cell><cell>Digits</cell><cell>SVM</cell><cell>ACC</cell><cell cols="2">125.169645 184.989553</cell></row><row><cell>PySOT</cell><cell>Digits</cell><cell>SVM</cell><cell>ACC</cell><cell cols="2">118.541295 277.484329</cell></row><row><cell>TuRBO+</cell><cell>Digits</cell><cell>SVM</cell><cell>ACC</cell><cell cols="2">116.889961 285.935578</cell></row><row><cell>BOHB-BB</cell><cell>Digits</cell><cell>SVM</cell><cell>ACC</cell><cell cols="2">111.935960 276.844235</cell></row><row><cell>Skopt</cell><cell>Digits</cell><cell>SVM</cell><cell>ACC</cell><cell cols="2">108.610278 358.177641</cell></row><row><cell cols="2">Nevergrad (1+1) Digits</cell><cell>SVM</cell><cell>ACC</cell><cell cols="2">93.702245 838.899480</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_72"><head>Table 71 :</head><label>71</label><figDesc>Digits with ACC loss for tuning Ada model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell cols="4">Dataset Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell>Digits</cell><cell>Ada</cell><cell>ACC</cell><cell>113.013741</cell><cell>8.281988</cell></row><row><cell>Opentuner</cell><cell>Digits</cell><cell>Ada</cell><cell>ACC</cell><cell>106.626931</cell><cell>57.406496</cell></row><row><cell>Skopt</cell><cell>Digits</cell><cell>Ada</cell><cell>ACC</cell><cell cols="2">95.538424 505.125378</cell></row><row><cell>PySOT</cell><cell>Digits</cell><cell>Ada</cell><cell>ACC</cell><cell cols="2">89.816945 200.472104</cell></row><row><cell>Hyperopt</cell><cell>Digits</cell><cell>Ada</cell><cell>ACC</cell><cell cols="2">85.187450 354.000313</cell></row><row><cell>Random-search</cell><cell>Digits</cell><cell>Ada</cell><cell>ACC</cell><cell cols="2">80.936224 197.588714</cell></row><row><cell>TuRBO</cell><cell>Digits</cell><cell>Ada</cell><cell>ACC</cell><cell cols="2">79.578370 286.188817</cell></row><row><cell>TuRBO+</cell><cell>Digits</cell><cell>Ada</cell><cell>ACC</cell><cell cols="2">77.540995 279.358366</cell></row><row><cell cols="2">Nevergrad (1+1) Digits</cell><cell>Ada</cell><cell>ACC</cell><cell cols="2">69.909418 239.320766</cell></row><row><cell>BOHB-BB</cell><cell>Digits</cell><cell>Ada</cell><cell>ACC</cell><cell cols="2">69.669304 174.603038</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_73"><head>Table 72 :</head><label>72</label><figDesc>Digits with ACC loss for tuning Knn model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell cols="4">Dataset Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell>Digits</cell><cell>Knn</cell><cell>ACC</cell><cell>100.040728</cell><cell>0.000000</cell></row><row><cell>BOHB-BB</cell><cell>Digits</cell><cell>Knn</cell><cell>ACC</cell><cell>100.039388</cell><cell>0.000036</cell></row><row><cell>Random-search</cell><cell>Digits</cell><cell>Knn</cell><cell>ACC</cell><cell>100.036708</cell><cell>0.000096</cell></row><row><cell>Hyperopt</cell><cell>Digits</cell><cell>Knn</cell><cell>ACC</cell><cell>100.031348</cell><cell>0.000172</cell></row><row><cell>Opentuner</cell><cell>Digits</cell><cell>Knn</cell><cell>ACC</cell><cell>99.657488</cell><cell>2.937454</cell></row><row><cell>PySOT</cell><cell>Digits</cell><cell>Knn</cell><cell>ACC</cell><cell>99.656148</cell><cell>2.936409</cell></row><row><cell>TuRBO+</cell><cell>Digits</cell><cell>Knn</cell><cell>ACC</cell><cell>98.884308</cell><cell>7.890293</cell></row><row><cell cols="2">Nevergrad (1+1) Digits</cell><cell>Knn</cell><cell>ACC</cell><cell>98.499729</cell><cell>9.868794</cell></row><row><cell>TuRBO</cell><cell>Digits</cell><cell>Knn</cell><cell>ACC</cell><cell cols="2">97.355369 14.097037</cell></row><row><cell>Skopt</cell><cell>Digits</cell><cell>Knn</cell><cell>ACC</cell><cell cols="2">97.351349 14.074483</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_74"><head>Table 73 :</head><label>73</label><figDesc>Digits with ACC loss for tuning Lasso model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell cols="3">Dataset Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell>Digits</cell><cell>Lasso ACC</cell><cell>99.812506</cell><cell>0.000015</cell></row><row><cell>TuRBO</cell><cell>Digits</cell><cell>Lasso ACC</cell><cell cols="2">99.579356 19.648246</cell></row><row><cell>TuRBO+</cell><cell>Digits</cell><cell>Lasso ACC</cell><cell>99.567177</cell><cell>9.142503</cell></row><row><cell>PySOT</cell><cell>Digits</cell><cell>Lasso ACC</cell><cell>98.824228</cell><cell>9.445856</cell></row><row><cell>Skopt</cell><cell>Digits</cell><cell>Lasso ACC</cell><cell>98.573678</cell><cell>4.903416</cell></row><row><cell>Hyperopt</cell><cell>Digits</cell><cell>Lasso ACC</cell><cell>98.320519</cell><cell>8.122499</cell></row><row><cell cols="2">Nevergrad (1+1) Digits</cell><cell>Lasso ACC</cell><cell cols="2">98.077799 42.798357</cell></row><row><cell>BOHB-BB</cell><cell>Digits</cell><cell>Lasso ACC</cell><cell>97.067772</cell><cell>9.136628</cell></row><row><cell>Opentuner</cell><cell>Digits</cell><cell>Lasso ACC</cell><cell cols="2">97.067772 14.404687</cell></row><row><cell>Random-search</cell><cell>Digits</cell><cell>Lasso ACC</cell><cell>95.815894</cell><cell>9.460591</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_75"><head>Table 74 :</head><label>74</label><figDesc>Digits with ACC loss for tuning Linear model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell cols="3">Dataset Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell>Digits</cell><cell>Linear ACC</cell><cell>92.457319</cell><cell>0.000058</cell></row><row><cell>PySOT</cell><cell>Digits</cell><cell>Linear ACC</cell><cell>92.439986</cell><cell>0.000736</cell></row><row><cell>Opentuner</cell><cell>Digits</cell><cell>Linear ACC</cell><cell>92.422652</cell><cell>0.001039</cell></row><row><cell>TuRBO</cell><cell>Digits</cell><cell>Linear ACC</cell><cell>92.104461</cell><cell>2.529241</cell></row><row><cell>Hyperopt</cell><cell>Digits</cell><cell>Linear ACC</cell><cell>92.092080</cell><cell>2.520398</cell></row><row><cell>Skopt</cell><cell>Digits</cell><cell>Linear ACC</cell><cell>91.351699</cell><cell>6.751230</cell></row><row><cell>TuRBO+</cell><cell>Digits</cell><cell>Linear ACC</cell><cell>90.680651</cell><cell>9.996151</cell></row><row><cell>BOHB-BB</cell><cell>Digits</cell><cell>Linear ACC</cell><cell>90.663318</cell><cell>9.987309</cell></row><row><cell>Random-search</cell><cell>Digits</cell><cell>Linear ACC</cell><cell cols="2">90.296841 11.103892</cell></row><row><cell cols="2">Nevergrad (1+1) Digits</cell><cell>Linear ACC</cell><cell cols="2">87.836744 43.931074</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_76"><head>Table 75 :</head><label>75</label><figDesc>Digits with NLL loss for tuning DT model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is TuRBO.</figDesc><table><row><cell>Algorithm</cell><cell cols="4">Dataset Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>TuRBO</cell><cell>Digits</cell><cell>DT</cell><cell>NLL</cell><cell>105.314498</cell><cell>15.866589</cell></row><row><cell>HEBO</cell><cell>Digits</cell><cell>DT</cell><cell>NLL</cell><cell>104.981244</cell><cell>18.690339</cell></row><row><cell>PySOT</cell><cell>Digits</cell><cell>DT</cell><cell>NLL</cell><cell>100.712129</cell><cell>25.343726</cell></row><row><cell>Skopt</cell><cell>Digits</cell><cell>DT</cell><cell>NLL</cell><cell>100.695390</cell><cell>158.169850</cell></row><row><cell>Hyperopt</cell><cell>Digits</cell><cell>DT</cell><cell>NLL</cell><cell>91.082975</cell><cell>73.272063</cell></row><row><cell>TuRBO+</cell><cell>Digits</cell><cell>DT</cell><cell>NLL</cell><cell>85.781199</cell><cell>240.286219</cell></row><row><cell>BOHB-BB</cell><cell>Digits</cell><cell>DT</cell><cell>NLL</cell><cell>83.167442</cell><cell>110.410120</cell></row><row><cell>Opentuner</cell><cell>Digits</cell><cell>DT</cell><cell>NLL</cell><cell>80.876262</cell><cell>324.737850</cell></row><row><cell>Random-search</cell><cell>Digits</cell><cell>DT</cell><cell>NLL</cell><cell>75.654077</cell><cell>204.351641</cell></row><row><cell cols="2">Nevergrad (1+1) Digits</cell><cell>DT</cell><cell>NLL</cell><cell cols="2">62.949280 1582.133768</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_77"><head>Table 76 :</head><label>76</label><figDesc>Digits with NLL loss for tuning MLP-adam model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is TuRBO.</figDesc><table><row><cell>Algorithm</cell><cell cols="2">Dataset Model</cell><cell cols="2">Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>TuRBO</cell><cell>Digits</cell><cell cols="2">MLP-adam NLL</cell><cell>104.936214</cell><cell>3.807905</cell></row><row><cell>HEBO</cell><cell>Digits</cell><cell cols="2">MLP-adam NLL</cell><cell>103.888110</cell><cell>2.816804</cell></row><row><cell>PySOT</cell><cell>Digits</cell><cell cols="2">MLP-adam NLL</cell><cell>102.476835</cell><cell>6.207587</cell></row><row><cell>Hyperopt</cell><cell>Digits</cell><cell cols="2">MLP-adam NLL</cell><cell>101.270862</cell><cell>5.540137</cell></row><row><cell>TuRBO+</cell><cell>Digits</cell><cell cols="2">MLP-adam NLL</cell><cell>100.550974</cell><cell>8.449016</cell></row><row><cell cols="2">Nevergrad (1+1) Digits</cell><cell cols="2">MLP-adam NLL</cell><cell cols="2">99.537718 15.391778</cell></row><row><cell>Opentuner</cell><cell>Digits</cell><cell cols="2">MLP-adam NLL</cell><cell cols="2">95.718504 17.973634</cell></row><row><cell>BOHB-BB</cell><cell>Digits</cell><cell cols="2">MLP-adam NLL</cell><cell cols="2">95.628722 17.000823</cell></row><row><cell>Random-search</cell><cell>Digits</cell><cell cols="2">MLP-adam NLL</cell><cell cols="2">95.303772 10.415455</cell></row><row><cell>Skopt</cell><cell>Digits</cell><cell cols="2">MLP-adam NLL</cell><cell cols="2">94.395163 12.085910</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_78"><head>Table 77 :</head><label>77</label><figDesc>Digits with NLL loss for tuning MLP-SGD model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell cols="2">Dataset Model</cell><cell cols="2">Metric Normalised Score Variance</cell></row><row><cell>HEBO</cell><cell>Digits</cell><cell cols="2">MLP-SGD NLL</cell><cell>100.316022 0.033631</cell></row><row><cell>TuRBO</cell><cell>Digits</cell><cell cols="2">MLP-SGD NLL</cell><cell>100.247280 0.020780</cell></row><row><cell>PySOT</cell><cell>Digits</cell><cell cols="2">MLP-SGD NLL</cell><cell>99.817072 0.191541</cell></row><row><cell>Hyperopt</cell><cell>Digits</cell><cell cols="2">MLP-SGD NLL</cell><cell>99.559253 0.118122</cell></row><row><cell>Opentuner</cell><cell>Digits</cell><cell cols="2">MLP-SGD NLL</cell><cell>99.166567 0.476585</cell></row><row><cell>Skopt</cell><cell>Digits</cell><cell cols="2">MLP-SGD NLL</cell><cell>99.120810 0.255960</cell></row><row><cell>TuRBO+</cell><cell>Digits</cell><cell cols="2">MLP-SGD NLL</cell><cell>98.906896 0.367730</cell></row><row><cell>BOHB-BB</cell><cell>Digits</cell><cell cols="2">MLP-SGD NLL</cell><cell>98.642683 0.487494</cell></row><row><cell cols="2">Nevergrad (1+1) Digits</cell><cell cols="2">MLP-SGD NLL</cell><cell>98.611544 3.487067</cell></row><row><cell>Random-search</cell><cell>Digits</cell><cell cols="2">MLP-SGD NLL</cell><cell>98.378973 0.516273</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_79"><head>Table 78 :</head><label>78</label><figDesc>Digits with NLL loss for tuning RF model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell cols="4">Dataset Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell>Digits</cell><cell>RF</cell><cell>NLL</cell><cell>124.468725</cell><cell>0.297160</cell></row><row><cell>PySOT</cell><cell>Digits</cell><cell>RF</cell><cell>NLL</cell><cell>124.073856</cell><cell>0.336978</cell></row><row><cell>TuRBO</cell><cell>Digits</cell><cell>RF</cell><cell>NLL</cell><cell>121.787294</cell><cell>21.428797</cell></row><row><cell>Skopt</cell><cell>Digits</cell><cell>RF</cell><cell>NLL</cell><cell>120.230526</cell><cell>2.533880</cell></row><row><cell>Hyperopt</cell><cell>Digits</cell><cell>RF</cell><cell>NLL</cell><cell>99.580977</cell><cell>158.609928</cell></row><row><cell cols="2">Nevergrad (1+1) Digits</cell><cell>RF</cell><cell>NLL</cell><cell cols="2">84.573865 3268.993667</cell></row><row><cell>TuRBO+</cell><cell>Digits</cell><cell>RF</cell><cell>NLL</cell><cell>81.415754</cell><cell>885.535630</cell></row><row><cell>Opentuner</cell><cell>Digits</cell><cell>RF</cell><cell>NLL</cell><cell>80.271515</cell><cell>997.408185</cell></row><row><cell>BOHB-BB</cell><cell>Digits</cell><cell>RF</cell><cell>NLL</cell><cell>69.251670</cell><cell>441.233007</cell></row><row><cell>Random-search</cell><cell>Digits</cell><cell>RF</cell><cell>NLL</cell><cell>67.481493</cell><cell>469.513384</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_80"><head>Table 79 :</head><label>79</label><figDesc>Digits with NLL loss for tuning SVM model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell cols="4">Dataset Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell>Digits</cell><cell>SVM</cell><cell>NLL</cell><cell>101.325277</cell><cell>5.161457</cell></row><row><cell>Opentuner</cell><cell>Digits</cell><cell>SVM</cell><cell>NLL</cell><cell>100.363949</cell><cell>7.971372</cell></row><row><cell>TuRBO</cell><cell>Digits</cell><cell>SVM</cell><cell>NLL</cell><cell>99.999409</cell><cell>4.000423</cell></row><row><cell>TuRBO+</cell><cell>Digits</cell><cell>SVM</cell><cell>NLL</cell><cell>99.759626</cell><cell>5.784031</cell></row><row><cell>Skopt</cell><cell>Digits</cell><cell>SVM</cell><cell>NLL</cell><cell>99.695385</cell><cell>9.680923</cell></row><row><cell>Hyperopt</cell><cell>Digits</cell><cell>SVM</cell><cell>NLL</cell><cell>98.923652</cell><cell>5.243537</cell></row><row><cell cols="2">Nevergrad (1+1) Digits</cell><cell>SVM</cell><cell>NLL</cell><cell cols="2">98.668450 15.533980</cell></row><row><cell>PySOT</cell><cell>Digits</cell><cell>SVM</cell><cell>NLL</cell><cell>98.531429</cell><cell>3.305281</cell></row><row><cell>Random-search</cell><cell>Digits</cell><cell>SVM</cell><cell>NLL</cell><cell>97.508303</cell><cell>4.801569</cell></row><row><cell>BOHB-BB</cell><cell>Digits</cell><cell>SVM</cell><cell>NLL</cell><cell>96.832038</cell><cell>8.596550</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_81"><head>Table 80 :</head><label>80</label><figDesc>Digits with NLL loss for tuning Ada model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell cols="4">Dataset Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell>Digits</cell><cell>Ada</cell><cell>NLL</cell><cell>111.973166</cell><cell>5.026622</cell></row><row><cell>Opentuner</cell><cell>Digits</cell><cell>Ada</cell><cell>NLL</cell><cell>108.973046</cell><cell>9.183543</cell></row><row><cell>Skopt</cell><cell>Digits</cell><cell>Ada</cell><cell>NLL</cell><cell>107.670995</cell><cell>53.503481</cell></row><row><cell>TuRBO</cell><cell>Digits</cell><cell>Ada</cell><cell>NLL</cell><cell>106.933065</cell><cell>66.341873</cell></row><row><cell cols="2">Nevergrad (1+1) Digits</cell><cell>Ada</cell><cell>NLL</cell><cell cols="2">103.648617 104.403755</cell></row><row><cell>PySOT</cell><cell>Digits</cell><cell>Ada</cell><cell>NLL</cell><cell>103.613606</cell><cell>18.883842</cell></row><row><cell>Hyperopt</cell><cell>Digits</cell><cell>Ada</cell><cell>NLL</cell><cell>102.669338</cell><cell>21.172717</cell></row><row><cell>TuRBO+</cell><cell>Digits</cell><cell>Ada</cell><cell>NLL</cell><cell>101.012913</cell><cell>48.423642</cell></row><row><cell>BOHB-BB</cell><cell>Digits</cell><cell>Ada</cell><cell>NLL</cell><cell cols="2">94.250499 161.887190</cell></row><row><cell>Random-search</cell><cell>Digits</cell><cell>Ada</cell><cell>NLL</cell><cell>93.337020</cell><cell>91.626773</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_82"><head>Table 81 :</head><label>81</label><figDesc>Digits with NLL loss for tuning Knn model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell cols="4">Dataset Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell>Digits</cell><cell>Knn</cell><cell>NLL</cell><cell>99.032850</cell><cell>0.000000</cell></row><row><cell>Opentuner</cell><cell>Digits</cell><cell>Knn</cell><cell>NLL</cell><cell>99.032850</cell><cell>0.000000</cell></row><row><cell>Random-search</cell><cell>Digits</cell><cell>Knn</cell><cell>NLL</cell><cell>98.292524</cell><cell>10.961651</cell></row><row><cell>PySOT</cell><cell>Digits</cell><cell>Knn</cell><cell>NLL</cell><cell>97.818694</cell><cell>8.793321</cell></row><row><cell>Hyperopt</cell><cell>Digits</cell><cell>Knn</cell><cell>NLL</cell><cell>97.009257</cell><cell>12.931355</cell></row><row><cell>TuRBO+</cell><cell>Digits</cell><cell>Knn</cell><cell>NLL</cell><cell>97.009257</cell><cell>12.931355</cell></row><row><cell>TuRBO</cell><cell>Digits</cell><cell>Knn</cell><cell>NLL</cell><cell>96.604538</cell><cell>14.483117</cell></row><row><cell>BOHB-BB</cell><cell>Digits</cell><cell>Knn</cell><cell>NLL</cell><cell>96.002435</cell><cell>31.845710</cell></row><row><cell cols="2">Nevergrad (1+1) Digits</cell><cell>Knn</cell><cell>NLL</cell><cell cols="2">89.407410 309.374772</cell></row><row><cell>Skopt</cell><cell>Digits</cell><cell>Knn</cell><cell>NLL</cell><cell cols="2">79.757469 260.984575</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_83"><head>Table 82 :</head><label>82</label><figDesc>Digits with NLL loss for tuning Lasso model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell cols="3">Dataset Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell>Digits</cell><cell>Lasso NLL</cell><cell>100.137158</cell><cell>0.000255</cell></row><row><cell>TuRBO</cell><cell>Digits</cell><cell>Lasso NLL</cell><cell>100.129152</cell><cell>0.000276</cell></row><row><cell>PySOT</cell><cell>Digits</cell><cell>Lasso NLL</cell><cell>100.041641</cell><cell>0.002026</cell></row><row><cell>Skopt</cell><cell>Digits</cell><cell>Lasso NLL</cell><cell>100.012443</cell><cell>0.016183</cell></row><row><cell>Opentuner</cell><cell>Digits</cell><cell>Lasso NLL</cell><cell>99.993744</cell><cell>0.034008</cell></row><row><cell>Hyperopt</cell><cell>Digits</cell><cell>Lasso NLL</cell><cell>99.916651</cell><cell>0.018303</cell></row><row><cell>Random-search</cell><cell>Digits</cell><cell>Lasso NLL</cell><cell>99.336829</cell><cell>0.333326</cell></row><row><cell>TuRBO+</cell><cell>Digits</cell><cell>Lasso NLL</cell><cell>99.205388</cell><cell>5.479509</cell></row><row><cell>BOHB-BB</cell><cell>Digits</cell><cell>Lasso NLL</cell><cell>98.596349</cell><cell>2.171619</cell></row><row><cell cols="2">Nevergrad (1+1) Digits</cell><cell>Lasso NLL</cell><cell cols="2">97.693951 18.423865</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_84"><head>Table 83 :</head><label>83</label><figDesc>Digits with NLL loss for tuning Linear model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell cols="3">Dataset Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell>Digits</cell><cell>Linear NLL</cell><cell>100.615794</cell><cell>0.000224</cell></row><row><cell>TuRBO</cell><cell>Digits</cell><cell>Linear NLL</cell><cell>100.608559</cell><cell>0.000513</cell></row><row><cell>PySOT</cell><cell>Digits</cell><cell>Linear NLL</cell><cell>100.585723</cell><cell>0.001391</cell></row><row><cell>Hyperopt</cell><cell>Digits</cell><cell>Linear NLL</cell><cell>100.428061</cell><cell>0.021496</cell></row><row><cell>Opentuner</cell><cell>Digits</cell><cell>Linear NLL</cell><cell>100.323235</cell><cell>0.175515</cell></row><row><cell>TuRBO+</cell><cell>Digits</cell><cell>Linear NLL</cell><cell>100.316800</cell><cell>0.117834</cell></row><row><cell>Skopt</cell><cell>Digits</cell><cell>Linear NLL</cell><cell>99.709029</cell><cell>2.042606</cell></row><row><cell>BOHB-BB</cell><cell>Digits</cell><cell>Linear NLL</cell><cell>98.930379</cell><cell>4.622083</cell></row><row><cell>Random-search</cell><cell>Digits</cell><cell>Linear NLL</cell><cell>98.655079</cell><cell>3.097357</cell></row><row><cell cols="2">Nevergrad (1+1) Digits</cell><cell>Linear NLL</cell><cell cols="2">98.269646 48.924742</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_85"><head>Table 84 :</head><label>84</label><figDesc>Iris with ACC loss for tuning DT model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell cols="4">Dataset Model Metric Normalised Score Variance</cell></row><row><cell>HEBO</cell><cell>Iris</cell><cell>DT</cell><cell>ACC</cell><cell>99.247213 2.809044</cell></row><row><cell>PySOT</cell><cell>Iris</cell><cell>DT</cell><cell>ACC</cell><cell>99.018466 3.525075</cell></row><row><cell>Skopt</cell><cell>Iris</cell><cell>DT</cell><cell>ACC</cell><cell>98.789719 2.478568</cell></row><row><cell cols="2">Nevergrad (1+1) Iris</cell><cell>DT</cell><cell>ACC</cell><cell>98.560972 1.872696</cell></row><row><cell>TuRBO</cell><cell>Iris</cell><cell>DT</cell><cell>ACC</cell><cell>98.446598 1.253054</cell></row><row><cell>TuRBO+</cell><cell>Iris</cell><cell>DT</cell><cell>ACC</cell><cell>98.217851 1.032737</cell></row><row><cell>Random-search</cell><cell>Iris</cell><cell>DT</cell><cell>ACC</cell><cell>97.874730 1.046507</cell></row><row><cell>Hyperopt</cell><cell>Iris</cell><cell>DT</cell><cell>ACC</cell><cell>97.760356 0.261627</cell></row><row><cell>BOHB-BB</cell><cell>Iris</cell><cell>DT</cell><cell>ACC</cell><cell>97.645982 0.000000</cell></row><row><cell>Opentuner</cell><cell>Iris</cell><cell>DT</cell><cell>ACC</cell><cell>97.645982 1.652379</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_86"><head>Table 85 :</head><label>85</label><figDesc>Iris with ACC loss for tuning MLP-adam model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell cols="2">Dataset Model</cell><cell cols="2">Metric Normalised Score Variance</cell></row><row><cell>HEBO</cell><cell>Iris</cell><cell cols="2">MLP-adam ACC</cell><cell>97.523595 2.387333</cell></row><row><cell>TuRBO</cell><cell>Iris</cell><cell cols="2">MLP-adam ACC</cell><cell>97.404537 1.477162</cell></row><row><cell>Opentuner</cell><cell>Iris</cell><cell cols="2">MLP-adam ACC</cell><cell>97.166421 1.954629</cell></row><row><cell>PySOT</cell><cell>Iris</cell><cell cols="2">MLP-adam ACC</cell><cell>97.047363 1.551766</cell></row><row><cell cols="2">Nevergrad (1+1) Iris</cell><cell cols="2">MLP-adam ACC</cell><cell>97.047363 2.745433</cell></row><row><cell>Skopt</cell><cell>Iris</cell><cell cols="2">MLP-adam ACC</cell><cell>96.928305 1.119062</cell></row><row><cell>TuRBO+</cell><cell>Iris</cell><cell cols="2">MLP-adam ACC</cell><cell>96.928305 2.312729</cell></row><row><cell>Hyperopt</cell><cell>Iris</cell><cell cols="2">MLP-adam ACC</cell><cell>96.213958 2.670829</cell></row><row><cell>Random-search</cell><cell>Iris</cell><cell cols="2">MLP-adam ACC</cell><cell>96.094900 4.416566</cell></row><row><cell>BOHB-BB</cell><cell>Iris</cell><cell cols="2">MLP-adam ACC</cell><cell>95.142437 4.177833</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_87"><head>Table 89 :</head><label>89</label><figDesc>Iris with ACC loss for tuning Ada model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is BOHB.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_88"><head>Table 92 :</head><label>92</label><figDesc>Iris with ACC loss for tuning Linear model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell cols="3">Dataset Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell>Iris</cell><cell>Linear ACC</cell><cell>102.086438</cell><cell>0.000000</cell></row><row><cell>PySOT</cell><cell>Iris</cell><cell>Linear ACC</cell><cell>102.086438</cell><cell>0.000000</cell></row><row><cell>Opentuner</cell><cell>Iris</cell><cell>Linear ACC</cell><cell>101.061848</cell><cell>20.995700</cell></row><row><cell>Skopt</cell><cell>Iris</cell><cell>Linear ACC</cell><cell>100.037258</cell><cell>39.781326</cell></row><row><cell>Hyperopt</cell><cell>Iris</cell><cell>Linear ACC</cell><cell>99.012668</cell><cell>56.356879</cell></row><row><cell>TuRBO</cell><cell>Iris</cell><cell>Linear ACC</cell><cell>99.012668</cell><cell>56.356879</cell></row><row><cell>TuRBO+</cell><cell>Iris</cell><cell>Linear ACC</cell><cell>99.012668</cell><cell>56.356879</cell></row><row><cell>BOHB-BB</cell><cell>Iris</cell><cell>Linear ACC</cell><cell>97.988077</cell><cell>70.722358</cell></row><row><cell>Random-search</cell><cell>Iris</cell><cell>Linear ACC</cell><cell cols="2">91.840537 110.503685</cell></row><row><cell cols="2">Nevergrad (1+1) Iris</cell><cell>Linear ACC</cell><cell>87.742176</cell><cell>92.823095</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_89"><head>Table 93 :</head><label>93</label><figDesc>Iris with NLL loss for tuning DT model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is Skopt.</figDesc><table><row><cell>Algorithm</cell><cell cols="4">Dataset Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>Skopt</cell><cell>Iris</cell><cell>DT</cell><cell>NLL</cell><cell>90.203906</cell><cell>10.555548</cell></row><row><cell>BOHB-BB</cell><cell>Iris</cell><cell>DT</cell><cell>NLL</cell><cell>89.526762</cell><cell>6.488902</cell></row><row><cell>Random-search</cell><cell>Iris</cell><cell>DT</cell><cell>NLL</cell><cell>89.082026</cell><cell>5.704656</cell></row><row><cell>HEBO</cell><cell>Iris</cell><cell>DT</cell><cell>NLL</cell><cell>88.967765</cell><cell>17.831672</cell></row><row><cell>TuRBO+</cell><cell>Iris</cell><cell>DT</cell><cell>NLL</cell><cell>88.562978</cell><cell>52.676524</cell></row><row><cell>PySOT</cell><cell>Iris</cell><cell>DT</cell><cell>NLL</cell><cell>87.994255</cell><cell>81.937308</cell></row><row><cell>Opentuner</cell><cell>Iris</cell><cell>DT</cell><cell>NLL</cell><cell>87.961800</cell><cell>83.095049</cell></row><row><cell>TuRBO</cell><cell>Iris</cell><cell>DT</cell><cell>NLL</cell><cell cols="2">86.393584 167.928590</cell></row><row><cell>Hyperopt</cell><cell>Iris</cell><cell>DT</cell><cell>NLL</cell><cell>86.051753</cell><cell>41.891055</cell></row><row><cell cols="2">Nevergrad (1+1) Iris</cell><cell>DT</cell><cell>NLL</cell><cell cols="2">72.442142 312.271956</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_90"><head>Table 94 :</head><label>94</label><figDesc>Iris with NLL loss for tuning MLP-adam model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_91"><head>Table 98 :</head><label>98</label><figDesc>Iris with NLL loss for tuning Ada model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is Opentuner.D.6 Wine Data Set</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_92"><head>Table 102 :</head><label>102</label><figDesc>Wine with ACC loss for tuning DT model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is PySOT.</figDesc><table><row><cell>Algorithm</cell><cell cols="4">Dataset Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>PySOT</cell><cell>Wine</cell><cell>DT</cell><cell>ACC</cell><cell>102.640360</cell><cell>4.994593</cell></row><row><cell cols="2">Nevergrad (1+1) Wine</cell><cell>DT</cell><cell>ACC</cell><cell cols="2">102.412606 12.231128</cell></row><row><cell>Skopt</cell><cell>Wine</cell><cell>DT</cell><cell>ACC</cell><cell>101.067267</cell><cell>5.199240</cell></row><row><cell>TuRBO</cell><cell>Wine</cell><cell>DT</cell><cell>ACC</cell><cell cols="2">99.968220 12.856860</cell></row><row><cell>HEBO</cell><cell>Wine</cell><cell>DT</cell><cell>ACC</cell><cell>99.608051</cell><cell>8.503162</cell></row><row><cell>TuRBO+</cell><cell>Wine</cell><cell>DT</cell><cell>ACC</cell><cell>99.152542</cell><cell>9.570280</cell></row><row><cell>Hyperopt</cell><cell>Wine</cell><cell>DT</cell><cell>ACC</cell><cell>98.697034</cell><cell>4.909287</cell></row><row><cell>Random-search</cell><cell>Wine</cell><cell>DT</cell><cell>ACC</cell><cell>97.534428</cell><cell>1.557289</cell></row><row><cell>BOHB-BB</cell><cell>Wine</cell><cell>DT</cell><cell>ACC</cell><cell>97.086864</cell><cell>0.453885</cell></row><row><cell>Opentuner</cell><cell>Wine</cell><cell>DT</cell><cell>ACC</cell><cell>97.086864</cell><cell>1.206916</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_93"><head>Table 103 :</head><label>103</label><figDesc>Wine with ACC loss for tuning MLP-adam model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell cols="2">Dataset Model</cell><cell cols="2">Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell>Wine</cell><cell cols="2">MLP-adam ACC</cell><cell>102.288087</cell><cell>3.576365</cell></row><row><cell>PySOT</cell><cell>Wine</cell><cell cols="2">MLP-adam ACC</cell><cell>101.781214</cell><cell>16.573344</cell></row><row><cell>TuRBO</cell><cell>Wine</cell><cell cols="2">MLP-adam ACC</cell><cell>100.990836</cell><cell>41.217673</cell></row><row><cell>Skopt</cell><cell>Wine</cell><cell cols="2">MLP-adam ACC</cell><cell>100.727377</cell><cell>8.173123</cell></row><row><cell>TuRBO+</cell><cell>Wine</cell><cell cols="2">MLP-adam ACC</cell><cell>97.943872</cell><cell>29.486961</cell></row><row><cell>Hyperopt</cell><cell>Wine</cell><cell cols="2">MLP-adam ACC</cell><cell>96.540664</cell><cell>18.874205</cell></row><row><cell cols="2">Nevergrad (1+1) Wine</cell><cell cols="2">MLP-adam ACC</cell><cell cols="2">92.328179 161.347003</cell></row><row><cell>Random-search</cell><cell>Wine</cell><cell cols="2">MLP-adam ACC</cell><cell>90.816151</cell><cell>63.213250</cell></row><row><cell>Opentuner</cell><cell>Wine</cell><cell cols="2">MLP-adam ACC</cell><cell cols="2">89.183849 103.560060</cell></row><row><cell>BOHB-BB</cell><cell>Wine</cell><cell cols="2">MLP-adam ACC</cell><cell>87.542955</cell><cell>57.246663</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_94"><head>Table 104 :</head><label>104</label><figDesc>Wine with ACC loss for tuning MLP-SGD model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell cols="2">Dataset Model</cell><cell cols="2">Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell>Wine</cell><cell cols="2">MLP-SGD ACC</cell><cell>98.576865</cell><cell>76.332793</cell></row><row><cell>Skopt</cell><cell>Wine</cell><cell cols="2">MLP-SGD ACC</cell><cell>96.955860</cell><cell>72.699785</cell></row><row><cell>TuRBO</cell><cell>Wine</cell><cell cols="2">MLP-SGD ACC</cell><cell cols="2">95.133181 291.308493</cell></row><row><cell>PySOT</cell><cell>Wine</cell><cell cols="2">MLP-SGD ACC</cell><cell cols="2">94.414003 344.889994</cell></row><row><cell>Hyperopt</cell><cell>Wine</cell><cell cols="2">MLP-SGD ACC</cell><cell>87.423896</cell><cell>76.509456</cell></row><row><cell cols="2">Nevergrad (1+1) Wine</cell><cell cols="2">MLP-SGD ACC</cell><cell cols="2">84.821157 393.474837</cell></row><row><cell>TuRBO+</cell><cell>Wine</cell><cell cols="2">MLP-SGD ACC</cell><cell cols="2">84.303653 163.191728</cell></row><row><cell>Random-search</cell><cell>Wine</cell><cell cols="2">MLP-SGD ACC</cell><cell>79.113394</cell><cell>53.744005</cell></row><row><cell>BOHB-BB</cell><cell>Wine</cell><cell cols="2">MLP-SGD ACC</cell><cell cols="2">78.576865 147.437959</cell></row><row><cell>Opentuner</cell><cell>Wine</cell><cell cols="2">MLP-SGD ACC</cell><cell cols="2">53.089802 324.782282</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_95"><head>Table 105 :</head><label>105</label><figDesc>Wine with ACC loss for tuning RF model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is Skopt.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_96"><head>Table 106 :</head><label>106</label><figDesc>Wine with ACC loss for tuning SVM model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell cols="4">Dataset Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell>Wine</cell><cell>SVM</cell><cell>ACC</cell><cell>101.089494</cell><cell>23.739951</cell></row><row><cell>Skopt</cell><cell>Wine</cell><cell>SVM</cell><cell>ACC</cell><cell>96.050584</cell><cell>42.094346</cell></row><row><cell>PySOT</cell><cell>Wine</cell><cell>SVM</cell><cell>ACC</cell><cell>95.603113</cell><cell>30.537137</cell></row><row><cell>TuRBO</cell><cell>Wine</cell><cell>SVM</cell><cell>ACC</cell><cell>95.486381</cell><cell>43.769737</cell></row><row><cell>Opentuner</cell><cell>Wine</cell><cell>SVM</cell><cell>ACC</cell><cell>92.743191</cell><cell>54.413749</cell></row><row><cell>Hyperopt</cell><cell>Wine</cell><cell>SVM</cell><cell>ACC</cell><cell>91.031128</cell><cell>20.295538</cell></row><row><cell cols="2">Nevergrad (1+1) Wine</cell><cell>SVM</cell><cell>ACC</cell><cell>90.408560</cell><cell>17.088987</cell></row><row><cell>TuRBO+</cell><cell>Wine</cell><cell>SVM</cell><cell>ACC</cell><cell cols="2">89.435798 134.768764</cell></row><row><cell>Random-search</cell><cell>Wine</cell><cell>SVM</cell><cell>ACC</cell><cell>83.229572</cell><cell>31.251121</cell></row><row><cell>BOHB-BB</cell><cell>Wine</cell><cell>SVM</cell><cell>ACC</cell><cell>82.140078</cell><cell>30.374578</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_97"><head>Table 107 :</head><label>107</label><figDesc>Wine with ACC loss for tuning Ada model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell cols="4">Dataset Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell>Wine</cell><cell>Ada</cell><cell>ACC</cell><cell>102.249135</cell><cell>61.472091</cell></row><row><cell>Opentuner</cell><cell>Wine</cell><cell>Ada</cell><cell>ACC</cell><cell>87.525952</cell><cell>89.791159</cell></row><row><cell>TuRBO</cell><cell>Wine</cell><cell>Ada</cell><cell>ACC</cell><cell cols="2">87.456747 186.588749</cell></row><row><cell cols="2">Nevergrad (1+1) Wine</cell><cell>Ada</cell><cell>ACC</cell><cell cols="2">87.024221 445.327648</cell></row><row><cell>PySOT</cell><cell>Wine</cell><cell>Ada</cell><cell>ACC</cell><cell>85.121107</cell><cell>95.696071</cell></row><row><cell>TuRBO+</cell><cell>Wine</cell><cell>Ada</cell><cell>ACC</cell><cell cols="2">83.044983 134.639949</cell></row><row><cell>Hyperopt</cell><cell>Wine</cell><cell>Ada</cell><cell>ACC</cell><cell>80.536332</cell><cell>97.458944</cell></row><row><cell>Skopt</cell><cell>Wine</cell><cell>Ada</cell><cell>ACC</cell><cell cols="2">77.595156 548.313094</cell></row><row><cell>Random-search</cell><cell>Wine</cell><cell>Ada</cell><cell>ACC</cell><cell>73.702422</cell><cell>43.178551</cell></row><row><cell>BOHB-BB</cell><cell>Wine</cell><cell>Ada</cell><cell>ACC</cell><cell>72.698962</cell><cell>81.226341</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_98"><head>Table 108 :</head><label>108</label><figDesc>Wine with ACC loss for tuning Knn model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell cols="4">Dataset Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell>Wine</cell><cell>Knn</cell><cell>ACC</cell><cell>100.000000</cell><cell>0.000000</cell></row><row><cell cols="2">Nevergrad (1+1) Wine</cell><cell>Knn</cell><cell>ACC</cell><cell>100.000000</cell><cell>0.000000</cell></row><row><cell>Opentuner</cell><cell>Wine</cell><cell>Knn</cell><cell>ACC</cell><cell>100.000000</cell><cell>0.000000</cell></row><row><cell>Hyperopt</cell><cell>Wine</cell><cell>Knn</cell><cell>ACC</cell><cell cols="2">89.612546 219.840273</cell></row><row><cell>TuRBO+</cell><cell>Wine</cell><cell>Knn</cell><cell>ACC</cell><cell cols="2">89.077491 317.790364</cell></row><row><cell>Random-search</cell><cell>Wine</cell><cell>Knn</cell><cell>ACC</cell><cell cols="2">83.994465 285.251803</cell></row><row><cell>Skopt</cell><cell>Wine</cell><cell>Knn</cell><cell>ACC</cell><cell cols="2">83.791513 325.106208</cell></row><row><cell>BOHB-BB</cell><cell>Wine</cell><cell>Knn</cell><cell>ACC</cell><cell cols="2">82.416974 287.574559</cell></row><row><cell>TuRBO</cell><cell>Wine</cell><cell>Knn</cell><cell>ACC</cell><cell cols="2">70.313653 597.435894</cell></row><row><cell>PySOT</cell><cell>Wine</cell><cell>Knn</cell><cell>ACC</cell><cell cols="2">70.036900 236.259826</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_99"><head>Table 109 :</head><label>109</label><figDesc>Wine with ACC loss for tuning Lasso model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is PySOT. Rivers, A. I., Palenicek, D., Moens, V.,Abdullah, M., Sootla, A., Wang, J., &amp; Bou- Ammar, H. (2020). SAMBA: Safe model-based &amp; active reinforcement learning. In arXiv preprint arXiv:2006.09436. Deb, K., Pratap, A., Agarwal, S., &amp; Meyarivan, T. (2002). A fast and elitist multiobjective genetic algorithm: NSGA-II. IEEE Transactions on Evolutionary Computation, 6 (2), 182-197. Dua, D., &amp; Graff, C. (2017). UCI machine learning repository. http://archive.ics.uci.edu/ml. Eriksson, D., Bindel, D., &amp; Shoemaker, C. A. (2019a). pySOT and POAP: An event-driven asynchronous framework for surrogate optimization. arXiv preprint arXiv:1908.00420. Eriksson, D., Pearce, M., Gardner, J., Turner, R. D., &amp; Poloczek, M. (2019b). Scalable global optimization via local Bayesian optimization. Advances in Neural Information Processing Systems, 32, 5496-5507.</figDesc><table><row><cell>Algorithm</cell><cell cols="3">Dataset Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>PySOT</cell><cell>Wine</cell><cell>Lasso ACC</cell><cell>98.390805</cell><cell>51.790197</cell></row><row><cell>HEBO</cell><cell>Wine</cell><cell>Lasso ACC</cell><cell cols="2">95.172414 139.015792</cell></row><row><cell>Hyperopt</cell><cell>Wine</cell><cell>Lasso ACC</cell><cell cols="2">87.068966 264.079243</cell></row><row><cell>TuRBO+</cell><cell>Wine</cell><cell>Lasso ACC</cell><cell cols="2">85.517241 597.089235</cell></row><row><cell>TuRBO</cell><cell>Wine</cell><cell>Lasso ACC</cell><cell cols="2">83.908046 708.707957</cell></row><row><cell>Opentuner</cell><cell>Wine</cell><cell>Lasso ACC</cell><cell cols="2">79.195402 685.469123</cell></row><row><cell>BOHB-BB</cell><cell>Wine</cell><cell>Lasso ACC</cell><cell cols="2">79.022989 358.508737</cell></row><row><cell>Skopt</cell><cell>Wine</cell><cell>Lasso ACC</cell><cell cols="2">67.701149 545.285131</cell></row><row><cell>Random-search</cell><cell>Wine</cell><cell>Lasso ACC</cell><cell cols="2">67.643678 327.273296</cell></row><row><cell cols="2">Nevergrad (1+1) Wine</cell><cell>Lasso ACC</cell><cell cols="2">66.091954 818.782986</cell></row><row><cell>Cowen-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_100"><head>Table 110 :</head><label>110</label><figDesc>Wine with ACC loss for tuning Linear model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell cols="3">Dataset Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell>Wine</cell><cell>Linear ACC</cell><cell>100.000000</cell><cell>0.000000</cell></row><row><cell>PySOT</cell><cell>Wine</cell><cell>Linear ACC</cell><cell>98.372093</cell><cell>53.001622</cell></row><row><cell>Opentuner</cell><cell>Wine</cell><cell>Linear ACC</cell><cell>98.313953</cell><cell>56.855057</cell></row><row><cell>Hyperopt</cell><cell>Wine</cell><cell>Linear ACC</cell><cell>96.686047</cell><cell>104.078307</cell></row><row><cell>TuRBO</cell><cell>Wine</cell><cell>Linear ACC</cell><cell>93.372093</cell><cell>300.603456</cell></row><row><cell>Random-search</cell><cell>Wine</cell><cell>Linear ACC</cell><cell>90.116279</cell><cell>240.030173</cell></row><row><cell>TuRBO+</cell><cell>Wine</cell><cell>Linear ACC</cell><cell>85.058140</cell><cell>634.094532</cell></row><row><cell cols="2">Nevergrad (1+1) Wine</cell><cell>Linear ACC</cell><cell cols="2">81.918605 1182.356750</cell></row><row><cell>BOHB-BB</cell><cell>Wine</cell><cell>Linear ACC</cell><cell>81.686047</cell><cell>289.043153</cell></row><row><cell>Skopt</cell><cell>Wine</cell><cell>Linear ACC</cell><cell>75.116279</cell><cell>910.933364</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_101"><head>Table 111 :</head><label>111</label><figDesc>Wine with NLL loss for tuning DT model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is PySOT.</figDesc><table><row><cell>Algorithm</cell><cell cols="4">Dataset Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>PySOT</cell><cell>Wine</cell><cell>DT</cell><cell>NLL</cell><cell>100.735933</cell><cell>7.443055</cell></row><row><cell>Skopt</cell><cell>Wine</cell><cell>DT</cell><cell>NLL</cell><cell>100.221883</cell><cell>3.394850</cell></row><row><cell>HEBO</cell><cell>Wine</cell><cell>DT</cell><cell>NLL</cell><cell>99.832918</cell><cell>3.739025</cell></row><row><cell>TuRBO+</cell><cell>Wine</cell><cell>DT</cell><cell>NLL</cell><cell>99.674754</cell><cell>3.371033</cell></row><row><cell>BOHB-BB</cell><cell>Wine</cell><cell>DT</cell><cell>NLL</cell><cell>98.178211</cell><cell>4.342658</cell></row><row><cell>TuRBO</cell><cell>Wine</cell><cell>DT</cell><cell>NLL</cell><cell>98.058048</cell><cell>83.877777</cell></row><row><cell>Random-search</cell><cell>Wine</cell><cell>DT</cell><cell>NLL</cell><cell>97.376968</cell><cell>10.077564</cell></row><row><cell>Opentuner</cell><cell>Wine</cell><cell>DT</cell><cell>NLL</cell><cell>97.049151</cell><cell>36.266865</cell></row><row><cell>Hyperopt</cell><cell>Wine</cell><cell>DT</cell><cell>NLL</cell><cell>96.334956</cell><cell>34.297016</cell></row><row><cell cols="2">Nevergrad (1+1) Wine</cell><cell>DT</cell><cell>NLL</cell><cell cols="2">89.297835 389.090524</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_102"><head>Table 112 :</head><label>112</label><figDesc>Wine with NLL loss for tuning MLP-adam model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is TuRBO.</figDesc><table><row><cell>Algorithm</cell><cell cols="2">Dataset Model</cell><cell cols="2">Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>TuRBO</cell><cell>Wine</cell><cell cols="2">MLP-adam NLL</cell><cell>100.249989</cell><cell>0.565669</cell></row><row><cell>Skopt</cell><cell>Wine</cell><cell cols="2">MLP-adam NLL</cell><cell>99.981264</cell><cell>0.167045</cell></row><row><cell>PySOT</cell><cell>Wine</cell><cell cols="2">MLP-adam NLL</cell><cell>99.926231</cell><cell>0.303040</cell></row><row><cell>HEBO</cell><cell>Wine</cell><cell cols="2">MLP-adam NLL</cell><cell>99.565384</cell><cell>0.763738</cell></row><row><cell>Hyperopt</cell><cell>Wine</cell><cell cols="2">MLP-adam NLL</cell><cell>99.449852</cell><cell>0.504218</cell></row><row><cell>Opentuner</cell><cell>Wine</cell><cell cols="2">MLP-adam NLL</cell><cell>98.831651</cell><cell>1.222083</cell></row><row><cell>TuRBO+</cell><cell>Wine</cell><cell cols="2">MLP-adam NLL</cell><cell>98.571847</cell><cell>2.666428</cell></row><row><cell>Random-search</cell><cell>Wine</cell><cell cols="2">MLP-adam NLL</cell><cell>98.111233</cell><cell>0.855310</cell></row><row><cell>BOHB-BB</cell><cell>Wine</cell><cell cols="2">MLP-adam NLL</cell><cell>97.819061</cell><cell>3.117717</cell></row><row><cell cols="2">Nevergrad (1+1) Wine</cell><cell cols="2">MLP-adam NLL</cell><cell cols="2">97.151962 33.698618</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_103"><head>Table 113 :</head><label>113</label><figDesc>Wine with NLL loss for tuning MLP-SGD model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell cols="2">Dataset Model</cell><cell cols="2">Metric Normalised Score Variance</cell></row><row><cell>HEBO</cell><cell>Wine</cell><cell cols="2">MLP-SGD NLL</cell><cell>100.065123 0.406916</cell></row><row><cell>Hyperopt</cell><cell>Wine</cell><cell cols="2">MLP-SGD NLL</cell><cell>99.764357 0.625817</cell></row><row><cell>TuRBO</cell><cell>Wine</cell><cell cols="2">MLP-SGD NLL</cell><cell>99.560475 0.802736</cell></row><row><cell>Skopt</cell><cell>Wine</cell><cell cols="2">MLP-SGD NLL</cell><cell>99.420960 0.566385</cell></row><row><cell cols="2">Nevergrad (1+1) Wine</cell><cell cols="2">MLP-SGD NLL</cell><cell>99.358015 0.910527</cell></row><row><cell>PySOT</cell><cell>Wine</cell><cell cols="2">MLP-SGD NLL</cell><cell>99.104753 0.965483</cell></row><row><cell>TuRBO+</cell><cell>Wine</cell><cell cols="2">MLP-SGD NLL</cell><cell>98.940260 0.886137</cell></row><row><cell>Random-search</cell><cell>Wine</cell><cell cols="2">MLP-SGD NLL</cell><cell>98.692717 0.489974</cell></row><row><cell>BOHB-BB</cell><cell>Wine</cell><cell cols="2">MLP-SGD NLL</cell><cell>98.358461 0.496547</cell></row><row><cell>Opentuner</cell><cell>Wine</cell><cell cols="2">MLP-SGD NLL</cell><cell>98.254971 0.292497</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_104"><head>Table 114 :</head><label>114</label><figDesc>Wine with NLL loss for tuning RF model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell cols="4">Dataset Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell>Wine</cell><cell>RF</cell><cell>NLL</cell><cell>104.649110</cell><cell>2.581885</cell></row><row><cell>PySOT</cell><cell>Wine</cell><cell>RF</cell><cell>NLL</cell><cell>104.355692</cell><cell>1.566376</cell></row><row><cell>Skopt</cell><cell>Wine</cell><cell>RF</cell><cell>NLL</cell><cell>104.170679</cell><cell>0.665546</cell></row><row><cell>TuRBO</cell><cell>Wine</cell><cell>RF</cell><cell>NLL</cell><cell>104.031688</cell><cell>4.984665</cell></row><row><cell cols="2">Nevergrad (1+1) Wine</cell><cell>RF</cell><cell>NLL</cell><cell cols="2">101.465863 15.172791</cell></row><row><cell>Hyperopt</cell><cell>Wine</cell><cell>RF</cell><cell>NLL</cell><cell>99.738821</cell><cell>6.443466</cell></row><row><cell>TuRBO+</cell><cell>Wine</cell><cell>RF</cell><cell>NLL</cell><cell>99.334611</cell><cell>7.498471</cell></row><row><cell>BOHB-BB</cell><cell>Wine</cell><cell>RF</cell><cell>NLL</cell><cell>97.437084</cell><cell>5.523676</cell></row><row><cell>Random-search</cell><cell>Wine</cell><cell>RF</cell><cell>NLL</cell><cell>97.361000</cell><cell>2.798394</cell></row><row><cell>Opentuner</cell><cell>Wine</cell><cell>RF</cell><cell>NLL</cell><cell>97.308388</cell><cell>2.661822</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_105"><head>Table 115 :</head><label>115</label><figDesc>Wine with NLL loss for tuning SVM model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell cols="4">Dataset Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell>Wine</cell><cell>SVM</cell><cell>NLL</cell><cell>93.675487</cell><cell>32.073349</cell></row><row><cell>PySOT</cell><cell>Wine</cell><cell>SVM</cell><cell>NLL</cell><cell>93.111866</cell><cell>33.743562</cell></row><row><cell>TuRBO</cell><cell>Wine</cell><cell>SVM</cell><cell>NLL</cell><cell>93.062091</cell><cell>34.028682</cell></row><row><cell>TuRBO+</cell><cell>Wine</cell><cell>SVM</cell><cell>NLL</cell><cell cols="2">91.816538 141.971101</cell></row><row><cell>Opentuner</cell><cell>Wine</cell><cell>SVM</cell><cell>NLL</cell><cell>91.608167</cell><cell>31.115479</cell></row><row><cell>Skopt</cell><cell>Wine</cell><cell>SVM</cell><cell>NLL</cell><cell>89.843405</cell><cell>31.673409</cell></row><row><cell>BOHB-BB</cell><cell>Wine</cell><cell>SVM</cell><cell>NLL</cell><cell>86.866351</cell><cell>32.611321</cell></row><row><cell>Hyperopt</cell><cell>Wine</cell><cell>SVM</cell><cell>NLL</cell><cell>86.472718</cell><cell>10.534871</cell></row><row><cell>Random-search</cell><cell>Wine</cell><cell>SVM</cell><cell>NLL</cell><cell>83.877620</cell><cell>47.933648</cell></row><row><cell cols="2">Nevergrad (1+1) Wine</cell><cell>SVM</cell><cell>NLL</cell><cell cols="2">80.529537 219.352311</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_106"><head>Table 118 :</head><label>118</label><figDesc>Wine with NLL loss for tuning Lasso model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell cols="3">Dataset Model Metric Normalised Score Variance</cell></row><row><cell>HEBO</cell><cell>Wine</cell><cell>Lasso NLL</cell><cell>99.827097 0.033778</cell></row><row><cell cols="2">Nevergrad (1+1) Wine</cell><cell>Lasso NLL</cell><cell>99.820084 0.022893</cell></row><row><cell>TuRBO</cell><cell>Wine</cell><cell>Lasso NLL</cell><cell>99.755145 0.024582</cell></row><row><cell>TuRBO+</cell><cell>Wine</cell><cell>Lasso NLL</cell><cell>99.708446 0.317074</cell></row><row><cell>PySOT</cell><cell>Wine</cell><cell>Lasso NLL</cell><cell>99.582104 0.021060</cell></row><row><cell>Hyperopt</cell><cell>Wine</cell><cell>Lasso NLL</cell><cell>99.469131 0.030572</cell></row><row><cell>Opentuner</cell><cell>Wine</cell><cell>Lasso NLL</cell><cell>99.448682 0.204748</cell></row><row><cell>BOHB-BB</cell><cell>Wine</cell><cell>Lasso NLL</cell><cell>99.393568 0.047506</cell></row><row><cell>Skopt</cell><cell>Wine</cell><cell>Lasso NLL</cell><cell>99.381857 1.114643</cell></row><row><cell>Random-search</cell><cell>Wine</cell><cell>Lasso NLL</cell><cell>99.110200 0.317902</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_107"><head>Table 119 :</head><label>119</label><figDesc>Wine with NLL loss for tuning Linear model, averaged over 20 seeds. Optimiser for this task with highest mean normalised score is HEBO.</figDesc><table><row><cell>Algorithm</cell><cell cols="3">Dataset Model Metric Normalised Score</cell><cell>Variance</cell></row><row><cell>HEBO</cell><cell>Wine</cell><cell>Linear NLL</cell><cell>99.532690</cell><cell>0.620171</cell></row><row><cell>TuRBO</cell><cell>Wine</cell><cell>Linear NLL</cell><cell>99.437383</cell><cell>0.618966</cell></row><row><cell>Opentuner</cell><cell>Wine</cell><cell>Linear NLL</cell><cell>98.930409</cell><cell>2.544485</cell></row><row><cell>PySOT</cell><cell>Wine</cell><cell>Linear NLL</cell><cell>98.501752</cell><cell>1.017364</cell></row><row><cell cols="2">Nevergrad (1+1) Wine</cell><cell>Linear NLL</cell><cell cols="2">97.886057 34.425637</cell></row><row><cell>TuRBO+</cell><cell>Wine</cell><cell>Linear NLL</cell><cell>97.200873</cell><cell>3.720058</cell></row><row><cell>Hyperopt</cell><cell>Wine</cell><cell>Linear NLL</cell><cell>96.307423</cell><cell>2.606512</cell></row><row><cell>Skopt</cell><cell>Wine</cell><cell>Linear NLL</cell><cell cols="2">95.972270 38.830724</cell></row><row><cell>Random-search</cell><cell>Wine</cell><cell>Linear NLL</cell><cell>94.013264</cell><cell>4.158990</cell></row><row><cell>BOHB-BB</cell><cell>Wine</cell><cell>Linear NLL</cell><cell>93.128692</cell><cell>5.936852</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/uber/bayesmark 2 It is these search spaces that are used by the random search baseline.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">One can make a case for augmenting the objective with a constraint such that updates for ? remain close to ? of the marginal likelihood. The ideal enforced proximity value however remains unclear in the robust acquisition literature to date<ref type="bibr" target="#b1">(Abdullah et al., 2019;</ref><ref type="bibr" target="#b30">Kirschner et al., 2020)</ref>.4 Here we use the common approach for proving stochastic expressions with high probability (see<ref type="bibr" target="#b60">(Tripuraneni et al., 2017)</ref>,<ref type="bibr" target="#b65">(Zhu &amp; Li, 2016)</ref>). Specifically, we show that for any confidence parameter ? ? (0, 1) the stochastic expression under consideration is valid with probability at least 1 ? ?.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14">We believe this due to the trust region not being modelled correctly with input warping.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15">For clarity we note that the input warping function used in<ref type="bibr" target="#b57">(Snoek et al., 2014)</ref> is the same one used in this work.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-objective Bayesian optimisation with preferences over objectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abdolshah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="12235" to="12245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Abdullah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Milenkovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.13196</idno>
		<title level="m">Wasserstein robust reinforcement learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Opentuner: An extensible framework for program autotuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ansel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Veeramachaneni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bosboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U.-M</forename><surname>O&amp;apos;reilly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amarasinghe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<title level="m">Proceedings of the 23rd International Conference on Parallel Architectures and Compilation</title>
		<meeting>the 23rd International Conference on Parallel Architectures and Compilation</meeting>
		<imprint>
			<biblScope unit="page" from="303" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BoTorch: A framework for efficient Monte-Carlo Bayesian optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Balandat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Karrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daulton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Letham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bakshy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hyperopt: a Python library for model selection and hyperparameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Komer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Eliasmith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Science &amp; Discovery</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">14008</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Nonconvex robust optimization for problems with constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bertsimas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nohadani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Teo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">INFORMS Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="44" to="58" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pymoo: Multi-objective optimization in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="89497" to="89509" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Recommender systems survey. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bobadilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guti?rrez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="109" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adversarially robust optimization with Gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bogunovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Scarlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cevher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5765" to="5775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An analysis of transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Box</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="211" to="243" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Bayesian modeling for optimization and control in robotics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Calandra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>Darmstadt, Technische Universit?t</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Manifold Gaussian processes for regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Calandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Deisenroth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3338" to="3345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scalable constrained Bayesian optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eriksson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poloczek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="730" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bohb: Robust and efficient hyperparameter optimization at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Falkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1437" to="1446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Survey of machine learning algorithms for disease diagnostic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fatima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pasha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent Learning Systems and Applications</title>
		<imprint>
			<biblScope unit="volume">09</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distribution-free two-sample tests for scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fligner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Killeen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">353</biblScope>
			<biblScope unit="page" from="210" to="213" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Derivative-free &amp; orderrobust optimisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gabillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tutunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bou-Ammar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Twenty Third International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="2293" to="2303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Achieving robustness to aleatoric uncertainty with heteroscedastic Bayesian optimisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-R</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Aldrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garcia-Ortegon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Lalchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.07779</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Modeling the multiwavelength variability of mrk 335 using Gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-R</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J K</forename><surname>Buisson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wilkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ingram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grupe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Parker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Astrophysical Journal</title>
		<imprint>
			<biblScope unit="volume">914</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">144</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hansen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.00772</idno>
		<title level="m">The CMA evolution strategy: A tutorial</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Portfolio allocation for Bayesian optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brochu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="327" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Portfolio allocation for bayesian optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brochu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="327" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sequential model-based optimization for general algorithm configuration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Hoos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Leyton-Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning and Intelligent Optimization</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="507" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-fidelity Bayesian optimisation with continuous approximations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dasarathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>P?czos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1799" to="1808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural architecture search with Bayesian optimisation and optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>P?czos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2020" to="2029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Tuning hyperparameters without grad students: Scalable and robust bayesian optimisation with dragonfly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Vysyaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Paria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">81</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Most likely heteroscedastic Gaussian process regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Plagemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pfaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Machine Learning</title>
		<meeting>the 24th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="393" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<editor>Bengio, Y., &amp; LeCun, Y.</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Auto-encoding variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations</title>
		<editor>Bengio, Y., &amp; LeCun, Y.</editor>
		<meeting><address><addrLine>Banff, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04-14" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Distributionally robust Bayesian optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirschner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bogunovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2174" to="2184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">RoBo: A flexible and robust Bayesian optimization framework in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Falkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mansur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems 2017 Workshop on Bayesian Optimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">GpflowOpt: a Bayesian optimization library using TensorFlow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Knudde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Der Herten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dhaene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Couckuyt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems 2017 Workshop on Bayesian Optimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Variable risk control via stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Kuindersma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Grupen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="806" to="825" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A generalized probability density function for double-bounded random processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kumaraswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Hydrology</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="79" to="88" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Variational heteroscedastic Gaussian process regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>L?zaro-Gredilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Titsias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on International Conference on Machine Learning</title>
		<meeting>the 28th International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="841" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Contributions to probability and statistics. Essays in Honor of Harold Hotelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Levene</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1960" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="278" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hyperband: A novel bandit-based approach to hyperparameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desalvo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="6765" to="6816" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On gradient descent ascent for nonconvex-concave minimax problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6083" to="6093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A survey on deep learning in medical image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kooi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Bejnordi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A A</forename><surname>Setio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ciompi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghafoorian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Van Der Laak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">I</forename><surname>S?nchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="60" to="88" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">On the limited memory BFGS method for large scale optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Programming</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="503" to="528" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">CMA-ES for hyperparameter optimization of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.07269</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Batch Bayesian optimization via multi-objective acquisition ensemble for automated analog circuit design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3306" to="3314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Maronas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hamelijnck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Knoblauch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Damoulas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.01596</idno>
		<title level="m">Transforming Gaussian processes with normalizing flows</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Knowing the what but not the where in Bayesian optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Osborne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7317" to="7326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Bock: Bayesian optimization with cylindrical kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3868" to="3877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Friedersdorf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04639</idno>
		<title level="m">Robust Gaussian process regression with a bias model</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Nevergrad -A gradient-free optimization platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rapin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Teytaud</surname></persName>
		</author>
		<ptr target="https://GitHub.com/FacebookResearch/Nevergrad" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<title level="m">Gaussian Processes for Machine Learning</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1530" to="1538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Compositionally-warped Gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tobar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="page" from="235" to="246" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Multi-fidelity black-box optimization with hierarchical partitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shakkottai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4538" to="4547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Taking the human out of the loop: A review of Bayesian optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shahriari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">104</biblScope>
			<biblScope unit="page" from="148" to="175" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">An entropy search portfolio for Bayesian optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shahriari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bouchard-C?t?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.4625</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Warped Gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Snelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="337" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Practical Bayesian optimization of machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2951" to="2959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Input warping for Bayesian optimization of non-stationary functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1674" to="1682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Bayesian optimization with robust Bayesian neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Falkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4134" to="4142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Domain randomization for transferring deep neural networks from simulation to the real world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tobin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="23" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Stochastic cubic regularization for fast nonconvex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tripuraneni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Regier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<idno>abs/1711.02838</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Bayesian optimization is superior to random search for machine learning hyperparameter tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eriksson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mccourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kiili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Laaksonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10201</idno>
	</analytic>
	<monogr>
		<title level="m">Analysis of the black-box optimization challenge 2020</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Gaussian processes for regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>MIT press Cambridge</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">The marginal value of adaptive gradient methods in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4151" to="4161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A new family of power transformations to improve normality or symmetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I.-K</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="954" to="959" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Even faster SVD decomposition yet without agonizing pain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">A</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1607.03463</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
