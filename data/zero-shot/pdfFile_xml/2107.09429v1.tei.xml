<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BoningKnife: JOINT ENTITY MENTION DETECTION AND TYPING FOR NESTED NER VIA PRIOR BOUNDARY KNOWLEDGE A PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiqiang</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoxin</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weile</forename><surname>Chen</surname></persName>
							<email>chen.weile7@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxi</forename><surname>Zhang</surname></persName>
							<email>chengxi_zhang@pku.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B?rje</forename><forename type="middle">F</forename><surname>Karlsson</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Peking University</orgName>
								<orgName type="institution" key="instit2">Microsoft Research ? Microsoft Azure AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">BoningKnife: JOINT ENTITY MENTION DETECTION AND TYPING FOR NESTED NER VIA PRIOR BOUNDARY KNOWLEDGE A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While named entity recognition (NER) is a key task in natural language processing, most approaches only target flat entities, ignoring nested structures which are common in many scenarios. Most existing nested NER methods traverse all sub-sequences which is both expensive and inefficient, and also don't well consider boundary knowledge which is significant for nested entities. In this paper, we propose a joint entity mention detection and typing model via prior boundary knowledge (BoningKnife) to better handle nested NER extraction and recognition tasks. BoningKnife consists of two modules, MentionTagger and TypeClassifier. MentionTagger better leverages boundary knowledge beyond just entity start/end to improve the handling of nesting levels and longer spans, while generating high quality mention candidates. TypeClassifier utilizes a two-level attention mechanism to decouple different nested level representations and better distinguish entity types. We jointly train both modules sharing a common representation and a new dual-info attention layer, which leads to improved representation focus on entity-related information. Experiments over different datasets show that our approach outperforms previous state of the art methods and achieves 86.41, 85.46, and 94.2 F1 scores on ACE2004, ACE2005, and NNE, respectively. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Named Entity Recognition (NER) is a fundamental tasks in natural language processing (NLP), which aims to extract and recognize named entities, like person names, organizations, geopolitical entities, etc., in unstructured text. However, in addition to flat entity mentions, nested or overlapping entities are commonplace in natural language. Such nested entities bring richer entity knowledge and semantics and can be critical to facilitate various downstream NLP tasks and real-world applications. As an example of their frequency, nested entities account for 35.19%, 30.80%, and 66.14% of mentions in standard datasets like ACE2004 Doddington et al. <ref type="bibr">[2004]</ref>, <ref type="bibr">ACE2005 Walker et al. [2006</ref>, and NNE <ref type="bibr" target="#b2">Ringland et al. [2019]</ref>, respectively. Nonetheless, the standard method for classic NER treats the problem as a sequence labeling task which has difficulty recognizing entities with nested structures directly <ref type="bibr" target="#b3">Alex et al. [2007]</ref>, <ref type="bibr" target="#b4">Lu and Roth [2015]</ref>, <ref type="bibr" target="#b5">Katiyar and Cardie [2018]</ref>. With that in mind, various approaches to recognizing nested entities have been proposed. From hyper-graph based methods <ref type="bibr" target="#b4">Lu and Roth [2015]</ref>, , <ref type="bibr" target="#b7">Marinho et al. [2019]</ref> which design expressive tagging schemas, to span-based methods which classify the categories of sub-sequences Sohrab and Miwa <ref type="bibr">[2018]</ref>, <ref type="bibr" target="#b9">Luan et al. [2019]</ref>, <ref type="bibr" target="#b10">Xia et al. [2019]</ref>, <ref type="bibr" target="#b11">Fisher and Vlachos [2019]</ref>. In order to improve output quality, most more recent approaches to nested NER adopt structures that require the enumeration or heuristic traversal of all sub-sequences, which leads to inefficiency, and lack effective use of boundary information, which is very significant for nested entities. Recently, <ref type="bibr" target="#b12">Zheng et al. [2019]</ref> and <ref type="bibr">Tan et al. [2020]</ref> explore using boundary knowledge to enhance recognition of nested entities. But both focus only on entity start/end information, which face limitations in handling long entity spans, the interaction of entity start/end(s), and lack region information. Moreover, decoupling the different nested levels of entity information remains a big problem in the nested NER task. For example, the spans "the leader of the Hezbollah in Syrian-occupied Lebanon", "the Hezbollah in Syrian-occupied Lebanon", and "Syrian-occupied Lebanon" all share the same end token (see <ref type="figure" target="#fig_0">Fig 1)</ref>. Shared contextual representations tend to focus on the outermost entity type (in the above case, PER).</p><p>In this paper, we propose a novel joint entity mention detection and typing model via prior boundary knowledge, BoningKnife, which can carve entity boundaries accurately and tease out type information more precisely. Our model consists of two main components, MentionTagger and TypeClassifier, which are jointly trained with a common encoder representation and a shared dual-info attention layer. MentionTagger performs mention detection by better leveraging boundary knowledge beyond just entity start/end to better handle nesting levels and longer spans. This improved representation of boundary knowledge both addresses limitations of previous systems and allows the generation of high quality mention candidates, which are critical for the overall system efficiency. TypeClassifier then utilizes a new two-level attention mechanism to decouple different nested level representations and better distinguish entity types. Moreover, the offshoots of MentionTagger entity token detection are further leveraged in the dual-info attention layer to improve joint training performance.</p><p>Experimental results on three datasets show that our approach achieves significant improvements over state-of-the-art methods across multiple nested NER datasets. Further analysis and case studies demonstrate the effectiveness of each component in the model and its different sub-task strategies of mention detection and typing attention layers. Moreover, our approach also achieves higher efficiency without drop in quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Traditionally, most approaches formalize the NER task as a sequence labeling problem, which assigns a single label to each token in a sentence. <ref type="bibr" target="#b14">Shen et al. [2003]</ref>, <ref type="bibr" target="#b15">Zhang et al. [2004]</ref>, <ref type="bibr" target="#b16">Zhou [2006]</ref> adopt bottom-up methods, which performs entity recognition from inner to outer mentions, following hand-crafted rules. <ref type="bibr" target="#b4">Lu and Roth [2015]</ref> introduces the idea of using a graph structure to connect tokens with multiple entities. While Muis and , , <ref type="bibr" target="#b5">Katiyar and Cardie [2018]</ref>, <ref type="bibr" target="#b18">Wang and Lu [2019]</ref> propose hypergraphs and different methods to utilize graph information for nested NER.</p><p>Transition-based models, which assemble a shift-reduce structure to detect a nested entity, have also been proposed.  builds a forest structure based on shift-reduce parsing. <ref type="bibr" target="#b7">Marinho et al. [2019]</ref> uses a stack structure to construct the transition-shift-reduce model. And <ref type="bibr" target="#b20">Ju et al. [2018]</ref> proposes a dynamically stacked multiple LSTM-CRF model to recognize the entity in an inside-out manner until no outer entity is plucked.</p><p>Span-based methods are another class of methods to recognize nested entities by classifying sub-sequences <ref type="bibr" target="#b10">Xia et al. [2019]</ref>. <ref type="bibr" target="#b9">Luan et al. [2019]</ref> proposes a graph-based model which leverages entity linking to improve NER performance. <ref type="bibr" target="#b11">Fisher and Vlachos [2019]</ref> introduces a merge-and-label method which uses nested entity hierarchy features. <ref type="bibr" target="#b21">Strakova et al. [2019]</ref> views the nested NER task as a seq2seq generator problem, in which the input is a list of sentences and output target entities list. <ref type="bibr" target="#b22">Shibuya and Hovy [2020]</ref> introduces an improved CRF model that recursively decoder the entity from outside to inside.</p><p>However, most previous methods need to traverse all sub-sequences and lack boundary knowledge, which is significant for nested entities. To try and mitigate such shortcomings, <ref type="bibr" target="#b12">Zheng et al. [2019]</ref> defines a boundary detection task to generate a mention candidate set based on the entity start/end, followed by typing all mentions in the candidate set. And <ref type="bibr">Tan et al. [2020]</ref> splits boundary information into two sub-tasks (entity start and entity end), before classifying candidates. While both works use boundary knowledge, they focus only on entity start/end, which does not fully represent boundary information and lead to issues such as not handling long spans well. BoningKnife jointly trains entity mention detection and typing modules and utilizes an extended representation of boundary knowledge to address such limitations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MentionTagger TypeClassifier</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dual-info Att</head><p>Two-level Att <ref type="figure">Figure 2</ref>: Framework of the proposed BoningKnife, including Encoder Layer, MentionTagger, and TypeClassifier.</p><p>In this section, we define the nested NER task, and then elaborate on our proposed solution. <ref type="figure">Fig 2 illustrates</ref> the framework of the proposed (BoningKnife). Specifically, we jointly train tagger and classifier, where the former (MentionTagger) extracts potential mention spans and generates mention candidates by leveraging an improved representation of boundary knowledge, and the later (TypeClassifier) classifies mention candidates into predefined entity types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Statement</head><p>Let s ? S denote sentence data and c ? C denote entity label data, where S and C are the vector space of sentences and labels. Given a sentence s = [t i ] 1?i?N , where N means the length of sentences s and t i represents the i-th token of s, the NER task object is to extract all semantic elements (l e , r e , c e ) e ? E where l e , r e are the element start/end indices, c e means the element corresponding to a predefined label, and E is the entity space.</p><p>Essentially, nested NER aims to learn a R N ?N ?|C| space representation (only for the upper triangular matrix), where |C| is the size of entity categories and non-entity, and each value in the matrix represents the span type probability P(e|s). We decompose the target probability P(e|s) into the product of two conditional probabilities (detection and typing) with a latent parameter (span).</p><formula xml:id="formula_0">P(e|s) = |E| i P(l i , r i |s) ? P(e|s, l i , r i )<label>(1)</label></formula><p>In the formula, we discard the term with a small span probability P(l i , r i |s) in order to reduce the amount of calculations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Encoder Layer</head><p>Because of the importance of context to entities, it is necessary to infuse information from different nested entities into one token. We propose a Dual-info attention structure to obtain entity semantic knowledge from both the token itself and others. This Dual-info attention representation is the input of two main sub-components of the model. For the attention architecture, we use a pre-LayerNorm residual connection and multi-head attention mechanism.</p><formula xml:id="formula_1">AttentionBlock(s, MASK s ) = MultiHead(LayerNorm(s), MASK s ) + s<label>(2)</label></formula><p>where MASK s ? R N ?N is the attention mask matrix. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dual-info</head><formula xml:id="formula_2">R i = Linear([AttentionBlock(b i , MASK global ); AttentionBlock(b i , MASK focus )])<label>(3)</label></formula><p>Global Masked Attention considers every token from the same sentence, which makes the representation more contextual, while Mention-focus Masked Attention uses entity detection from MentionTagger (Sec 3.3) and local context to construct attention weights. For tokens not in mention candidates, we encode them from the representation of mention candidates' tokens and local context. Otherwise, we encode mention candidates' tokens by calculating the attention weighted sum of all tokens except itself. This approach tries to emphasise information related to: entity to entity, token to entity, and entity types. The ablation experiments (Sec 5.1) and attention discussion (Sec 5.3) further showcase its effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">MentionTagger</head><p>The mention tagger module aims to extract entity mention candidates and compute their corresponding probabilities in a sentence.</p><p>It is onerous to learn the mention detection matrix in R N ?N ?2 space directly. A basic idea is to traverse all subsequences based on a shared representation. Building a high dimensional matrix concatenating the representation of entity's start/end token. However, this method misses the interaction between start and end tokens, and increases the risk of over-fitting in the training stage.</p><p>The other extreme is to treat entity boundary information as two completely independent variables, like recent mention detection models <ref type="bibr" target="#b12">Zheng et al. [2019]</ref>, <ref type="bibr">Tan et al. [2020]</ref> that only consider entity start/end tokens, which lack enough region contextual information. MentionTagger circumvents these two problems and utilizes three types of boundary information: entity start/end token, entity token itself, and mention region; which we term prior boundary knowledge. To infuse prior boundary distribution, we use three sub-tasks (start/end detection, entity detection, and mention detection) in training the tagger module.</p><p>Start/End detection Inspired by the biaffine model <ref type="bibr" target="#b24">Dozat and Manning [2017]</ref>, we use two MLPs (MLP start and MLP end ) to get a low dimension start/end representation h start , h end and compute the span representation S e (i) for span e(l i , r i ). We also use a start/end detection sub-task to enhance start/end representation h start , h end and apply two other MLPs(MLP startPoint and MLP endPoint ) to project h start /h end to the category space. For span e(l i , r i ), these are:</p><formula xml:id="formula_3">h start (i) = MLP start (R li ) h end (i) = MLP end (R ri )<label>(4)</label></formula><p>where R li and R ri are the Dual-info Attention representation of the span e(l i , r i )'s start/end token.</p><p>And the span vector S e (i) generated from the low dimension start/end representation h start , h end ,</p><formula xml:id="formula_4">S e (i) = h start (i)U span h end (i) + h start (i)u start + h end (i)u end + b span (5) where U span ? R N ?N ?dspan , u start ? R dlow?dspan , u end ? R dlow?dspan are self-learned parameters. P ? (Y start ) = softmax(MLP startPoint (h start (i))) P ? (Y end ) = softmax(MLP endPoint (h end (i)))<label>(6)</label></formula><p>where P ? (Y start ) and P ? (Y end ) are the output probability of the start/end detection sub-task.</p><formula xml:id="formula_5">L start = 1 N i?[1,N ] ? log P ? (Y start,i = y start,i ) L end = 1 N i?[1,N ] ? log P ? (Y end,i = y end,i )<label>(7)</label></formula><p>where y start,i are the ground truth labels of the start/end detection sub-task and L start , L end are the training losses of the start/end sub-task, respectively.</p><p>Entity detection Notice that only using the start/end information does not define a span boundary. Two high probability start i/end j don't mean the probability of span e(l i , r i ) is high. For example, in the sentence "Joe went to school.", the probability of start token"Joe" and end token "school" are both high, but the probability of span "Joe went to school" being an entity is low. Applying the entity detection information (verdict token w/o belonging to at least one entity) can help address this problem. We reduce the span probability with large spacing by accumulating the entity detection probability values.</p><p>For the token index k in a sentence, we have:</p><formula xml:id="formula_6">P ? (Y enityDetection ) = softmax(MLP enityDetection (R k ))<label>(8)</label></formula><p>where P ? (Y enityDetection ) means the output probability of token k belonging to a span.</p><p>In this sub-task, the entity detection loss function L entityDetection is defined as:</p><formula xml:id="formula_7">L entityDetection = 1 N i?[1,N ] ? log P ? (Y enityDetection,i = y enityDetection,i )<label>(9)</label></formula><p>where y entityDetection,i are the ground truth labels of the entity detection sub-task.</p><p>Mention detection Using the span representation S e (i) and entity detection probability P ? (Y enityDetection ), we compute the mention detection probability for all sub-sequences in the sentence. For the span e(l i , r i ), these are:</p><formula xml:id="formula_8">P ? (Y mention ) = softmax(MLP mention (S e (i) ? k?[li,ri] P ? (Y enityDetection )))<label>(10)</label></formula><p>where P ? (Y mention ) means the output probability of the mention detection sub-task.</p><p>The mention detection loss function L mention is calculated as follows:</p><formula xml:id="formula_9">L mention = 1 N 2 i?[1,N ] j?[1,N ] ? log P ? (Y mention,i,j = y mention,i,j )<label>(11)</label></formula><p>where y entityDetection,i are the ground truth labels of the mention detection sub-task. After getting the mention probability P ? (Y mention ) of each mention pair e(l i , r i ), we use a threshold hyper-parameter to generate the mention candidate M candidate . MentionTagger not only outputs a mention candidate set M candidate as the input of TypeClassifier, but the offshoots of its internal entity token detection P ? (Y enityDetection ) are fed to the shared Dual-info attention layer as a mention-focus mask matrix MASK focus , improving its representation of entity semantic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">TypeClassifier</head><p>After obtaining mention candidates from MentionTagger, TypeClassifier aims to predict the probability of entity types for each candidate. We utilize a mention decoupling layer (MDL) to focus on the current mention semantic information, including two-level attention and a four-level representation (see <ref type="figure">Fig 2)</ref>, for each span.</p><p>We apply a dimensional position embedding P i over the Dual-info Attention encoder embedding R i , as position-wise token representation H i , and consider H i as the input to the two-level attention component.</p><formula xml:id="formula_10">H i = Linear([R i ; P i ])<label>(12)</label></formula><p>where P i is a learnable position embedding.</p><p>Two-level attention combines mention-level attention MentionAttention(s) (to make semantic information focus on internal spans) and neighbor-level attention NeighborAttention(s) (which emphasizes the knowledge from span boundaries and contextual tokens). Both utilize have different mask matrices. Mention-level attention can only see the mention tokens, while neighbor-level attention can only see the remaining tokens. This attention is defined as:</p><formula xml:id="formula_11">MentionAttention(s) = AttentionBlock(H, MASK mention ) NeighborAttention(s) = AttentionBlock(H, MASK neighbor )<label>(13)</label></formula><p>where MASK mention , MASK neighbor are the difference mask matrices, and AttentionBlock is the attention architecture mentioned in eq <ref type="formula" target="#formula_1">(2)</ref>.</p><p>Four-level representation For each span, we align and combine four fine-grained representations, thereby improving the model's understanding of entity boundaries and entity semantics. Taking the span e(l i , r i ), the detail representation encompasses:</p><p>? Sentence-level Representation,</p><formula xml:id="formula_12">E sentence (s) = Linear([R [CLS] ; R [SEP] ])<label>(14)</label></formula><p>? These four different features are combined into TypeClassifer's representation:</p><formula xml:id="formula_13">T (s) = Linear([E sentence ; E position ; E mention ; E neighbor ])<label>(18)</label></formula><p>In the same way, the predicted classification probability is output through the Softmax function.</p><formula xml:id="formula_14">P ? (Y type ) = softmax(MLP type (T (s)))<label>(19)</label></formula><p>The corresponding loss function L type is:</p><formula xml:id="formula_15">L type = 1 |M candidate | i?[1,|Mcandidate|] ? log P ? (Y type,i = y type,i )<label>(20)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Optimization Objective</head><p>In MentionTagger, we joint optimize the above detection sub-tasks. All losses are based on cross entropy loss. To balance difference loss, we apply a focal loss style self-adjusting weight strategy. For larger losses, the weights will be correspondingly scaled up to improve the learning process.</p><p>L mentionTagger = ? start L start + ? end L end + ? entityDetection L entityDetection + ? mention L mention <ref type="formula" target="#formula_0">(21)</ref> where,</p><formula xml:id="formula_16">LearnScore i = (1 ? exp(?L i )) ? ? i = LearnScore i j LearnScore j (22)</formula><p>where LearnScore i is a score to judge degree of sub-task training, which borrows from focal loss <ref type="bibr" target="#b25">Lin et al. [2017]</ref> and ? i is a normalization version of LearnScore i , j ? {start, end, entityDetection, mention}.</p><p>We jointly train MentionTagger and TypeClassifier as a multi-task process alternately, where the shared representation layer and the entity detection prediction results in Dual-info attention enhance the connections between the two components. The overall optimization goal is: </p><formula xml:id="formula_17">L train = L mentionTagger + L type<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline and Experimental Settings</head><p>We compare BoningKnife with a set of representative models and the recent state of the art.</p><p>? , a graph-based model using LSTM to learn a feature encoder;</p><p>? <ref type="bibr" target="#b10">Xia et al. [2019]</ref>, which is a detect-classify model without boundary knowledge;</p><p>? <ref type="bibr" target="#b9">Luan et al. [2019]</ref>, a graph-based model which leverages entity linking to improve NER; <ref type="bibr" target="#b11">? Fisher and Vlachos [2019]</ref>, a merge-and-label model with hierarchical features;</p><p>? <ref type="bibr" target="#b21">Strakova et al. [2019]</ref>, which treats the nested NER task as a seq2seq problem;</p><p>? <ref type="bibr" target="#b22">Shibuya and Hovy [2020]</ref>, which extracts nested entities recursively with CRF;</p><p>? <ref type="bibr">Tan et al. [2020]</ref>, which combines entity start/ end probabilities.</p><p>Moreover, we utilize BERT base itself as a lower-bound baseline for the BERT-based models. <ref type="bibr">3</ref> We perform a random search strategy for hyperparameter optimization and select the best settings on the development sets. We initialize the loss weight parameter as ? = 0.5 in MentionTagger and set the max neighbor window size to  128 in neighbor-level attention block NeighborAttention to control memory size. The hidden sizes of low dimension start/end representation h start , h end are 84, the number of attention heads is 16, and the windows size of mention-focus attention mask matrix MASK focus is 2. Except for BERT base , our model has around 24M parameters. We employ AdamW <ref type="bibr" target="#b27">Loshchilov and Hutter [2019]</ref> as optimizer during training. Experiments are repeated 5 times for different random seeds on each corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results and Discussion</head><p>Tables 2 and 3 report the results of our model and the different baselines on the ACE2004/ACE2005 and NNE datasets. It can be seen that our proposed method outperforms all previous state-of-the-art methods, reaching <ref type="bibr">86.41, 85.46, and 94.24</ref> in average micro-F1 score, on ACE2004, ACE2005, and NNE respectively.</p><p>Compared with the latest boundary-enhanced method <ref type="bibr">[Tan et al., 2020]</ref>, our method achieves 1.11 and 1.56 absolute point gains on ACE2004 and ACE2005 5 . The boost comes mainly from recall improvements (2.06 to 2.26 points). MentionTagger is able to produce more precise mention candidates, which allows TypeClassifier to focus on distinguishing entity types, instead of filtering candidates as not viable. The improved precision of MentionTagger is further evidenced in <ref type="table">Table 6</ref>.</p><p>In the NNE corpus, BoningKnife achieves 94.24 F1-score; an improvement of 1.05 points over the previous SOTA. We hypothesize that as NNE datasets has deeper nesting levels, <ref type="bibr">[Tan et al., 2020]</ref>'s approach leads to error transmission in their recursive encoding process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ablation &amp; Flat/Nested Performance</head><p>To validate the contributions and effectiveness of different components in the proposed model, we introduce the following model variants to perform an ablation study:  <ref type="table">Table 4</ref> highlights the performance contributions of each component in our proposed model, and removing any of them will generally lead to substantial performance drops. It can be seen that quality decreases significantly when either removing MentionTagger or its sub-tasks (entity token and start/end detection) sub-task, which indicates the proposed model makes effective usage of boundary knowledge (for example, to better handle long length entity spans). Without the proposed two-level attention in TypeClassifier, it becomes harder for the model to separate nested information and assign the proper type for nested entities; even more so than removing only the neighbor-level component of the two-level attention . This further demonstrates the benefits of the two-level structure and its ability to combine clear boundary and local context information. Lastly, while the effects of removing mention-focus mask attention are less prominent, it's still noticeable and removing this component leads to slower overall model convergence. Furthermore, <ref type="table" target="#tab_6">Table 5</ref> reports the Flat/Nested performance across datasets. It can be seen that BoningKnife excels in nested entities while remaining competitive on flat results; which further evidences the overall effectiveness of the model in leveraging boundary knowledge. Similarly to <ref type="bibr">Tan et al. [2020]</ref>, our method substantially improves the time complexity over typical span-based methods by generating high-quality candidates, which greatly reduce complexity and training time. Span-based models, which require traversing all sub-sequences, have O(m ? n 2 ) time complexity, where m is the count of tags. Efficiently reducing the number of candidates is key in a two-step system like BoningKnife, as span classifying time complexity is determined by the number of candidates in its input. To measure the speedup from our approach due to its improved candidate generation and provide a comparison with <ref type="bibr">Tan et al. [2020]</ref>, we run two experiments: i) the complete BoningKnife Sentence: The Coventry University researchers who report the findings in the British journal of sports medicine say anxiety and depression are common among those so injured, possibly as a result of pain and impaired mobility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SPAN</head><p>pstart p end pmention ps ptype ytype y golden (a) The Coventry University researchers who report the 1.000 0.999 1.000 0.942 1.000 PER PER findings in the British journal of sports medicine (b) Coventry University 0.595 1.000 0.945 0.502 0.998 ORG ORG (c) who 1.000 0.999 1.000 0.997 1.000 PER PER (d) those so injured 1.000 0.483 0.953 0.045 1.000 PER PER (e) those so injured, possibly as a result of pain and 1.000 0.000 0.000 0.573 0.000 Non-entity Non-entity impaired mobility (f) . 0.000 0.000 0.000 0.501 0.000 Non-entity Non-entity <ref type="table">Table 7</ref>: An example where BoningKnife leverages prior boundary knowledge to better predict nested entity type. p s from the ablation experiment "w/o ED subtask".</p><p>(a) Global Mask (b) Mention-focus Mask <ref type="figure">Figure 4</ref>: Visualization of the Dual-info Attention weights for the case study sentence <ref type="table">(Table 7)</ref>. system and ii) BoningKnife -MentionTagger + mention strategy in <ref type="bibr">Tan et al. [2020]</ref>. The experiments were run on a Ubuntu 16.04.6 server with Intel Xeon CPU E5-2690v3 @ 2.60GHz and one P100 GPU. <ref type="table">Table 6</ref> reports the comparison between both experiments. We can see that our method provides significant speedup over the simpler modeling of boundary knowledge approach, especially with deeper nesting levels. BoningKnife is 1.75x, 1.83x, and 4.18x faster in ACE2004, ACE2005, and NNE, respectively, while still achieving higher quality. <ref type="table">Table 7</ref> shows an example of BoningKnife prediction in ACE 2004. Span (d), "those so injured", is a correct mention, but the probability of end token "injured" is small. For S/E based methods like <ref type="bibr">Tan et al. [2020]</ref>, <ref type="bibr" target="#b12">Zheng et al. [2019]</ref>, this span would likely be discarded, but in our method it is correctly identified. Compared with p s only, the entity token detection knowledge reduces the number of high probability mentions (like (e) and (f)) inconsistent with the prior information, while not discarding very long entities, like mention (a). <ref type="figure">Fig 4 shows</ref> the Dual-info attention weights for the sentence in <ref type="table">Table 7</ref>. The global attention weights all focus on common keywords like "university", "the". While the mention-focus attention, focus on specific token neighbors, like "report" focusing on "researchers" and "findings", which improve their semantic information. Also, additional tokens focus on the relevant entity tokens, like "who" focusing on the same entity type word "those" instead of on itself in global attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Case Study and Attention Weight Visualization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper we propose a novel joint entity mention detection and typing model via prior boundary knowledge for the nested NER task. The proposed method effectively incorporates prior boundary knowledge information to generate high quality mention candidates, which greatly improves efficiency of the whole system. By introducing a Dual-info attention layer at the mention classification stage, it facilitates mention decoupling and more accurate mention classification at different levels. Experiments show that our system, BoningKnife, achieves state-of-the-art results on three standard benchmark datasets; and an ablation study further demonstrates the effectiveness of its components.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example of nested mentions in ACE2004.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Mask matrix of Dual-info attention (windows size is 2). Left: global mask. Right: mention-focus mask which fuses "entity detection" subtasks and local context.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Attention consists of Global Masked attention and Mention-focus Masked attention layers. Fig 3 shows the masked matrix example of Dual-info attention based on BERT base Devlin et al. [2018]. We use b i , the BERT representation of the i-th token in the sentence, to compute the Dual-info attention representation R i .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>3 Problem and Methodology BERT Encoder Global Masked Att Mention-Focus Masked Att MLPstart MLPend MLPentity_t Biaffine MLPsp MLPep Mention Detection ? Start Detection End Detection Entity Detection</head><label></label><figDesc></figDesc><table><row><cell cols="2">MLPtype</cell><cell cols="2">Entity Type</cell></row><row><cell>RCLS/</cell><cell>Hstart</cell><cell>Mstart</cell><cell>Nstart-1/</cell></row><row><cell>RSEP</cell><cell>/Hend</cell><cell>/Mend</cell><cell>Nend+1</cell></row><row><cell>Sentence Repr</cell><cell>Pos</cell><cell>Mention-level Att</cell><cell>Neighbor-level Att</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>:</head><label></label><figDesc><ref type="bibr" target="#b26">Lin et al. [2019]</ref>,<ref type="bibr" target="#b2">Ringland et al. [2019]</ref>. 2 .Table 1shows the proportions of nested entities in the datasets range from 30.80% to 66.14%. ACE2004 and ACE2005 include 7 entity types, while NNE has 114 entity types. We report precision, recall, and micro-F1 metrics for all experiments. Statistics of the ACE2004, ACE2005, and NNE nested NER datasets.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>23)</cell></row><row><cell>4 Experiments</cell><cell></cell><cell></cell><cell></cell></row><row><cell>4.1 Datasets</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">We evaluate our model on three nested NER datasets, ACE2004 Doddington et al. [2004], ACE2005 Walker et al.</cell></row><row><cell cols="4">[2006], and NNE Ringland et al. [2019]; using the same splits as previous work Lu and Roth [2015], Wang and Lu</cell></row><row><cell cols="4">[2018], ACE2004 ACE2005 NNE</cell></row><row><cell>Documents</cell><cell>443</cell><cell>464</cell><cell>2,312</cell></row><row><cell>Sentences</cell><cell>8,507</cell><cell>9,311</cell><cell>49,208</cell></row><row><cell>Mentions</cell><cell>27,753</cell><cell cols="2">31,102 279,795</cell></row><row><cell>Entity overlaps</cell><cell>9,767</cell><cell cols="2">9,579 185,054</cell></row><row><cell>Overlap ratio</cell><cell cols="3">35.19% 30.80% 66.14%</cell></row><row><cell cols="2">% of overlaps over all sub-sequences 1.11%</cell><cell>1.30%</cell><cell>1.47%</cell></row><row><cell>Table 1</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>BoningKnife ? 85.98 ? 0.36 86.86 ? 0.39 86.41 ? 0.24 84.77 ? 0.31 86.16 ? 0.43 85.46 ? 0.32</figDesc><table><row><cell>Model</cell><cell></cell><cell>ACE2004</cell><cell></cell><cell></cell><cell>ACE2005</cell><cell></cell></row><row><cell></cell><cell>P(%)</cell><cell>R(%)</cell><cell>F1(%)</cell><cell>P (%)</cell><cell>R(%)</cell><cell>F1 (%)</cell></row><row><cell>Wang and Lu [2018]</cell><cell>78.0</cell><cell>72.4</cell><cell>75.1</cell><cell>76.8</cell><cell>72.3</cell><cell>74.5</cell></row><row><cell>Xia et al. [2019] [ELMO]</cell><cell>81.7</cell><cell>77.4</cell><cell>79.5</cell><cell>79.0</cell><cell>77.3</cell><cell>78.2</cell></row><row><cell>Luan et al. [2019]</cell><cell>-</cell><cell>-</cell><cell>84.7</cell><cell>-</cell><cell>-</cell><cell>82.9</cell></row><row><cell>BERT merge outmost &amp; inmost  ?</cell><cell>80.55</cell><cell>79.23</cell><cell>79.88</cell><cell>78.12</cell><cell>82.71</cell><cell>80.35</cell></row><row><cell>Fisher and Vlachos [2019]  ?</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>82.7</cell><cell>82.1</cell><cell>82.4</cell></row><row><cell>Strakova et al. [2019]  ?</cell><cell>-</cell><cell>-</cell><cell>84.4</cell><cell>-</cell><cell>-</cell><cell>84.3</cell></row><row><cell>Shibuya and Hovy [2020]  ?</cell><cell>85.23</cell><cell>84.72</cell><cell>84.97</cell><cell cols="3">83.30 ? 0.22 84.69 ? 0.37 83.99 ? 0.27</cell></row><row><cell>Tan et al. [2020]  ?</cell><cell>85.8</cell><cell>84.8</cell><cell>85.3</cell><cell>83.8</cell><cell>83.9</cell><cell>83.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Results of the proposed BoningKnife and prior state-of-the-art methods over the ACE2004/2005 test sets. ? denotes models utilizing BERT. '-' denotes results not reported. Results of the proposed model and baselines over the NNE dataset.</figDesc><table><row><cell>Model</cell><cell>P(%)</cell><cell>R(%)</cell><cell>F1(%)</cell></row><row><cell>Wang et al. [2018]</cell><cell>77.4</cell><cell>70.1</cell><cell>73.6</cell></row><row><cell cols="2">BERT base merge outmost &amp; inmost 80.43</cell><cell>74.94</cell><cell>77.59</cell></row><row><cell>Wang and Lu [2018]</cell><cell>91.8</cell><cell>91.1</cell><cell>91.4</cell></row><row><cell>Shibuya and Hovy [2020] 4</cell><cell>93.03</cell><cell>93.34</cell><cell>93.19</cell></row><row><cell>BoningKnife</cell><cell>93.74</cell><cell>94.75</cell><cell>94.24</cell></row><row><cell></cell><cell cols="3">(? 0.33) (? 0.24) (? 0.05)</cell></row><row><cell>Table 3:</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Ablation study of the proposed BoningKnife over the ACE 2004 test set, where numbers in parenthesis denote performance change.</figDesc><table><row><cell>Method</cell><cell>P(%) R(%)</cell><cell>F1(%)</cell></row><row><cell>BoningKnife</cell><cell>85.98 86.86</cell><cell>86.41</cell></row><row><cell cols="3">-w/o EntityDetection subtask 84.74 85.95 85.35 (-1.06)</cell></row><row><cell>-w/o Start/End sub-task</cell><cell cols="2">85.71 85.18 85.44 (-0.97)</cell></row><row><cell cols="3">-w/o Neighbor-level attention 85.44 86.56 86.00 (-0.41)</cell></row><row><cell>-w/o Two-level attention</cell><cell cols="2">84.75 86.43 85.58 (-0.83)</cell></row><row><cell cols="3">-w/o Mention-focus attention 85.64 86.63 86.13 (-0.28)</cell></row><row><cell cols="3">-w/o MentionTagger Stage 6 87.25 83.86 85.52 (-0.89)</cell></row><row><cell>Table 4:</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>MentionTagger 83.56 86.27 84.16 85.45 83.89 93.65 Shibuya and Hovy [2020] 84.45 85.14 84.86 83.13 84.26 93.67 Flat/Nested F1-scores over the ACE2004, ACE2005, and NNE test sets. Comparison of mention precision and training time cost per epoch, between E2E system using MentionTagger and mention strategy from Tan et al. [2020].</figDesc><table><row><cell></cell><cell></cell><cell>ACE2004</cell><cell cols="2">ACE2005</cell><cell>NNE</cell></row><row><cell></cell><cell cols="5">Flat Nested Flat Nested Flat Nested</cell></row><row><cell>BoningKnife</cell><cell cols="5">84.32 87.10 84.54 86.23 84.45 94.73</cell></row><row><cell>-w/o 5.2 Time Complexity</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="2">ACE2004</cell><cell cols="2">ACE2005</cell><cell>NNE</cell></row><row><cell></cell><cell cols="5">P Time (s) P Time (s) P Time (s)</cell></row><row><cell>BoningKnife</cell><cell>89.46</cell><cell>479</cell><cell>87.22</cell><cell>649</cell><cell>95.48 7841</cell></row><row><cell cols="2">-w/o MentionTagger 31.54</cell><cell>839</cell><cell cols="3">32.58 1190 30.71 32758</cell></row><row><cell>TimeRatio</cell><cell></cell><cell>1.75X</cell><cell></cell><cell>1.83X</cell><cell>4.18X</cell></row><row><cell>Table 6:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">ACE2004 / ACE2005 as in https://statnlp-research.github.io/publications/, and NNE as in https: //github.com/nickyringland/nested_named_entities 3 NNE results for<ref type="bibr" target="#b22">Shibuya and Hovy [2020]</ref> are reported by using their public code in https://github.com/yahshibu/ nested-ner-2019-bert.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Unfortunately Tan et al. [2020] does not report results over NNE and did not release their code for further experiments</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Replacing MentionTagger with using entity start/end to generate mention candidates, similarly toTan et al. [2020].</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The automatic content extraction (ace) program-tasks, data, and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>George R Doddington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Przybocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><forename type="middle">M</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><forename type="middle">M</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weischedel</surname></persName>
		</author>
		<ptr target="http://www.lrec-conf.org/proceedings/lrec2004/pdf/5.pdf" />
	</analytic>
	<monogr>
		<title level="m">Lrec</title>
		<meeting><address><addrLine>Lisbon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Ace 2005 multilingual training corpus. Linguistic Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Medero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuaki</forename><surname>Maeda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<pubPlace>Philadelphia, 57</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">NNE: A dataset for nested named entity recognition in english newswire</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicky</forename><surname>Ringland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Hachey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarvnaz</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?cile</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/P19-1510" />
	</analytic>
	<monogr>
		<title level="m">Proc. Annu. Meeting Assoc. for Computational Linguistics (ACL)</title>
		<meeting>Annu. Meeting Assoc. for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recognising nested named entities in biomedical text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Alex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Grover</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/W07-1009" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on BioNLP 2007: Biological, Translational, and Clinical Language Processing</title>
		<meeting>the Workshop on BioNLP 2007: Biological, Translational, and Clinical Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Joint mention extraction and classification with mention hypergraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D15-1102" />
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process. (EMNLP)</title>
		<meeting>Conf. Empirical Methods Natural Language ess. (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Nested named entity recognition revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arzoo</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/N18-1079" />
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. North American Assoc. for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<meeting>Conf. North American Assoc. for Computational Linguistics: Human Language Technologies (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="861" to="871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural segmental hypergraphs for overlapping mention recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bailin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D18-1019" />
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process. (EMNLP)</title>
		<meeting>Conf. Empirical Methods Natural Language ess. (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hierarchical nested named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zita</forename><surname>Marinho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfonso</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastiao</forename><surname>Miranda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Nogueira</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/W19-1904" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Clinical Natural Language Processing Workshop</title>
		<meeting>the 2nd Clinical Natural Language Processing Workshop</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="28" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep exhaustive model for nested named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golam</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Sohrab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miwa</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D18-1309" />
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process. (EMNLP)</title>
		<meeting>Conf. Empirical Methods Natural Language ess. (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2843" to="2849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A general framework for information extraction using dynamic span graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/N19-1308" />
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. North American Assoc. for Computational Linguistics (NAACL)</title>
		<meeting>Conf. North American Assoc. for Computational Linguistics (NAACL)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-grained named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congying</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fenglong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/P19-1138" />
	</analytic>
	<monogr>
		<title level="m">Proc. Annu. Meeting Assoc. for Computational Linguistics (ACL)</title>
		<meeting>Annu. Meeting Assoc. for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Merge and label: A novel neural network architecture for nested ner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/P19-1585" />
	</analytic>
	<monogr>
		<title level="m">Proc. Annu. Meeting Assoc. for Computational Linguistics (ACL)</title>
		<meeting>Annu. Meeting Assoc. for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A boundary-aware neural model for nested named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changmeng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guandong</forename><surname>Ho-Fung Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1034</idno>
		<ptr target="https://www.aclweb.org/anthology/D19-1034" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11" />
			<biblScope unit="page" from="357" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Boundary enhanced neural span classification for nested named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mosha</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Proc. Conference on Alien Intelligence(AAAI)</title>
		<meeting>Int. Conf. . Conference on Alien Intelligence(AAAI)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Effective adaptation of a hidden markov model-based named entity recognizer for biomedical domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chew-Lim</forename><surname>Tan</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/" />
	</analytic>
	<monogr>
		<title level="m">Proc. Annu. Meeting Assoc. for Computational Linguistics (ACL)</title>
		<meeting>Annu. Meeting Assoc. for Computational Linguistics (ACL)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Enhancing hmm-based biomedical named entity recognition by studying special phenomena</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chew-Lim</forename><surname>Tan</surname></persName>
		</author>
		<ptr target="https://www.ncbi.nlm.nih.gov/pubmed/15542015" />
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="411" to="422" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recognizing names in biomedical texts using mutual information independence model and svm plus sigmoid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gd Zhou</surname></persName>
		</author>
		<ptr target="http://nlp.suda.edu.cn/~gdzhou/publication/zhougd2006_IJMI_BiomedicalNamedEntityRecognition.pdf" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Medical Informatics</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="456" to="467" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Labeling gaps between words: Recognizing overlapping mentions with mention separators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aldrian</forename><surname>Obaja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muis</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D17-1276" />
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process. (EMNLP)</title>
		<meeting>Conf. Empirical Methods Natural Language ess. (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Combining spans into entities: A neural two-stage approach for recognizing discontiguous entities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bailin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D19-1644" />
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process. (EMNLP)</title>
		<meeting>Conf. Empirical Methods Natural Language ess. (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A neural transition-based model for nested mention recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bailin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Jin</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D18-1124" />
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process. (EMNLP)</title>
		<meeting>Conf. Empirical Methods Natural Language ess. (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A neural layered model for nested named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meizhi</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/N18-1131" />
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. North American Assoc. for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<meeting>Conf. North American Assoc. for Computational Linguistics: Human Language Technologies (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1446" to="1459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural architectures for nested ner through linearization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Strakova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajic</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/" />
	</analytic>
	<monogr>
		<title level="m">Proc. Annu. Meeting Assoc. for Computational Linguistics (ACL)</title>
		<meeting>Annu. Meeting Assoc. for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="19" to="1527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Nested named entity recognition via second-best sequence learning and decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Shibuya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00334</idno>
		<ptr target="https://aclanthology.org/2020.tacl-1.39" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="605" to="620" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/N19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. North American Assoc. for Computational Linguistics (NAACL)</title>
		<meeting>Conf. North American Assoc. for Computational Linguistics (NAACL)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep biaffine attention for neural dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learning Representations (ICLR</title>
		<meeting>Int. Conf. Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<idno>abs/1708.02002</idno>
		<ptr target="http://arxiv.org/abs/1708.02002" />
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sequence-to-nuggets: Nested entity mention detection via anchorregion networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaojie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/P19-1511" />
	</analytic>
	<monogr>
		<title level="m">Proc. Annu. Meeting Assoc. for Computational Linguistics (ACL)</title>
		<meeting>Annu. Meeting Assoc. for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fixing weight decay regularization in adam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno>abs/1711.05101</idno>
		<ptr target="http://arxiv.org/abs/1711.05101" />
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learning Representations (ICLR)</title>
		<meeting>Int. Conf. Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
