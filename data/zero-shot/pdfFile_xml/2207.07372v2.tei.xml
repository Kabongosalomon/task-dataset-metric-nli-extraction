<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3D Instances as 1D Kernels ?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizheng</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Image Processing and Intelligent Control</orgName>
								<orgName type="department" key="dep2">Ministry of Education School of AIA</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Image Processing and Intelligent Control</orgName>
								<orgName type="department" key="dep2">Ministry of Education School of AIA</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaiyuan</forename><surname>Du</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Image Processing and Intelligent Control</orgName>
								<orgName type="department" key="dep2">Ministry of Education School of AIA</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Image Processing and Intelligent Control</orgName>
								<orgName type="department" key="dep2">Ministry of Education School of AIA</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Cao</surname></persName>
							<email>zgcao@hust.edu.cnzhongweicai@huawei.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Image Processing and Intelligent Control</orgName>
								<orgName type="department" key="dep2">Ministry of Education School of AIA</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weicai</forename><surname>Zhong</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Huawei CBG Consumer Cloud Service Search &amp; Maps BU</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">3D Instances as 1D Kernels ?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Instance kernel</term>
					<term>point cloud</term>
					<term>instance segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a 3D instance representation, termed instance kernels, where instances are represented by one-dimensional vectors that encode the semantic, positional, and shape information of 3D instances. We show that instance kernels enable easy mask inference by simply scanning kernels over the entire scenes, avoiding the heavy reliance on proposals or heuristic clustering algorithms in standard 3D instance segmentation pipelines. The idea of instance kernel is inspired by recent success of dynamic convolutions in 2D/3D instance segmentation. However, we find it non-trivial to represent 3D instances due to the disordered and unstructured nature of point cloud data, e.g., poor instance localization can significantly degrade instance representation. To remedy this, we construct a novel 3D instance encoding paradigm. First, potential instance centroids are localized as candidates. Then, a candidate merging scheme is devised to simultaneously aggregate duplicated candidates and collect context around the merged centroids to form the instance kernels. Once instance kernels are available, instance masks can be reconstructed via dynamic convolutions whose weights are conditioned on instance kernels. The whole pipeline is instantiated with a dynamic kernel network (DKNet). Results show that DKNet outperforms the state of the arts on both ScanNetV2 and S3DIS datasets with better instance localization. Code is available: https://github.com/W1zheng/DKNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>3D Instance segmentation aims to predict point-level instance labels <ref type="bibr" target="#b10">[8,</ref><ref type="bibr" target="#b14">12]</ref>. Standard approaches heavily rely on proposals <ref type="bibr" target="#b34">[28,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b22">17]</ref> or heuristic clustering algorithms <ref type="bibr" target="#b14">[12,</ref><ref type="bibr" target="#b1">2]</ref>. In this work, we show that instance masks can be reconstructed by scanning a scene with instance kernels, a representation for 3D instances, which simultaneously encodes the positional, semantic, and shape information of 3D instances.</p><p>3D instance representation addresses two fundamental problems: i) how to localize an instance precisely and ii) how to aggregate features effectively to depict the instance. Unlike 2D instances that can be directly encoded via grid sampling <ref type="bibr" target="#b32">[26]</ref> or dynamic kernel assigning <ref type="bibr">[31]</ref>, in the 3D domain, the disordered and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Centroids cluster</head><p>Instance masks probability <ref type="figure">Fig. 1</ref>. Comparison of inferred centroid clusters and instance masks. Compared with HAIS <ref type="bibr" target="#b1">[2]</ref>, our DKNet generates more focused centroid clusters that can guide precise localization such that small and close instances can be discriminated and large instances have consistent predictions. Best viewed by zooming in and in color.</p><p>unstructured nature of point cloud data renders difficulties for precise instance localization and reliable representation; and top-performing approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">16,</ref><ref type="bibr" target="#b14">12]</ref> implicitly localize instances with centroid offsets <ref type="bibr" target="#b27">[22]</ref>, which only provides coarse information for instance representation, as shown in <ref type="figure">Fig. 1</ref>. Our instance kernel also draws inspiration from DyCo3D <ref type="bibr" target="#b11">[9]</ref>, which first applies dynamic convolution into 3D instance segmentation. However, DyCo3D is built upon the existing bottom-up segmentation pipeline <ref type="bibr" target="#b14">[12]</ref>, leaving the fundamental problems of instance encoding unsolved. To alleviate the difficulties above, we design a novel instance encoding paradigm that efficiently localizes different instances and encodes the semantic, positional, and shape information of instances into instance kernels for mask generation.</p><p>We further incorporate the kernel encoding paradigm into a dynamic kernel network (DKNet) for 3D instance segmentation. To localize instances, as shown in <ref type="figure">Fig. 1</ref>, DKNet predicts a centroid map for instances and extracts centroids via a customized Non-Maximum Suppression (NMS) operator with local normalization. Observing that duplicated candidates may be predicted for a single instance (especially for large ones), we design an iterative aggregation mechanism to merge duplicated candidates guided by a predicted merging score map. The score map indicates the probability whether each paired candidates should be merged. Afterwards, the merged instances are encoded into instance kernels by adaptively fusing the point features around the localized instance centroids. Finally, instance masks can be reconstructed with a few convolution layers, whose weights are conditioned on the generated instance kernels.</p><p>We evaluate DKNet on two popular 3D instance segmentation datasets, including ScanNetV2 <ref type="bibr" target="#b3">[3]</ref> and S3DIS <ref type="bibr" target="#b0">[1]</ref>. The results show that DKNet outperforms previous state-of-the-art approaches, ranking the first AP among published methods on the ScanNetV2 online leaderboard. <ref type="bibr" target="#b0">1</ref> Thanks to the instance kernels and the specially designed instance localization pipeline, DKNet can better distinguish instances from dense areas than current top-performing approaches, as shown in <ref type="figure">Fig. 1</ref>. A series of ablation studies also demonstrate that the proposed instance localization and aggregation pipeline can greatly enhance the instance representation.</p><p>Our contributions are two-fold:</p><p>-We extend the idea of dynamic convolution into instance kernel, a comprehensive representation for 3D instances in point clouds; -We propose a dynamic kernel network for 3D instance segmentation, with a novel instance kernel encoding paradigm;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Here we briefly review the 3D instance segmentation approaches and kernelbased instance segmentation.</p><p>Proposal-Based 3D Instance Segmentation. Proposal-based approaches <ref type="bibr" target="#b35">[29]</ref> assign instances with proposals and the instance masks are generated upon proposals. 3D-BoNet <ref type="bibr" target="#b34">[28]</ref> directly predicts the bounding boxes, within instance masks are generated. 3D-MPA <ref type="bibr" target="#b4">[4]</ref> samples proposals from predicted centroids; masks of proposals are then clustered to form the instance masks. GICN <ref type="bibr" target="#b22">[17]</ref> simultaneously predicts the centroids and sizes of instances to obtain bounding box proposals. Predictions from proposal-based approaches show good objectness, while two major drawbacks exist: 1) the multi-stage training and the proposal generation process introduce large computational overhead; 2) the results highly rely on the proposals. Propoal-Free 3D Instance Segmentation. Proposal-free approaches cluster points into instances in a bottom-up manner. SSTNet <ref type="bibr" target="#b20">[16]</ref> models the entire scene by constructing a tree of superpoints and uses top-bottom traversal to aggregate nodes and form instance masks. PointGroup <ref type="bibr" target="#b14">[12]</ref> clusters points using semantic labels and centroid offsets as clues. PE <ref type="bibr" target="#b36">[30]</ref> encodes points into an embedding space where points from the same instances are close. Then clustering are performed in this embedding space. Considering that clustering points into instances with various sizes in one shot is difficult, HAIS <ref type="bibr" target="#b1">[2]</ref> proposes a novel hierarchical clustering pipeline to gradually refine the aggregation results. However, even with the implicit guide of object signals, the objectness of predictions is still low, as shown in <ref type="figure">Fig. 1</ref>. Hence, directly adding kernel-based dynamic convolution modules upon existing proposal-free approaches cannot bring out the best of kernel-based instance segmentation paradigm. Kernel-Based Instance Segmentation. Kernel-based instance segmentation uses instance-aware kernels to scan the whole scene to reconstruct instance masks, the pivot of which is to represent or associate instances with different kernels. After obtaining the kernels, the common solutions are scanning the scene via dot product or dynamic convolution <ref type="bibr" target="#b13">[11,</ref><ref type="bibr" target="#b24">19]</ref>. CondInst <ref type="bibr" target="#b30">[24]</ref> predicts instance proposals by object detection and encodes the proposal features into kernels. K-Net [31] associates a fixed number of kernels with instances in certain regions and dynamically updates them. SOLOv2 <ref type="bibr" target="#b32">[26]</ref> partitions the feature maps into grids and generates a kernel for each grid. However, representing instances as kernels in the 3D domain is non-trivial due to the disordered and unstructured nature of point cloud data. DyCo3D <ref type="bibr" target="#b11">[9]</ref> first introduce the kernel-based paradigm in 3D instance segmentation, which is built upon existing bottom-up approach <ref type="bibr" target="#b14">[12]</ref>. However, they focus on the concrete implementation of dynamic convolutions, and bypass the core of the kernel-based paradigm: how to encode instances into kernels? In this work, we further explore the underlying relation between the discriminative representation of instances and effective kernel-based segmentation, resulting in a novel localize-then-aggregate instance kernel encoding paradigm.</p><p>3 Dynamic Kernel Network for 3D Instance Segmentation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>As illustrated in <ref type="figure">Fig. 2</ref>, at the core of our dynamic kernel network (DKNet) is to encode instances into discriminative instance kernels. The encoding process consists of three key stages: 1) processing raw point clouds with a UNet-like backbone and predicting point features, centroid offsets, and semantic masks; 2) localizing centroids for instances with a candidate mining branch; 3) merging duplicated candidates and collecting context around instance centroids to form instance kernels. Once the instance kernels are acquired, the instance masks can be obtained by processing the point cloud features with a few convolution layers, whose weights are conditioned on instance kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Point-wise Feature Extraction</head><p>Following recent proposal-free approaches <ref type="bibr" target="#b14">[12,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b20">16]</ref>, we adopt the backbone from PointGroup <ref type="bibr" target="#b14">[12]</ref> for feature extraction. Given the raw point cloud P ? R N ?6</p><p>with N points, a 3D UNet-like <ref type="bibr" target="#b28">[23]</ref> backbone with sparse convolutions <ref type="bibr" target="#b5">[5]</ref> outputs point features F p ? R N ?D . F p is then fed to a semantic branch which predicts semantic mask S ? R N ?C , and additionally, a centroid offset branch which infers the offset O ? R N ?3 of each point to the corresponding instance centroid. The semantic branch is a Multi-Layer Perceptron (MLP) with softmax activation at the output layer. Cross-entropy loss and multi-class dice loss <ref type="bibr" target="#b25">[20]</ref> are used to supervise the training of this branch. Similar to the semantic branch, the centroid offset branch maps F p into offsets O. For each point P i , O i ? R 3 is a vector pointing to the centroid of instance that covers this point. Further details of the backbone can be referred to the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Finding Instances</head><p>To generate the kernel for each instance, we should first find all the instances. However, the top-performing proposal-free approaches predict centroid offsets as implicit object signals, which is rather coarse to precisely localize instances, as shown in <ref type="figure">Fig.1</ref>. Hence, learning from proposal-based approaches, we propose a candidate mining branch to generate centroid maps, followed by a searching algorithm to localize the instance candidates. As shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, point features F p ? R N ?D and centroid offsets O ? R N ?3 are jointly concatenated to form the input F c ? R N ?(D+3) for the centroid mining branch. Then F c is fed into an MLP with softmax activation at the output layer to obtain sharp centroid heatmap H ? R N . Each element H i indicates the probability of the i th point being an instance centroid.</p><p>During training, we place a 3D Gaussian kernel on every instance centroid to form a pseudo ground truth heatmap as? i = exp(?? ? d 2 i /r 2 i ) , where d i denotes the distance between point i to the centroid of the instance covering it. r i , which controls the variance of the Gaussian kernel, equals the maximum side length of the axis-aligned bounding box of the corresponding instance. Hence, the Gaussian kernels are geometry adaptive w.r.t. the size of different instances. ? is set to 25 to keep the average of? around 0.1. To supervise the training, the loss function L center for the candidate mining branch is defined as:</p><formula xml:id="formula_0">L center = 1 N i=1 I(P i ) N i=1 |H i ?? i | ? I(P i ) ,<label>(1)</label></formula><p>where I(P i ) is an indicator function that outputs 1 when i th point belongs to an instance, otherwise outputs 0. With the predicted heatmap H, we iteratively search the local maximum as instance candidates with a customized local normalized NMS (LN-NMS) strategy. During each iteration, the algorithm localizes the point with the highest centroid score among the foreground points; the centroid scores of other points in its neighbor with radius R are then normalized via the division of the maximum value in this R-radius neighbor. If the normalized centroid score is larger than a threshold T ? , this point will be considered a candidate and all other points within its R-radius neighbor are suppressed and excluded in the next iteration, no matter whether the point is chosen as the candidate. We set T ? = 0.5 and the radius R = 0.3m according to the average size of instances in ScanNet <ref type="bibr" target="#b3">[3]</ref>. The iteration ends when no point remains or N ? candidates have been found. N ? is empirically set to 200. Finally, a candidate set Q ? R N ? can be collected, where N ? denotes the number of candidates. Refer to supplementary for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Representing Instances as Kernels</head><p>After localizing the instance centroids, we represent these candidates as instance kernels. We expect that one kernel is extracted for one instance, and the kernel should be discriminative. Therefore, we design a duplicated candidate aggregation strategy that simultaneously eliminates extra candidates and adaptively fuses features around candidates for instance representation.</p><p>Aggregating Duplicate Candidates. We judge whether two candidates should be aggregated based on the context of each candidate. For each raw candidate, we use the features from its "foreground points", "background points" to describe the context. The "foreground points" denote points with the same semantic label within a R-radius neighbor of each candidate, while the "background points" denote all the points with different semantic labels within a 2R-radius neighbor of each candidate. To aggregate features from the foreground and background points, we first process point feature F p with an MLP for dimensionality reduction. Then, the output features of the MLP w.r.t. "foreground points" and "background points" are averaged and respectively form the descriptive feature F n ? R N ? ?D ? and the background feature F b ? R N ? ?D ? for each candidate. As the above two features only encode semantic and shape information, we concatenate them with the shifted coordinate (add the raw coordinates with the centroid offset vectors) of each candidate as positional information to form the aggregation feature F a ? R N ? ?(2D ? +3) for duplicated candidates aggregation. As shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, for each candidate, its aggregation feature F a,i is subtracted from the aggregation feature of all other candidates. By repeating this process for every instance, a candidate difference matrix reflecting how similar Weighted Average</p><p>Difference Matrix each pair of candidates will be generated. Taking the absolute values of the difference matrix as inputs, an MLP with sigmoid function outputs the merging score map A ? R N ? ?N ? , where A ij indicates the probability that the i th and j th candidates shall be merged because they can belong to the same instance.</p><formula xml:id="formula_1">: ? ? ? : ? ? ? : ? ? 3 (a) (b)</formula><p>Once the merging score map A is obtained, a simple greedy algorithm will be used to iteratively merge candidates. We first initialize an instance centroid map M ins ? R N where M ins,i = i. M ins records the indices of instance centroids that each candidate belongs to, and we define candidates with the same index as an instance group. Before aggregation, the instance centroids of candidates are themselves, and each candidate is an instance group. During each iteration, if A ij is the maximum in A excluding diagonal elements, all candidates in the i th and j th instance groups will be merged. The instance centroid indices of these candidates will also be unified to the index of candidate with the highest centroid score among them. Since candidates within the same instance group can no longer be merged, we then update all merging scores between them to be 0. The iteration ends when all merging scores are below a predefined threshold, which is set to 0.5. The candidates with the same index are treated as a predicted instance. After aggregation, supposing I instance groups are generated, the centroid coordinates C ins ? R I?3 of new instances are set to the coordinates of the center candidates, while the features of new instances F ins ? R I?D ? are obtained by a weighted average upon descriptive features of grouped candidates w.r.t. their neighbor sizes. The average can help dynamically aggregate the features of instances with different sizes. As shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, if multiple candidates are predicted on one large instance, information from all the candidates will be propagated to the instance centroids to describe instances.</p><p>During training, it is easy to figure out which pairs of candidates are duplicated, and accordingly a ground truth merging map? can be generated. The standard binary cross entropy loss (BCELoss) is adopted as the loss function L aggre for candidate aggregation, which is defined as:</p><formula xml:id="formula_2">L aggre = BCELoss(A,?),<label>(2)</label></formula><p>where? i,j = 1 if the candidate q i and q j belong to the same ground truth instance, and 0 otherwise. Aside from aggregating duplicated candidates, by guiding the network to distinguish whether two candidates need to be merged, the representation ability of point feature F p can also be enhanced, as the learning of aggregation encourages points from the same instance to be close in the feature space, and vice versa, similar to the idea of contrast learning <ref type="bibr" target="#b8">[7]</ref>.</p><p>Encoding Instance Kernels. After candidate aggregation, all instances in the scene are assigned with centroids, denoted by C ins , and corresponding features, denoted by F ins . F ins is fed into an MLP to generate instance kernels W ? R I?L , where L is the length of instance kernels. In analogous to CondInst <ref type="bibr" target="#b30">[24]</ref> and DyCo3D <ref type="bibr" target="#b11">[9]</ref>, the instance kernels are transformed into the weights for a few convolution layers in the instance decoding stage. Hence, L depends on the specific configurations of the convolution layers, which can be computed by Eq. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Generating Masks with Instance Kernels</head><p>The instance kernels, denoted by W, have encoded the positional, semantic, and shape characteristics of instances. To decode instances, the instance kernels are transformed into the weights with a few convolution layers, which are applied to augmented point cloud features to reconstruct instance masks.</p><p>To augment the point cloud features, an MLP further extracts the mask feature F m ? R N ?D ? from the point feature F p . To inject instance-aware positional information into F m , inspired by DyCo3d <ref type="bibr" target="#b11">[9]</ref>, the offset between each point to the instance centroids are added to F m before convolution. E.g., for each point P i , we compute its offset to the centroid of the k th instance as Z k,i = C ins,k ?X i . Then, the point decoding feature F d ? R N ?(D ? +3) for the k th instance is generated by concatenating F m and Z k along the channel dimension. Although DyCo3D also generates instance masks via instance-specific kernels, the kernels are only applied to the points within the same semantic category. In contrast, the instance kernels in our approach scan the entire scene, which avoids the reliance of semantic prediction. Hence, the decoding process can correct some errors in semantic prediction. The bottom-up approaches <ref type="bibr" target="#b14">[12,</ref><ref type="bibr" target="#b1">2]</ref> cannot correct such errors. The decoding process outputs the instance mask M ? R I?N by</p><formula xml:id="formula_3">M i = Conv(F d , W i ), i ? [1, I] ,<label>(3)</label></formula><p>where W i ? R L is transformed into the weights and biases via two 1 ? 1 convolution layers. The first layer has 16 output channels with ReLU activation function and the second one has 1 output channel with sigmoid for mask decoding. To The use of dynamic convolution is the same as <ref type="bibr" target="#b11">[9,</ref><ref type="bibr" target="#b30">24]</ref>. Implementation details are depicted in the supplementary.</p><p>To supervise the generation of instance masks, we first match the predicted instances with actual instances with Hungarian algorithm <ref type="bibr" target="#b16">[13]</ref> according to a cost matrix. Then, we apply the BCELoss and dice loss <ref type="bibr" target="#b25">[20]</ref> for supervision. Supposing M ? R I?N with I instance masks is generated, andM ? R G?N with G ground-truth instance masks is provided, the cost matrix C ? R I?G for the Hungarian algorithm is obtained by:</p><formula xml:id="formula_4">C i,j = ?C ins,i ? C gt,j ? 2 + I(S ins,i == S gt,j ) ,<label>(5)</label></formula><p>where C ins and C gt are centroid coordinates of predicted an ground truth instances, respectively, and S ins and S gt are corresponding semantic labels. We determine the instance semantic label S ins by voting within the predicted instance. With the cost matrix, one predicted instance mask is expected to be matched to a instance with the closet centroid and identical semantic label. After the matching process, the predicted instance mask M is assigned with the ground truth instance masksM ? R G?N . Then, BCELoss L bce and dice loss L dice are computed by:</p><formula xml:id="formula_5">L mask = 1 I ? I k=1 (BCE(M k ,M k ) + (1 ? 2 M k ?M k |M k | + |M k | )) ? I(iou k &gt; 0.25) ,<label>(6)</label></formula><p>whereM k denotes the ground truth instance mask for the k th instance, and M k denotes the predicted one. iou k denotes the Intersection-over-Union (IoU) between M k andM k , and I is an indicator function. We add the constraints so that the loss will only be computed when instances are correctly matched and I ? is defined by I ? = I k=1 I(iou k &gt; 0.25).</p><p>Inference Post-processing. During inference, to convert the soft instance masks M k 's into hard instance labels and filter out potentially wrong predictions, a simple yet effective two-stage refinement pipeline is proposed. First, some small duplicated fragments or noise is removed. In the second stage, superpoints <ref type="bibr" target="#b18">[15]</ref> are applied to refine the shapes of generated instance masks. Unlike proposalfree approaches <ref type="bibr" target="#b14">[12,</ref><ref type="bibr" target="#b11">9,</ref><ref type="bibr" target="#b1">2]</ref>, our approach do not need NMS or ScoreNet in postprocessing, which is efficient.</p><p>In the first stage, given the predicted soft instance mask M ? R I?N , we first generate the raw instance label by selecting the label of the instance with the highest score in M . Then, we define a coverage score S c ? R I by: where N inter,k and N intra,k denote the number of "inter-point" and "intra-point" for the k th instance, respectively. The "inter-point" denotes the number of points being assigned to the k th instance in the raw instance label, while the "intrapoint" denotes the number of points in the soft instance mask of the k th instance that are above a threshold T m,k . T m,k is determined by the Otsu algorithm <ref type="bibr" target="#b26">[21]</ref> that is adaptive to different instances. The cover score indicates the completeness and independence of each instance prediction. We then multiply the coverage score S c with M to generate the refined soft instance mask. By taking the instance with the highest score in the refined mask, the final hard instance labels can be obtained. Refer to supplementary for more details on computing T m . In the second stage, we first aggregate the raw point clouds into superpoints <ref type="bibr" target="#b18">[15]</ref>. Points within a certain superpoint should belong to the same instance. Hence, we unify the instance label in each superpoint to be the one that most points belong to. Aside from the instance label R, we need to assign a confidence score for each predicted instance that indicates the prediction quality for evaluation. This score is obtained by multiplying the average of instance score and semantic score of the "intra-point".</p><formula xml:id="formula_6">S c,k = N inter,k /N intra,k ,<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we first compare the proposed Dynamic Kernel Network (DKNet) with other state-of-the-art approaches on two 3D instance segmentation benchmarks: ScanNetV2 <ref type="bibr" target="#b3">[3]</ref> and S3DIS <ref type="bibr" target="#b0">[1]</ref>. Then, we verify the effectiveness of different components in DKNet via a controlled ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>Training Details. For data preparation, the coordinates and colors are concatenated together to form 6D vectors for each point. The network is trained on a single RTX 3090 GPU with a batch size of 4 for 400 epochs. We use the AdamW <ref type="bibr" target="#b23">[18]</ref> optimizer with an initial learning rate of 0.001, which is adjusted by a cosine scheduler <ref type="bibr" target="#b33">[27]</ref> during training. Weight decay is set to 1e-5. Following previous methods <ref type="bibr" target="#b14">[12]</ref>, we voxelize the point clouds with the size of 0.02m for ScanNetV2 and 0.05m for S3DIS. The overall training loss combines the loss from the semantic prediction, offset prediction, candidate mining, candidate aggregation branch, and the mask generation process, which can be defined as:</p><formula xml:id="formula_7">L = L sem + L of f + L center + L aggre + L mask ,<label>(8)</label></formula><p>where L sem and L of f are the losses for semantic segmentation and centroid offsets prediction, respectively. Datasets. We use ScanNetV2 and S3DIS for training and evaluation. Scan-NetV2 includes 1, 613 scenes with 20 different semantic categories. 1, 201, 312 and 100 scenes are selected as the training, validation and test set, respectively. Note that the labels for test set are hidden for a fair comparison. Following official evaluation protocol, we use mean average precisions (mAPs) under different IoU thresholds as the evaluation metrics. AP @25 and AP @50 denote the average precision scores with IoU thresholds set to 25% and 50%. mAP denotes the average of all the AP s with IoU thresholds ranging from 50% to 95% with a step size of 5%. S3DIS dataset consists of 271 scenes collected from 6 different areas with 13 different object categories. Following previous approaches <ref type="bibr" target="#b14">[12,</ref><ref type="bibr" target="#b11">9,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b20">16]</ref>, we train and evaluate our approach in two ways: to 0.5, we report coverage (mCov), weighted coverage (mWCov), mean precision (mPrec), and mean recall (mRec) as evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with the state of the arts.</head><p>ScanNetV2. Comparisons with the state of the arts on the ScanNetV2 test set are shown in <ref type="table" target="#tab_1">Table 1</ref>. Our approach achieves an mAP of 53.2%, outperforming previous state-of-the-art approaches. The proposed DKNet obtains significant improvement on small instances like chairs or pictures, and competitive results on large instances like beds or tables. We also notice that, compared with a recent well-designed bottom-up method SoftGroup <ref type="bibr" target="#b31">[25]</ref>, DKNet shows inferior AP @50. The plausible reason are two fold. First, we find the DKNet retains relative high AP under strict IoU thresholds (&gt;0.6), indicating that the predicted masks can well preserve the instance shapes. However, under lower IoU thresholds (&lt;0.6), DKNet becomes less advantageous. Second, our pure data-driven candidate merging process shows mistakes on some difficult scenes, such as bookshelves with vague boundaries, which can be better tackled by well-designed bottom-up clustering. These results suggest that DKNet can be improved with careful design to further boost the potential of the proposed kernel-based paradigm.</p><p>To evaluate the instance localization performance of 3D instance segmentation approaches, we compare different 3D instance approaches under object detection metrics on ScanNetV2 validation set in <ref type="table" target="#tab_2">Table 2</ref>. Predicted masks are converted into axis-aligned bounding boxes following DyCo3D <ref type="bibr" target="#b11">[9]</ref>. Our approach achieves the best performance in AP @25 of 67.4% and AP @50 of 59.0%, which demonstrate that kernels are extracted from solid instance localization results. S3DIS. As shown in <ref type="table">Table 3</ref>, DKNet is comparable on scenes in area-5, while outperforms other approaches in 3 out of 4 metrics on 6-fold cross validation.</p><p>Since most categories in S3DIS are large ones like ceiling, wall, and bookcase, the results can reflect the robustness of the proposed instance encoding paradigm on large instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Qualitative Evaluation</head><p>We visualize the predicted masks in <ref type="figure">Fig. 5</ref>. As is shown in column 2, excluding candidate aggregation (baseline) leads to severe over-segmentation as multiple kernels will be generated for one instance. The over-segmentation can be effectively alleviated with the candidate aggregation, which will simultaneously guides the generation of instance-aware features. Comparing with the common NMS in GICN <ref type="bibr" target="#b22">[17]</ref>, the proposed LN-NMS can better localize instances with different sizes, while common NMS omits some small instances (garbage bins in row 2 and chairs in row 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>Here, we first compare different candidate mining and aggregation strategies. Then, we analyze how to better represent instances as kernels. Candidate Mining. Here we verify the effectiveness of the proposed LN-NMS algorithm for candidate mining. As in <ref type="table" target="#tab_4">Table 4</ref>, three different ways are compared. Random (row 1) denotes points above a threshold is randomly selected (at most 200) as instance candidates. NMS denotes the searching algorithm with common NMS used in GICN <ref type="bibr" target="#b22">[17]</ref>. And the proposed LN-NMS algorithm shown in Sec. 3.3. Results in row 1 show that: 1) poor instance localization can significantly degrade instance segmentation performance; 2) even selecting points randomly in the heatmaps as candidates can yield competitive results, which demonstrates the robustness of the subsequent candidate aggregation module. Comparing results in row 2 with row 3, LN-NMS improves AP @50 by 2.1%. Instance Candidates Aggregations. Comparison of different candidate aggregation strategies are shown in <ref type="table">Table 5</ref>. "W/o" means no candidate aggregation are preformed. "In training" means only optimizing aggregation loss during training, while the raw candidates will not be aggregated for inference. "All phases" denotes our full approach. When adding the aggregation loss only for supervision (row 2), AP @50 increases by 1.9%; Motivated by the aggregation loss, there is an instance-clustering trend for the identical instances in the feature space, which is similar to contrast learning <ref type="bibr" target="#b8">[7]</ref>. By aggregating the duplicated candidates, the full approach promotes the AP , AP @50 and AP @25 by 3.1%, 4.1% and 2.0% comparing with baseline, which demonstrates the effectiveness of candidate aggregation. Generating Instance Kernels. With the raw instance candidates and merging map M ins marking which candidates shall be aggregated, naturally, there are 2 different ways to represent each instance: 1) only using the feature from the candidate with the highest centroids scores; 2) aggregating features from all the merged candidates. As in <ref type="table" target="#tab_5">Table 6</ref>, the latter way (default) obtains better results, which demonstrates that aggregating features from different candidates benefits the representation. In addition, we test an ideal representation in row 3 where features are collected from all the points within each instance. It can be observed that the performances of our instance representation approach are comparable with this ideal representation. This indicates that representative and discriminative instance contexts are obtained in our kernel generation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We introduce a 3D instance representation, termed instance kernels, which encodes the positional, semantic, and shape information of instances into a 1D vector. We find that the difficulty in representing 3D instances lies in precisely localizing instances and collecting discriminative features. Accordingly, we design a novel instance encoding paradigm that first mines centroids candidates for localization. Then, an aggregation process simultaneously eliminates duplicated candidates and gathers features around each instance for representation. We incorporate the instance kernel into a Dynamic Kernel Network (DKNet), which outperforms previous state-of-the-art approaches on public benchmarks. 31. Zhang, W., Pang, J., Chen, K., Loy, C.C.: K-net: Towards unified image segmentation. In: Proceedings of Advances in Neural Information Processing Systems (NeurIPS). pp. 10326-10338 (2021)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1 Backbone and Loss Function</head><p>As mentioned in Sec. 3.2, a 3D UNet-like backbone <ref type="bibr" target="#b28">[23]</ref> is adopted to extract point features F p ? R N ?D . And two multi-layer perceptrons (MLPs) are used to predict semantic masks and centroid offsets. We specify the detailed architectures of these three components in <ref type="figure">Fig. A1</ref>. These three components are adopted from PointGroup <ref type="bibr" target="#b14">[12]</ref>, which is common for recent top-performing methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">16]</ref>.</p><p>Loss for Semantic Branch. The semantic prediction branch outputs S ? R N ?C , where C is the number of categories. For point P i , S i denotes the probability of this point belonging to different semantic categories. Given the one-hot ground truth semantic label? i , the semantic loss L sem can be computed as:</p><formula xml:id="formula_8">L sem = 1 N N i=1 CE(S i ,? i ) + 1 ? 2 N i=1 S T i? i N i=1 S T i S i + N i=1? T i? i ,<label>(A9)</label></formula><p>where CE(x, y) denotes the cross entropy loss. The second term in Eq. A9 is the multi-class dice loss <ref type="bibr" target="#b25">[20]</ref>, which can help address the imbalance between different semantic categories.</p><p>Loss for Offset Branch. The offset branch estimates the centroid offsets for all points, i.e., O ? R N ?3 . Given a point P i , we define the centroid of the instance that covers this point as C p,i . Both the Euclidean norm and the direction are considered to measure the difference between the estimated centroid offset vector O i and the ground truth offset C p,i ? X i , where X i denotes the 3D coordinate of the point P i . Then, the offset loss L of f is computed as:</p><formula xml:id="formula_9">L of f = 1 N ? N i=1 (?O i ? (C p,i ? X i )? + O i ? (C p,i ? X i ) ?O i ? ? ?C p,i ? X i ? ) ? I(P i ),<label>(A10)</label></formula><p>where I(P i ) is an indicator function that outputs 1 when point P i belongs to one instance, otherwise outputs 0. N ? denotes the number of points (excluding background points), which can be obtained via N ? = N i=1 I(P i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2 Algorithms</head><p>Candidate Mining Algorithm As mentioned in Sec R N ?C . B i equals the category label with the maximum score in S i . Note that, apart from localizing instance centroids, the candidate mining algorithm fetches the "foreground points" and "background points" of each candidate to describe the candidates for further processing, as mentioned in Sec.3.4 (line 244).</p><p>Candidate Merging Algorithm As mentioned in Sec. 3.4, to aggregate duplicated candidates with the predicted merging score map A , we design a candidates merging algorithm. Algorithm 2 illustrates this merging process in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A3 Instance kernels</head><p>To obtain a discriminative instance kernel, we encode semantic, positional and shape information to represent instance. The position and semantic information comes from candidate coordinates and point features. Shape information is thus encoded by splitting 'foreground points' and 'background points'. As illustrated in <ref type="figure" target="#fig_1">Fig. A3</ref>, 'foreground points' sketch a basic shape of instance (a chair). In <ref type="table" target="#tab_1">Table A1</ref> we show the performance when the position (coordinates) or shape (mixing foreground and background points for feature pooling) is ablated. One can observe that both information is vital for instance kernels. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A4 Dynamic Convolution</head><p>To generate instance masks, point features are fed into different instance decoders consisting of a few dynamic convolution layers. The parameters of dynamic convolutions are conditioned on the corresponding instance kernels. As shown in <ref type="figure">Fig. A2</ref>, we instantiate the instance decoder with two convolution layers, which have 16 and 1 output channels (its kernel shape is <ref type="bibr" target="#b20">[16,</ref><ref type="bibr" target="#b0">1]</ref>). The elements in one instance kernel are sequentially inserted into the weight vectors and biases of these two convolution layers. Hence, the length of instance kernels L depends on the specific configuration of the instance decoder. As for the instance decoder in <ref type="figure">Fig. A2</ref>, L can be computed by: To evaluate the effect of kernel size, we design a series of ablation experiments. As illustrated in <ref type="table" target="#tab_2">Table A2</ref>, DKNet is stable under varying instance kernel sizes and dynamic convolution layer shapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A5 Thresholds for Soft Instance Mask</head><p>As mentioned in Section 3.5, in post-processing, we use the Otsu algorithm <ref type="bibr" target="#b26">[21]</ref> to binarize the predicted soft instance masks. Otsu algorithm divides the pixels in a grey image into the foreground or the background category, whose essential idea is to maximize the inter-class variance. We re-purpose this idea to binarize</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Candidates</head><p>Foreground point Background point the soft instance masks M ? R I?N . As the original algorithm functions on pixels with discrete gray levels, similarly, we discreteize the value ([0, 1]) of soft instance mask into K confidence levels. Therefore, the quantified instance mask M ? can be obtained by:</p><formula xml:id="formula_10">M ? k = ?M k * K? ,<label>(A14)</label></formula><p>where M k denotes the k th instance mask. Taking the quantified masks as inputs, Otsu algorithm processes each point in M ? k as a pixel in an image, and outputs T m,k for each instance. Instead of using fixed threshold, Otsu algorithm can adaptively generate thresholds, which can better preserve the shape of instance with weak responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A6 More Visualizations</head><p>More visualizations of instance segmentation results and intermediate centroid maps are shown in <ref type="figure" target="#fig_3">Fig. A4</ref>. Base denotes the baseline method without candidate aggregation while Full denotes our full method. We also observe that, by reconstructing instance masks from instance kernels, some errors in semantic predictions can be corrected. We show some examples in <ref type="figure">Fig A5.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A7 Efficiency</head><p>Here we specify the training and inference time of the proposed DKNet. Training DKNet on ScanNetV2 <ref type="bibr" target="#b3">[3]</ref> with default settings consumes about 72 GPU hours on an single RTX 3090. In terms of inference, DKNet is relatively efficient; the average inference time (per scene) of DKNet on a Titan XP is 521 ms, which is on  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>?</head><label></label><figDesc>Y. Wu and M. Shi contributed equally. Z. Cao is the corresponding author. arXiv:2207.07372v2 [cs.CV] 18 Jul 2022</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Centroid mining branch. (a) The input of candidate mining branch; (b) The customized Non-Maximum Suppression with local normalization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Candidates aggregation module. (a) The process of predicting the merging score map; (b) The process of generating instance kernels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>fit the number of parameters of convolution layers, the length of the instance kernel L can be computed by L = (16 + 3) ? 16(weight) + 16(bias) + 16 ? 1(weight) + 1(bias) = 337 . (4)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>. 3 . 3 (</head><label>33</label><figDesc>line 219), we design a customized non-maximum suppression algorithm with local normalization (LN-NMS) to localize instance centroids from the predicted heatmaps. The detailed candidate mining process is described in Algorithm 1. The semantic label B ? R N denotes the hard semantic label derived from the soft semantic masks S ? . Detailed network architecture. (a) Architecture of backbone. (b) Architecture of semantic branch and offset branch. V denotes the number of voxels. The number near linear layer denotes the number of output channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. A3 .Fig. A4 .</head><label>A3A4</label><figDesc>Shape modeling. Visualizations on ScanNetV2 validation set.Fig. A5. The robustness against errors in semantic predictions. Although errors occur in the semantic predictions (left parts), the instance decoder can still recover correct instance masks (right parts).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>DKNet Feature Visualization &amp; Results Data flow Instance decoder Feature Visualization Pooling Kernels ? Instance masks I?N foreground &amp; background points</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>MLP</cell><cell>:</cell><cell>Candidate mining</cell><cell>:N?</cell><cell>Candidate aggregation</cell><cell>: I?D?</cell><cell>MLP</cell></row><row><cell></cell><cell></cell><cell></cell><cell>: ?3</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Offset branch</cell><cell></cell><cell></cell><cell></cell></row><row><cell>encoder</cell><cell>decoder</cell><cell>: ?D</cell><cell>Semantic branch</cell><cell>: ?1</cell><cell></cell><cell></cell><cell>:N??D?</cell></row><row><cell cols="2">Unet</cell><cell></cell><cell></cell><cell></cell><cell>MLP</cell><cell>: ?D?</cell><cell>:N??D?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MLP</cell><cell>: N?D?</cell></row></table><note>Fig. 2. Pipeline of the Dynamic Kernel Network.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Quantitative results on the ScanNetV2 test set. Refer to supplementary full results with all the 20 categories. 76.1 80.8 84.5 71.6 86.2 82.4 65.5 62.0 73.4 69.9 79.1 84.4 76.9 59.4 Ours 53.2 71.8 81.4 78.2 61.9 87.2 75.1 56.9 67.7 58.5 72.4 63.3 81.9 73.6 61.7</figDesc><table><row><cell>approaches</cell><cell>mAP</cell><cell>AP @50</cell><cell>bed</cell><cell>booksh.</cell><cell>cabinet</cell><cell>chair</cell><cell>curtain</cell><cell>desk</cell><cell>door</cell><cell>otherfu.</cell><cell>picture</cell><cell>refrige.</cell><cell>sofa</cell><cell>table</cell><cell>window</cell></row><row><cell cols="16">3D-BoNet[28] 25.3 48.8 67.2 59.0 30.1 48.4 62.0 30.6 34.1 25.9 12.5 43.4 49.9 51.3 43.9</cell></row><row><cell>3D-SIS[10]</cell><cell cols="15">16.1 38.2 43.2 24.5 19.0 57.7 26.3 3.3 32.0 24.0 7.5 42.2 69.9 27.1 23.5</cell></row><row><cell>MTML[14]</cell><cell cols="15">28.2 54.9 80.7 58.8 32.7 64.7 81.5 18.0 41.8 36.4 18.2 44.5 68.8 57.1 39.6</cell></row><row><cell>3D-MPA[4]</cell><cell cols="15">35.5 61.1 83.3 76.5 52.6 75.6 58.8 47.0 43.8 43.2 35.8 65.0 76.5 55.7 43.0</cell></row><row><cell cols="16">PointGroup[12] 40.7 63.6 76.5 62.4 50.5 79.7 69.6 38.4 44.1 55.9 47.6 59.6 75.6 55.6 51.3</cell></row><row><cell>GICN[17]</cell><cell cols="15">34.1 63.8 89.5 80.0 48.0 67.6 73.7 35.4 44.7 40.0 36.5 70.0 83.6 59.9 47.3</cell></row><row><cell>DyCo3D[9]</cell><cell cols="15">39.5 64.1 84.1 89.3 53.1 80.2 58.8 44.8 43.8 53.7 43.0 55.0 76.4 65.7 56.8</cell></row><row><cell>Occuseg[6]</cell><cell cols="15">48.6 67.2 75.8 68.2 57.6 84.2 50.4 52.4 56.7 58.5 45.1 55.7 79.7 56.3 46.7</cell></row><row><cell>PE[30]</cell><cell cols="15">39.6 64.5 77.3 79.8 53.8 78.6 79.9 35.0 43.5 54.7 54.5 64.6 76.1 55.6 50.1</cell></row><row><cell>SSTNet[16]</cell><cell cols="15">50.6 69.8 69.7 88.8 55.6 80.3 62.6 41.7 55.6 58.5 70.2 60.0 72.0 69.2 50.9</cell></row><row><cell>HAIS[2]</cell><cell cols="15">45.7 69.9 84.9 82.0 67.5 80.8 75.7 46.5 51.7 59.6 55.9 60.0 76.7 67.6 56.0</cell></row><row><cell cols="2">SoftGroup[25] 50.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table><row><cell cols="3">Object detection re-</cell><cell cols="4">Table 3. Quantitative results on S3DIS</cell></row><row><cell cols="3">sults on ScanNetV2 valida-</cell><cell cols="4">dataset. We report mCov, mWCov, mPre, and</cell></row><row><cell cols="3">tion set. We report per-class</cell><cell cols="4">mRec. approaches with  ? marks are evaluated on</cell></row><row><cell cols="3">mAP with an IoU of 25% and</cell><cell cols="4">scenes in Area-5. The others are evaluated via 6-</cell></row><row><cell cols="3">50%. The IoU is computed on</cell><cell cols="2">fold cross validation.</cell><cell></cell><cell></cell></row><row><cell cols="3">bounding boxes. We evaluate the</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">performance of HAIS with the</cell><cell>approach</cell><cell cols="3">mCov mWCov mPre mRec</cell></row><row><cell cols="3">provided model. "Ours ? " de-notes the model without candi-dates aggregation part.</cell><cell>PointGroup  ? [12] DyCo3D  ? [9] SSTNet  ? [16]</cell><cell>-63.5 -</cell><cell>-64.6 -</cell><cell>61.9 62.1 64.3 64.2 65.5 64.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>HAIS  ? [2]</cell><cell>64.3</cell><cell>66.0</cell><cell>71.1 65.0</cell></row><row><cell>approach</cell><cell cols="2">AP @25 AP @50</cell><cell>Ours  ?</cell><cell>64.7</cell><cell>65.6</cell><cell>70.8 65.3</cell></row><row><cell cols="2">VoteNet [22] 58.6</cell><cell>33.5</cell><cell>OccuSeg [6]</cell><cell>-</cell><cell>-</cell><cell>72.8 60.3</cell></row><row><cell>3DSIS [10]</cell><cell>40.2</cell><cell>22.5</cell><cell>GICN [17]</cell><cell>-</cell><cell>-</cell><cell>68.5 50.8</cell></row><row><cell>3D-MPA [4]</cell><cell>64.2</cell><cell>49.2</cell><cell>PointGroup [12]</cell><cell>-</cell><cell>-</cell><cell>69.6 69.2</cell></row><row><cell>DyCo3D [9]</cell><cell>58.9</cell><cell>45.3</cell><cell>SSTNet [16]</cell><cell>-</cell><cell>-</cell><cell>73.5 73.4</cell></row><row><cell>HAIS [2]</cell><cell>66.0</cell><cell>54.2</cell><cell>HAIS [2]</cell><cell>67.0</cell><cell>70.4</cell><cell>73.2 69.4</cell></row><row><cell>Ours ?</cell><cell>65.4</cell><cell>57.9</cell><cell>SoftGroup [25]</cell><cell>69.3</cell><cell>71.7</cell><cell>75.3 69.8</cell></row><row><cell>Ours</cell><cell>67.4</cell><cell>59.0</cell><cell>Ours</cell><cell>70.3</cell><cell>72.8</cell><cell>75.3 71.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>1) scenes from area-5 are used for testing while scenes in other areas are used for training; 2) 6-fold cross validation where each area is used in turn for testing. On S3DIS, with the threshold IoU set</figDesc><table><row><cell>ground truth</cell><cell>baseline</cell><cell>w/ cand. aggre.</cell><cell>w/ cand. aggre.</cell><cell>w/ cand. aggre.</cell></row><row><cell></cell><cell></cell><cell>only in training</cell><cell>&amp; common NMS</cell><cell>&amp; LN-NMS</cell></row></table><note>Fig. 5. Qualitative results of our approaches on ScanNetV2 validation set. We highlight the key details with red marks. Best viewed by zooming in and in color.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Comparison of different candidate mining algorithms on ScanNetV2 validation set.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Table 5. Comparison of different instance</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">aggregation strategies on ScanNetV2 val-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>idation set.</cell><cell></cell></row><row><cell cols="3">Strategy AP AP @50 AP @25</cell><cell>Phase</cell><cell cols="2">AP AP @50 AP @25</cell></row><row><cell cols="2">Random 48.0 63.7</cell><cell>73.5</cell><cell>W/o</cell><cell>47.7 62.6</cell><cell>74.9</cell></row><row><cell>NMS</cell><cell>49.7 64.6</cell><cell>75.7</cell><cell cols="2">In training 48.4 64.5</cell><cell>76.1</cell></row><row><cell cols="2">LN-NMS 50.8 66.7</cell><cell>76.9</cell><cell cols="2">All phases 50.8 66.7</cell><cell>76.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Comparisons of different candidates aggregation approaches on ScanNetV2 validation set.</figDesc><table><row><cell>Strategy</cell><cell cols="3">AP AP @50 AP @25</cell></row><row><cell cols="2">Candidates elimination 50.4</cell><cell>65.2</cell><cell>75.8</cell></row><row><cell cols="2">Candidates aggregation 50.8</cell><cell>66.7</cell><cell>76.9</cell></row><row><cell>Full instances</cell><cell>51.5</cell><cell>67.0</cell><cell>77.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table A1 .Table A2 .</head><label>A1A2</label><figDesc>Component analysis. Kernel shape analysis.</figDesc><table><row><cell cols="3">Info. components mAP AP @50 AP @25</cell><cell cols="3">Kernel shape/size mAP AP@50 AP@25</cell></row><row><cell>W/o coord.</cell><cell>49.5 66.3</cell><cell>75.9</cell><cell>[8, 1]/169</cell><cell>50.6 67.8</cell><cell>76.4</cell></row><row><cell>W/o shape</cell><cell>49.3 64.3</cell><cell>75.2</cell><cell>[16, 1]/337</cell><cell>50.8 66.7</cell><cell>76.9</cell></row><row><cell>W/o both</cell><cell>47.9 63.6</cell><cell>73.8</cell><cell>[16, 8, 1]/465</cell><cell>51.2 67.1</cell><cell>77.0</cell></row><row><cell>Full</cell><cell>50.8 66.7</cell><cell>76.9</cell><cell>[16, 16, 1]/609</cell><cell>50.9 66.0</cell><cell>76.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Mins Instance Kernels Input Output R Instance decoder ?</head><label></label><figDesc>Conv1 #weight = (16 + 3) ? 1 ? 1 ? 16 = 304, #bias = 16 ? 1 = 16 , (A11) Candidate merging algorithm Input: merging score map A, centroids map H, candidates Q. Output: instance centroid map Mins ? R N ? 1: initialize an array Mins = arange(N Fig. A2. Details of instance decoder. The instance decoder consists of two convolution layers. The elements in instance kernels are sequentially inserted into the weights and biases for convolution layers.</figDesc><table><row><cell cols="5">Algorithm 2 ? )</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">2: while A.max() &gt; 0.5 do</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3:</cell><cell cols="3">i, j = col, row of A.argmax()</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4:</cell><cell cols="8">Gi = (Mins == Mins[i]), Gj = (Mins == Mins[j]), G = where(Gi ? Gj)</cell></row><row><cell>5:</cell><cell cols="2">c = H[Q[G]].argmax()</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>6:</cell><cell cols="2">update Mins[G] = Mins[c]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>7:</cell><cell>for m in G do</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>8:</cell><cell cols="2">for n in G do</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>9:</cell><cell cols="2">update A[m,n] = 0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Instance decoder Localization Representation 10: return Related offsets</cell><cell>Instance Kernel :</cell><cell>?</cell><cell>+</cell><cell>+</cell><cell>+</cell><cell>Mask =</cell><cell>Mask</cell></row><row><cell></cell><cell>: ? Mask features : ?</cell><cell>?</cell><cell>Conv1</cell><cell>?</cell><cell>Conv2</cell><cell>? Mask</cell><cell></cell></row><row><cell cols="9">Conv2 #weight = 16 ? 1 ? 1 ? 1 = 16, #bias = 1 ? 1 = 1 ,</cell><cell>(A12)</cell></row><row><cell cols="3">L = 304 + 16 + 16 + 1 = 337 ,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(A13)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table A3 .</head><label>A3</label><figDesc>Inference time of different stages in DKNet on RTX 3090. Total Backbone Encoding Decoding Post-processing 357.5ms 160.6ms 129.3ms 16.3ms 45.9ms Table A4. Full quantitative results of AP @50 on the ScanNetV2 test set. Best performance is in boldface.</figDesc><table><row><cell>approaches</cell><cell>AP @50</cell><cell>bathtub</cell><cell>bed</cell><cell>booksh.</cell><cell>cabinet</cell><cell>chair</cell><cell>counter</cell><cell>curtain</cell><cell>desk</cell><cell>door</cell><cell>otherfu.</cell><cell>picture</cell><cell>refrige.</cell><cell>s. curtain</cell><cell>sink</cell><cell>sofa</cell><cell>table</cell><cell>toilet</cell><cell>window</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table A5 .</head><label>A5</label><figDesc>Full quantitative results of mAP on the ScanNetV2 test set. Best performance is in boldface. BoNet[28] 25.3 51.9 32.4 25.1 13.7 34.5 3.1 41.9 6.9 16.2 13.1 5.2 20.2 33.8 14.7 30.1 30.3 65.1 17.8 MTML[14] 28.2 57.7 38.0 18.2 10.7 43.0 0.1 42.2 5.7 17.9 16.2 7.0 22.9 51.1 16.1 49.1 31.3 65.0 16.2 3D-MPA[4] 35.5 45.7 48.4 29.9 27.7 59.1 4.7 33.2 21.2 21.7 27.8 19.3 41.3 41.0 19.5 57.4 35.2 84.9 21.3 PointGroup[12] 40.7 63.9 49.6 41.5 24.3 64.5 2.1 57.0 11.4 21.1 35.9 21.7 42.8 66.0 25.6 56.2 34.1 86.0 29.1 GICN[17] 34.1 58.0 37.1 34.4 19.8 46.9 5.2 56.4 9.3 21.2 21.2 12.7 34.7 53.7 20.6 52.5 32.9 72.9 24.1 DyCo3D[9] 39.5 64.2 51.8 44.7 25.9 66.6 5.0 25.1 16.6 23.1 36.2 23.2 33.1 53.5 22.9 58.7 43.8 85.0 31.7 Occuseg[6] 48.6 80.2 53.6 42.8 36.9 70.2 20.5 33.1 30.1 37.9 47.4 32.7 43.7 86.2 48.5 60.1 39.4 84.6 27.3 SSTNet[16] 50.6 73.8 54.9 49.7 31.6 69.3 17.8 37.7 19.8 33.0 46.3 57.6 51.5 85.7 49.4 63.7 45.7 94.3 29.0 HAIS[2] 45.7 70.4 56.1 45.7 36.4 67.3 4.6 54.7 19.4 30.8 42.6 28.8 45.4 71.1 26.2 56.3 43.4 88.9 34.4 Ours 53.2 81.5 62.4 51.7 37.7 74.9 10.7 50.9 30.4 43.7 47.5 58.1 53.9 77.5 33.9 64.0 50.6 90.1 38.5</figDesc><table><row><cell>approaches</cell><cell>mAP</cell><cell>bathtub</cell><cell>bed</cell><cell>booksh.</cell><cell>cabinet</cell><cell>chair</cell><cell>counter</cell><cell>curtain</cell><cell>desk</cell><cell>door</cell><cell>otherfu.</cell><cell>picture</cell><cell>refrige.</cell><cell>s. curtain</cell><cell>sink</cell><cell>sofa</cell><cell>table</cell><cell>toilet</cell><cell>window</cell></row><row><cell>3D-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://kaldir.vc.in.tum.de/scannet_benchmark/semantic_instance_3d</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work was supported in part by the National Key R&amp;D Program of China (No.2018YFB1305504) and the DigiX Joint Innovation Center of Huawei-HUST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D Instances as 1D Kernels</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Q.append(q) 21:</p><p>Fn.append(fn/kn) 22:</p><p>F b .append(f b /k b ) 23:</p><p>k + + 24: return Q, Fn, F b</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">3d semantic parsing of large-scale indoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Brilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.170</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.170" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1534" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hierarchical aggregation for 3d instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15447" to="15456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/ICCV48922.2021.01518</idno>
		<ptr target="https://doi.org/10.1109/ICCV48922.2021.01518" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.261</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.261" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2432" to="2443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">3d-mpa: Multiproposal aggregation for 3d semantic instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bokeloh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR42600.2020.00905</idno>
		<ptr target="https://doi.org/10.1109/CVPR42600.2020.00905" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9028" to="9037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Occuseg: Occupancy-aware 3d instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2937" to="2946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/CVPR42600.2020.00301</idno>
		<ptr target="https://doi.org/10.1109/CVPR42600.2020.00301" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9726" to="9735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/CVPR42600.2020.00975</idno>
		<ptr target="https://doi.org/10.1109/CVPR42600.2020.00975" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.322</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2017.322" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dyco3d: Robust instance segmentation of 3d point clouds through dynamic convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="354" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3d-sis: 3d semantic instance segmentation of RGB-D scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00455</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2019.00455" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4421" to="4430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Advances in Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="667" to="675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pointgroup: Dual-set point grouping for 3d instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4866" to="4875" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/CVPR42600.2020.00492</idno>
		<ptr target="https://doi.org/10.1109/CVPR42600.2020.00492" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Naval research logistics quarterly</title>
		<imprint>
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3d instance segmentation via multi-task metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lahoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Oswald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00935</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2019.00935" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9255" to="9265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/CVPR.2018.00479</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2018.00479" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Instance segmentation in 3d scenes using semantic superpoint tree networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2763" to="2772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/ICCV48922.2021.00278</idno>
		<ptr target="https://doi.org/10.1109/ICCV48922.2021.00278" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">Learning gaussian instance segmentation in point clouds. arXiv Computer Research Repository</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<title level="m">Decoupled weight decay regularization. arXiv Computer Research Repository</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Index networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="242" to="255" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Ahmadi</surname></persName>
		</author>
		<idno type="DOI">10.1109/3DV.2016.79</idno>
		<ptr target="https://doi.org/10.1109/3DV.2016.79" />
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A threshold selection method from gray-level histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep hough voting for 3d object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00937</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2019.00937" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9276" to="9285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)</title>
		<meeting>International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)</meeting>
		<imprint>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Conditional convolutions for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58452-8_17</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58452-817" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="282" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Softgroup for 3d instance segmentation on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Solov2: Dynamic and fast instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Advances in Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stochastic gradient descent with step cosine warm restarts for pathological lymph node image classification via pet/ct images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Signal and Image Processing</title>
		<imprint>
			<publisher>ICSIP</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning object bounding boxes for 3d instance segmentation on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Advances in Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6737" to="6746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">GSPN: generative shape proposal network for 3d instance segmentation in point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00407</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2019.00407" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3947" to="3956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Point cloud instance segmentation using probabilistic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8883" to="8892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Note that, DKNet is much more efficient than recent top-down approach such as GICN [17] (8615 ms). The detailed inference time of different steps in DKNet is shown Table A3. The core operation: encoding instance kernels consumes the second most time (36.19%). To further boost the efficiency, we build a CUDA library for core operations in DKNet, e.g., the LN-NMS and Otsu thresholding mentioned in Sec. 3.4 and 3.5. Compared with naive python implementation, the CUDA version LN-NMS reduces its inference time from 187 ms to 79 ms. Note that, these operations are parallelized with naive implementations</title>
		<idno>3D-BoNet[28] 48recent bottom-up approaches PointGroup [12] (452 ms) and HAIS [2] (339</idno>
		<imprint/>
	</monogr>
	<note>ms) on the same device, only introducing limited latency (100-200 ms). with much room to be optimized. Hence, the inference time can probably be further reduced</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
