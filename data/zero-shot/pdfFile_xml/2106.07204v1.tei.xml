<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HARD SAMPLES RECTIFICATION FOR UNSUPERVISED CROSS-DOMAIN PERSON RE-IDENTIFICATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Ting</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate Institute of Electronics Engineering</orgName>
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man-Yu</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate Institute of Electronics Engineering</orgName>
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsai-Shien</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate Institute of Electronics Engineering</orgName>
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shao-Yi</forename><surname>Chien</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate Institute of Electronics Engineering</orgName>
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">HARD SAMPLES RECTIFICATION FOR UNSUPERVISED CROSS-DOMAIN PERSON RE-IDENTIFICATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Person re-identification</term>
					<term>unsupervised learn- ing</term>
					<term>computer vision</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Person re-identification (re-ID) has received great success with the supervised learning methods. However, the task of unsupervised cross-domain re-ID is still challenging. In this paper, we propose a Hard Samples Rectification (HSR) learning scheme which resolves the weakness of original clustering-based methods being vulnerable to the hard positive and negative samples in the target unlabelled dataset. Our HSR contains two parts, an inter-camera mining method that helps recognize a person under different views (hard positive) and a part-based homogeneity technique that makes the model discriminate different persons but with similar appearance (hard negative). By rectifying those two hard cases, the re-ID model can learn effectively and achieve promising results on two large-scale benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Person re-identification (re-ID) tackles the problem of matching images of the same person in a camera network, which has drawn much attention in recent years because of its wide applications in the intelligent surveillance system. Many existing works obtained great success by adopting supervised deep learning approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>; however, it is impractical in realworld scenarios owing to the high annotation costs. Thus, how to perform re-ID in an unsupervised manner would be a critical yet challenging issue to be solved. Cross-domain re-ID, which aims at learning re-ID on the target unlabelled domain with the aid of labelled data on a source domain, is one of the unsupervised problems that has been continuously addressed. Some works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> utilize image-to-image translation with Generative Adversarial Network (GAN) <ref type="bibr" target="#b4">[5]</ref> to translate images from source to target domain. However, those methods depend on the quality of generated fake images and ignore the inherent data distribution in the unlabelled domain.</p><p>To exploit the discriminative characteristics accessible in the target domain, the recent works mainly focus on clustering-based methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>  the estimated correspondence of images can help for the unsupervised training. However, an underlying main problem of clustering-based methods is that the capability of re-ID model highly relies on the "quality" of the clustering results. In other words, the inconsistency between the generated pseudo labels and the unknown ground truth labels would undesirably degrade the re-ID performance, which generally arose from the misclustered hard training pairs. For instance, the same identity pairs captured under different cameras with intensive variations of the appearance could be possibly misclustered to different groups (we call it the hard positive). Or two people with similar appearance but only with subtle difference are likely to be clustered into the same group and be assigned with the same pseudo label (we call it the hard negative). These two situations are harmful for re-ID model learning because they all degrade the discriminative ability for identifying people. With the above observations, we propose a Hard Samples Rectification (HSR) learning scheme which contains two components, an inter-camera mining (ICM) and a part-based homogeneity (PBH) techniques.</p><p>Because the camera ID information of each image is easily available in the dataset, based on the data feature similarity, our ICM will additionally mine and pull close those possible hard positive pairs which are mutually similar but with different camera views. This data mining is beyond the original clustering results and can steadily rectify the cluster quality afterwards. For refining the cluster containing hard negative pairs, the proposed PBH technique will forcibly partition off and regroup the imperfect cluster with the part-based features. The idea behind is that the part-based feature gives a finer insight of a person; thus, with our PBH, the hard negative samples among a cluster will have the chance be rectified and assigned with different pseudo labels. The main contributions of this work can be summarized as follows:</p><p>? We propose an inter-camera mining technique (ICM) to mine potentially hard positive samples and alleviate the clustering bias of human appearance. ? The proposed part-based homogeneity technique (PBH) effectively regroups the imperfect clusters containing hard negative samples. ? We conduct extensive experiments on two large-scale benchmarks and our HSR achieves promising performances in cross-domain unsupervised person re-ID. 2. PROPOSED METHODS 2.1. Overview of our HSR learning scheme We first define the notation to be used in this paper. Given an unlabelled target dataset I t c,i Nt i=1 containing total N t training images, where c denotes the camera ID of image I t c,i , and a source labelled dataset which serves as a preliminary knowledge base for learning re-ID, the goal of our model is to learn a discriminative ability to perform person re-ID on the target dataset. The learning scheme is shown in <ref type="figure" target="#fig_1">Fig. 1</ref>. Typically, a feature extractor ? will be first learnt on the labelled source dataset as a pretrained feature embedding function ?(?, ? s ), where ? s is the parameters learned on the source domain. Then, similar to <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b7">8]</ref>, a clustering algorithm called DBSCAN <ref type="bibr" target="#b10">[11]</ref>, which does not require the exact number of clusters or identities, will be used to generate the pseudo labels for the target unlabelled images based on the extracted feature vectors ?(I t c,i , ? s ). With "estimated" pseudo labels y t i , we can learn the re-ID model in the typical supervised manner, which consists of the cross-entropy loss (L CE ) that helps correctly classify the identities <ref type="bibr" target="#b11">[12]</ref> and the triplet loss (L trip ) for controlling the distance of the positive and negative pairs in the embedding feature space <ref type="bibr" target="#b12">[13]</ref>. The clustering and network optimization stages will be conducted iteratively, and the performance of re-ID model and the quality of clustering results will improve steadily. However, it will reach a bottleneck owing to the situations caused by the hard samples as mentioned above. To further enhance the model ability, we propose Hard Samples Rectification (HSR) learning scheme, which dually rectifies the hard positive and negative samples with two components: inter-camera mining (ICM) and part-based homogeneity (PBH) techniques, as shown in <ref type="figure" target="#fig_1">Fig. 1</ref>. During training, ICM will mine possible hard positive pairs with different camera views and apply triplet loss to pull close those pairs in the feature space. On the other hand, PBH technique will refine the potential imperfect clusters by splitting the hard negative pairs within the same group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Inter-Camera Mining</head><p>As mentioned in Section 1, hard positive pairs may be assigned to different pseudo labels due to the variance of ap-</p><formula xml:id="formula_0">Algorithm 1: Inter-Camera Mining Input: Image feature vectors ?(I t i ) N t i=1 and its camera ID {ci} N t i=1 on target domain Output: Possible hard positive pairs 1: Calculate similarity matrix S ? R N t ?N t . 2: for i=1 ; i ? Nt ; i=i+1 do 3:</formula><p>Sort S[i] in descending order. <ref type="bibr" target="#b3">4</ref>:</p><formula xml:id="formula_1">Rank(I t i ) = top-K images I t j K j=1 in S[i] with cj = ci 5: end for 6: Choose image pairs (I t i , I t j ) conform to I t j ? Rank(I t i ) and I t i ? Rank(I t j )</formula><p>. 7: return all chosen pairs. pearance under different cameras. After several iterations of clustering and network training, it will leads to a vicious cycle that the positive pairs used to optimize the model are only those with similar content, which goes against the goal of person re-ID to match people across cameras. Thus, we propose an inter-camera mining technique as a role of assisting the original clustering method to mine the hard positive samples.</p><p>In practice, shown in Algorithm 1, we first compute the similarity matrix S ? R Nt?Nt for all target images, where the element in the i-th row and j-th column is the negative Euclidean distance of ?(I t i ) and ?(I t j ). Then, after sorting each row in descending order, we form the possible hard positive ranking list of each image by selecting its top-K closest images according to the matrix S, denoted as Rank(I t i ) with a total length of K. It is worth noting that in order to emphasize on "inter-camera" positive pairs, we remove those images captured by the camera same as the image I t i . To ensure the robustness and correctness of our inter-camera mining, inspired by Dekel et al. <ref type="bibr" target="#b13">[14]</ref>, we additionally conduct a K mutually best-buddies pairs technique. That is to say, for every image I t j in Rank(I t i ), I t i should as well be in Rank(I t j ). Thus, only the image pair (I t i , I t j ) that meets the above requirement would be taken into account as a reliable hard positive pair in the following CNN training.</p><p>With the mining hard positive pairs, we additionally apply the triplet loss L ICM , where the selection of positive samples is based on our ICM mining results. Notes that it differs from the original L trip which samples the positives based on the pseudo labels generated by the clustering algorithm. As for the choice of negative samples of each anchor I t i in L ICM , we choose images with different pseudo labels from I t i and at the same time not in its rank list Rank(I t i ). Different from <ref type="bibr" target="#b7">[8]</ref>, we embed the accessible camera information and the mutual similarity, which benefits the correctness and the robustness of additional triplet pairs mining. With our L ICM iteratively shortening the distance of these mined hard positive samples, it can progressively ensure the ability of our model to match person regardless of the variation between camera views and at the same time improve the quality of the clustering results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Part-based Homogeneity</head><p>Different people with only subtle difference are possibly assigned with the same pseudo labels, which would degrade the model ability to discriminatively identify people in detail. In the aim of separating imperfect clusters which possibly contain hard negative pairs, we develop a novel method called part-based homogeneity (PBH) as a rectification technique by utilizing the local features which provide finer information other than the global one. First, we need to define the imperfectness of a cluster and select the candidates for applying our PBH. To this end, we utilize Silhouette score <ref type="bibr" target="#b14">[15]</ref>, which is an evaluation metric for measuring how well a sample is clustered to its group without the requirement of the ground truth labels. By computing the mean Silhouette score of data in each cluster i, denoted as mSil(i), we can further select the imperfect cluster with its mSil(i) smaller than an empirically predefined threshold ?. Our proposed PBH technique is then applied on every selected cluster to refine the original clustering results, as illustrated in <ref type="figure" target="#fig_2">Fig. 2</ref>.</p><p>To start with, we split and pool the output feature maps of every sample in the selected cluster j into two parts: upper and lower features, which are formulated as {f u,i } Nj i=1 and {f l,i } Nj i=1 , where N j is the number of samples in cluster j. Then, we respectively employ the K-means clustering with K = 2 on {f u,i } Nj i=1 and {f l,i } Nj i=1 to observe the data distribution of the finer local features. Consequently, each sample is assigned with two temporary labels based on the groups of its upper and lower features, denoted as y u and y l . With the part-based label pair (y u , y l ), we can re-assign new pseudo labels to the samples in cluster j according to a look-up table, as shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. The idea behind is that only the data with both similar local parts, which means the same (y u , y l ), can be assigned with the same pseudo label. Notably, because the number of contained ground truth labels is unknown, we suppose that if the cluster is defined as an imperfect one, it would contain at least two ground truth labels. Furthermore, the progress of iterative learning can ensure that even the selected imperfect cluster contains more than two ground truth labels, the split clusters would still have the chance to be de- fined as imperfect ones in the next iteration. In summary, by considering the local features, our PBH maintains the homogeneity within the new cluster and avoids assigning the same pseudo label to globally similar hard negative pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Optimization Procedure</head><p>For each iteration, after clustering the unlabelled data by DB-SCAN, we would first verify the imperfect clusters and adopt the proposed PBH technique to refine the original estimated pseudo labels. Then, we jointly utilize the triplet loss (L trip ) and the cross-entropy loss (L CE ) to optimize the CNN network with those updated pseudo labels. Besides the positive and negative pairs sampled from the pseudo labels, we also jointly adopted the triplet loss L ICM according to our ICM sampling technique. The overall loss function can be written as follows:</p><formula xml:id="formula_2">L total = L CE + L trip + L ICM<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS 3.1. Datasets and Evaluation Protocol</head><p>We evaluate our approach on two large-scale person re-ID benchmarks: Market-1501 <ref type="bibr" target="#b15">[16]</ref> and DukeMTMC-ReID <ref type="bibr" target="#b16">[17]</ref>, abbreviated as "Market" and "Duke" in the following sections. The Market and Duke datasets contain 1501 and 1404 identities respectively, and each of the identity was captured by at most 6 or 8 cameras. In our experiments, the label information of the training data in the target domain is not available during the whole learning process. Rank-1 (R1) accuracy (%) and the Mean Average Precision (mAP, %) are used to evaluate the re-ID performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Implementation Details</head><p>We adopt ResNet-50 <ref type="bibr" target="#b17">[18]</ref> as our feature extractor ? and use the last 2048-d feature vector to represent the data in both training and clustering. Notes that we split the last feature map before average pooling into the upper and lower local feature parts in our proposed PBH technique. The image size is 256 ? 128 and augmented with random erasing and horizontal flip. Each mini-batch is with size 32, which consists of 8 randomly sampled pseudo identities, and for L trip , each contains 4 sampled images in their cluster, but for L ICM , the 4 samples come from the possible hard positive ranking list. Empirically, we set K = 10 in the ICM, the number of local features = 2 (upper and lower) in PBH, and set the threshold ? = mean(mSil) ? 3std(mSil) in our PBH, where mean(mSil) and std(mSil) denote the average and standard deviation of mSil of all clusters. We choose the SGD optimizer with the learning rate = 0.005 to optimize the model for 10 epochs in each iteration, where the total #iterations is 30.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Comparison with State-of-the-arts</head><p>We compare our proposed HSR with existing state-of-theart unsupervised cross-domain re-ID methods on Market and Duke datasets in <ref type="table" target="#tab_0">Table 1</ref>. Based on the common settings, we use Duke as the source dataset when test on Market and vice versa. We can see that our HSR outperforms all the compared methods significantly on both datasets. Among the compared methods, the works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b5">6]</ref> and some latest approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref> also aim to exploit discriminative information in target domain based on pseudo label estimation. Different from them, our HSR focuses on mining hard positive and hard negative samples to calibrate the unreliable clustering results, and thus acquires a promising gain in the performance. Specifically, HSR achieves R1 = 85.3% and mAP = 65.2%, which outperforms the best of these approaches by margins of 4.2% and 4.0% in Market. Similar improvement can be seen in Duke by achieving R-1 = 76.1% and mAP = 58.0%, with margins of 3.0% and 3.0%. In summary, our method effectively enhances the model capability by alleviating the effect of hard cases in clustering-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Ablation Study</head><p>We perform ablation study to evaluate the effectiveness of each proposed component. Results are shown in <ref type="table" target="#tab_1">Table 2</ref>. First, we directly apply the model pretrained on source dataset to the target dataset, denoted as "Direct Transfer". The in- ferior performance due to the discrepancy between domains reveals the necessity of applying unsupervised method on target domain. Next, for the "Baseline" method, we utilize DB-SCAN to cluster and generate pseudo labels for learning re-ID model on target domain, as shown in the second row of the <ref type="table" target="#tab_1">Table 2</ref>; the baseline can achieve 46.3% and 42.2% in terms of mAP on the two datasets respectively. But the unsatisfactory results can be explained by the inaccurate pseudo labels, thus degrades the model capability to identify persons. Effectiveness of PBH and ICM. The hard negative pairs being clustered to a same group is a critical factor that hinders the model ability to distinguish different identities in details. With our PBH, the imperfect clusters will be split into multiple groups and re-assigned new pseudo labels. The third row of <ref type="table" target="#tab_1">Table 2</ref> (Baseline + PBH) shows that with PBH, all performance results improve on the two datasets compared to the baseline method. We then validate the proposed ICM in the fourth row of <ref type="table" target="#tab_1">Table 2</ref> (Baseline + L ICM ). A significant improvement can be observed compared to baseline, which gains 17.0% and 12.2% in mAP on Market and Duke. This demonstrates that our inter-camera mining is able to assist the model learning to identify people regardless of the crosscamera scene variation, which is exactly the goal of re-ID. We also calculate the precision of the selected positive pairs from ICM. Specifically, for every sample I t i in the target domain, we compute the average of true positive rate of their corresponding Rank(I t i ). It can progressively rise up to 82% in our iterative learning process, and notably, we can also reach a maximum rate of 17% in our Rank(I t i ) of "hard positive samples", which possesses the same ground truth identities yet are assigned into "different" clusters. This shows that our ICM has strong capability of rectifying the original clustering results and favorably generate possible hard positive for model learning.</p><p>4. CONCLUSION In this paper, we introduce a hard samples rectification (HSR) learning scheme to address the issue of hard samples that degrades the performance in clustering-based methods. Specifically, we propose an inter-camera mining technique to match people under various camera views, and a part-based homogeneity technique to split hard negative pair within same cluster in a part-based manner. With our HSR, the model can learn a discriminative representation for unlabelled target images and receive a significant improvement of re-ID performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>which generate pseudo identity labels by clustering the unlabelled data. Thus, denotes equal contribution This research was supported in part by the Ministry of Science and Technology of Taiwan (MOST 110-2218-E-002 -025 -), National Taiwan University (NTU-108L104039), Intel Corporation, Delta Electronics and Compal Electronics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>Overview of the proposed HSR learning scheme. This learning scheme is conducted iteratively with clustering and network training. ICM are used to create additional hard positive training pairs and PBH is used to rectify the hard negative pairs in original clustering results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Illustration of part-based homogeneity technique. We extract local features of upper part and lower part for each sample in the imperfect cluster and apply K-means clustering respectively on both local features to obtain two kinds of part-based labels. With the two temporary local labels, the cluster is then split into at most four different groups according to the look-up table.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparisons with state-of-the-arts unsupervised re-ID methods on Market and Duke.</figDesc><table><row><cell>Methods</cell><cell cols="4">Duke ? Market Market ? Duke R1 mAP R1 mAP</cell></row><row><cell>PUL [7]</cell><cell>45.5</cell><cell>20.5</cell><cell>30.0</cell><cell>16.4</cell></row><row><cell>CAMEL [6]</cell><cell>54.5</cell><cell>26.3</cell><cell>-</cell><cell>-</cell></row><row><cell>SPGAN [3]</cell><cell>58.1</cell><cell>26.9</cell><cell>46.9</cell><cell>26.4</cell></row><row><cell>HHL [4]</cell><cell>62.2</cell><cell>31.4</cell><cell>46.9</cell><cell>27.2</cell></row><row><cell>MAR [19]</cell><cell>67.7</cell><cell>40.0</cell><cell>67.1</cell><cell>48.0</cell></row><row><cell>PAST [8]</cell><cell>78.4</cell><cell>54.6</cell><cell>72.4</cell><cell>54.3</cell></row><row><cell>SSG [9]</cell><cell>80.0</cell><cell>58.3</cell><cell>73.0</cell><cell>53.4</cell></row><row><cell cols="2">pMR-SADA [20] 83.0</cell><cell>59.8</cell><cell>74.5</cell><cell>55.8</cell></row><row><cell>GDS-H [10]</cell><cell>81.1</cell><cell>61.2</cell><cell>73.1</cell><cell>55.1</cell></row><row><cell>HSR (Ours)</cell><cell>85.3</cell><cell>65.2</cell><cell>76.1</cell><cell>58.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation studies of the proposed methods in terms of R1 and mAP (%).</figDesc><table><row><cell>Experimental setting</cell><cell cols="4">Duke ? Martket Market ? Duke R1 mAP R1 mAP</cell></row><row><cell>Direct Transfer</cell><cell>50.1</cell><cell>20.9</cell><cell>36.2</cell><cell>18.3</cell></row><row><cell>Baseline</cell><cell>72.9</cell><cell>46.3</cell><cell>60.2</cell><cell>42.2</cell></row><row><cell>Baseline + PBH</cell><cell>74.5</cell><cell>47.1</cell><cell>63.5</cell><cell>44.6</cell></row><row><cell>Baseline + L ICM</cell><cell>83.8</cell><cell>63.3</cell><cell>73.5</cell><cell>54.4</cell></row><row><cell>HSR (Ours)</cell><cell>85.3</cell><cell>65.2</cell><cell>76.1</cell><cell>58.0</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="480" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning discriminative features with multiple granularities for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanshuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufeng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM Multimedia Conference on Multimedia Conference</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="274" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Image-image domain adaptation with preserved self-similarity and domaindissimilarity for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generalizing a person retrieval model hetero-and homogeneously</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="172" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cross-view asymmetric metric learning for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Xing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ancong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="994" to="1002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification: Clustering and fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehe</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenggang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">83</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Communications, and Applications (TOMM)</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Self-training with progressive augmentation for unsupervised cross-domain person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiewei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8222" to="8231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Self-similarity grouping: A simple unsupervised cross domain adaptation approach for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanshuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6112" to="6121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Global distance-distributions separation for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A density-based algorithm for discovering clusters in large spatial databases with noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiaowei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>AAAI Press</publisher>
			<pubPlace>Menlo Park, CA (United States</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Person reidentification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoyan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1367" to="1376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">In defense of the triplet loss for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Best-buddies similarity for robust template matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaul</forename><surname>Oron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Avidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2021" to="2029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Silhouettes: a graphical aid to the interpretation and validation of cluster analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rousseeuw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational and applied mathematics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="53" to="65" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scalable person reidentification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1116" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by gan improve the person reidentification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification by soft multilabel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Xing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ancong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Huang</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2148" to="2157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Smoothing adversarial domain attack and p-memory reconsolidation for cross-domain person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangcong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Huang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10568" to="10577" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
