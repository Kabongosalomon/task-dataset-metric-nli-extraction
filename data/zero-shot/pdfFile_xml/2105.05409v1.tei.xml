<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Large-Scale Benchmark for Food Image Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiongwei</forename><surname>Wu</surname></persName>
							<email>xwwu@smu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">Singapore Management University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Fu</surname></persName>
							<email>xinfu@bjtu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Beijing Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Liu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Singapore Management University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ee-Peng</forename><surname>Lim</surname></persName>
							<email>eplim@smu.edu.sg</email>
							<affiliation key="aff3">
								<orgName type="institution">Singapore Management University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Salesforce Research Asia</orgName>
								<orgName type="institution">Management University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
							<email>qianrusun@smu.edu.sg</email>
							<affiliation key="aff5">
								<orgName type="institution">Singapore Management University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Large-Scale Benchmark for Food Image Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Food image segmentation is a critical and indispensible task for developing health-related applications such as estimating food calories and nutrients. Existing food image segmentation models are underperforming due to two reasons: (1) there is a lack of high quality food image datasets with fine-grained ingredient labels and pixel-wise location masks-the existing datasets either carry coarse ingredient labels or are small in size; and (2) the complex appearance of food makes it difficult to localize and recognize ingredients in food images, e.g., the ingredients may overlap one another in the same image, and the identical ingredient may appear distinctly in different food images.</p><p>In this work, we build a new food image dataset FoodSeg103 (and its extension FoodSeg154) containing 9,490 images. We annotate these images with 154 ingredient classes and each image has an average of 6 ingredient labels and pixel-wise masks. In addition, we propose a multi-modality pre-training approach called ReLeM that explicitly equips a segmentation model with rich and semantic food knowledge. In experiments, we use three popular semantic segmentation methods (i.e., Dilated Convolution based [17], Feature Pyramid based <ref type="bibr" target="#b21">[22]</ref>, and Vision Transformer based [54]) as baselines, and evaluate them as well as ReLeM on our new datasets. We believe that the FoodSeg103 (and its extension FoodSeg154) and the pre-trained models using ReLeM can serve as a benchmark to facilitate future works on fine-grained food image understanding. We make all these datasets and methods public at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Food computing has attracted increasing public attention in recent years, as it provides the core technologies for food and healthrelated research and applications. <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b42">43]</ref>. One of the important goals of food computing is to automatically recognize different types of food and profile their nutrition and calorie values. In computer vision, the related works include dish classification <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b51">52]</ref>, recipe generation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b45">46]</ref>, and food image retrieval <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b41">42]</ref>. Most of them focus on representing and analysing the food image as a whole, and do not explicitly localize or classify its individual ingredients-the visible components in the cooked food. We call the former food image classification and the latter food image segmentation. Between the two, food image segmentation is more complex as it aims to recognize each ingredient category as well as its pixel-wise locations in the food image. As shown in <ref type="figure" target="#fig_1">Figure 1</ref>, given an "hamburger" example image, a good segmentation model  needs to recognize and mask out "beef", "tomato", "lettuce", "onion" and "bread roll" ingredients. Compared to semantic segmentation on general object images <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22]</ref>, food image segmentation is more challenging due to the large diversity in food appearances and the often imbalanced distribution of categories of ingredients. First, an ingredient cooked differently can vary a lot visually, e.g., "pineapples" cooked with meat in <ref type="figure" target="#fig_1">Figure 1</ref> (a) versus the "pineapples" in a fruit platter in <ref type="figure" target="#fig_1">Figure 1</ref> (b). Different ingredients may look very similar, e.g., "pineapples" cooked with meat cannot be easily distinguished from "potatoes" cooked with meat, as shown in <ref type="figure" target="#fig_1">Figures 1 (a)</ref> and (c) respectively. Second, food datasets usually suffer from imbalanced distributionboth food classes and ingredient classes often exist in long-tailed distributions. This is inevitable due to two reasons: 1) large number of food images are dominated by very few popular food classes while vast majority of food classes are unpopular; and 2) there is a selection bias in the construction of food image collection <ref type="bibr" target="#b43">[44]</ref>. We will elaborate the detailed distribution analysis in Section 3.</p><p>Existing food image datasets, such as ETH Food101 <ref type="bibr" target="#b0">[1]</ref>, Recipe1M <ref type="bibr" target="#b40">[41]</ref>, and Geo-Dish <ref type="bibr" target="#b51">[52]</ref>, mainly facilitate the research of dish classification or recipe generation. They do not have fine-grained ingredient masks or labels. UECFoodPix <ref type="bibr" target="#b12">[13]</ref> and UECFoodPixComplete <ref type="bibr" target="#b34">[35]</ref> are the only two public datasets for food image segmentation. However, their segmentation masks are annotated at dish level only. That is, each mask covers the region of an entire dish instead of that of food ingredients. We elaborate more dataset comparison in Section 3.3.</p><p>Dataset contribution: To facilitate fine-grained food image segmentation, we build a large-scale dataset called FoodSeg103, for which we have defined 103 ingredient classes and annotated 7,118 western food images using these labels together with the corresponding segmentation masks. Besides, we annotated an additional set of 2,372 images of Asian food which covers more diverse set of ingredients making these images more challenging than those in the main set (FoodSeg103). For this set, we defined 112 ingredient classes-55% overlap with the ingredient classes of the main set. In total, we annotated 154 classes of ingredients with around 60k masks (in the two datasets). We name the combined dataset as FoodSeg154. During the annotation, we carried out careful data selection, iterative refinement of labels and masks (to be further elaborated in Section 3.2), so as to guarantee high quality labels and masks in the dataset. Our annotation is thus expensive and time-consuming. In experiments, we use FoodSeg103 for in-domain training and testing, and use the additional set in FoodSeg154 for out-domain testing.</p><p>Model contribution: The source images of FoodSeg103 are from another existing food dataset Recipe1M <ref type="bibr" target="#b40">[41]</ref>-millions of images and cooking recipes, used for recipe generation. Each recipe contains not only "how to cook" but also "what ingredient to use". In our work, we leverage these recipe information as auxiliary information to train semantic segmentation models. We call this multi-modality knowledge transfer and name our training method ReLeM. Specifically, ReLeM integrates food recipe data, in the format of language embedding, with the visual representation of the food image. In this way, it forces the visual representation of an ingredient appearing in different dishes to have their appearances "connected" in the feature space through a common language embedding (extracted from the ingredient's label and its cooking instructions).</p><p>Experiment contribution: We validate our proposed ReLeM model by plugging it into the state-of-the-art semantic segmentation models such as CCNet <ref type="bibr" target="#b16">[17]</ref>, Sem-FPN <ref type="bibr" target="#b21">[22]</ref> and SeTR <ref type="bibr">[54]</ref>. In experiments, we compare ReLeM-variants with these baseline models using both convolutional networks and transformer backbones. Our experiments show that ReLeM is generic to be applied into multiple segmentation frameworks, and it helps to achieve significant accuracy improvement when incorporated into the SOTA CNNbased model CCNet. This validates that our knowledge transfer approach works more efficient on stronger models-a characteristic preferred by the multimedia community.</p><p>Our contributions are thus three-fold. i) We build a large-scale food image segmentation dataset called FoodSeg103 (and its extension FoodSeg154). It can facilitate a promising and challenging benchmark for the task of semantic segmentation in food images. ii) We propose a knowledge transfer approach ReLeM that utilizes the multi-modality information of recipe datasets. It can be incorporated into different semantic segmentation methods to boost the model performance. iii) We conduct extensive experiments that reveal the challenges of segmenting food on our FoodSeg103 dataset, and validate the efficiency of our ReLeM based on multiple baseline methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>Food Image Datasets. In recent years, the scale of food-related datasets has grown rapidly. For example, Bossard et al <ref type="bibr" target="#b0">[1]</ref> built one large-scale food dataset ETH Food101, which contains 101 classes with 1,000 images per class. Matsuda et al. <ref type="bibr" target="#b29">[30]</ref> constructed a Japanese food dataset UEC Food100 with 15K images in 100 dish categories. In comparison, ISIA Food500 <ref type="bibr" target="#b33">[34]</ref> contains nearly 400k food images in 500 categories, which is the largest food image recognition. In addition, there are also recipe-related datasets. Salvador et al. <ref type="bibr" target="#b40">[41]</ref> built the Recipe1M, with nearly 900k images and 1 million recipes, which is widely used in multi-modal learning between images and recipes. Based on Recipe1M, an even larger dataset Recipe1M+ <ref type="bibr" target="#b27">[28]</ref> was constructed with more than 13 millions of food images. However, these datasets are mainly built to support food recognition and recipe generation research rather than food image segmentation, so they do not segment food images into multiple masks and labels of ingredient . UECFoodPix <ref type="bibr" target="#b12">[13]</ref> and UECFoodPixComplete <ref type="bibr" target="#b34">[35]</ref> are the only two datasets for food image segmentation, which contains 10,000 images with more than 100 categories. Nevertheless, their annotation are limited to dish-wise masks so they cannot be used for ingredient segmentation.</p><p>In this paper, we built FoodSeg103 dataset with 7,118 images and more than 40k masks covering 103 food ingredients. In addition, we have collected another image set for Asian food with 2,372 images (for cross-domain evaluation of the models). Combining the main set and the Asian set, we get the FoodSeg154 with nearly 10k images and 60k ingredient masks. To our best knowledge, FoodSeg154 is the first and the largest ingredient-level dataset for fine-grained food image segmentation. Dataset is a key step in developing deep learning based methods. We hope our dataset can inspire more efforts for the task of food image segmentation. Semantic Segmentation in Images. Deep learning based semantic segmentation is a super hot topic in recent years. Fully convolutional neural network (FCN) <ref type="bibr" target="#b26">[27]</ref> is the first semantic segmentation framework based on deep convolutional neural networks. It predicts pixel-wise masks by replacing the fully connected layers with convolution layers and achieves a clear margin of improvement on the model performance. Chen et al. <ref type="bibr" target="#b2">[3]</ref> proposed DeepLab which applies dilated convolutional layers in vanilla FCN. The trained model is more effective as the dilation mechanism enlarges the receptive fields while maintaining a high resolution in feature maps. Chen et al <ref type="bibr" target="#b3">[4]</ref> proposed the DeepLab v2, which adds an ASPP module to integrate features of different dilation rates. To further include contextual cues, PSPNet <ref type="bibr" target="#b52">[53]</ref> proposed a PPM module that aggregates the contextual information using different-size pooling layers. Wang et al. <ref type="bibr" target="#b47">[48]</ref> proposed the non-local networks to encode the relationship between each pair of pixels in the feature map. Based on the non-local networks, CCNet [17] adopted a criss-cross attention layer to significantly economize the computation costs of calculating attentions. Most recently, vision transformer (attentionbased) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b44">45]</ref> was adapted to tackle semantic segmentation problems in <ref type="bibr">[54]</ref>. recently and achieves state-of-the-art results <ref type="bibr">[54]</ref>. In this paper, we conduct extensive experiments on our dataset using  three representative semantic segmentation methods: CCNet <ref type="bibr" target="#b16">[17]</ref>, FPN <ref type="bibr" target="#b21">[22]</ref> and SeTR <ref type="bibr">[54]</ref>. We also plug the proposed ReLeM into these methods to show its general efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">FOOD IMAGE SEGMENTATION DATASET</head><p>FoodSeg103 is a subset of FoodSeg154, and the latter includes an additional subset of Asian food images and annotations. Some example images and their annotations can be found in <ref type="figure" target="#fig_3">Figure 2</ref>.</p><p>In FoodSeg103, we have defined 103 ingredient categories and assigned these category labels as well as the segmentation masks to 7,118 images. The images are from an existing recipe dataset called Recipe1M <ref type="bibr" target="#b40">[41]</ref>. For the additional subset in FoodSeg154, we specially collect 2,372 images of Asian food which is of larger diversity than the Western food in FoodSeg103. We use this subset to evaluate the domain adaptation performance of our food image segmentation models. We release FoodSeg103 to facilitate public research, but currently we cannot make the Asian food set public due to the confidentiality of the images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Collecting Food Images</head><p>We use FoodSeg103 as an example to elaborate the dataset construction process. We elaborate the image source, category compilation and image selection as follows. Source: We used Recipe1M <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b40">41]</ref> as our source dataset. This dataset contains 900k images with cooking instructions and ingredient labels, which are used for food image retrieval and recipe generation tasks. Categories: First, we counted the frequency of all ingredient categories in Recipe1M. While there are around 1.5k ingredient categories <ref type="bibr" target="#b39">[40]</ref>, most of them are not easy to be masked out from images. Hence, we kept only the top 124 ingredient categories (with further refinement, this number became 103) and assigned ingredients with the "others" category when they do not fall under the above 124 categories. Finally, we grouped these categories into 14 superclass categories, e.g., "Main" (i.e., main staple) is a superclass category covering more fine-grained categories such as "noodle" and "rice". Images:</p><p>In each fine-grained ingredient category, we sampled Recipe1M images based on the following two criteria: 1) the image should contain at least two ingredients (with the same or different categories) but not more than 16 ingredients; and 2) the ingredients should be visible in the images and easy-to-annotate. Finally, we obtained 7,118 images to annotate masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Annotating Ingredient Labels and Masks</head><p>Given the above images, the next step is to annotate segmentation masks, i.e., the polygons covering the pixel-wise locations of different ingredients. This effort includes the mask annotation and mask refinement steps. Annotation: We engaged a data annotation company to perform mask annotation, a laborious and painstaking job. For each image, a human annotator first identifies the categories of ingredients in the image, tags each ingredient with the appropriate category label and draws the pixel-wise mask. We asked the annotators to ignore tiny image regions (even if it may contain some ingredients) with area covering less than 5% of the whole image. Refinement: After receiving all masks from the annotation company, we further conducted an overall refinement. We followed three refinement criteria: 1) correcting mislabeled data; 2) deleting unpopular category labels that are assigned to less than 5 images, and 3) merging visually similar ingredient categories, such as orange and citrus. After refinement, we reduced the initial set of 125 ingredient categories to 103. <ref type="figure" target="#fig_8">Figure 5</ref> shows some examples refined by us. The annotation and refinement works took around one year. We show some data examples in <ref type="figure" target="#fig_3">Figure 2</ref>. In <ref type="figure" target="#fig_3">Figure 2</ref> (a), we give some easy cases where the boundaries of ingredients are clear and the image compositions are not complex. In <ref type="figure" target="#fig_3">Figure 2</ref> (b) and (c), we show some difficult cases with overlapped ingredient regions and complex compositions in the images. <ref type="figure" target="#fig_5">Figure 3</ref> shows the distributions of fine-grained ingredient categories and superclass categories. Figures 3(a) and 3(c) show partial statistics for small subsets of categories due to page limit. The complete statistics will be published when releasing the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparing with Food Image Datasets</head><p>Food Image Datasets. We summarize the comparison results in Table 1. We only include datasets that are mainly used for food recognition tasks. They contain images and dish-level labels, and therefore    they do not have any ingredient-level annotations. Recipe1M and Recipe1M+ include ingredient labels for each images but not the segmentation masks. Notably, there are two datasets for food image segmentation: UECFoodPix <ref type="bibr" target="#b12">[13]</ref> and UECFoodPixComplete <ref type="bibr" target="#b34">[35]</ref>. Below, we compare these two with our datasets FoodSeg103 and FoodSeg154 in detail. Food Image Segmentation Datasets. UECFoodPix and UECFood-PixComplete (UECFoodPixComp.) are two public datasets for food image segmentation, with 10k images and 102 dish categories. Detailed comparison numbers are given in <ref type="table" target="#tab_2">Table 2</ref>. We highlight three advantages of our FoodSeg103 and FoodSeg154: 1) the number of pixel-wise masks of FoodSeg (40k and 60k) is significantly larger than UEC dataset (only 10k); 2) the annotation mask in UECFood-Pix and UECFoodPixComp covers entire dish but not ingredients (dish components), while our FoodSeg154 and FoodSeg103 have ingredient-wise masks, which better capture the characteristic of the food. Illustrative comparisons are given in <ref type="figure" target="#fig_7">Figure 4</ref>. In <ref type="table" target="#tab_2">Table 2</ref>, we not only present the statistic numbers but also evaluate FoodSeg103, UECFoodPix and UECFoodPixComplete using deeplabv3+ as a baseline model. The last row of the table shows that FoodSeg103 serves as a more challenging benchmark for semantic segmentation. Moreover, fine-grained ingredient annotations in our datasets are more useful for analyzing food nutrition and estimating calories in health-related applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">FOOD IMAGE SEGMENTATION FRAMEWORK</head><p>As shown in <ref type="figure">Figure 6</ref>, our food image segmentation framework contains two modules. One is the recipe learning module (ReLeM) to incorporate recipes in the form of language embedding into the visual representation of a food image. We call this approach multi-modality knowledge transfer. In this approach, we explicitly force the visual representations of the same ingredient appearing in different dishes to be "connected" in the feature space through</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Year Type #Dish #Ingr. Images PFID <ref type="bibr" target="#b4">[5]</ref> 2009 CLS 101 0 4,545 Food50 <ref type="bibr" target="#b17">[18]</ref> 2010 CLS 50 0 5,000 Food85 <ref type="bibr" target="#b15">[16]</ref> 2010 CLS 85 0 5,500 UEC Food100 <ref type="bibr" target="#b29">[30]</ref> 2012 CLS 100 0 14,361 UEC Food256 <ref type="bibr" target="#b19">[20]</ref> 2014 CLS 256 0 25,088 ETH Food-101 <ref type="bibr" target="#b0">[1]</ref> 2014 CLS 101 0 101,000 UPMC Food-101 <ref type="bibr" target="#b48">[49]</ref> 2015 CLS 101 0 90,840 Geo-Dish <ref type="bibr" target="#b51">[52]</ref> 2015 CLS 701 0 117,504 Sushi-50 <ref type="bibr" target="#b35">[36]</ref> 2019 CLS 50 0 3,963 FoodX-251 <ref type="bibr" target="#b18">[19]</ref> 2019 CLS 251 0 158,846 ISIA Food-200 <ref type="bibr" target="#b32">[33]</ref> 2019 CLS 200 0 197,323 FoodAI-756 <ref type="bibr" target="#b37">[38]</ref> 2019 CLS 756 0 400,000 Recipe1M <ref type="bibr" target="#b40">[41]</ref> 2017 Recipe 0 1488 1M Recipe1M+ <ref type="bibr" target="#b27">[28]</ref> 2019 Recipe 0 1488 14M UECFoodPix <ref type="bibr" target="#b12">[13]</ref> 2019 SEG 102 0 10,000 UECFoodPixComp. <ref type="bibr">[</ref>   the common language embedding (extracted from the ingredient label and its cooking instructions), so as to handle the high variance of the ingredient appearing in different dishes. The other module of our framework is the encoder-decoder based image segmentation. Its encoder is initialized using the one trained by ReLeM, and its decoder is randomly initialized and trained with the segmentation masks. We next introduce the two modules in detail. Food image segmentation can be viewed as a special type of semantic segmentation <ref type="bibr" target="#b24">[25,</ref><ref type="bibr">54]</ref>. It is more difficult than normal image segmentation due to: 1) the ingredient cooked with different methods can vary a lot by appearances, and 2) ingredient distribution is inevitably long-tailed making the data very sparse for ingredients in the long tail. Given a food image, the Segmenter identifies the ingredient categories and also mask out the corresponding pixels for each category (class). The common metrics for measuring Segmenter's performance include mIoU (mean IoU over each class), mACC (mean accuracy over all classes) and aAcc (over all pixels), See <ref type="figure" target="#fig_10">Figure 7</ref> for more details of IoU and accuracy (Acc) calculation.  <ref type="figure">Figure 6</ref>: Our food image segmentation framework consists of two modules: Recipe Learning Module (ReLeM) and Image Segmentation Module (Segmenter). For ReLeM, we encode the recipe information into the visual representation of the food image. We deploy the cosine similarity to compute the distance between two distinct-modality models, together with a semantic loss <ref type="bibr" target="#b40">[41]</ref>. After training, we use the trained encoder to initialize the encoder of the Segmenter. The decoder of the Segmenter is trained with the segmentation masks from a random initialization.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Recipe Learning Module (ReLeM)</head><p>Overview. We propose ReLeM to reduce the large intra-variance of ingredients caused by different cooking methods mentioned in the recipes. Specifically, our training method integrates the recipe information into the visual representation of the corresponding image. Assume an ingredient in two different images are cooked in different methods. The visual representations of the ingredients from vision encoder are denoted as 1 and 2 , where 1 and 2 have significant difference in the visual space. ReLeM aims to reduce this difference according to its word embedding of the cooking instructions of the two recipes 1 and 2 respectively in the language space.</p><formula xml:id="formula_0">| ( 1 | 1 ) ? ( 2 | 2 )| &lt; | ( 1 ) ? ( 2 )|<label>(1)</label></formula><p>where is the vision decoder in the Segmenter (elaborated in Section 4.2). Our ReLeM is optimized by using two loss terms: cosine similarity loss between features, and semantic loss (distance) between the text representation and the visual representation of the same image:</p><formula xml:id="formula_1">cosine (( , ), ) = 1 ? ( , ) = 1 (0, ( , ) ? ) = ?1 (2) semantic (( , ), , ) = CE( , ) + CE( , )<label>(3)</label></formula><p>where denotes whether and are from the same recipe. and denote the semantic class of and respectively, and is the margin parameter, which is set to 0.1. As Recipe1M does not contain specific semantic labels (i.e., dish names), we define 2,000 semantic labels for it by selecting the most frequent dish names appeared in its recipe titles. Preprocessing. Each recipe contains ingredients and cooking instructions. Some preprocessing steps are required to encode ingredients and instructions from raw text into the fixed length vectors before they are fed into the text encoder. Specifically, we first extract useful ingredient and instruction texts from the raw recipe data by removing redundant words. For each ingredient, we learn a word2vec <ref type="bibr" target="#b31">[32]</ref> representation using a bi-directional LSTM. As the sequence of instructions can be long, it is difficult for LSTM to encode them, due to the gradient vanishing issue. Following a previous work <ref type="bibr" target="#b40">[41]</ref>, we encode the instructions with a skip-instructions <ref type="bibr" target="#b22">[23]</ref> to generate the feature vectors with a fixed length. Text Encoder. The text encoder is a general module to extract text knowledge from ingredient labels and cooking instructions. We use two types of text encoders: LSTM-based encoder and transformerbased encoder. For LSTM-based, we use a bi-directional LSTM to encode ingredient features and a LSTM to encode instruction features. For transformer-based model, we use two light-weight transformers, each of which contains 2 transformer layers with 4-head self-attention modules. Vision Encoder. The vision encoder used in ReLeM aims to extract the visual knowledge from the input image, and the weights will initialize the vision encoder in the segmenter. In this paper, two vision encoders are used: ResNet-50 <ref type="bibr" target="#b14">[15]</ref> based on convolutional neural network and ViT-16/B <ref type="bibr" target="#b11">[12]</ref> based on vision transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Image Segmentation Module (Segmenter)</head><p>Our framework follows the standard paradigm of semantic segmentation, where the input image is first encoded in a vision encoder, and then goes through a vision decoder for mask prediction. The existing segmentation models can be roughly divided into three groups, based on the different designs of encoder and decoder: Dilation based, Feature Pyramid Networks (FPN) based and Transformer based. Dilation based. Dilation convolution layers aim to enlarge the receptive fields without sacrificing the resolution, as shown in <ref type="figure" target="#fig_12">Figure 8 (a)</ref>. In its decoder, only the last-layer feature maps are used for prediction <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17]</ref>, as shown in <ref type="figure">Figure 9 (a)</ref>. FPN based. FPN integrates feature maps in different layers by the lateral connection. The shallow-layer image representation is enhanced by integrating the feature maps generated in deep layers, as shown in <ref type="figure" target="#fig_12">Figure 8 (b)</ref>. In its decoder, a set of feature pyramids are merged together followed with a mask predictor, as shown in <ref type="figure">Figure 9</ref> (b). Transformer based. Transformer is based on attention, which suits semantic segmentation tasks well--the contextual information is important in segmenting objects. Moreover, the receptive fields can be enlarged via attention mechanism <ref type="bibr" target="#b44">[45,</ref><ref type="bibr">54]</ref>. The transformerbased model reshapes the image into a sequence of regions and then encodes them by a sequence of attention modules, as shown in <ref type="figure" target="#fig_12">Figure 8</ref> (c). Its decoder predicts segmentation masks on the last-layer feature maps, as shown in <ref type="figure">Figure 9</ref>(c).</p><p>In this paper, we conduct experiments using three representative frameworks of these three types, respectively, i.e., CCNet (Dilation) <ref type="bibr" target="#b16">[17]</ref>, FPN <ref type="bibr" target="#b21">[22]</ref> and SeTR (Transformer) <ref type="bibr">[54]</ref>. Note that the encoder of Segmenter is pre-trained by our ReLeM. With LSTM and transformer-based text encoding, we arrive at 6 different ReLeM models, i.e., ReLeM-{ CCNet, FPN, SeTR}? ({ LSTM, Transformer}). We use the standard pixel-wise cross-entropy loss to optimize segmentation models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We conduct extensive experiments on our dataset FoodSeg103 and implement our proposed ReLeM by incorporating three baseline methods of semantic segmentation. Below, we first elaborate the experimental settings and the results of an ablation study. Then, we show the performance gaps of the top model in the typical semantic segmentation task and our food image segmentation task. We also evaluate the model adaptability using the Asian food data splits in our FoodSeg154. Lastly, we provide some qualitative results of our best segmentation models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implementation Details</head><p>Dataset Settings In our experiments, we use FoodSeg103 for indomain training and testing, and use the additional Asian food set for out-domain testing. We randomly divide FoodSeg103 dataset into two splits: training set and testing set, according to the 7:3 ratio. Our training set contains 4,983 images with 29,530 ingredient masks, while testing set contains 2,135 images with 12,567 ingredient masks. For ReLeM training, we use the training set of Recipe1M+ to learn the recipe representations (with test images in FoodSeg103 hidden from training). Segmenter Settings We conduct experiments based on two types of vision encoders: ResNet-50 <ref type="bibr" target="#b14">[15]</ref> based on convolutional neural networks, and ViT-16/B <ref type="bibr" target="#b11">[12]</ref> based on vision transformer. ResNet-50 is initialized from the pre-training model on ImageNet-1k <ref type="bibr" target="#b9">[10]</ref>, which is widely used in multiple vision tasks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b36">37]</ref>. ViT-16/B <ref type="bibr" target="#b11">[12]</ref> is a transformer-based model, which is initialized from the pretraining model on ImageNet-21k. ViT-16/B contains 12 transformer encoders with 12-head self-attention modules. We use the bilinear interpolation method to reinitialize the pre-trained positional embedding. In this paper, we use three types of segmentors: CC-Net <ref type="bibr" target="#b16">[17]</ref>, FPN <ref type="bibr" target="#b21">[22]</ref> and SeTR <ref type="bibr">[54]</ref>. CCNet and FPN are based on ResNet-50, while SeTR is based on ViT-16/B. Notably, SeTR extracts feature maps from 12 th transformer encoders, followed by two sets of convolution layers for prediction. Other components of the segmentors follow the default settings with random initialization. ReLeM Settings We use two types of vision encoders in ReLeM: ResNet-50 and ViT-16/B, which follow the same setting as Segmenter. In text preprocessing step, we use the skip-instruction models from the pre-trained weights in <ref type="bibr" target="#b28">[29]</ref>. Learning Parameters of Segmenter Each image will be resized into a fixed size of 2049 ? 1024 pixels with a ratio range from 0.5   <ref type="figure">Figure 9</ref>: Different types of decoder for food image segmentation to 2.0. A 768 ? 768 patch is cropped from the resized images, and random horizontal flipping and color jitter are applied. We trained the models with 80k iterations based on 8 images per batch, and optimized the models by SGD solvers, with a momentum as 0.9 and weight decay as 0.0005. For CCNet and FPN, we set the initial learning rate to 1e-3, while for SeTR we set initial learning rate to 1e-3. According to the general settings <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b46">47]</ref>, the learning rate is decayed by a power of 0.9 according to the polynomial decay schedule. For simplicity, we do not apply hard negative mining during training, and our framework is based on the widely used platform mmsegmentation <ref type="bibr" target="#b6">[7]</ref>. All experiments were conducted on 4 Tesla-V100 GPU cards. Learning Parameters of ReLeM Each input image are resized into a size of 256 ? 256 pixels and a 224 ? 224 patch is cropped from the resized images as the input of the vision encoder. The model is trained for 720 epochs and each batch contains 160 images. We use Adam solver <ref type="bibr" target="#b20">[21]</ref> to optimize the models, with a learning rate of 1e-4, Here we follow a two-stage optimization strategy. We first freeze the weights of the vision encoder and optimize the text encoder. After the text encoder converges, we start to train the vision encoder and freeze the parameters of the text encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results and Observations</head><p>The experiment results of CCNet, FPN and SeTR on FoodSeg103 are shown in <ref type="table">Table 3</ref>. The Segmenters of all CCNet, FPN and SeTR achieve significant improvements when incorporating with either LSTM-based or transformer-based ReLeM (1.3%, 1.3% and 2.6% improvement). This confirms that ReLeM is effective in enhancing both convolution based and transformer based semantic segmentation models. Besides, we can see that the performance of using LSTM-based ReLeM is consistently superior than using transformer-based ReLeM across all the model configurations.  <ref type="table">Table 3</ref>: Semantic segmentation results of our ReLeM plugged into three baseline methods (on the FoodSeg103 dataset). We implement two variants of ReLeM using LSTM and Transformer, respectively, to encode recipes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparing FoodSeg103 with Cityscapes</head><p>We compare the food image segmentation task with conventional semantic segmentation to compare the degree of difficulty of the two types of segmentation tasks. We include three types of state-ofthe-art segmentation algorithms, CCNet, SeTR and FPN. They are evaluated on FoodSeg103 and Cityscapes <ref type="bibr" target="#b7">[8]</ref> datasets. Cityscapes contains around 5,000 images captured on the streets of German cities, and 20 types of objects as segmentation targets. As we can see from <ref type="table" target="#tab_6">Table 4</ref>, all baseline methods achieve satisfactory results on Cityscapes, but suffer significant performance drops on our FoodSeg103. This indirectly shows the greater level of difficulty in the food image segmentation problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Qualitative Examples</head><p>In <ref type="figure" target="#fig_1">Figure 10</ref>, we show some qualitative results of using CCNet and ReLeM-CCNet on the testing set of FoodSed103. The first two rows clearly show that ReLeM-CCNet produces more accurate and    detailed predictions than the vanilla CCNet, demonstrating the effectiveness of ReLeM. In the last row, we show a failure case. It is actually a hard example with no clear boundaries among different ingredients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Cross-Domain Evaluation</head><p>We conduct an out-domain model evaluation using the Asian food data set in FoodSeg154. With the model trained on FoodSeg103, we adapt it to the subset of FoodSeg154, the Asian food data set. Specifically, the Asia food set is evenly divided into the training and testing splits. We fine-tune the trained model on the training set and then run the model on the testing data. In <ref type="table" target="#tab_7">Table 5</ref>, we show the performances of three models trained with the following settings: 1) without ReLeM, 2) with ReLeM and 3) with ReLeM and fine-tuned on the training split of the Asian food set. For the first two settings, we only evaluate the 62 classes in Asian food set overlapped with FoodSeg103, and for the last setting, we evaluate 112 classes (all). From the results in <ref type="table" target="#tab_7">Table 5</ref>, we observe that using ReLeM consistently outperforms baselines in both cases-with and without model fine-tuning on the training split of Asian food data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>We construct a large-scale image dataset FoodSeg103 (and its extension FoodSeg154) for food image segmentation research. We use around 10k images and annotate 60k segmentation masks in total, covering highly diverse appearances among 154 ingredients. In addition, we propose a multi-modality based pre-training method ReLeM, and validate its effectiveness by incorporating three baseline semantic segmentation methods and conducting extensive experiments on the FoodSeg103, i.e., using the typical setting, as well as on the FoodSeg154, i.e., using the challenging cross-domain setting.  <ref type="table" target="#tab_10">Table 6</ref>, and the more detailed statistic can be found in <ref type="table">Table 9</ref>. In our experiments, we use FoodSeg103 for in-domain training and testing, and use the additional Asian set for out-domain evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ACKNOWLEDGEMENT</head><p>Structure of FoodSeg103 FoodSeg103 contains 103 ingredient categories which belong to 15 super categories. In <ref type="figure" target="#fig_1">Figure 12</ref>, we show the dataset structure of FoodSeg103, where the inner circle plots the names of super classes, and the outer circle plots the corresponding ingredient categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Visualization</head><p>Visualization of FoodSeg103. In <ref type="figure" target="#fig_1">Figure 11</ref>, we show more visualization examples of the source image and its corresponding mask annotation in FoodSeg103.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Analysis on Transformer-based Models</head><p>Vision Transformers have been intensively studied recently, and a bunch of new algorithms have been proposed. The new proposed vision transformers have achieved significantly better performance than conventional CNN-based models in multiple vision tasks. In this section, we explore the performance of applying vision transformers into food image segmentation task. We adopt the vision transformers: ViT <ref type="bibr" target="#b11">[12]</ref>, Swin <ref type="bibr" target="#b25">[26]</ref> and PVT <ref type="bibr" target="#b46">[47]</ref> as segmentation encoders. We follow the default design of decoders, where FPN is used in PVT models and UperNet <ref type="bibr" target="#b50">[51]</ref> is used in Swin models. For ViT models, we use the two default settings in SeTR: Naive and MLA, as decoders. All the models are trained with the default learning settings with 80k iterations. The results are shown in <ref type="table" target="#tab_12">Table 8</ref>. ReLeM-variants show consistent improvement on both PVT and ViT-Naive models (0.7% and 2.6% improvement). However, in ViT-MLA model, the baseline shows better performance. In MLA decoder, feature maps from different level transformer encoders are integrated for final prediction. In ReLeM, however, only the last feature map is extracted for recipe learning. We argue ReLeM can also learn strong multi-level representation by extracting feature maps of different levels for recipe learning, and we leave it as the future work. In addition, larger backbones cannot guarantee improvement and may even hurt the performances (44.5% vs 45.1% in ViT, and 41.2% vs 41.6% in Swin). Besides, Swin achieves much better performance than ViT in other vision tasks <ref type="bibr" target="#b25">[26]</ref>, but in food image segmentation, the performance of Swin is much worse than ViT models, even with more parameters. These results show that food image segmentation task is more challenging and naively boosting the power of backbone cannot guarantee performance gain. Finally, decoders play important roles in transformer-based segmenters, but few efforts have been made to design a food-aware decoders, which is also an important research problem in the future.    All models are trained based on the default learning settings with 4 images per batch for 80k iterations. "S", "B" and "L" denote "Small", "Base" and "Large" models respectively.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>The first row shows a source image and its segmentation masks on our FoodSeg103. The second row shows example images to reveal the difficulties of food image segmentation, e.g., the pineapples in (a) and (b) look different, while the pineapple in (a) and the potato in (c) look quite similar.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Foodseg103 examples: source images (left) and annotations (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Statistic for Ingredients in Asian food set (partial) Statistics of fine-grained ingredient categories (partial) Statistic of 14 super classes in Asian food set (b) Statistics of 15 super classes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Category statistics for our FoodSeg103 dataset in (a) and (b), and the Asian food image set (i.e., the additional set in FoodSeg154) in (c) and (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Comparison of different annotation styles for masking food images: (a) source images, and (b) ingredientlevel annotation (ours), and (c) dish-level annotation<ref type="bibr" target="#b34">[35]</ref>. Ingredient-level annotation contains more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Examples of dataset refinement. (a) sources images (b) before refinement (wrong or confusing labels exist), and (c) after refinement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Calculating IoU and Acc, taking the "cake" mask as an example. IoU = ( TP TP+FP+FN ) and Acc = ( TP TP+FN ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 :</head><label>8</label><figDesc>Different types of encoder for food image segmentation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 :</head><label>10</label><figDesc>Visualization results on FoodSeg103. ReLeM-CCNet can make more accurate predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 11 :</head><label>11</label><figDesc>More annotation examples of FoodSeg103. The source images are in the left hand, while the annotation masks are in the right hand.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 12 :</head><label>12</label><figDesc>The dataset structure of FoodSeg103. The inner circle plots the super classes and the outer circle plots the corresponding sub-classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>Statistics</cell><cell cols="4">FoodSeg103 FoodSeg154 UECFood UECFoodComp.</cell></row><row><cell># Dish</cell><cell>730</cell><cell>730</cell><cell>102</cell><cell>102</cell></row><row><cell># Ingr.</cell><cell>103</cell><cell>154</cell><cell>0</cell><cell>0</cell></row><row><cell># images</cell><cell>7,118</cell><cell>9,490</cell><cell>10,000</cell><cell>10,000</cell></row><row><cell># masks</cell><cell>42,097</cell><cell>59,773</cell><cell>14,011</cell><cell>16,060</cell></row><row><cell>mean image width</cell><cell>771 pixels</cell><cell>776 pixels</cell><cell>442 pixels</cell><cell>442 pixels</cell></row><row><cell>mean image height</cell><cell>647 pixels</cell><cell>656 pixels</cell><cell>349 pixels</cell><cell>349 pixels</cell></row><row><cell>mIoU@deeplabv3+</cell><cell>34.2</cell><cell>N.A.</cell><cell>41.6</cell><cell>55.5</cell></row></table><note>A global view of existing food image datasets. (CLS: no recipe and masks, Recipe: with recipe, SEG: with segmen- tation masks )</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Data summary and comparison with existing food image segmentation datasets.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell cols="4">Semantic segmentation results on Cityscape [8] and</cell></row><row><cell cols="4">our FoodSeg103, showing that our FoodSeg103 is much more</cell></row><row><cell cols="4">challenging than the object image dataset for the task of se-</cell></row><row><cell>mantic segmentation.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="3">mIoU mAcc aAcc</cell></row><row><cell>CCNet</cell><cell>28.6</cell><cell>47.8</cell><cell>78.9</cell></row><row><cell>ReLeM-CCNet</cell><cell>29.2</cell><cell>47.5</cell><cell>79.3</cell></row><row><cell>CCNet-Finetune</cell><cell>41.3</cell><cell>53.8</cell><cell>87.7</cell></row><row><cell cols="2">ReLeM-CCNet-Finetune 47.1</cell><cell>59.5</cell><cell>85.5</cell></row><row><cell>FPN</cell><cell>21.9</cell><cell>41.7</cell><cell>75.5</cell></row><row><cell>ReLeM-FPN</cell><cell>22.9</cell><cell>42.3</cell><cell>77.0</cell></row><row><cell>FPN-Finetune</cell><cell>27.1</cell><cell>38.0</cell><cell>82.6</cell></row><row><cell>ReLeM-FPN-Finetune</cell><cell>30.8</cell><cell>40.7</cell><cell>78.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Cross-domain adaptation results. We use LSTM based ReLeM.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>This research is supported by the National Research Foundation, Singapore under its International Research Centres in Singapore Funding Initiative. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore. It is also partially supported by A*STAR under its AME YIRG Grant (Project No. A20E6c0101).[54] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. 2020. Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers. arXiv preprint arXiv:2012.15840 (2020).</figDesc><table><row><cell>APPENDIX: MORE DETAILS OF FOODSEG103</cell></row><row><cell>AND FOODSEG154</cell></row><row><cell>7.1 Statistics</cell></row><row><cell>Image Collection. For FoodSeg103, we first shuffle all the images</cell></row><row><cell>and randomly select 70% images (4983 images) as training set and</cell></row><row><cell>the left 30% images as testing set. For Asian Set, we randomly</cell></row><row><cell>sample 50% images (1186 images) for each dish class, and the left</cell></row><row><cell>50% are used for testing. The basic information of training and</cell></row><row><cell>testing set is listed in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Statistic of training and testing set for FoodSeg103, Asian Set and FoodSeg154.</figDesc><table><row><cell cols="3">S-classes Number S-classes</cell><cell cols="3">Number S-classes Number</cell></row><row><cell>Dessert</cell><cell>3913</cell><cell>Meat</cell><cell>4956</cell><cell>Soy</cell><cell>148</cell></row><row><cell>Beverage</cell><cell>844</cell><cell>Condiment</cell><cell>1543</cell><cell>Vegetable</cell><cell>15719</cell></row><row><cell>Nut</cell><cell>912</cell><cell>Seafood</cell><cell>920</cell><cell>Fungus</cell><cell>592</cell></row><row><cell>Egg</cell><cell>424</cell><cell>Soup</cell><cell>121</cell><cell>Salad</cell><cell>23</cell></row><row><cell>Fruit</cell><cell>6007</cell><cell>Main</cell><cell>5634</cell><cell>Others</cell><cell>341</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>The ingredient number of all super classes in Food-Seg103.</figDesc><table><row><cell>Encoder</cell><cell cols="4">Decoder mIoU mAcc Model Size</cell></row><row><cell>PVT-S</cell><cell>FPN</cell><cell>31.3</cell><cell>43.0</cell><cell>202M</cell></row><row><cell>ReLeM-PVT-S</cell><cell>FPN</cell><cell>32.0</cell><cell>44.1</cell><cell>202M</cell></row><row><cell>ViT-16/B</cell><cell>Naive</cell><cell>41.3</cell><cell>52.7</cell><cell>723M</cell></row><row><cell>ReLeM-ViT-16/B</cell><cell>Naive</cell><cell>43.9</cell><cell>57.0</cell><cell>723M</cell></row><row><cell>ViT-16/B</cell><cell>MLA</cell><cell>45.1</cell><cell>57.4</cell><cell>711M</cell></row><row><cell>ReLeM-ViT-16/B</cell><cell>MLA</cell><cell>43.3</cell><cell>55.9</cell><cell>711M</cell></row><row><cell>ViT-16/L</cell><cell>MLA</cell><cell>44.5</cell><cell>56.6</cell><cell>2.4G</cell></row><row><cell>Swin-S</cell><cell>Uper</cell><cell>41.6</cell><cell>53.6</cell><cell>931M</cell></row><row><cell>Swin-B</cell><cell>Uper</cell><cell>41.2</cell><cell>53.9</cell><cell>1.4G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Semantic segmentation results of different vision transformers.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FoodSeg103</head><p>Asian Set FoodSeg154 <ref type="table">Train Test Total  Train Test  Total  1  candy  58  43  101  0  0  101  2  egg tart  8  6  14  0  0  14  3  french fries  190  87  277  95  83  455  4  chocolate  158  59  217  0  0  217  5  biscuit  393  122  515  4  1  520  6</ref> popcorn 37 <ref type="bibr" target="#b10">11</ref>   <ref type="table">535  213  748  0  0  748  11  wine  117  50  167  15  19  201  12  milkshake  107  32  139  0  0  139  13  coffee  136  62  198  8  12  218  14  juice  157  64  221  71  72  364  15  milk  48  36  84  5  4  93  16  tea  29  6  35  15  6  56  17  almond  268  74  342  0  0  342  18  red beans  46  27  73  0  0  73  19  cashew  44  43  87  0  0  87  20 dried cranberries  79  55  134  0  0  134  21  soy  41  18  59  0  0  59  22  walnut  100  81  181  0  0  181  23  peanut  16  20  36  93  95  224  24  egg  321  103  424  162  161  747  25  apple  195  80  275  29  49  353  26  date  14  3  17  51  43  111  27  apricot  39  18  57  0  0  57  28  avocado  104  35  139  9  19</ref>   <ref type="table">Train Test Total  Train Test  Total  38  pear  55  21  76  0  0  76  39  fig  51  9  60  0  0  60  40  pineapple  205  81  286  32  37  355  41  grape  189  48  237  0  0  237  42  kiwi  69  21  90  0  0  90  43  melon  44  7  51  0  0  51  44  orange  283  110  393  54  48  495  45  watermelon  68  18  86  0  0  86  46  steak  987  483 1470  0  0  1470  47  pork  646  261  907  0  0  907  48  chicken duck  1160 508 1668  0  0  1668  49  sausage  372  93  465  32  34  531  50  fried meat  209  118  327  0  0  327  51  lamb  85  34  119  0  0  119  52  sauce  1124 419 1543  19  15  1577  53  crab  19  11  30  38  37  105  54  fish  348  138  486  103  126  715  55  shellfish  77  27  104  37  40  181  56  shrimp  211  89  300  51  54  405  57  soup  92  29  121  0  0  121  58  bread  1698 738 2436  49  40  2525  59  corn  411  170  581  29  35  645  60  hamburg  7  1  8  0  0  8  61  pizza  83  22  105  0  0  105  62  hanamaki baozi  22  14  36  0  0  36  63 wonton dumplings  10  10  20  165  149  334  64  pasta  171  59  230  18  3  251  65  noodles  337  140  477  811  836  2124  66  rice  655  277  932  294  306  1532  67  pie  563  246  809  20  17  846  68  tofu  111  37  148  73  57  278  69  eggplant  34  9  43  38  12  93  70  potato  1041 400 1441  110  111  1662  71  garlic  143  29  172  40  36  248  72  cauliflower  237  100  337  43  32  412  73  tomato  1404 687 2091  124  100  2315  74  kelp  4  5  9  0  0  9</ref> A Large-Scale Benchmark for Food Image Segmentation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class Id Class Name</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FoodSeg103</head><p>Asian Set FoodSeg154 <ref type="table">Train Test Total  Train Test  Total  75  seaweed  16  10  26  29  29  84  76  spring onion  285  113  398  556  561  1515  77  rape  59  23  82  360  429  871  78  ginger  25  12  37  24  34  95  79  okra  35  9  44  31  18  93  80  lettuce  748  338 1086  245  230  1561  81  pumpkin  114  25  139  0  0  139  82  cucumber  568  267  835  234  203  1272  83  white radish  56  34  90  63  52  205  84  carrot  1407 670 2077  156  143  2376  85  asparagus  325  139  464  24  23  511  86  bamboo shoots  8  7  15  0  0  15  87  broccoli  966  427 1393  35  49  1477  88  celery stick  233  91  324  36  35  395  89  cilantro mint  1045 466 1511  323  320  2154  90  snow peas  103  49  152  6  16  174  91  cabbage  139  39  178  25  13  216  92  bean sprouts  35  20  55  34  34  123  93  onion  732  304 1036  85  103  1224  94  pepper  552  242  794  189  191  1174  95  green beans  237  125  362  40  37  439  96  French beans  360  168  528  39  34  601  97 king oyster mushroom  12  3  15  0  0  15  98  shiitake  185  106  291  167  205  663  99  enoki mushroom  9  5  14  25  31  70  100  oyster mushroom  11  4  15  0  0  15  101 white button mushroom  195  62  257  35  26  318  102  salad  12  11  23  0  0  23  103  other ingredients  230  111  341  667  738  1746  104  water  0  0  0  2  4  6  105  goji berry  0  0  0  33  50  83  106  ribs  0  0  0  148  135  283  107  tripe  0  0  0  31  36  67  108  meat slices  0  0  0  135  170  305  109  minced meat  0  0  0  95  69  164  110  pork belly  0  0  0  87  76  163  111  pork intestine  0  0  0  16  16  32  112  pork skin  0  0  0  33  15  48  113  blood  0  0  0  4  4  8  114  pork liver  0  0  0  26  16  42  115</ref> shredded pork 0 0 0 25 34 59  <ref type="table">0  0  0  26  33  59  145  preserved vegetable  0  0  0  7  17  24  146  salted vegetables  0  0  0  32  25  57  147  pea seedlings  0  0  0  13  15  28  148  kai lan  0  0  0  6  11  17  149  lotus root  0  0  0  26  26  52  150  amaranth  0  0  0  23  16  39  151  millet spicy  0  0  0  64  65  129  152  bitter gourd  0  0  0  16  17  33  153  daylily  0  0  0  1  5  6  154  agaric  0  0  0  33  42  75  -Summary  29530 12567 42097  8795 8881  59773   Table 9</ref>: Statistic of ingredients per class for FoodSeg103, Asian set and FoodSeg154.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class Id Class Name</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Food-101-mining discriminative components with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="446" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Training in cognitive strategies reduces eating and improves food choice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rebecca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendy</forename><surname>Boswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shosuke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedy</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kober</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>PNAS</publisher>
			<biblScope unit="page" from="11238" to="11247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">PFID: Pittsburgh fast-food image dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kapil</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="289" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning CNN-based features for retrieval of food images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianluigi</forename><surname>Ciocca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Napoletano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raimondo</forename><surname>Schettini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIAP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="426" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">MMSegmentation: OpenMMLab Semantic Segmentation Toolbox and Benchmark</title>
		<idno>MMSegmentation Contributors. 2020</idno>
		<ptr target="https://github.com/open-mmlab/mmsegmentation" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Uwe Franke, Stefan Roth, and Bernt Schiele. 2016. The Cityscapes Dataset for Semantic Urban Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Global diets link environmental sustainability and human health</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tilman</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clark</forename><surname>Michael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="page" from="518" to="540" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mixed-dish recognition with contextual relation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixi</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyan</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM international conference on Multimedia</title>
		<meeting>ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="112" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Jakob Uszkoreit, and Neil Houlsby. 2021. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A New Large-scale Food Image Segmentation Dataset and Its Application to Food Calorie Estimation Based on Grains of Rice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takumi</forename><surname>Ege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keiji</forename><surname>Yanai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="82" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">RecipeGPT: Generative pre-training based cooking recipe generation and evaluation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helena</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Palakorn</forename><surname>Achananuparp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Philips Kokoh Prasetyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ee-Peng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lav R</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Varshney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="181" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image recognition of 85 food categories by feature fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Hoashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taichi</forename><surname>Joutou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keiji</forename><surname>Yanai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISM</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="296" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">CCNet: Criss-Cross Attention for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A food image recognition system with multiple kernel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taichi</forename><surname>Joutou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keiji</forename><surname>Yanai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="285" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">FoodX-251: A Dataset for Fine-grained Food Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parneet</forename><surname>Kaur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Divakaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic expansion of a food image dataset leveraging existing categories with domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiyuki</forename><surname>Kawano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keiji</forename><surname>Yanai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6399" to="6408" />
		</imprint>
	</monogr>
	<note>Kaiming He, and Piotr Doll?r</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06726</idno>
		<title level="m">Skip-thought vectors</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. 2017. Feature Pyramid Networks for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<title level="m">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recipe1M+: A Dataset for Learning Cross-Modal Embeddings for Cooking Recipes and Food Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aritro</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferda</forename><surname>Ofli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Hynes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaia</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="page" from="187" to="203" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Recipe1M+: A Dataset for Learning Cross-Modal Embeddings for Cooking Recipes and Food Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Mar?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aritro</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferda</forename><surname>Ofli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Hynes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaia</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="page" from="187" to="203" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Multiple-food recognition considering cooccurrence employing manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keiji</forename><surname>Yanai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2017" to="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Im2Calories: towards an automated mobile vision food diary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Gorban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1233" to="1241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Ingredient-Guided Cascaded Multi-Attention Network for Food Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiqing</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqiang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM international conference on Multimedia</title>
		<meeting>ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1331" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">ISIA Food-500: A Dataset for Large-Scale Food Recognition via Stacked Global-Local Attention Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiqing</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiling</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM international conference on Multimedia</title>
		<meeting>ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="393" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">UEC-FoodPIX Complete: A Large-scale Food Image Segmentation Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaimu</forename><surname>Okamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keiji</forename><surname>Yanai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MADiMa</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mining Discriminative Food Regions for Accurate Food Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianing</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-W</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingnan</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benny</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">FoodAI: Food Image Recognition via Deep Learning for Smart Food Logging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doyen</forename><surname>Sahoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiongwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Palakorn</forename><surname>Achananuparp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ee-Peng</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2260" to="2268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Inverse cooking: Recipe generation from food images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaia</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Giro-I Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10453" to="10462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Inverse Cooking: Recipe Generation From Food Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaia</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Giro-I Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10453" to="10462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning cross-modal embeddings for cooking recipes and food images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaia</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Hynes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferda</forename><surname>Ofli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3020" to="3028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning food image similarity for food image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wataru</forename><surname>Shimoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keiji</forename><surname>Yanai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BigMM</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="165" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Nutrition5k: Towards Automatic Nutritional Understanding of Generic Food</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quin</forename><surname>Thames</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Karpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Norris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangting</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liviu</forename><surname>Panait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Sim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1521" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Attention is All you Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Structure-Aware Generation Network for Recipe Generation from Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="359" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12122</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Abhinav Gupta, and Kaiming He</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
	<note>Non-local neural networks</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Recipe recognition with large multimodal food dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devinder</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Precioso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Mixed dish recognition through multi-label learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing-Jing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyan</forename><surname>Ming</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Workshop on Multimedia for Cooking and Eating Activities</title>
		<meeting>the 11th Workshop on Multimedia for Cooking and Eating Activities</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="418" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Geolocalized modeling for dish recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Herranz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="page" from="1187" to="1199" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
