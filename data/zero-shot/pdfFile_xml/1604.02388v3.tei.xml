<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">STD2P: RGBD Semantic Segmentation using Spatio-Temporal Data-Driven Pooling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<orgName type="institution">Saarland Informatics Campus</orgName>
								<address>
									<settlement>Saarbr?cken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chen</forename><surname>Chiu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<orgName type="institution">Saarland Informatics Campus</orgName>
								<address>
									<settlement>Saarbr?cken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Mannheim</orgName>
								<address>
									<settlement>Mannheim</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<orgName type="institution">Saarland Informatics Campus</orgName>
								<address>
									<settlement>Saarbr?cken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">STD2P: RGBD Semantic Segmentation using Spatio-Temporal Data-Driven Pooling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel superpixel-based multi-view convolutional neural network for semantic image segmentation. The proposed network produces a high quality segmentation of a single image by leveraging information from additional views of the same scene. Particularly in indoor videos such as captured by robotic platforms or handheld and bodyworn RGBD cameras, nearby video frames provide diverse viewpoints and additional context of objects and scenes. To leverage such information, we first compute region correspondences by optical flow and image boundary-based superpixels. Given these region correspondences, we propose a novel spatio-temporal pooling layer to aggregate information over space and time. We evaluate our approach on the NYU-Depth-V2 and the SUN3D datasets and compare it to various state-of-the-art single-view and multi-view approaches. Besides a general improvement over the stateof-the-art, we also show the benefits of making use of unlabeled frames during training for multi-view as well as single-view prediction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Consumer friendly and affordable combined image and depth-sensors such as Kinect are nowadays commercially deployed in scenarios such as gaming, personal 3D capture and robotic platforms. Interpreting this raw data in terms of a semantic segmentation is an important processing step and hence has received significant attention. The goal is typically formalized as predicting for each pixel in the image plane the corresponding semantic class.</p><p>For many of the aforementioned scenarios, an image sequence is naturally collected and provides a substantially richer source of information than a single image. Multiple images of the same scene can provide different views that change the observed context, appearance, scale and occlusion patterns. The full sequence provides a richer observa- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sequence</head><p>Single-View Predictions <ref type="figure">Figure 1</ref>: An image sequence can provide rich context and appearance, as well as unoccluded objects for visual recognition systems. Our Spatio-Temopral Data-Driven Pooling (STD2P) approach integrates the multi-view information to improve semantic image segmentation in challenging scenarios.</p><p>tion of the scene and propagating information across views has the potential to significantly improve the accuracy of semantic segmentations in more challenging views as shown in <ref type="figure">Figure 1</ref>. Hence, we propose a multi-view aggregation method by a spatio-temporal data-driven pooling (STD2P) layer which is a principled approach to incorporate multiple frames into any convolutional network architecture. In contrast to previous work on superpixel-based approaches <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b2">3]</ref>, we compute correspondences over time which allows for knowledgeable and consistent prediction over space and time.</p><p>As dense annotation of full training sequences is time consuming and not available in current datasets, a key feature of our approach is training from partially annotated sequences. Notably, our model leads to improved semantic segmentations in the case of multi-view observation as well as single-view observation at test time. The main contributions of our paper are:</p><p>? We propose a principled way to incorporate superpixels and multi-view information into state-of-theart convolutional networks for semantic segmentation. Our method is able to exploit a variable number of frames with partial annotation in training time.</p><p>? We show that training on sequences with partial annotation improves semantic segmentation for multi-view observation as well as single-view observation.</p><p>? We evaluate our method on the challenging semantic segmentation datasets NYU-Depth-V2 and SUN3D. There, it outperforms several baselines as well as the state-of-the-art. In particular, we improve on difficult classes not well captured by other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work 2.1. Context modeling for fully convolutional networks</head><p>Fully convolutional networks (FCN) <ref type="bibr" target="#b27">[28]</ref>, built on deep classification networks <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b35">36]</ref>, carried their success forward to semantic segmentation networks that are end-to-end trainable. Context information plays an important role in semantic segmentation <ref type="bibr" target="#b29">[30]</ref>, so researchers tried to improve the standard FCN by modeling or providing context in the network. Liu et al. <ref type="bibr" target="#b25">[26]</ref> added global context features to a feature map by global pooling. Yu et al. <ref type="bibr" target="#b40">[41]</ref> proposed dilation convolutions to aggregate wider context information. In addition, graphical models are applied to model the relationship of neuron activation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b23">24]</ref>. Particularly, Chen et al. <ref type="bibr" target="#b5">[6]</ref> combined the strengths of conditional random field (CRF) with CNN to refine the prediction, and thus achieved more accurate results. Zheng et al. <ref type="bibr" target="#b41">[42]</ref> formulated CRFs as recurrent neural networks (RNN), and trained the FCN and the CRF-RNN end-to-end. Recurrent neural networks have also been used to replace graphical models in learning context dependencies <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b22">23]</ref>, which shows benefits in complicated scenarios.</p><p>Recently, incorporating superpixels in convolutional networks has received much attention. Superpixels are able to not only provide precise boundaries, but also to provide adaptive receptive fields. For example, Dai et al. <ref type="bibr" target="#b8">[9]</ref> designed a convolutional feature masking layer for semantic segmentation, which allows networks to extract features in unstructured regions with the help of superpixels. Gadde et al. <ref type="bibr" target="#b12">[13]</ref> improved the semantic segmentation using superpixel convolutional networks with bilateral inception, which can merge initial superpixels by parameters and generate different levels of regions. Caesar et al. <ref type="bibr" target="#b4">[5]</ref> proposed a novel network with free-form ROI pooling which leverages superpixels to generate adaptive pooling regions. Arnab et al. <ref type="bibr" target="#b2">[3]</ref> modeled a CRF with superpixels as higher order potentials, and achieved better results than previous CRF based methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b41">42]</ref>. Both methods showed the merit of providing superpixels to networks, which can generate more accurate segmentations. Different from prior works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b4">5]</ref>, we introduce superpixels at the end of convolutional networks instead of in the intermediate layers and also integrate the response from multiple views with average pooling, which has been used to replace the fully connected layers in classification <ref type="bibr" target="#b24">[25]</ref> and localization <ref type="bibr" target="#b42">[43]</ref> tasks successfully.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Semantic segmentation with videos</head><p>The aim of multi-view semantic segmentation is to employ the potentially richer information from diverse views to improve over segmentations from a single view. Couprie et al. <ref type="bibr" target="#b7">[8]</ref> performed single image semantic segmentation with learned features with color and depth information, and applied a temporal smoothing in test time to improve the performance of frame-by-frame estimations. Hermans et al. <ref type="bibr" target="#b17">[18]</ref> used the Bayesian update strategy to fuse new classification results and a CRF model in 3D space to smooth the segmentation. St?ckler et al. <ref type="bibr" target="#b36">[37]</ref> used random forests to predict single view segmentations, and fused all views to the final output by a simultaneous localization and mapping (SLAM) system. Kundu et al. <ref type="bibr" target="#b21">[22]</ref> built a dense 3D CRF model with correspondences from optical flow to refine semantic segmentation from video. Recently, McCormac et al. <ref type="bibr" target="#b28">[29]</ref> proposed a CNN based semantic 3D mapping system for indoor scenes. They applied a SLAM system to build correspondences, and mapped semantic labels predicted from CNN to 3D point cloud data. Mustikovela et al. <ref type="bibr" target="#b30">[31]</ref> proposed to generate pseudo ground truth annotations for auxiliary data with a CRF based framework. With the auxiliary data and their generated annotations, they achieved a clear improvement. In contrast to the above methods, instead of integrating multi-view information by using graphical models, we utilize optical flow and image superpixels to establish region correspondences, and design a superpixel based multi-view network for semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Fully convolutional multi-view segmentation with region correspondences</head><p>Our goal is a multi-view semantic segmentation scheme, that integrates seamlessly into exciting deep architectures and produces highly accurate semantic segmentation of a single view. We further aim at facilitating training from partially annotated input sequences, so that existing datasets can be used and the annotation effort stays moderate for new datasets. To this end, we draw on prior work on high quality non-semantic image segmentation and optical flow which is input to our proposed Spatio-Temporal Data-Driven Pooling (STD2P) layer.</p><p>Overview. As illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>, our method starts from an image sequence. We are interested in providing an accurate semantic segmentation of one view in the sequence, called target frame, which can be located at any position in the image sequence. The two components that distinguish our approach from a standard fully convolutional architecture for semantic segmentation are, first, the computation of region correspondences and, second, the novel spatio-temporal pooling layer that is based on these correspondences.</p><p>We first compute the superpixel segmentation of each frame and establish region correspondences using optical flow. Then, the proposed data-driven pooling allows to aggregate information first within superpixels and then along their correspondences inside a CNN architecture. Thus, we achieve a tight integration of the superpixel segmentation and multi-view aggregation into a deep learning framework for semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Region correspondences</head><p>Motivated by the recent success of superpixel based approaches in deep learning architectures <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b9">10]</ref> and the reduced computational load, we decide for a region-based approach. In the following, we motivate and detail our approach on establishing robust correspondences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motivation.</head><p>One key idea of our approach is to map information from potentially unlabeled frames to the target frame, as diverse view points can provide additional context and resolve challenges in appearance and occlusion as illustrated in <ref type="figure">Figure 1</ref>. Hence, we do not want to assume visibility or correspondence of objects across all frames (e.g. the nightstand in the target frame as shown in <ref type="figure" target="#fig_1">Figure 2</ref>). Therefore, video supervoxel methods such as <ref type="bibr" target="#b14">[15]</ref> that force interframe correspondences and do not offer any confidence measure are not suitable. Instead, we establish the required correspondences on a frame-wise region level.</p><p>Superpixels &amp; optical flow. We compute RGBD superpixels <ref type="bibr" target="#b16">[17]</ref> in each frame to partition a RGBD image into regions, and apply Epic flow <ref type="bibr" target="#b32">[33]</ref> between each pair of consecutive frames to link these regions. To take advantage of the depth information, we utilize the RGBD version of the structured edge detection <ref type="bibr" target="#b10">[11]</ref> to generate boundary estimates. Then, Epic flow is computed in forward and backward directions.</p><p>Robust spatio-temporal matching. Given the precomputed regions in the target frame and all unlabeled frames as well as the optical flow between those frames, our goal is to find highly reliable region correspondences. For any two regions R t in the target frame f t and R u in an unlabeled frame f u , we compute their matching score from their intersection over union (IoU). Let us assume w.l.o.g. that u &lt; t. Then, we warp R u from f u to R u in f t using forward optical flow. The IoU between R t and R u is denoted by ? ? ? IoU tu . Similarly, we compute ? ? ? IoU tu with backward optical flow. We regard R t and R u as a successful match if their matching score meets min(</p><formula xml:id="formula_0">? ? ? IoU tu , ? ? ? IoU tu ) &gt; ? .</formula><p>We keep the one with the highest matching score if R t has several successful matches. We show the statistics of region correspondences on the NYUDv2 dataset in <ref type="figure">Figure 3</ref>.</p><p>It shows that 87.17% of the regions are relatively small (less than 2000 pixels) The plot on the right shows that those small regions generally only find less than 10 matches in a whole video. Contrariwise, even slightly bigger regions can be matched more easily and they cover large portions of images. They usually have more than 40 matches in a whole video, and thus provide adequate information for our multi-view network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Spatio-Temporal Data-Driven Pooling (STD2P)</head><p>Here, we describe our Spatio-Temporal Data-Driven Pooling (STD2P) model that uses the spatio-temporal structure of the computed region correspondences to aggregate information across views as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. While the proposed method is highly compatible with recent CNN and FCN models, we build on a per frame model using <ref type="bibr" target="#b27">[28]</ref>. In more detail, we refine the output of the deconvolution layer with superpixels and aggregate the information from multiple views by three layers: a spatial pooling layer, a temporal pooling layer and a region-to-pixel layer.</p><p>Spatial pooling layer. The input to the spatial pooling layer is a feature map I s ? R N ?C?H?W for N frames, C channels with size H ? W and a superpixel map S ? R N ?H?W encoded with the region index. It generates the output O s ? R N ?C?P , where P is the maximum number of superpixels. The superpixel map S guides the forward and backward propagation of the layer. Here, ? ij = {(x, y)|S(i, x, y) = j} denotes a superpixel in the i-th frame with region index j. Then, the forward propagation of spatial average pooling can be formulated as</p><formula xml:id="formula_1">O s (i, c, j) = 1 |? ij | (x,y)??ij I s (i, c, x, y)<label>(1)</label></formula><p>for each channel index c of the i-th frame and region index j. We train our model using stochastic gradient descent. The gradient of the input I s (i, c, x, y), where (x, y) ? ? ij , in our spatial pooling is calculated by back propagation <ref type="bibr" target="#b33">[34]</ref>,</p><formula xml:id="formula_2">?L ?I s (i, c, x, y) = ?L ?O s (i, c, j) ?O s (i, c, j) ?I s (i, c, x, y) = 1 |? ij | ?L ?O s (i, c, j)</formula><p>.</p><p>(2)</p><p>Temporal pooling layer. Similarly, we formulate our temporal pooling which fuses the information from N frames I t ? R N ?C?P , which is the output of spatial pooling layer, to one frame O t ? R C?P . This layer also needs superpixel information ? ij , which is the superpixel with index j of the i-th input frame. If ? ij = ?, there exists correspondence. The forward propagation can be expressed as</p><formula xml:id="formula_3">O t (c, j) = 1 K ?ij =? I t (i, c, j)<label>(3)</label></formula><p>for channel index c and region index j, where K = |{i|? ij = ?, 1 ? i ? N }|, which is the number of matched frames for j-th region. The gradient is calculated by</p><formula xml:id="formula_4">?L ?I t (i, c, j) = ?L ?O t (c, j) ?O t (c, j) ?I t (i, c, j) = 1 K ?L ?O t (c, j) .<label>(4)</label></formula><p>Region-to-pixel layer. To directly optimize a semantic segmentation model with dense annotations, we map the region based feature map I r ? R C?P to a dense pixel-level prediction O r ? R C?H?W . This layer needs a superpixel map on the target frame S target ? R H?W to perform forward and backward propagation. The forward propagation is expressed as</p><formula xml:id="formula_5">O r (c, x, y) = I r (c, j), S target (x, y) = j.<label>(5)</label></formula><p>The gradient is computed by  <ref type="figure">Figure 4</ref>: Visualization examples of the semantic segmentation on NYUDv2. Column 1 shows the RGB images and column 2 shows the ground truth (black represents the unlabeled pixels). Columns 3 to 6 show the results from CRF-RNN <ref type="bibr" target="#b41">[42]</ref>, DeepLab-LFOV <ref type="bibr" target="#b6">[7]</ref>, BI(3000) <ref type="bibr" target="#b12">[13]</ref> and E2S2 <ref type="bibr" target="#b4">[5]</ref>, respectively. Columns 7 to 9 show the results from FCN <ref type="bibr" target="#b27">[28]</ref>, single-view superpixel and multi-view pixel baselines. The results from our whole system are shown in column 10. Best viewed in color.</p><formula xml:id="formula_6">?L ?I r (c, j) = Starget(x,y)=j ?L ?O r (c, x, y) ?O r (c, x, y) ?I r (c, j) = Starget(x,y)=j ?L ?O r (c, x, y) .<label>(6)</label></formula><p>Implementation details. We regard the frames with groundtruth annotations as target frames. For each target frame, we equidistantly sample up to 100 frames around it with the static interval of 3 frames. Next, we compute the superpixels <ref type="bibr" target="#b16">[17]</ref> and Epic flow <ref type="bibr" target="#b32">[33]</ref> with the default settings provided in the corresponding source codes. The threshold ? for the computation of region correspondence is 0.4 (cf. section 3.1). Finally, for each RGBD sequence, we randomly sample 11 frames including the target frame together with their correspondence maps as the input for our network. We use RGB images and HHA representations of depth <ref type="bibr" target="#b16">[17]</ref> and train the network by stochastic gradient descent with momentum term. Due to the memory limitation, we first run FCN and cache the output pool4 rgb and pool4 hha. Then, we finetune the layers after pool4 with a new network which is the copy of the higher layers in FCN. We use a minibatch size of 10, momentum 0.9, weight decay 0.0005 and fixed learning rate 10 ?14 . We finetune our model by using cross entropy loss with 1000 iterations for all our models in the experiments. We implement the proposed network using the Caffe framework <ref type="bibr" target="#b18">[19]</ref>, and the source code is available at https: //github.com/SSAW14/STD2P.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and analysis</head><p>We evaluate our approach on the 4-class <ref type="bibr" target="#b31">[32]</ref>, 13class <ref type="bibr" target="#b7">[8]</ref>, and 40-class <ref type="bibr" target="#b15">[16]</ref> tasks of the NYU-Depth-V2 (NYUDv2) dataset <ref type="bibr" target="#b31">[32]</ref>, and 33-class task of the SUN3D dataset <ref type="bibr" target="#b39">[40]</ref>.</p><p>The NYUDv2 dataset contains 518 RGBD videos, which have more than 400,000 images. Among them, there are 1449 densely labeled frames, which are split into 795 train-ing images and 654 testing images. We follow the experimental settings of <ref type="bibr" target="#b9">[10]</ref> to test on 65 labeled frames. We compare our models of different settings to previous stateof-the-art multi-view methods as well as single-view methods, which are summarized in <ref type="table" target="#tab_1">Table 1</ref>. We report the results on the labeled frames, using the same evaluation protocol and metrics as <ref type="bibr" target="#b27">[28]</ref>, pixel accuracy (Pixel Acc.), mean accuracy (Mean Acc.), region intersection over union (Mean IoU), and frequency weighted intersection over union (f.w. IoU). 4.1. Results on NYUDv2 40-class task <ref type="table">Table 2</ref> evaluates performance of our method on NYUDv2 40-class task and compares to state-of-the-art methods and related approaches <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b12">13</ref>, 5] 1 . We include 3 versions of our approach:</p><p>Our superpixel model is trained on single frames without additional unlabeled data, and tested using a single target frame. It improves the baseline FCN on all four metrics by at least 2 percentage points (pp), and it achieves in particular <ref type="table">Table 2</ref>: Performance of the 40-class semantic segmentation task on NYUDv2. We compare our method to various state-ofthe-art methods: <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b11">12]</ref> are also based on convolutional networks, <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b6">7]</ref> are the models based on convolutional networks and CRF, and <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b9">10]</ref> are region labeling methods, and thus related to ours. We mark the best performance in all methods with BOLD font, and the second best one is written with UNDERLINE. - - better performance than recently proposed methods based on superpixels and CNN <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b4">5]</ref>.</p><formula xml:id="formula_7">- - - - - - - - - - - - - - Multi-Scale CNN [12] - - - - - - - - - - - - - - - CRF-</formula><formula xml:id="formula_8">- - - - - - - - - - - - - - Multi-Scale CNN [12] - - - - - - - - - - - - - - - CRF-RNN [</formula><p>Our superpixel+ model leverages additional unlabeled data in the training while it only uses the target frame for test. It obtains 3.4pp, 2.1pp, 1.1pp improvements over the superpixel model on Mean Acc., Mean IoU and f.w. IoU, leading to more favorable performance than many state-ofthe-art methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b4">5]</ref>. This highlights the benefits of leveraging unlabeled data.  <ref type="figure">Figure 4</ref> demonstrates that our method is able to produce smooth predictions with accurate boundaries. We present the most related methods, which either apply CRF <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b6">7]</ref> or incorporate superpixels <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b4">5]</ref>, in the columns 3 to 6 of this figure. According to the qualitative comparison to these approaches, we can see the benefit of our method. It captures small objects like chair legs, as well as large areas like floormat and door. In addition, we also present FCN and the superpixel model at the 7-th and 8-th column of <ref type="figure">Figure 4</ref>. The FCN is boosted by introducing superpixels but not as precise as our full model using unlabeled data.</p><p>Average vs. max spatio-temporal data-driven pooling. Our data-driven pooling aggregates the local information from multiple observations within a segment and across multiple views. Average pooling and max pooling are canonical choices used in many deep neural network architectures. Here we test average pooling and max pooling both in the spatial and temporal pooling layer, and show the results in <ref type="table" target="#tab_5">Table 3</ref>. All the models are trained with multiple frames, and tested on multiple frames. Average pooling turns out to perform best for spatial and temporal pooling. This result confirms our design choice.</p><p>Region vs. pixel correspondences. We compare our full model, which is built on the region correspondences, to the model with pixel correspondences. It only uses the per-pixel correspondences by optical flow and applies average pooling to fuse the information from multiple view. The visualization results of this baseline are presented in column 9 of <ref type="figure">Figure 4</ref>. Obtaining accurate pixel correspondences is challenging because the optical flow is not perfect and the error can accumulate over time. Consequently, the model with pixel correspondences only improves slightly over the FCN baseline, as it is also reflected in the numbers in <ref type="table" target="#tab_6">Table 4</ref>. Establishing region correspondences with the proposed re-  jection strategy described in section 3.1 seems indeed to be favorable over pixel correspondences. Our full model shows a significant improvement over the pixel-correspondence baseline and FCN in all 4 measures.</p><p>Analysis of multi-view prediction. In our multi-view model, we subsample frames from a whole video for computational considerations. There is a trade-off between close-by and distant frames to be made. If we select frames far away from the target frames, they can provide more diverse views of an object, while matching is more challenging and potentially less accurate than for close-by frames. Hence, we analyze the influence of the distance of selected frames to target frames, and report the Mean Acc. and Mean IoU in <ref type="figure" target="#fig_3">Figure 5</ref>. In results, providing wider views is helpful, as the performance is improved with the increase of max distance. And selecting the data in the future, which is another way to provide wider views, also contributes to the improvements of performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results on NYUDv2 4-class and 13-class tasks</head><p>To show the effectiveness of our multi-view semantic segmentation approach, we compare our method to previous state-of-the-art multi-view semantic segmentation methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b28">29]</ref> on the 4-class and 13-class tasks of NYUDv2 as shown in <ref type="table" target="#tab_7">Table 5</ref>. Besides, we also present previous state-of-the-art single-view methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b37">38]</ref>. We observe that our superpixel+ model already outperforms all the multi-view competitors, and the proposed temporal pooling scheme further boosts Pixel Acc. and Mean Acc. by more than 1pp and then outperforms the state-of-the-art Image GT CRF-RNN DeepLab-LFOV BI(3000) E2S2 FCN Our full model <ref type="figure">Figure 6</ref>: Qualititive results of the SUN3D dataset. For each example, the images are arranged from top to bottom, from left to right as color image, groundtruth, CRF-RNN <ref type="bibr" target="#b41">[42]</ref>, DeepLab-LFOV <ref type="bibr" target="#b6">[7]</ref>, BI <ref type="bibr" target="#b12">[13]</ref>, E2S2 <ref type="bibr" target="#b4">[5]</ref>, FCN <ref type="bibr" target="#b27">[28]</ref> and ours.  <ref type="bibr" target="#b11">[12]</ref>. In particular, the recent proposed method by McCormac et al. <ref type="bibr" target="#b28">[29]</ref> is also built on CNN, however, their performance on 13-class task is about 5pp worse than ours. <ref type="table" target="#tab_8">Table 6</ref> shows the results of our method and baselines on the SUN3D dataset. We follow the experimental settings of <ref type="bibr" target="#b9">[10]</ref> to test all the methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b27">28]</ref> on all 65 labeled frames in SUN3D, which are trained with the NYUDv2 40-class annotations. After computing the 40class prediction, we map 7 unseen semantic classes into 33 classes. Specifically, floormat is merged to floor, dresser is merged to other furni and five other classes are merged to other props. Among all the methods, we achieve the best Mean IoU score that our superpixel+ and full model are 1.2pp and 4.7pp better than <ref type="bibr" target="#b9">[10]</ref> and <ref type="bibr" target="#b6">[7]</ref> . For Pixel Acc., our method is comparable to the previous state of the art <ref type="bibr" target="#b9">[10]</ref>. In addition, we observe that our superpixel+ model boosts the baseline FCN by 3.7pp, 2.3pp, 3.3pp, 3.9pp on the four metrics, and applying multi-view information further improves 3.0pp, 0.4pp, 3.5pp, 3.7pp, respectively. Besides, we achieve much better performance than DeepLab- LFOV, which is comparable to our model on the NYUDv2 40-class task. This illustrates the generalization capability of our model, even without finetuning on the new domain or dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results on SUN3D 33-class task</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented a novel semantic segmentation approach using image sequences. We design a superpixelbased multi-view semantic segmentation network with spatio-temporal data-driven pooling which can receive multiple images and their correspondence as input. We propagate the information from multiple views to the target frame, and significantly improve the semantic segmentation performance on the target frame. Besides, our method can leverage large scale unlabeled images for training and test, and we show that using unlabeled data also benefits single image semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Materials</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Analysis of semantic segmentation boundary accuracy</head><p>In order to quantify the improvement on semantic boundary localization based on the proposed data-driven pooling scheme, we use Boundary Precision Recall (BPR), as also used in image or video segmentation benchmark <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b1">2]</ref> for evaluation. <ref type="figure" target="#fig_4">Figure 7</ref> shows the resulting semantic boundary average precision-recall curve. We conclude that our method generates more accurate boundaries than FCN, which achieve 0.477 BPR score while our method achieves 0.647. Besides, our method even improves on the superpixel <ref type="bibr" target="#b16">[17]</ref> we build on, which means our method can successfully merge over-segmentations or non-semantic boundaries between adjacent instances of the same semantic class. B. Oracle performance using groundtruth labels We perform two best-case analysis by computing an oracle performance where groundtruth labels are available for either reference or target frames. The first row of <ref type="table" target="#tab_9">Table 7</ref> shows the achievable performance by performing a majority vote of the groundtruth pixel labels on the employed superpixels from <ref type="bibr" target="#b16">[17]</ref>. Thereby we achieve an upper bound of 96.2% on the pixel accuracy that is implied by the superpixel over-segmentation. In order to evaluate the effectiveness of our region correspondence, we use groundtruth labels of reference frames in the sequence. We collect 143 views to conduct this experiment in NYUDv2, which have corresponding regions in target frames. We ignore regions without correspondence in the next frame to compute the quantitative results, which are presented in <ref type="table" target="#tab_9">Table 7</ref>. This best-case analysis for correspondence results in a pixel accuracy of 84.7%. Both oracle performances indicate a strong potential for performance improvements in our setup in all 4 reported measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Groundtruth analysis</head><p>At a closer look, it turns out that at least part of the performance loss in the best-case analysis for the correspondence is not due to bad matches between regions. In <ref type="figure">Fig.  8</ref>, we present some examples of the annotations provided in the dataset. In several cases, as the ones shown in the figure, the labeling is inconsistent and object labels are changed during the sequence. From left to right in <ref type="figure">Fig. 8</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Qualitative results on NYUDv2 4-class and 13class task</head><p>We provide the qualitative results of 4-class and 13-class tasks of NYUDv2 dataset in <ref type="figure">Figure 9</ref> and <ref type="figure">Figure 10</ref> respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Qualitative results on NYUDv2 40-class task</head><p>We provide more qualititative results in the following figures. We pick up some major scene categories from the test set including bedroom <ref type="figure">(Figure 11</ref>), living room ( <ref type="figure" target="#fig_1">Figure  12</ref>), dining room ( <ref type="figure">Figure 13</ref>), kitchen ( <ref type="figure">Figure 14</ref>), bathroom ( <ref type="figure" target="#fig_3">Figure 15</ref>), office ( <ref type="figure">Figure 16</ref>) and classroom ( <ref type="figure" target="#fig_4">Figure 17</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Failure cases</head><p>In this section, we present some failure cases of our methods in <ref type="figure">Figure 18</ref>. In those views, our method does not achieve better result. In the first two rows, we cannot segment the regions marked with white bounding box. This is because the superpixel in this two views cannot successful segment the regions. We use the same parameter for all views, so it fails to provide good superpixels for our system, but we believe that it is not difficult to get better superpixels for those failure views by adjusting the parameter of superpixel. In the third and fourth rows, we recognize the region as "cabinet" and "floormat" while groundtruth are "dresser" and "floor", which are also difficult for human  <ref type="figure">Figure 8</ref>: Example of groundtruth limitation and segmentation results of oracle case. Row 3 and 2 draw color images of target frame and next labeled frame, respectively. And row 4 and 1 draw their groundtruth. The segmentation result with groundtruth of target frame is shown in row 5, and the result with groundtruth of next frame is shown in row 6. We point out the regions in different frames with white bounding box, which are the same object of different views but labeled as different classes. beings to classify. In the last two rows, we show some challenges, which make our system fail to correctly recognize the region.  <ref type="figure">Figure 18</ref>: Some failure cases that our method is not able to improve FCN.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Pipeline of the proposed method. Our multi-view semantic segmentation network is built on top of a CNN. It takes a RGBD sequence as input and computes the semantic segmentation of a target frame with the help of unlabeled frames. We use superpixels and optical flow to establish region correspondences, and fuse the posterior from multiple views with the proposed Spatio-Temporal Data-Driven Pooling (STD2P).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>4 Figure 3 :</head><label>43</label><figDesc>Statistics of region correspondences on the NYUDv2 dataset. (left) Distribution of region sizes; (right) Histogram of the average number of matches over region sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>The performance of multi-view prediction with varying maximum distance. Green lines show the results of using future and past views. Blue lines show the results of only using past views.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Superpixel ([Gupta et. al.]) FCN (BPR=0.477) Proposed Method (BPR=0.647) Precision-recall curve on semantic boundaries on the NYUDv2 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :Figure 11 :Figure 12 :Figure 13 :Figure 14 :Figure 15 :Figure 16 :Figure 17 :</head><label>911121314151617</label><figDesc>Semantic segmentation results of 4-class task on NYUDv2. Semantic segmentation results of bedroom scenes on NYUDv2. Semantic segmentation results of living room scenes on NYUDv2. Semantic segmentation results of dining room scenes on NYUDv2. Semantic segmentation results of kitchen scenes on NYUDv2. Semantic segmentation results of bathroom scenes on NYUDv2. Semantic segmentation results of office scenes on NYUDv2. Semantic segmentation results of classroom scenes on NYUDv2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Configurations of competing methods</figDesc><table><row><cell></cell><cell>RGB</cell><cell>RGBD</cell></row><row><cell>Single-View</cell><cell>[12, 20]</cell><cell>[5, 6, 7, 10, 13, 17, 28, 38, 39, 42]</cell></row><row><cell>Multi-View</cell><cell>/</cell><cell>[8, 18, 37, 29]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Comparison of average and max spatio-temporal data-driven pooling.</figDesc><table><row><cell>Spatial/Temporal</cell><cell>Pixel Acc.</cell><cell>Mean Acc.</cell><cell>Mean IoU</cell><cell>f.w. IoU</cell></row><row><cell>AVG / AVG</cell><cell>70.1</cell><cell>53.8</cell><cell>40.1</cell><cell>55.7</cell></row><row><cell>AVG / MAX</cell><cell>69.4</cell><cell>51.0</cell><cell>38.0</cell><cell>54.4</cell></row><row><cell>MAX / AVG</cell><cell>66.4</cell><cell>45.4</cell><cell>33.8</cell><cell>49.6</cell></row><row><cell>MAX / MAX</cell><cell>64.9</cell><cell>44.5</cell><cell>32.1</cell><cell>47.9</cell></row><row><cell cols="5">Our full model leverages additional unlabeled data both</cell></row><row><cell cols="5">in the training and test. It achieves a consistent im-</cell></row><row><cell cols="5">provement over the superpixel+ model and outperforms</cell></row><row><cell cols="5">all competitors in Mean Acc., Mean IoU and f.w. IoU</cell></row><row><cell cols="5">by 0.9pp, 0.7pp, 1.0pp respectively. Particularly strong</cell></row><row><cell cols="5">improvements are observed on challenging object classes</cell></row><row><cell cols="5">such as dresser(+7.2pp), door(+4.8pp), bed(+4.7pp) and</cell></row><row><cell>TV(+3.1pp).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell cols="5">Comparison results with baselines on NYUDv2</cell></row><row><cell>40-class task</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell>Pixel Acc.</cell><cell>Mean Acc.</cell><cell>Mean IoU</cell><cell>f.w. IoU</cell></row><row><cell>FCN [28]</cell><cell>65.4</cell><cell>46.1</cell><cell>34.0</cell><cell>49.5</cell></row><row><cell>Pixel Correspondence</cell><cell>66.2</cell><cell>45.9</cell><cell>34.6</cell><cell>50.2</cell></row><row><cell>Superpixel Correspondence</cell><cell>70.1</cell><cell>53.8</cell><cell>40.1</cell><cell>55.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell cols="5">: Performance of the 4-class (left) and 13-class</cell></row><row><cell cols="4">(right) semantic segmentation tasks on NYUDv2.</cell><cell></cell></row><row><cell>Methods</cell><cell>Pixel Acc.</cell><cell>Mean Acc.</cell><cell>Pixel Acc.</cell><cell>Mean Acc.</cell></row><row><cell>Couprie et al. [8]</cell><cell>64.5</cell><cell>63.5</cell><cell>52.4</cell><cell>36.2</cell></row><row><cell>Hermans et al. [18]</cell><cell>69.0</cell><cell>68.1</cell><cell>54.2</cell><cell>48.0</cell></row><row><cell>St?ckler et al. [37]</cell><cell>70.6</cell><cell>66.8</cell><cell>-</cell><cell>-</cell></row><row><cell>McCormac et al. [29]</cell><cell>-</cell><cell>-</cell><cell>69.9</cell><cell>63.6</cell></row><row><cell>Wang et al. [38]</cell><cell>-</cell><cell>65.3</cell><cell>-</cell><cell>42.2</cell></row><row><cell>Wang et al. [39]</cell><cell>-</cell><cell>74.7</cell><cell>-</cell><cell>52.7</cell></row><row><cell>Eigen et al. [12]</cell><cell>83.2</cell><cell>82.0</cell><cell>75.4</cell><cell>66.9</cell></row><row><cell>Ours (superpixel+)</cell><cell>82.7</cell><cell>81.3</cell><cell>74.8</cell><cell>67.0</cell></row><row><cell>Ours (full model)</cell><cell>83.6</cell><cell>82.5</cell><cell>75.8</cell><cell>68.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Performance of the 33-class semantic segmentation task on SUN3D. All 65 images are used as the test set.</figDesc><table><row><cell>Methods</cell><cell>Pixel Acc.</cell><cell>Mean Acc.</cell><cell>Mean IoU</cell><cell>f.w. IoU</cell></row><row><cell>Mutex Constraints [10]</cell><cell>65.7</cell><cell>-</cell><cell>28.2</cell><cell>51.0</cell></row><row><cell>CRF-RNN [42]</cell><cell>59.8</cell><cell>-</cell><cell>25.5</cell><cell>43.3</cell></row><row><cell>DeepLab [6]</cell><cell>60.9</cell><cell>30.7</cell><cell>24.0</cell><cell>44.1</cell></row><row><cell>DeepLab-LFOV [7]</cell><cell>62.3</cell><cell>35.3</cell><cell>28.2</cell><cell>46.2</cell></row><row><cell>BI (1000) [13]</cell><cell>53.8</cell><cell>31.1</cell><cell>20.8</cell><cell>37.1</cell></row><row><cell>BI (3000) [13]</cell><cell>53.9</cell><cell>31.6</cell><cell>21.1</cell><cell>37.4</cell></row><row><cell>E2S2 [5]</cell><cell>56.7</cell><cell>47.7</cell><cell>27.2</cell><cell>43.3</cell></row><row><cell>FCN [28]</cell><cell>58.8</cell><cell>38.5</cell><cell>26.1</cell><cell>43.9</cell></row><row><cell>Ours (superpixel+)</cell><cell>62.5</cell><cell>40.8</cell><cell>29.4</cell><cell>47.8</cell></row><row><cell>Ours (full model)</cell><cell>65.5</cell><cell>41.2</cell><cell>32.9</cell><cell>51.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>The performance of oracle case using groundtruth to label the regions.</figDesc><table><row><cell>Groundtruth</cell><cell>Pixel Acc.</cell><cell>Mean Acc.</cell><cell>Mean IoU</cell><cell>f. w. IoU</cell></row><row><cell>Current Frame</cell><cell>96.2</cell><cell>94.0</cell><cell>90.2</cell><cell>92.7</cell></row><row><cell>Next Frame</cell><cell>84.7</cell><cell>76.2</cell><cell>63.4</cell><cell>74.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>, table changes to desk, table changesto dresser, floor changes to floor mat, bookshelf changes to shelves, cabinet changes to other-furniture, and window changes to blinds. Consequently, we see mistakes in the last two rows corresponding to the best case results due to inconsistent labelings.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>table desk table floor dresser window blinds shelves floor mat bookshelf other furniture cabinet</head><label>desk</label><figDesc></figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For<ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b11">12]</ref>, we copy the performance from their paper. For<ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b4">5]</ref>, we run the code provided by the authors with RGB+HHA images. Specifically, for<ref type="bibr" target="#b12">[13]</ref>, we also increase the maximum number of superpixels from 1000 to 3000. The original coarse version and the fine version are abbreviated as BI(1000) and BI(3000).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Figure 10: Semantic segmentation results of 13-class task on NYUDv2.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by the German Research Foundation (DFG CRC 1223) and the ERC Starting Grant VideoLearn.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantic segmentation using regions and parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Higher order conditional random fields in deep neural networks. In ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scene labeling with lstm recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Byeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Raue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Region-based semantic segmentation with end-to-end training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Indoor semantic segmentation using depth information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional feature masking for joint object and stuff segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semantic segmentation of rgbd images with mutex constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Jan</forename><surname>Latecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Structured forests for fast edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Superpixel convolutional networks using bilateral inceptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A unified video segmentation benchmark: Annotation, metrics and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Galasso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Cardenas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient hierarchical graph based video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Perceptual organization and recognition of indoor scenes from rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning rich features from rgb-d images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dense 3d semantic mapping of indoor scenes from rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Floros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Bayesian segnet: Model uncertainty in deep convolutional encoder-decoder architectures for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vijay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02680</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Feature space optimization for semantic video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Lstmcf: Unifying context modeling and fusion with lstms for rgbd scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04579</idno>
		<title level="m">Parsenet: Looking wider to see better</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semantic image segmentation via deep parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Semanticfusion: Dense 3d semantic mapping with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mccormac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leutenegger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.05130</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Can ground truth label propagation from video help semantic segmentation?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Mustikovela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop on Video Segmentation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Nathan Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Epicflow: Edge-preserving interpolation of correspondences for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive modeling</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dag-recurrent neural networks for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dense real-time mapping of object-class semantics from rgb-d video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>St?ckler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Waldvogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Real-Time Image Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multimodal unsupervised feature learning for rgb-d scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Cham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning common and specific features for rgb-d semantic segmentation with deconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Sun3d: A database of big spaces reconstructed using sfm and object labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning Deep Features for Discriminative Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
