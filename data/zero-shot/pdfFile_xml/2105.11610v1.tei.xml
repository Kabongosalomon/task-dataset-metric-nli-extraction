<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Scale-consistent Depth Learning from Video</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Wang</forename><surname>Bian</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huangying</forename><surname>Zhan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Le Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
						</author>
						<title level="a" type="main">Unsupervised Scale-consistent Depth Learning from Video</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Received: 7 September 2020 / Accepted: 24 May 2021</note>
					<note>International Journal of Computer Vision manuscript No. (will be inserted by the editor)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Unsupervised Depth Estimation</term>
					<term>Scale Consistency</term>
					<term>Visual SLAM</term>
					<term>Pseudo-RGBD SLAM</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a monocular depth estimator SC-Depth, which requires only unlabelled videos for training and enables the scale-consistent prediction at inference time. Our contributions include: (i) we propose a geometry consistency loss, which penalizes the inconsistency of predicted depths between adjacent views; (ii) we propose a self-discovered mask to automatically localize moving objects that violate the underlying static scene assumption and cause noisy signals during training; (iii) we demonstrate the efficacy of each component with a detailed ablation study and show high-quality depth estimation results in both KITTI and NYUv2 datasets. Moreover, thanks to the capability of scale-consistent prediction, we show that our monocular-trained deep networks are readily integrated into ORB-SLAM2 system for more robust and accurate tracking. The proposed hybrid Pseudo-RGBD SLAM shows compelling results in KITTI, and it generalizes well to the KAIST dataset without additional training. Finally, we provide several demos for qualitative evaluation. The source code is released on GitHub.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The CNN-based monocular depth estimation <ref type="bibr" target="#b16">(Eigen et al. 2014</ref>) has shown significant promise for many Computer Vision tasks. The supervised methods <ref type="bibr" target="#b19">(Fu et al. 2018;</ref><ref type="bibr" target="#b85">Yin et al. 2019</ref>) achieve high performance, while they require expensive range sensors to capture the groundtruth data for training. To this end, recent work explores unsupervised learning for monocular depth estimation, which either uses the calibrated stereo pairs <ref type="bibr" target="#b20">(Garg et al. 2016;</ref><ref type="bibr" target="#b24">Godard et al. 2017)</ref> or unlabelled videos <ref type="bibr" target="#b88">(Yin &amp; Shi 2018;</ref><ref type="bibr" target="#b94">Zhou et al. 2017)</ref> for training. In these frameworks, the color consistency between multiple views serves as the main supervision signal. Since they do not require ground truth labels, and particularly the recent method <ref type="bibr" target="#b26">(Gordon et al. 2019)</ref> showing unsupervised depth estimation can work with the unknown camera intrinsics, these methods attract a lot of interest in the Computer Vision community. In this paper, we are interested in the videobased unsupervised learning framework because it has a minimum requirement for training data.</p><p>Compared with stereo-based learning, video-based learning <ref type="bibr" target="#b94">(Zhou et al. 2017</ref>) is often more challenging due to the unknown camera motion. More importantly, due to scale ambiguity, a natural issue in monocular vision, the predicted depth by the latter has an unknown scaling to the real world. This is the so-called relative depth, as opposed to the metric depth in the previous setting. The relative depth is also widely used, e.g., ORB-SLAM <ref type="bibr" target="#b56">(Mur-Artal et al. 2015)</ref> and COLMAP <ref type="bibr" target="#b69">(Schonberger &amp; Frahm 2016)</ref> both generate results up to an unknown scale. However, one critical issue that we identify in video-based learning is that methods may generate scale-inconsistent predictions over different frames since they suffer from a perframe scale ambiguity. This does not impact the singleimage based tasks, while it is critical for video-based applications, e.g., inconsistent depths cannot be used for camera tracking in the Visual SLAM system-See <ref type="figure" target="#fig_7">Fig. 9</ref>.</p><p>In this paper, we propose an improved unsupervised learning framework for higher depth accuracy and consistency. First, we propose a geometry consistency loss (L G ) to encourage networks to predict scale-consistent depths. It explicitly penalizes the pixel-wise inconsistency of predicted depths between adjacent frames during training. It enables more effective learning and allows for more consistent predictions at inference time-See Tab. 6. Second, we propose a self-discovered mask (M s ) for handling mov- The scene is so challenging that ORB-SLAM2 (Mur-Artal &amp; Tard?s 2017) failed to initialize or quickly lost tracking after initialization, while our Pseudo-RGBD SLAM system can provide an accurate trajectory, which is consistent with the Google Map. See more details in <ref type="bibr">Sec. 5.4.</ref> ing objects during training, which violates the underlying static scene assumption. It improves the performance significantly (See Tab. 2) and does not require additional overhead since the proposed mask is simply derived from L G .</p><p>To show the benefits from scale-consistent depth prediction and demonstrate our contribution for downstream tasks, we integrate our trained networks into the ORB-SLAM2 (Mur-Artal &amp; Tard?s 2017) system for more accurate and robust tracking. The proposed hybrid Pseudo-RGBD SLAM system has distinct advantages over traditional monocular systems, including a) it starts tracking at any frame without latency; b) it enables more robust and accurate tracking with the help of predicted depths; and c) it allows for dense 3D reconstruction-See <ref type="figure">Fig. 12</ref>. We report comprehensive quantitative results and provide several demos in Sec. 5.4 for qualitative evaluation. An example is shown in <ref type="figure">Fig. 1</ref>, where we visualize the depth, point cloud, and camera trajectory generated by our method on a real-world driving video.</p><p>Our preliminary version was presented in NeurIPS 2019 <ref type="bibr" target="#b2">(Bian et al. 2019b)</ref>, where we propose an unsupervised learning framework for scale-consistent depth and pose estimation. In this paper, we i) add more technical details of our proposed method; ii) make a more clear explanation of our contribution and distinguish our method from existing methods; iii) improve our learning framework by changing network architectures and integrating effective components from related work; iv) conduct a more comprehensive evaluation and show the potential of our method to Visual SLAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Single-view depth estimation. The depth estimation problem was mainly solved by using traditional geometry based methods <ref type="bibr" target="#b23">(Geiger et al. 2011;</ref><ref type="bibr" target="#b70">Sch?nberger et al. 2016</ref>) before deep learning based methods emerged. They rely on correspondences search <ref type="bibr" target="#b1">(Bian et al. 2019a;</ref><ref type="bibr" target="#b51">Lowe 2004)</ref>, model fitting <ref type="bibr" target="#b3">(Bian et al. 2019c;</ref><ref type="bibr" target="#b90">Zhang 1998)</ref>, and multiview triangulation <ref type="bibr" target="#b29">(Hartley &amp; Zisserman 2003)</ref>. Therefore, at least two different views of the scene are required as input for computing the depth. In contrast, recent deep learning based methods leverage the expressive power of convolutional neural networks, and they are able to regress the depth from a single image only. According to the training data, we can categorize learningbased methods into four classes: First, <ref type="bibr" target="#b16">(Eigen et al. 2014)</ref> use the sensor captured depths (e.g., LiDAR or RGB-D devices) as the ground truth for training. The following work <ref type="bibr" target="#b19">(Fu et al. 2018;</ref><ref type="bibr" target="#b21">Garg et al. 2019;</ref><ref type="bibr" target="#b32">Huynh et al. 2020;</ref><ref type="bibr" target="#b48">Liu et al. 2016;</ref><ref type="bibr" target="#b85">Yin et al. 2019)</ref> proposes more advanced network architectures or learning objectives to improve the performance. These methods achieve high performance, while it is expensive to capture groundtruth data in many real-world scenes. Second, <ref type="bibr" target="#b10">(Chen et al. 2019a;</ref><ref type="bibr" target="#b46">Li et al. 2019b;</ref><ref type="bibr" target="#b47">Li &amp; Snavely 2018;</ref><ref type="bibr" target="#b77">Wang et al. 2019;</ref><ref type="bibr" target="#b81">Xian et al. 2018;</ref><ref type="bibr" target="#b86">Yin et al. 2020</ref>) collect stereo images or videos from the web and use off-the-shelf tools (e.g., stereo matching <ref type="bibr" target="#b31">(Hirschmuller 2005)</ref> or multi-view stereo <ref type="bibr" target="#b70">(Sch?nberger et al. 2016)</ref>) to compute dense groundtruth depths. Besides, <ref type="bibr" target="#b64">(Ranftl et al. 2020</ref>) export perfect depths from the synthetic 3D movies <ref type="bibr" target="#b6">(Butler et al. 2012)</ref>. Although these methods can obtain cheap ground-truth data, there often exists a domain gap between the collected data and the desired scenes. More importantly, the learned scale information is hard to generalize across different scenes so that they often predict the relative depth. This prevents them from predicting consistent depths on a video. Third, <ref type="bibr" target="#b20">(Garg et al. 2016</ref>) use the calibrated stereo images for training models, where they warp images using the predicted depth with the known camera baseline and use the photometric loss to penalize the warping error. Then <ref type="bibr" target="#b24">(Godard et al. 2017</ref>) exploit the left-right consistency in image pairs, and <ref type="bibr" target="#b89">(Zhan et al. 2018</ref>) exploit the temporary consistency in videos. <ref type="bibr" target="#b61">(Pilzer et al. 2018</ref>) leverage adversarial learning, and <ref type="bibr" target="#b62">(Poggi et al. 2020</ref>) study the uncertainty of predicted depths. These methods can predict the metric depth, while it requires well-calibrated stereo cameras to collect training data. Fourth, <ref type="bibr" target="#b94">(Zhou et al. 2017)</ref> train models from unlabelled videos, where they jointly train the depth and pose networks using adjacent frames with photometric loss and differentiable warping <ref type="bibr" target="#b33">(Jaderberg et al. 2015)</ref>. Due to the simplicity and generality,, it attracts a lot of researchers' interests and inspires a series of works, including <ref type="bibr" target="#b11">(Chen et al. 2019b;</ref><ref type="bibr" target="#b25">Godard et al. 2019;</ref><ref type="bibr" target="#b26">Gordon et al. 2019;</ref><ref type="bibr">Guizilini et al. 2020a,b;</ref><ref type="bibr" target="#b37">Klingner et al. 2020;</ref><ref type="bibr" target="#b54">Mahjourian et al. 2018;</ref><ref type="bibr" target="#b65">Ranjan et al. 2019;</ref><ref type="bibr" target="#b78">Wang et al. 2018;</ref><ref type="bibr" target="#b88">Yin &amp; Shi 2018;</ref><ref type="bibr" target="#b91">Zhao et al. 2020;</ref><ref type="bibr" target="#b92">Zhou et al. 2019;</ref><ref type="bibr" target="#b96">Zou et al. 2018)</ref>. Our method falls into this category, and we target improving the depth accuracy and consistency for advancing downstream video-based tasks.</p><p>Scale consistency. It is an important problem in Visual SLAM <ref type="bibr" target="#b56">(Mur-Artal et al. 2015)</ref>, but to the best of our knowledge, we are the first ones to discuss the scale inconsistency behind unsupervised video-based depth learning. Nevertheless, we find that our proposed geometry consistency loss is technically similar to two previous methods. First, <ref type="bibr" target="#b54">(Mahjourian et al. 2018</ref>) propose a 3D ICP loss to penalize the misalignment of predicted depths, where they approximate gradients for depth and pose networks independently because the ICP is not differentiable. This ignores second-order effects between depth and pose networks, and hence it limits the performance. By contrast, our geometry consistency loss is naturally differentiable and results in better performance. Second, <ref type="bibr" target="#b96">(Zou et al. 2018</ref>) propose a depth consistency loss, which enforces corresponding points in two images to have identical depth predictions. This is physically incorrect because the scene depth is view-dependent, i.e., it should be different in different views. We instead synthesize the depth for the second view using the predicted depth in the first view via rigid transformation, and we penalize the difference between predicted depths and synthesized depths in the second view. Not only does our approach improve the depth accuracy, but also it enables scaleconsistent depth prediction for advancing video-based applications such Visual SLAM (Mur-Artal &amp; Tard?s 2017). After the publication of our conference paper, we notice that more recent works pay attention to consistent depth prediction, including <ref type="bibr" target="#b53">(Luo et al. 2020;</ref><ref type="bibr" target="#b75">Tiwari et al. 2020;</ref><ref type="bibr" target="#b91">Zhao et al. 2020;</ref><ref type="bibr" target="#b95">Zou et al. 2020)</ref>.</p><p>Moving objects. As the moving objects violate the underlying static world assumption for learning depths, related work often detects dynamic regions and masks them out when computing the photometric loss. <ref type="bibr" target="#b94">(Zhou et al. 2017)</ref> predict a mask from a pair of images by using the neural network. However, due to lacking effective supervision signals, the performance is limited. <ref type="bibr" target="#b76">(Vijayanarasimhan et al. 2017</ref>) learn a moving object mask from synthetic data <ref type="bibr" target="#b55">(Menze &amp; Geiger 2015)</ref>, which is often hard to generalize to real-world scenes. <ref type="bibr" target="#b11">(Chen et al. 2019b;</ref><ref type="bibr" target="#b65">Ranjan et al. 2019;</ref><ref type="bibr" target="#b88">Yin &amp; Shi 2018;</ref><ref type="bibr" target="#b96">Zou et al. 2018</ref>) additionally train an optical flow network and compare the optical flow with depth-based mapping for detecting moving objects. This is effective, but training an optical flow network is time-consuming due to the complex correlation computation. <ref type="bibr">(Casser et al. 2019a,b;</ref><ref type="bibr" target="#b26">Gordon et al. 2019;</ref><ref type="bibr" target="#b28">Guizilini et al. 2020b;</ref><ref type="bibr" target="#b32">Huynh et al. 2020)</ref> leverage the semantic information for localizing dynamic objects. They either require the pretrained semantic segmentation network or need the manually labelled class labels for multitask training. <ref type="bibr" target="#b25">(Godard et al. 2019</ref>) mask out the moving objects that have the same velocity as the camera, while it cannot handle other object motions. Compared with previous methods, our method does not require semantic inputs and does not require training additional networks. Our proposed mask is analytically derived from the geometry consistency loss, and it is able to handle arbitrary object motions and occlusions. After ours, <ref type="bibr" target="#b41">(Li et al. 2020)</ref> propose to learn the dense 3D translation field of objects relative to the scene by using the neural network, which is also efficient and effective.</p><p>Depth estimation for Visual SLAM. Traditional methods use either feature matching <ref type="bibr" target="#b23">(Geiger et al. 2011;</ref><ref type="bibr" target="#b36">Klein &amp; Murray 2007)</ref> or direct image alignment <ref type="bibr" target="#b17">(Engel et al. 2017;</ref><ref type="bibr" target="#b18">Forster et al. 2014)</ref> for camera tracking and mapping. Recently, <ref type="bibr" target="#b87">(Yin et al. 2017</ref>) use a supervised depth estimation model to help recover the absolute scale for monocular methods. CNN-SLAM <ref type="bibr" target="#b74">(Tateno et al. 2017)</ref> uses the depth estimation network within a monocular SLAM system for dense reconstruction. CodeSLAM <ref type="bibr" target="#b5">(Bloesch et al. 2018</ref>) jointly optimizes the depth and pose via a learned latent code. Although promising results are reported, these methods rely on supervised training, which is not always available in real-world scenarios. UndeepVO  and <ref type="bibr" target="#b89">(Zhan et al. 2018</ref>) train depth and pose networks on the calibrated stereo videos using the photometric loss, and they show that the learned pose network can inference on monocular videos like a visual odometer. CNN-SVO <ref type="bibr" target="#b49">(Loo et al. 2019</ref>) combines the stereo-learned depth network and SVO <ref type="bibr" target="#b18">(Forster et al. 2014</ref>) for more accurate trajectory estimation. DVSO <ref type="bibr" target="#b83">(Yang et al. 2018a)</ref> and D3VO <ref type="bibr" target="#b82">(Yang et al. 2020</ref>) also train depth models on stereo videos, and they further conduct geometric optimization. Note that all the aforementioned methods do not suffer from the scale ambiguity issue, as opposed to ours, because they can recover the metric depth. In this paper, we show that the monocular-trained model can predict the scale-consistent results, and it can be used for visual odometry. After ours, <ref type="bibr" target="#b95">(Zou et al. 2020)</ref> propose to model the long-term dependency by using a twolayer convolutional LSTM module, which improves the pose prediction accuracy significantly. However, the pure learning-based methods are easy to overfit, and we believe that combing deep learning and geometry-based methods is a more promising direction. As a result, our hybrid system generalizes well to the previously unseen dataset and to our self-captured videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SC-Depth</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Framework Overview</head><p>Our goal is to train depth and pose CNNs from unlabeled videos. Given two adjacent frames (I a , I b ) randomly sampled from a video, their depth maps (D a , D b ) and relative 6-DoF camera pose P ab are first estimated by the depth and pose CNNs, respectively. With the predicted depth and pose, we can synthesize the reference image I a using the source image I b by differentiable warping <ref type="bibr" target="#b33">(Jaderberg et al. 2015)</ref>, which generates I a . Then the network is supervised by the photometric loss between the real I a and the synthesized I a . To explicitly constrain the depth CNN to predict scale-consistent results on adjacent frames, we propose a geometry consistency loss L G . To handle invalid cases such as static frames and dynamic objects, we introduce two masks. First, a self-discovered mask M s (Eqn. 7) is introduced to reason the dynamics and occlusions by checking the depth consistency. <ref type="figure">Fig. 2</ref> illustrates the proposed loss and mask. Second, we use the automask (M a ) <ref type="bibr" target="#b25">(Godard et al. 2019)</ref> to remove stationary points on image pairs where the camera is not moving.</p><p>Our objective function is formulated as follows:</p><formula xml:id="formula_0">L = ?L M P + ?L S + ?L G ,<label>(1)</label></formula><p>where L M P stands for the photometric loss L P weighted by the proposed M s . L S stands for the smoothness loss, and L G is the geometric consistency loss. <ref type="bibr">[?, ?, ?]</ref> are the loss weighting terms. The loss is averaged over valid points, which are determined by M a . In the following sections, we first introduce the photometric loss and smoothness loss in Sec. 3.2, then we describe the proposed geometric consistency loss L G in Sec. 3.3 and the self-discovered mask M s in Sec. 3.4.1, and finally, we elaborate the automask M a in Sec. 3.4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Photometric and Smoothness Loss</head><p>Leveraging brightness constancy and spatial smoothness priors is ubiquitous in classical dense correspondence algorithms <ref type="bibr" target="#b0">(Baker &amp; Matthews 2004)</ref>. Previous works <ref type="bibr" target="#b65">(Ranjan et al. 2019;</ref><ref type="bibr" target="#b88">Yin &amp; Shi 2018;</ref><ref type="bibr" target="#b94">Zhou et al. 2017</ref>) have used the photometric loss between the warped frame and the reference frame as an unsupervised loss function for network training. With the predicted depth D a and pose P ab , we synthesize I a by warping I b , where differentiable warping <ref type="bibr" target="#b33">(Jaderberg et al. 2015)</ref> is used. With the synthesized I a and the reference image I a , we formulate the objective function as</p><formula xml:id="formula_1">L P = 1 |V| p?V (? I a (p)?I a (p) 1 +(1??) 1 ? SSIM aa (p) 2 ),<label>(2)</label></formula><p>where V stands for the set of valid points that are successfully projected from I a to the image plane of I b , and p stands for a generic point in V. We choose L 1 loss due to its robustness to outliers. Besides, SSIM aa stands for the element-wise similarity between I a and I a by the SSIM function <ref type="bibr" target="#b80">(Wang et al. 2004)</ref>. This is used to better handle complex illumination changes since it normalizes the pixel illumination. More specifically,</p><formula xml:id="formula_2">SSIM (x,y) = (2? x ? y + C 1 )(2? xy + C 2 ) (? 2 x + ? 2 y + C 1 )(? 2 x + ? 2 y + C 2 ) ,<label>(3)</label></formula><p>where x, y stands for two 3 by 3 patches around the central pixel. C 1 and C 2 are constants. ? and ? are local statistics of the image color, i.e., mean and variance, respectively. Following <ref type="bibr" target="#b24">(Godard et al. 2017;</ref><ref type="bibr" target="#b65">Ranjan et al. 2019;</ref><ref type="bibr" target="#b88">Yin &amp; Shi 2018)</ref>, we use C 1 = 0.0001, C 2 = 0.0009, and ? = 0.15. As the photometric loss is not informative in lowtexture regions of the scene, existing work also incorporates a smoothness prior to regularize the estimated depth map. We adopt the edge-aware smoothness loss used in <ref type="bibr" target="#b65">(Ranjan et al. 2019)</ref>. Formally,</p><formula xml:id="formula_3">L S = p (e ??Ia(p) ? ?D a (p)) 2 ,<label>(4)</label></formula><p>where ? is the first derivative along spatial directions. It ensures smoothness to be guided by image edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Geometry Consistency Loss</head><p>To explicitly enforce geometry consistency, we constrain that the predicted D a and D b (related by P ab ) conform the same 3D structure by penalizing their inconsistency. Specifically, we propose a differentiable depth inconsistency operation to compute the pixel-wise inconsistency between two depth maps, as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. Here, D a b is the synthesized depth for I b , which is generated by D a and pose P ab with the underlying rigid transformation. D b is an interpolation of D b for aligning and comparing with D a b . Given them, we compute the depth inconsistency map D diff for each p ? V as:</p><formula xml:id="formula_4">D diff (p) = |D a b (p) ? D b (p)| D a b (p) + D b (p) ,<label>(5)</label></formula><p>where we normalize depth differences by their summation. This works better than the absolute distance in practice as it treats points at different absolute depths equally in optimization. Besides, the function is symmetric, and the outputs are naturally ranging from 0 to 1, which makes the training more stable. Illustration of the proposed geometry consistency loss and self-discover mask. Given two consecutive frames (Ia, I b ), we first estimate their depth maps (Da, D b ) and relative pose (P ab ) using the network. Then we compute the D diff (Eqn. 5), i.e., pixel-wise depth inconsistency between Da and D b . Finally, we derive our geometric consistency loss L G (Eqn. 6) and self-discovered mask Ms (Eqn. 7) from D diff to regularize the network training and hanlding dynamics and occlusions ( <ref type="figure" target="#fig_3">Fig. 4</ref>). For clarity, the photometric loss and smoothness loss are not shown in this figure. Firstly, we project Da to 3D space and then to the image plane of D b using P ab , obtaining the D a b that stands for the synthesized D b . Then, we hope to compute the difference between D a b and D b . However, it is not practical because the projection does not religiously lie in the grid of D b . Therefore, we obtain the D b by using the differentiable bilinear interpolation <ref type="bibr" target="#b33">(Jaderberg et al. 2015)</ref>. Finally, we compare D a b with D b to obtain the depth inconsistency (D diff ). Here, we use the relative loss (Eqn. 5), although other loss functions such as L1 and L2 are also applicable.</p><p>With the inconsistency map, we define the proposed geometry consistency loss as:</p><formula xml:id="formula_5">L G = 1 |V| p?V D diff (p),<label>(6)</label></formula><p>which minimizes the geometric inconsistency of predicted depths over two views. By minimizing the depth inconsistency between samples in a batch, we naturally propagate such consistency to the entire sequence: the depth of I 1 agrees with the depth of I 2 in a batch; the depth of I 2 agrees with the depth of I 3 in another training batch. Eventually, depths of I i of a sequence should all agree with each other, leading to scale-consistent results over the entire sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Masking Scheme</head><p>The assumption of a moving camera and a static scene is underlying in the unsupervised depth learning framework, where the moving objects in the scene and image pairs with identity camera pose provide invalid signals. To be specific, the moving objects create the non-rigid flow that cannot be represented by the depth-based mapping, and the static camera consistently creates the identical flow that is independent to the depth prediction. Therefore, we propose to mask out these regions by introducing a self-discovered mask (M s ) and adopting the auto-mask (M a ) by Monodepth2 <ref type="bibr" target="#b25">(Godard et al. 2019</ref>). The proposed M s computes weights (ranging from 0 to 1) for points in V by checking their depth consistency, and the M a simply removes invalid points from V. The proposed two masks are readily integrated into the proposed learning framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Self Discovered Mask</head><p>As moving objects and occlusions naturally violate the geometry consistency assumption, they will cause large depth inconsistency in our pre-computed D diff (Eqn. 5). This encourages us to define the M s as:</p><formula xml:id="formula_6">M s = 1 ? D diff ,<label>(7)</label></formula><p>where the M s is in [0, 1] and it attentively assign low weights for geometrically inconsistent pixels and high weights for consistent pixels. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Auto-Mask</head><p>To remove the invalid points in static pairs, e.g., two images are captured at the same position, we use the automask M a that is proposed in <ref type="bibr" target="#b25">(Godard et al. 2019)</ref>. It compares the photometric losses between the mapping by depth and pose and the identity mapping, and it removes the points where the identity mapping leads to a lower loss. Formally, for each p ? V, we have</p><formula xml:id="formula_7">M a (p) = 1 if I a (p) ? I a (p) 1 &lt; I a (p) ? I b (p) 1 , 0 otherwise.<label>(8)</label></formula><p>Here M a is a binary mask for each point in V (valid points), and I a is the warped image from the source image I b using the estimated depth and pose. It makes the network to ignore objects that move at the same velocity as the camera, and it even ignores whole frames when the relative pose is identity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">How to use masks</head><p>First, to use M a in our loss function, we remove invalid points in V that have M a (p) = 0. When training the network, we only compute losses on the remaining valid points. Second, we use the proposed M s to re-weight the photometric loss in Eqn. 2 by:</p><formula xml:id="formula_8">L M P = 1 |V| p?V (M s (p) ? L P (p)).<label>(9)</label></formula><p>This mitigates the noisy signals caused by moving objects and occlusions. <ref type="figure" target="#fig_3">Fig. 4</ref> shows visual results for the two types of masks, which coincides with our anticipation. The dark regions in M s correspond to moving objects that violate the static scene assumption, e.g., the car region and human ride region. In the binary M a , black regions correspond to pixels that have similar speed with the camera, e.g., the moving vehicle in the left example and the static scene in the right example. Tab. 2 shows the ablation study results, which shows that the proposed masks results in a significant performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Pseudo-RGBD SLAM</head><p>In this section, we present a Pseudo-RGBD SLAM system, which is based on our trained models and existing SLAM systems. We overview the system pipeline in Sec. 4.1, followed by elaborating each component in Sec. 4.2, and finally, we discuss the advantages and limitations of the proposed system in Sec. 4.3. <ref type="figure">Fig. 5</ref> shows an overview of the proposed method, which is composed of our SC-Depth, ORB-SLAM2 (Mur-Artal &amp; Tard?s 2017), and InfiniTAMv3 <ref type="bibr" target="#b63">(Prisacariu et al. 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">System Overview</head><p>The whole system takes a monocular RGB video as input and outputs a globally consistent 6-DoF camera trajectory and sparse/dense 3D maps. First, we initialize the tracking and mapping by using the predicted depth on the starting frame I 0 , which creates an initial 3D map. Second, for a new frame I t , we estimate its depth and relative pose to the previous view I t?1 using our trained networks. As the camera pose of I t?1 has been known from previous tracking or initialization, we can obtain the pose estimate for the current view by accumulation. Third, we feed the color image, depth map, and the estimated pose for I t as input into ORB-SLAM2 (Mur-Artal &amp; Tard?s 2017), which performs matching and optimization, resulting in an optimized camera pose as well as an increased map. In such an incremental way, we eventually obtain a globally consistent camera trajectory and a sparse 3D map from the video. Finally, we feed the color images, depth maps, and camera trajectories into Infini-</p><formula xml:id="formula_9">Concat Depth Net Pose Net ? ? P t-1 P t ORB-SLAM2 (RGB-D) Optimized P t InfiniTAM v3</formula><p>Dense 3D Model Sparse 3D Model <ref type="figure">Fig. 5</ref> Pipeline of Pseudo-RGBD SLAM. For the current frame It, we first estimate its depth Dt using our trained depth CNN. Then, we estimate its relative pose to previous frame I t?1 (its pose P t?1 has been known in previous tracking) to recover the current pose. Next, we feed the color images, predicted depths, and estimated poses into ORB-SLAM2 (Mur-Artal &amp; Tard?s 2017), which outputs the accurate camera trajectory and a sparse 3D map. Finally, given the consistent depth and camera trajectory, we construct the dense voxel representation using InfiniTAMv3 <ref type="bibr" target="#b63">(Prisacariu et al. 2017)</ref>. Note that the dense reconstruction here is only used for qualitative demonstration.</p><p>TAMv3 <ref type="bibr" target="#b63">(Prisacariu et al. 2017)</ref>, which fuses depth maps to construct the dense and textured voxel volumes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">System Details</head><p>ORB-SLAM2. The original RGB-D system takes the sensor captured depth as input, while we use the estimated depth. It relies on ORB features <ref type="bibr" target="#b67">(Rublee et al. 2011</ref>) to generate correspondences, and it minimizes the reprojection error for pose optimization. Poor correspondences (beyond the error threshold) are detected and removed as outliers, and the remaining correspondences are used for all sub-tasks, including tracking, mapping, loop closing, and re-localization. We will elaborate on how our predicted depth and pose influence the correspondence and optimization in the system.</p><p>Depth. The predicted depths are used to initialize a 3D map at the beginning, and they are used in the objective function during optimization. Specifically, beyond 2d reprojection error, the system also minimizes the difference between the projected depth (from the 3D map to the image) and the predicted depth. Formally,</p><formula xml:id="formula_10">? ? ? E 2D = (p x ? p x ) 2 + (p y ? p y ) 2 E 3D = (p x ? p x ) 2 + (p y ? p y ) 2 + (p d ? p d ) 2 ,<label>(10)</label></formula><p>where p stands for points in current image plane, and p stands for points projected from 3D map. p d and p d are their disparities, i.e., inverse depths. Note that p d is computed from our predicted depth map, so it is unavailable in the monocular system. This extends the reprojection error from 2D into 3D, which greatly improves the performance. Consequently, the consistency of estimated depths is vital in tracking. For example, inconsistent depths would increase the reprojection error, and correct matches would be wrongly removed as outliers, which causes the system to fail-See <ref type="figure" target="#fig_7">Fig. 9</ref>.</p><p>Pose. The predicted pose is used as the initial pose during tracking, in which the system first projects the sparse keypoints in a 3D map to the live view using the estimated pose and then searches for correspondences in the neighboring regions. The camera pose is optimized through the Bundle Adjustment (Mur-Artal &amp; Tard?s 2017). After tracking, we enrich the 3D map by unprojecting the keypoints detected in the live view to the map using the optimized camera pose. The original ORB-SLAM2 uses the constant velocity motion model for initial pose, which simply assumes that the camera motion is the same as the previous frame. Formally,</p><formula xml:id="formula_11">T t?t+1 = T t?1?t ORB-SLAM2, PoseNet(I t , I t+1 ) Ours,<label>(11)</label></formula><p>where T stands for relative pose. However, this assumption is often violated in real scenarios, such as abrupt motion in driving scenes. Though these frames are few in the sequence, they usually contribute the most of the drift in the final evaluation. Our trained pose CNN has the potential to cope with these cases.</p><p>InfiniTAMv3. It takes RGB-D videos and can densely reconstruct the scene structure. We disable the internal tracking module and use our optimized camera poses and predicted depths for reconstruction. This is only used for visualization purposes, and it is also a demonstration of our consistent results. Note that the dense reconstruction is very sensitive to geometry consistency, i.e., it will crash when depths are not sufficiently consistent. <ref type="figure">Fig. 11</ref> shows the screenshot of our demo, which can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discussion</head><p>Our proposed SLAM system leverages the advantage of deep learning, and it optimizes the predicted poses in the multi-view geometry-based framework. This has distinctive advantages over existing solutions.</p><p>Advantages. Compared with classical monocular SLAM systems such as ORB-SLAM2 (Mur-Artal &amp; Tard?s 2017), our advantages include:</p><p>1. ORB-SLAM2 is often hard to initialize because it requires the multi-view triangulation, while our method can initialize at any time without latency by using the estimated dense depth-See <ref type="figure" target="#fig_7">Fig. 9</ref>. 2. ORB-SLAM2 often loses tracking when the 3D map is over-sparse, while our method is more robust because we can enrich the map by using the predicted dense depth-See <ref type="figure" target="#fig_7">Fig. 9</ref>. 3. ORB-SLAM2 can only provide a sparse map, while our method enables dense 3D reconstruction by using the predicted dense depth-See <ref type="figure">Fig. 12</ref>.</p><p>Compared with learning-based methods ), our advantage is the post geometric optimization e.g., Loop Closing (Mur-Artal &amp; Tard?s 2014), which can effectively correct drifts and improve the performance, as shown in <ref type="figure" target="#fig_6">Fig. 8</ref>.</p><p>Limitations. Our method cannot recover the absolute scale because only monocular videos are used. However, in realworld applications, the metric scale can be recovered by using other sensors and cues, like IMU and road landmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implementation details</head><p>Network architecture. Our depth network takes a single RGB image as input and outputs an inverse depth map. It is a U-Net structure <ref type="bibr" target="#b66">(Ronneberger et al. 2015)</ref>, and we use the ResNet50 <ref type="bibr" target="#b30">(He et al. 2016)</ref> encoder to extract features. The decoder is the DispNet as used in <ref type="bibr" target="#b94">(Zhou et al. 2017</ref> Single scale supervision. Previous methods compute the losses on an image pyramid, i.e., usually four layers. They either work on the decoder's side outputs <ref type="bibr" target="#b88">(Yin &amp; Shi 2018;</ref><ref type="bibr" target="#b94">Zhou et al. 2017;</ref><ref type="bibr" target="#b96">Zou et al. 2018)</ref> or upsample them to the original image resolution <ref type="bibr" target="#b25">(Godard et al. 2019</ref>). However, it introduces great computational overhead in training. By contrast, we only compute the loss on the original image resolution. This has a less computational cost and achieves on par performance with the multi-scale solution in MonoDepth2 <ref type="bibr" target="#b25">(Godard et al. 2019)</ref>, as shown in Tab. 2. The motivation is that we empirically find that the supervision on low-resolution images is inaccurate, and the camera movement between training image pairs is small so that the multi-scale solution is unnecessary.</p><p>Training details. We implement the proposed method using the PyTorch <ref type="bibr" target="#b59">(Paszke et al. 2017)</ref>. Following <ref type="bibr" target="#b65">(Ranjan et al. 2019;</ref><ref type="bibr" target="#b78">Wang et al. 2018;</ref><ref type="bibr" target="#b94">Zhou et al. 2017</ref>), we use a snippet of three sequential video frames as a training sample. We compute the projection and losses from the second frame to others and reverse them again for maximizing the data usage. The images are augmented with random scaling, cropping, and horizontal flips during training. We use ADAM (Kingma &amp; Ba 2014) optimizer and set the learning rate to be 10 ?4 . During training, we set ? = 1.0, ? = 0.1, and ? = 0.5 in Eqn. 1. For fast convergence, we initialize the encoder of our networks by using the pre-trained model on ImageNet <ref type="bibr" target="#b14">(Deng et al. 2009</ref>).</p><p>Datasets. For depth estimation evaluation, we use both the KITTI <ref type="bibr" target="#b22">(Geiger et al. 2013</ref>) and NYUv2 <ref type="bibr" target="#b72">(Silberman et al. 2012)</ref> datasets. In KITTI, we use the same training/testing split as in <ref type="bibr" target="#b94">(Zhou et al. 2017)</ref>. The 697 images are used for testing. We train the network for 200K iterations, where we set the batch size to be 4 and resize images to 832?256 resolution for training. In NYUv2, we use the officially provided 654 densely labeled images for testing, and use the rest sequences (no overlapping with the testing scenes) for training. We extract one frame from every 10 frame in the original video to remove redundant frames, and we resize images to 320 ? 256 resolution as input of the network. We train models for 50 epochs, and the batch size is 8. For visual odometry evaluation, we use the KITTI odometry dataset (Seq. 00-08) for training, and we test our method on the Seq. 09-10. Moreover, we use the KAIST urban dataset <ref type="bibr" target="#b34">(Jeong et al. 2019)</ref> to validate the zero-shot generalization ability of our method. We use one of the hardest scenes (urban39pankyo), which contains more than 18000 street-view images, and we split it into 9 sequences with each sequence containing 2000 images for testing.</p><p>Evaluation metrics. For depth evaluation, following previous methods <ref type="bibr" target="#b85">(Yin et al. 2019;</ref><ref type="bibr" target="#b94">Zhou et al. 2017)</ref>, we use the mean absolute relative error (AbsRel), mean log10 error (Log10), root mean squared error (RMS), root mean squared log error (RMSlog), and the accuracy under threshold (? i &lt; 1.25 i , i = 1, 2, 3). As unsupervised methods cannot recover the absolute scale, we multiply the predicted depth maps by a scalar that matches the median with  <ref type="bibr" target="#b22">(Geiger et al. 2013</ref>   ation, we follow the standard evaluation metrics, including the translational (t err ) and rotational errors (r err ) averaged over the entire sequence <ref type="bibr" target="#b22">(Geiger et al. 2013)</ref>, and the absolute trajectory error (ATE) <ref type="bibr" target="#b73">(Sturm et al. 2012)</ref>. Single-view depth estimation results on NYUv2 <ref type="bibr" target="#b72">(Silberman et al. 2012)</ref>. Legends: D-depth supervision; M-unsupervised training using monocular snippets; F-joint learning with the optical flow; WR-weak rectification <ref type="bibr" target="#b4">(Bian et al. 2020</ref>) which preprocesses the hand-held camera captured videos for better training. More specifically, <ref type="bibr" target="#b4">(Bian et al. 2020)</ref> remove the relative rotation between training pairs since they find that it is hard for the pose network to learn image rotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Resolution Supervision</head><p>Error ? Accuracy ? AbsRel Log10 RMS ? 1 ? 2 ? 3 Make3D <ref type="bibr" target="#b68">(Saxena et al. 2006</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Depth Estimation</head><p>Results on KITTI. Tab. 1 shows the results, which shows that the supervised methods <ref type="bibr" target="#b19">(Fu et al. 2018;</ref><ref type="bibr" target="#b85">Yin et al. 2019</ref>) are best-performing, followed by the stereo trained models <ref type="bibr" target="#b82">(Yang et al. 2020</ref>). Besides, it shows that learning with semantic labels <ref type="bibr" target="#b28">(Guizilini et al. 2020b)</ref> or optical flow  can effectively improve the performance of monocular methods. We are here more interested in the monocular methods that do not use additional information. In this category, our method outperforms previous methods (before 2020), and it shows on par performance with the MonoDepth2 <ref type="bibr" target="#b25">(Godard et al. 2019</ref>). However, we argue that our advantage against Monodepth2 is the depth consistency (Tab. 6), which has important implications on downstream video-based tasks. For example, contributed to the consistent depth prediction, our method can be readily plugged into the Visual SLAM systems, while the Monodepth2 is unable-See <ref type="figure" target="#fig_7">Fig. 9</ref> for detailed analysis.</p><p>Efficacy of the proposed methods. Multi-scale supervision. Tab. 2 shows the results of our method with the modified multi-scale solution proposed in <ref type="bibr" target="#b25">(Godard et al. 2019)</ref>. It upsamples the predicted four depth maps to original image resolution and then computes losses instead of downsampling the original color image <ref type="bibr" target="#b94">(Zhou et al. 2017)</ref>. The result demonstrates that our method could hardly benefit from that, and it requires two times longer time for training. Therefore, we use single-scale supervision in our framework.</p><p>SSIM vs NCC. Tab. 2 shows the results of our method with the normalized cross-correlation (NCC) loss, in which we replace the SSIM. Both losses compute the local image similarity on a 3 by 3 patch. The results show that SSIM leads to better performance than NCC in our unsupervised learning framework.  <ref type="bibr" target="#b22">(Geiger et al. 2013)</ref>. S/M stands for training on stereo/monocular videos, and G stands for geometric optimization. stands for failure in initialization or tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Types Seq. 09 Seq. 10 t err (%)r err ( ? /100m)ATE(m)t err (%)r err ( ? /100m)ATE(m) Depth-VO-Feat <ref type="bibr" target="#b89">(Zhan et al. 2018</ref>  Results on NYUv2. Tab. 4 shows the results, which shows that our method outperforms previous unsupervised methods by a large margin. Besides, following <ref type="bibr" target="#b4">(Bian et al. 2020)</ref>, we remove the relative rotation between training image pairs since they find that it is hard for the pose network to learn image rotation. This leads to a significant improvement because rotation is the dominate ego-motion in hand-held camera captured videos. ) solves the problem by replacing the Pose CNN with a traditional geometry-based pose solver. The qualitative results are shown in <ref type="figure" target="#fig_5">Fig. 7</ref>. We find that Mon-oDepth2 <ref type="bibr" target="#b25">(Godard et al. 2019</ref>) often collapses in train-ing, so we report the best result. Compared with the supervised methods, our method is inferior to the state-ofthe-art <ref type="bibr" target="#b85">(Yin et al. 2019</ref>) but outperforms many previous methods <ref type="bibr" target="#b9">(Chakrabarti et al. 2016;</ref><ref type="bibr" target="#b15">Eigen &amp; Fergus 2015;</ref><ref type="bibr" target="#b42">Li et al. 2017;</ref><ref type="bibr" target="#b48">Liu et al. 2016;</ref><ref type="bibr" target="#b68">Saxena et al. 2006;</ref><ref type="bibr" target="#b79">Wang et al. 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Visual Odometry</head><p>Comparing with deep learning based methods. Tab. 5 shows the visual odometry results on KITTI. For methods that   <ref type="table">Table 7</ref> Visual odometry results on KITTI. We evaluate the results on all frames and on keyframes that are selected by the ORB-SLAM2 since the latter cannot provide results for the full sequence due to unsuccessful initialization or tracking failure. The ATE (m) metric is used. We use 2K keypoints as default, and we analyze the effect of keypoint numbers on system performance by increasing it to 8K. train on monocular videos, we align the scale of their predicted results with the ground truth by using the 7-DoF optimization. The results show that the proposed SC-Depth outperforms the previous monocular alternatives, and it even shows on par performance with the stereo trained method ). However, it is not as good as the very recent approach <ref type="bibr" target="#b95">(Zou et al. 2020</ref>) that models the long-term geometry by using the LSTM module. Besides, the results show that the proposed Pseudo-RGBD SLAM system improves the accuracy significantly  <ref type="figure">Fig. 10</ref> Estimated trajectory on Seq. 08. ORB-SLAM2 is hard to maintain consistent scales over a long video (e.g., left is small, and right is big), while our method is able by leveraging the scale-consistent depth prediction. over our SC-Depth, which is contributed to the geometric optimization. The success of D3VO <ref type="bibr" target="#b82">(Yang et al. 2020</ref>) also confirms the importance of geometric optimization for odometry accuracy. However, note that stereo-trained methods can estimate depths at the metric scale, which are readily optimized in existing SLAM frameworks. By contrast, the monocular trained methods suffer from the scale inconsistency issue, which makes the post-optimization non-trivial-See <ref type="figure" target="#fig_7">Fig. 9</ref>. Our contribution here is enabling the monocular trained methods to predict the scale consistent results so that it allows for optimizing the predicted depths and poses by using the classical geometric frameworks. A qualitative comparison is provided in <ref type="figure" target="#fig_6">Fig. 8, which</ref> shows that the trajectory optimized by our Pseudo-RGBD SLAM is more well-aligned with the ground truth than our SC-Depth and other learning-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ORB KeyFrames All Frames</head><p>Depth consistency evaluation. We evaluate the geometry consistency of predicted depths by using the point cloud registration metric that is implemented in the Open3D library <ref type="bibr" target="#b93">(Zhou et al. 2018)</ref>. To be specific, we use the "open3d.registration.evaluate registration" function. It computes the RMSE of two aligned point clouds and recognizes the inlier correspondences by a constant threshold. <ref type="figure">Fig. 11</ref> Dense multi-view reconstruction on Seq. 09. The left column shows the reconstructed 3D voxels. The right column shows the input RGB image, estimated depth map. We use the depth CNN trained on Seq. 00-08, and the predicted depth is cropped and masked by using our proposed Ms.</p><p>Then it measures the overlapping area of point clouds by counting the ratio of inlier correspondences in all the target points. More details can be founded in the Open3D library. For a given testing sequence, we predict the depth and relative pose for every adjacent image pair, and we convert the depth into point clouds for evaluation, where all the depth maps are resized to 832 ? 256 resolution for a fair comparison. Tab. 6 shows the results, where we compare our method with Monodepth2 <ref type="bibr" target="#b25">(Godard et al. 2019)</ref>. It shows that our predicted depths are significantly more consistent than the latter, and we hypothesize this is the reason why our method can be readily plugged into the ORB-SLAM2 system while the Monodepth2 fails. We conduct a more detailed comparison by reporting the number of tracked keypoints in each frame. The results are shown in <ref type="figure" target="#fig_7">Fig. 9</ref>.</p><p>Pose network or motion model. Tab. 5 shows the results, where using the built-in motion model in ORB-SLAM or using our pose CNN for pose initialization leads to similar performance. We conjecture the reason is that the motion model is satisfied in most driving scenarios, where forward motion is dominant. However, we believe that using the pose network is a more general solution because the constant velocity model is violated when abrupt motion occurs.</p><p>Comparing with ORB-SLAM2. Tab. 7 shows the odometry results on KITTI <ref type="bibr" target="#b22">(Geiger et al. 2013)</ref>. We evaluate results on all frames and on keyframes that are selected by ORB-SLAM2 (Mur-Artal &amp; Tard?s 2017), since the latter cannot provide results for the full sequence due to unsuccessful initialization or tracking failure. The results on eleven sequences show that our method either achieves on par accuracy with the ORB-SLAM2 or significantly outperforms the latter. Besides tracking accuracy, our system is more robust than the ORB-SLAM2. A detailed comparison is provided in <ref type="figure" target="#fig_7">Fig. 9</ref>, where our method always tracks more points than the latter (e.g., about 800 vs 100). Moreover, we find that ORB-SLAM2 sometimes suffers from heavy scale drifts in long sequences-See <ref type="figure">Fig. 10</ref> where ORB-SLAM2 provides inconsistent scales between left and right parts. In this scenario, our method can maintain a consistent scale over the entire sequence by leveraging the scale-consistent depth prediction. <ref type="figure">Fig. 12</ref> Point cloud visualization on Seq. 09. For each incoming image (right 1st row), we predict the depth map (right 2nd row) using our trained network and convert it to a 3D point cloud, which is rendered using the color image and visualized in an eye-bird view (left).</p><p>Using more or less keypoints. Tab. 7 shows the ablation study results, where our system with 2K keypoints is more accurate than that with 8K keypoints. We conjecture the reason is that using more keypoints would also introduce more outliers, while the geometric optimization requires only a few accurate sparse points. We hence recommend users choosing keypoint numbers by considering the trade-off between the system accuracy and robustness ( <ref type="figure" target="#fig_7">Fig. 9</ref>).</p><p>Zero-short generalization. We validate the generalization ability of our proposed Pseudo-RGBD SLAM on KAIST urban dataset <ref type="bibr" target="#b34">(Jeong et al. 2019)</ref>, where our models are trained on KITTI. The results are reported in Tab. 8. It shows that our method consistently outperforms ORB-SLAM2, which demonstrates the robustness of our proposed system. Moreover, we demonstrate the generalization ability of our method by presenting a real-world demo-See <ref type="figure">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Qualitative Evaluation</head><p>We provide several demos in the supplementary material, which are briefly described below.</p><p>Per-frame 3D visualization. <ref type="figure">Fig. 12</ref> shows the visualization for predicted depths and textured point clouds on Seq. 09. We use the model trained on Seq. 00-08. This demo is to show that the predicted scene or object structure by our trained depth CNN is visually reasonable, and their scales are consistent over time. Note that inconsistent prediction would cause flickering videos, while it is doesn't occur in our demo.</p><p>Dense multiple view reconstruction. <ref type="figure">Fig. 11</ref> shows our dense reconstruction demo. As the depth range is wild in outdoor scenes, we have to reduce the voxel size of TSDF <ref type="bibr" target="#b13">(Curless &amp; Levoy 1996)</ref> for affording the memory requirement, which degrades the reconstruction quality. Although the reconstruction is inferior to the state-ofthe-art methods, this demo clearly demonstrates the high consistency of our estimated depths.</p><p>Generalization on real-world videos. <ref type="figure">Fig. 1</ref> shows the depth and camera trajectory generated by our method on a selfcaptured driving video. The video is captured in Adelaide, Australia. We use a single camera, which is mounted on a driving car. Due to the lack of an accurate ground truth trajectory, we use the Google map for qualitative evaluation. The scene is so challenging that ORB-SLAM2 (Mur-Artal &amp; Tard?s 2017) is unable to generate a complete trajectory, while the proposed Pseudo-RGBD SLAM performs well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper proposes a video-based unsupervised depth learning method. Thanks to the proposed geometry consistency loss and masking scheme, our trained network can predict scale-consistent and accurate depths over a video. The depth accuracy is comprehensively evaluated in both indoor and outdoor scenes, and the quantitative results are attached. Besides, we demonstrate better consistency against the related work which shows on par depth accuracy to our method, and we show that such consistency enables our method to be readily plugged into the existing Visual SLAM system. This shows the possibility of leveraging the depth network that is unsupervised trained from monocular videos for camera tracking and dense reconstruction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Predicted depth and textured point cloudStart End Car on roadside (b) Qualitative evaluation of estimated trajectory Fig. 1 Generalization on our self-captured video. The data is collected in Adelaide, an Australia city. Our depth and pose networks are trained on KITTI without additional finetuning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fig. 2 Illustration of the proposed geometry consistency loss and self-discover mask. Given two consecutive frames (Ia, I b ), we first estimate their depth maps (Da, D b ) and relative pose (P ab ) using the network. Then we compute the D diff (Eqn. 5), i.e., pixel-wise depth inconsistency between Da and D b . Finally, we derive our geometric consistency loss L G (Eqn. 6) and self-discovered mask Ms (Eqn. 7) from D diff to regularize the network training and hanlding dynamics and occlusions (Fig. 4). For clarity, the photometric loss and smoothness loss are not shown in this figure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3</head><label>3</label><figDesc>Differentiable depth inconsistency computation. This operation takes two depth maps (Da, D b ) and their relative pose (P ab ) as input and outputs the pixel-wise inconsistency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4</head><label>4</label><figDesc>Visual results of depth and masking. Top to bottom: sample image, estimated depth, self-discovered mask Ms, and automask Ma<ref type="bibr" target="#b25">(Godard et al. 2019</ref>). The proposed Ms detects dynamics and occlusions (dark regions), and the binary mask Ma finds invalid stationary points (black pixels).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6</head><label>6</label><figDesc>Qualitative comparison with the Monodepth2 (Godard et al. 2019) on KITTI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7</head><label>7</label><figDesc>Qualitative comparison with the state-of-the art unsupervised methods on NYUv2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8</head><label>8</label><figDesc>Estimated trajectory on Seq. 09 (left) and 05 (right). The results optimized by the proposed Pseudo-RGBD SLAM are more accurate than our SC-Depth and other learning-based methods, and the improvement is especially large when loops are detected and closed. For example, the terr is reduced from 5.91 to 1.67 on Seq. 05.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9</head><label>9</label><figDesc>Number of tracked keypoints on Seq. 09. We extract 2000 feature points for all methods, and the values in the figure are smoothed for visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Single-view depth estimation results on KITTI</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>). Legends: D-depth supervision; S-stereo pairs; M-monocular snippets; L-semantic labels or networks; F-joint learning with optical flow. Ablation study results on KITTI. We use the ResNet18 model, and the image resolution is 416 ? 128.</figDesc><table><row><cell>Error ?</cell><cell>Accuracy ?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>Trade-offs between image resolution, network, and speed. We train models on KITTI using a TESLA V100 GPU and test the inference speed in an RTX 2080 GPU.</figDesc><table><row><cell>image</cell><cell>monodepth2</cell><cell>ours</cell></row><row><cell>Resolution Model AbsRel ? 1 Train Infer 0.132 0.835 10h 228 fps 416 ? 128 R18 R50 0.126 0.840 16h 110 fps</cell><cell></cell><cell></cell></row><row><cell>0.119 0.863 29h 133 fps R50 0.114 0.873 37h 59 fps 832 ? 256 R18</cell><cell></cell><cell></cell></row><row><cell>that of the ground truth, as in (Zhou et al. 2017). The</cell><cell></cell><cell></cell></row><row><cell>predicted depths are capped at 80m/10m in KITTI and</cell><cell></cell><cell></cell></row><row><cell>NYUv2 datasets, respectively. For visual odometry evalu-</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Tab. 2 summarizes the results. It shows that the proposed L G makes training more stable by enforcing depth consistency, and the proposed M s can boost performance significantly by handling scene dynamics. Besides, it shows that using M</figDesc><table><row><cell>a can</cell></row><row><cell>contribute to extra marginal performance improvement</cell></row><row><cell>by removing the stationary points. Consequently, the fi-</cell></row><row><cell>nal solution (with all terms) can achieve the best per-</cell></row><row><cell>formance. Moreover, Tab. 3 shows the relation between</cell></row><row><cell>depth accuracy, training time, inference speed, network</cell></row><row><cell>architecture, and image resolution.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc>Visual odometry results on KITTI</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6</head><label>6</label><figDesc>Depth consistency results on Seq. 09.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Fitness measures</cell></row><row><cell cols="4">the overlapping area of two point clouds (# of inlier correspon-</cell></row><row><cell cols="4">dences / # of points in target). RMSE is averaged over all inlier</cell></row><row><cell cols="2">correspondences (#Corr).</cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="3">Fitness (?) RMSE (?) #Corr (?)</cell></row><row><cell>MonoDepth2</cell><cell>0.384</cell><cell>9.84e-3</cell><cell>80.776K</cell></row><row><cell>Ours (w/o L G ) Ours</cell><cell>0.663 0.689</cell><cell cols="2">8.90e-3 8.71e-3 134.956K 129.825K</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8</head><label>8</label><figDesc>Zero-short generalization on KAIST dataset<ref type="bibr" target="#b34">(Jeong et al. 2019)</ref>. We compare our method with ORB-SLAM2 using the ATE (m) metric.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">ORB KeyFrames</cell><cell>All Frames (2K)</cell></row><row><cell></cell><cell cols="4">Seq Frames ORB Ours</cell><cell>Ours</cell></row><row><cell></cell><cell>00</cell><cell>189</cell><cell cols="2">7.04 2.35</cell><cell>2.21</cell></row><row><cell></cell><cell>01</cell><cell>286</cell><cell cols="2">21.06 3.90</cell><cell>4.29</cell></row><row><cell></cell><cell>02</cell><cell>231</cell><cell cols="2">11.95 4.65</cell><cell>5.11</cell></row><row><cell></cell><cell>03</cell><cell>150</cell><cell cols="2">11.67 2.71</cell><cell>2.59</cell></row><row><cell></cell><cell>04</cell><cell>140</cell><cell cols="2">3.80 2.00</cell><cell>1.67</cell></row><row><cell></cell><cell>05</cell><cell>201</cell><cell cols="2">55.87 27.46</cell><cell>28.34</cell></row><row><cell></cell><cell>06</cell><cell cols="3">306 136.85 7.47</cell><cell>7.78</cell></row><row><cell></cell><cell>07</cell><cell>304</cell><cell cols="2">10.41 16.27</cell><cell>16.48</cell></row><row><cell></cell><cell>08</cell><cell>185</cell><cell cols="2">2.48 1.66</cell><cell>1.44</cell></row><row><cell></cell><cell>400</cell><cell>GT</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>ORB-SLAM</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">P-RGBD SLAM</cell><cell></cell></row><row><cell></cell><cell>300</cell><cell></cell><cell></cell><cell></cell></row><row><cell>z (m)</cell><cell>200</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>100</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>?400</cell><cell cols="2">?200</cell><cell>0 x (m)</cell><cell>200</cell><cell>400</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The University of Adelaide, Australia 2 Australian Centre for Robotic Vision, Australia 3 TuSimple, China 4 Agency for Science, Technology and Research, Singapore 5 TKLNDST, CS, Nankai University, China</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was in part supported by the Australian Centre of Excellence for Robotic Vision CE140100016, and the ARC Laureate Fellowship FL130100102 to Prof. Ian Reid. This work was supported by Major Project for New Generation of AI (No. 2018AAA0100403), Tianjin Natural Science Foundation (No. 18JCYBJC41300 and No. 18ZXZNGX00110), and NSFC (61922046) to Prof. Ming-Ming Cheng. We also thank anonymous reviewers for their valuable suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Lucas-kanade 20 years on: A unifying framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">56</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">GMS: Grid-based motion statistics for fast, ultra-robust feature correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>IJCV</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised scale-consistent depth and ego-motion learning from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An evaluation of feature matchers for fundamental matrix estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Unsupervised depth learning in challenging indoor video: Weak rectification to rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.02708</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Codeslam-learning a compact, optimisable representation for dense visual slam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bloesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Czarnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2560" to="2568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="611" to="625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Depth prediction without the sensors: Leveraging structure for unsupervised learning from monocular videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Casser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth and ego-motion learning with structure and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Casser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop on Visual Odometry and Computer Vision Applications Based on Location Cues (VOCVALC)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Depth from a single image by harmonizing overcomplete local network predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning single-image depth from videos using quality assessment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5604" to="5613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Selfsupervised learning with geometric constraints in monocular video: Connecting flow, depth, and camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7063" to="7072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07289</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A volumetric method for building complex models from range images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="1996" />
			<publisher>CUMINCAD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Direct sparse odometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Recognition and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="611" to="625" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Svo: Fast semi-direct monocular visual odometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Forster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pizzoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="15" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2002" to="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised cnn for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Bg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning single camera depth estimation using dual-pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wadhwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ansari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7628" to="7637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Vision meets Robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>IJRR</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Stereoscan: Dense 3d reconstruction in real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Digging into self-supervised monocular depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Depth from videos in the wild: Unsupervised monocular depth learning from unknown cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jonschkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">3d packing for self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raventos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semantically-guided representation learning for selfsupervised monocular depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Accurate and efficient stereo processing by semi-global matching and mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hirschmuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Guiding monocular depth estimation using depthattention volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen-Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heikkil?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="581" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Complex urban dataset with multi-level sensors from highly diverse urban environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Roh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="page">0278364919843996</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">ADAM: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Parallel tracking and mapping for small ar workspaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE and ACM international symposium on mixed and augmented reality</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="225" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Self-supervised monocular depth estimation: Solving the dynamic object problem by semantic guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-A</forename><surname>Term?hlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fingscheidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="582" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Semisupervised deep learning for monocular depth map prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kuznietsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stuckler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning monocular depth in dynamic scenes via instance-aware projection consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth learning in dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Casser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A two-streamed network for estimating fine-scaled depth maps from single rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Undeepvo: Monocular visual odometry through unsupervised deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7286" to="7291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Pose graph optimization for unsupervised monocular visual odometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="5439" to="5445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning the depths of moving people by watching frozen people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4521" to="4530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Megadepth: Learning single-view depth prediction from internet photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2041" to="2050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images using deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Recognition and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">38</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Cnn-svo: Improving the mapping in semi-direct visual odometry using single-image depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Loo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Amiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mashohor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="5218" to="5223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Distinctive image features from scaleinvariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">60</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Every pixel counts++: Joint learning of geometry and motion with 3d holistic understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Recognition and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="2624" to="2641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Consistent video depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Matzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3061" to="3070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">ORB-SLAM: a versatile and accurate monocular slam system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M M</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Tardos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Fast relocalisation and loop closing in keyframe-based slam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Tard?s</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="846" to="853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">ORB-SLAM2: an opensource SLAM system for monocular, stereo and RGB-D cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Tard?s</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>De-Vito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS-W</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Superdepth: Selfsupervised, super-resolved monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ambru?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9250" to="9256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Unsupervised adversarial depth estimation using cycled generative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pilzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Puscas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="587" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">On the uncertainty of self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Aleotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Prisacariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>K?hler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Golodetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sapienza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cavallari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">InfiniTAM v3: A Framework for Large-Scale 3D Reconstruction with Loop Closure</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints. 1708.00783</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lasinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Recognition and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Competitive Collaboration: Joint unsupervised learning of depth, camera motion, optical flow and motion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Orb: An efficient alternative to sift or surf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rublee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Structure-frommotion revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Schonberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4104" to="4113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Pixelwise view selection for unstructured multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Sch?nberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Beyond photometric loss for self-supervised ego-motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6359" to="6365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">A benchmark for the evaluation of rgb-d slam systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Engelhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Cnnslam: Real-time dense monocular slam with learned depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tateno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6243" to="6252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Pseudo rgb-d for self-improving monocular slam and depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-H</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="437" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.07804</idno>
		<title level="m">Sfm-net: Learning of structure and motion from video</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Web stereo video supervision for depth prediction from dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="348" to="357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Learning depth from monocular videos using direct methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Towards unified depth and semantic prediction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Image Quality Assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Monocular relative depth perception with web stereo data supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="311" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">D3vo: Deep depth, deep pose and deep uncertainty for monocular visual odometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Stumberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1281" to="1292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Deep virtual stereo odometry: Leveraging deep depth prediction for monocular direct sparse odometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stuckler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Unsupervised learning of geometry with edge-aware depthnormal consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Enforcing geometric constraints of virtual normal for depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Learning to recover 3d scene shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Scale recovery for monocular visual odometry using depth estimated with deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5870" to="5878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">GeoNet: Unsupervised learning of dense depth, optical flow and camera pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Unsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Saroj Weerasekera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Determining the epipolar geometry and its uncertainty: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="161" to="195" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Towards better generalization: Joint depth-pose learning without posenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Moving indoor: Unsupervised video depth learning in challenging environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.09847</idno>
		<title level="m">Open3D: A modern library for 3D data processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Learning monocular visual odometry via selfsupervised long-term modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-H</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">DF-Net: Unsupervised joint learning of depth and flow using cross-task consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
