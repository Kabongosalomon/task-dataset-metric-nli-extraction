<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Evolving Neural Architecture Using One Shot Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nilotpal</forename><surname>Sinha</surname></persName>
							<email>nilotpalsinha.cs06g@nctu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="institution">National Chiao Tung University Hsinchu City</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Wen</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">National Chiao Tung University</orgName>
								<address>
									<settlement>Hsinchu City</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Evolving Neural Architecture Using One Shot Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural Architecture Search (NAS) is emerging as a new research direction which has the potential to replace the hand-crafted neural architectures designed for specific tasks. Previous evolution based architecture search requires high computational resources resulting in high search time. In this work, we propose a novel way of applying a simple genetic algorithm to the NAS problem called EvNAS (Evolving Neural Architecture using One Shot Model) which reduces the search time significantly while still achieving better result than previous evolution based methods. The architectures are represented by using the architecture parameter of the one shot model which results in the weight sharing among the architectures for a given population of architectures and also weight inheritance from one generation to the next generation of architectures. We propose a decoding technique for the architecture parameter which is used to divert majority of the gradient information towards the given architecture and is also used for improving the performance prediction of the given architecture from the one shot model during the search process. Furthermore, we use the accuracy of the partially trained architecture on the validation data as a prediction of its fitness in order to reduce the search time. EvNAS searches for the architecture on the proxy dataset i.e. CIFAR-10 for 4.4 GPU day on a single GPU and achieves top-1 test error of 2.47% with 3.63M parameters which is then transferred to CIFAR-100 and Im-ageNet achieving top-1 error of 16.37% and top-5 error of 7.4% respectively. All of these results show the potential of evolutionary methods in solving the architecture search problem.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Convolutional neural networks have been instrumental in solving various problems in the field of computer vision. However, the network designs were mainly done by humans (like AlexNet <ref type="bibr" target="#b11">[12]</ref>, ResNet <ref type="bibr" target="#b7">[8]</ref>, DenseNet <ref type="bibr" target="#b9">[10]</ref>, VGGNet <ref type="bibr" target="#b22">[23]</ref>) on the basis of their intuition and understanding of the specific problem. This has led to the growing interest in the automated search of neural architecture called Neural Architecture Search (NAS) <ref type="bibr" target="#b6">[7]</ref>[30] <ref type="bibr" target="#b19">[20]</ref>. NAS has shown some promising results in the field of computer vision but most of these methods demand a considerable amount of computational power. For example, obtaining the state-of-the-art architecture for CIFAR-10 required 3150 GPU days of evolution <ref type="bibr" target="#b20">[21]</ref> and 1800 GPU days of reinforcement learning (RL) <ref type="bibr" target="#b30">[31]</ref>. This can be mainly attributed to the evaluation of the architectures during the search process of the different NAS methods because most NAS methods train each architecture individually for certain number of epochs in order to evaluate its performance on the validation data. Recent works <ref type="bibr" target="#b19">[20]</ref> <ref type="bibr" target="#b0">[1]</ref> have reduced the search time by weight sharing among the architectures. DARTS <ref type="bibr" target="#b16">[17]</ref> further improves upon the scalability by relaxing the search space to a continuous space in order to use gradient descent for optimizing the architecture. But these gradient based methods are highly dependent on the search space and they tend to overfit to operations in the search space that lead to faster gradient descent <ref type="bibr" target="#b27">[28]</ref>.</p><p>In this work, we propose a method called EvNAS (Evolving Neural Architecture using One Shot Model) which involves evolving a convolutional neural network architecture with weight sharing among the architectures in the population for the image classification task. The work is inspired in part by the representation used for the architectures of the network in DARTS <ref type="bibr" target="#b16">[17]</ref> and a random search <ref type="bibr" target="#b12">[13]</ref> using the same representation as DARTS which achieved a competitive result on the CIFAR-10 dataset. By replacing the idea of using a random search with a genetic algorithm on the representation used in DARTS, we introduce a directional component to the otherwise directionless random search <ref type="bibr" target="#b12">[13]</ref> through the use of crossover with tournament selection and elitism. The stochastic nature of the algorithm also ensures that the algorithm does not get stuck in the local minima.</p><p>The objective of the paper is to show how to apply a simple genetic algorithm to the neural architecture search problem while reducing the high search time associated with the evolution based search algorithm. Our experiments (Section 4) involves neural architecture search on a proxy dataset i.e. CIFAR-10 which achieves the test error of 2.47% with 3.63M parameters on this dataset while using minimal computational resources <ref type="bibr">(4.4</ref> GPUs day on a single GPU). The discovered architecture is then transferred to CIFAR-100 and ImageNet achieving top-1 error of 16.37% and top-5 error of 7.4% respectively.</p><p>Our contributions can be summarized as follows:</p><p>? We introduce a novel method of applying a simple genetic algorithm to the NAS problem with reduced computational requirements.</p><p>? We propose a decoding technique for each architecture in the population which diverts a majority of the gradient information to the current architecture during the training phase and is used to calculate the fitness of the architecture from the one shot model during the fitness evaluation phase.</p><p>? We propose a crossover operation that is guided by the predicted fitness of the partially trained architectures of the previous generation and does not require keeping track of the ancestors of the parent architectures.</p><p>? We achieved remarkable efficiency in the architecture search achieving test error of 2.47% with 3.63M parameters on CIFAR-10 and showed that the architecture learned by EvNAS is transferable to CIFAR-100 and ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Automated Neural Architecture Search is an alternative to the hand-crafted architectures where the machine designs the best suited architecture for a specific problem. Several search methods have been proposed to explore the space of neural architectures, such as evolutionary algorithm (EA) <ref type="bibr">[</ref>  <ref type="bibr" target="#b27">[28]</ref>. These can be grouped into two groups: gradient-based methods and non-gradient based methods.</p><p>Gradient Based Methods: In these methods, the neural architecture is directly optimized using the gradient information based on the performance on the validation data. In <ref type="bibr" target="#b15">[16]</ref> <ref type="bibr" target="#b16">[17]</ref>, the discrete architecture search space is relaxed to a continuous search space by using a one shot model and the performance of the model on the validation data is used for updating the architecture using gradients. This method reduces the search time significantly but suffers from the overfitting problem wherein the searched architecture performs very well on the validation data but exhibits poor performance on the test data. This is mainly attributed to the preference of parameter-less operations during the search process as it leads to a rapid gradient descent <ref type="bibr" target="#b1">[2]</ref>. Many regularizations have been introduced to tackle the problem such as early stopping <ref type="bibr" target="#b27">[28]</ref>, search space regularization <ref type="bibr" target="#b1">[2]</ref> and architecture refinement <ref type="bibr" target="#b1">[2]</ref>. Contrary to the gradient based methods, the proposed method does not suffer from the overfitting problem because of the stochastic nature introduced by the mutation operation.</p><p>Non-Gradient Based Methods: These methods include reinforcement learning (RL) and evolutionary algorithm (EA). In RL methods, an agent is trained to generate a neural architecture through its action in order to maximize the expected accuracy on the validation data. In <ref type="bibr" target="#b29">[30]</ref> <ref type="bibr" target="#b30">[31]</ref>, a recurrent neural network (RNN) is used as an agent which samples neural architectures which are then trained to convergence in order to obtain their accuracies on the validation data. These accuracies are then used to update the weights of RNN by using policy gradient methods. Both of these methods suffered from huge computational requirements. This was improved upon in <ref type="bibr" target="#b19">[20]</ref>, where all the sampled architectures were forced to share weights by using a single directed acyclic graph (DAG) resulting in the reduction of computational resources. Early approaches based on EA such as <ref type="bibr" target="#b24">[25]</ref>[24] optimized both the neural architectures and the weights of the network which limited their usage to relatively smaller networks. Then, methods such as <ref type="bibr" target="#b25">[26]</ref>[21] used evolution to search for the architecture and gradient descent for optimizing the weights of each architecture which made it possible to search for relatively large networks. However, this resulted in huge computational requirements. To speed up the training of each individual architecture, weight inheritance was introduced in <ref type="bibr" target="#b21">[22]</ref> wherein a child network inherits the parent networks' weights. In this work, we used both weight inheritance and weight sharing among the architectures to speed up the search process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>This section discusses different parts of the proposed algorithm and its relationship to prior works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Representation of Architecture</head><p>The proposed algorithm deals with a population of architectures in each generation during the search process. Instead of having a separate model for each architecture in a population <ref type="bibr" target="#b25">[26]</ref> <ref type="bibr" target="#b20">[21]</ref>, we used a one shot model which treats all the architectures as subgraphs of the supergraph while sharing the weights among all the architectures. The one shot model is composed of repeatable cells which are stacked together to form the convolutional network. The one shot model has two types of convolutional cells: normal cell and reduction cell. A normal cell uses operations with stride 1 whereas reduction cell uses operations with stride The edge between node i and node j can be written as:</p><formula xml:id="formula_0">f (i,j) (x) = op?O exp(? i,j op ) op ?O exp(? i,j op ) op(x).<label>(1)</label></formula><p>Where ? i,j op refers to the weight of the operation op in the operation space O between node i and node j. The architecture is represented by two matrices, one for normal cell and one for reduction cell, where the row represents the edge between two nodes and the column represents the weights of different operations from the operation space as shown in <ref type="figure" target="#fig_0">Figure 1</ref>(a). Please refer to the original DARTS paper <ref type="bibr" target="#b16">[17]</ref> for more technical details. The design choice results in weight sharing among the architectures in a given population of architectures. It also results in weight inheritance from one generation of architectures to the next generation of architectures i.e. the next generation architectures are not trained from scratch but inherit the partially trained weights from the previous generation architectures. All of these ultimately leads to the reduction of the architecture search time using evolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Decoding Architecture Parameter</head><p>Architecture parameter, ?, gives variable weights to the operations in any particular architecture which results in very noisy estimate of fitness of the architecture. This results in the algorithm performing marginally better than the random algorithm as discussed in Section 4.3. We propose a decoding technique, which is a process of giving equal higher weight to the operations of the actual architecture/subgraph according to the architecture parameter, ? and equal smaller weights to the operations of the other architectures. This can be thought of as decoding/mapping the genotype, i.e. ?, to the phenotype, i.e. actual architecture <ref type="bibr" target="#b5">[6]</ref>. The process has the following two steps:</p><p>? For any ?, derive the discrete architecture, arch dis , from ? as shown in <ref type="figure" target="#fig_0">Figure 1</ref>(b).</p><p>? On the basis of the discrete architecture, arch dis , create another architecture parameter called decoded architecture parameter ,? (as shown in <ref type="figure" target="#fig_0">Figure 1</ref>(c)), with the following entries:</p><formula xml:id="formula_1">? i,j op = k, if op between node i and j present in arch dis 0, otherwise<label>(2)</label></formula><p>where k is an integer. The design ensures that the current architecture according to ? gets a majority of the gradient information to update its parameters while the rest of the gradient information is distributed equally among all the other architectures to update their parameters. This results in making sure that the weights of an architecture does not get co-dependent with the weights of the other architecture due to the weight sharing nature of the one shot model. It also helps in improving the estimation of the fitness of each architecture in the population, as it gives higher equal weight to that particular architecture operations while giving lower equal weights to the other architecture operations. This results in the higher contribution from a particular architecture while very low contribution by other architectures during the fitness evaluation step of that particular architecture from the one shot model. This is in contrast to the variable architecture contribution, used in the original DARTS paper, wherein an architecture is evaluated using ? which results in very noisy estimate of its performance. We empirically find that k = 1 gives a good result and increasing the value of k from 1 tends to deteriorate the accuracy as shown in <ref type="figure" target="#fig_1">Figure 2</ref>(a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training and Performance Estimation</head><p>The sharing of the network weights among the architectures in the population, due to the one shot model representation <ref type="bibr" target="#b16">[17]</ref>, helps in exchanging information to the next generation population, wherein the architectures of the next generation do not start training from scratch. This can be thought of as child architecture model inheriting the weights of the parent architecture model, also known as weight inheritance. Therefore, instead of the full training of each architecture in the population from scratch, EvNAS partially trains the inherited architecture model weights by using the training data. This is done by first copying the decoded architecture parameter,?, in Section 3.2, for the individual architecture in the population to the one shot model and then training the network for a certain number of batches of training examples.</p><p>To evaluate the performance of each individual architecture, its decoded architecture parameter,?, from Section 3.2, is first copied to the one shot model. The model is then evaluated on the basis of its accuracy on the validation data, which becomes the fitness of the architecture. Note that the fitness value of each architecture is a noisy estimate of its true accuracy on the validation data as the architecture has been trained partially on a certain number of training batches while inheriting its weights from the previous generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Evolutionary Algorithm</head><p>The evolutionary algorithm (EA) starts with a population of architectures, which are sampled from a uniform distribution on the interval [0, 1), and it runs for G generations. In each generation, the one shot model is trained on the training data by using the decoded architecture parameter? of each individual architecture in the population in a round-robin fashion. Then, the fitness of each individual architecture is estimated using the decoded architecture parameter?. The population is then evolved using crossover and mutation operations to create the next generation population replacing the previous generation population. The best architecture in each generation does not undergo any modification and is automatically copied to the next generation. This ensures that the algorithm does not forget the best architecture learned thus far and gives an opportunity to old generation architecture to compete against the new generation architecture; this is known as elitism. The best architecture is returned after G generations. The entire process is summarized as Algorithm 1 in the supplementary.</p><p>Mutation Operation: It refers to a random change to an individual architecture in the population. The algorithm uses the mutation rate <ref type="bibr" target="#b5">[6]</ref>, which decides the probability of changing the architecture parameter, ? i,j , between node i and node j. This is done by re-sampling ? i,j from a uniform distribution on the interval [0, 1) as illustrated in <ref type="figure">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3: Illustration of mutation operation</head><p>Crossover Operation: It is a process of combining parent architectures to create a new child architecture, which may perform better than the parents. EvNAS uses tournament selection <ref type="bibr" target="#b5">[6]</ref> for the parent selection process to generate the next generation architecture population. In tour- and parent2 and are used to create a single child architecture. This is done by copying the architecture parameters, [? i,j ] parent1 and [? i,j ] parent2 between node i and node j, from parent1 and parent2, respectively, with a certain probability to the child architecture parameter, [? i,j ] child between node i and node j as illustrated in <ref type="figure" target="#fig_3">Figure 5</ref>. This can be formulated as follows:</p><formula xml:id="formula_2">[? i,j ] child = [? i,j ] parent1 , with probability 0.5 [? i,j ] parent2 , otherwise<label>(3)</label></formula><p>Note that as all the architectures are sub-graph of the supergraph, i.e. the one shot model, so, we do not have to keep track of the ancestors in order to apply the crossover operation, as was done in <ref type="bibr" target="#b28">[29]</ref>[25]. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Relationship to Prior Works</head><p>Weight inheritance was used in <ref type="bibr" target="#b21">[22]</ref> during architecture search using evolution but in the proposed method, the architectures in a given generation share weights and inherit the weights from the previous generation (i.e weight inheritance) because of the use of the one shot model.</p><p>FairNAS <ref type="bibr" target="#b2">[3]</ref>, NSGANetV2 <ref type="bibr" target="#b17">[18]</ref> has also proposed evolutionary search with weight sharing which has two steps for searching neural architecture where they optimizes the supernet in the first step and then FairNAS performs architecture search using evolutionary method with the trained supernet as the evaluator in the second step while NS-GANetV2 uses the weights from trained supernet to warm start gradient descent for an architecture during the architecture search. In contrast, our method combines both the training and search process in one single stage. FairNAS and NSGANetV2 solves the search problem as a multiobjective problem whereas our method solves it as a single objective problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head><p>In this section, we report the performance of the proposed algorithm EvNAS in terms of a neural architecture search on the CIFAR-10 dataset <ref type="bibr" target="#b10">[11]</ref> and the performance of the found architectures on the CIFAR-100 dataset <ref type="bibr" target="#b10">[11]</ref> and the ImageNet dataset <ref type="bibr" target="#b3">[4]</ref>. We then present an ablation study showing the importance of the proposed decoded architecture parameter,?, crossover and mutation operations during the search process.</p><p>Initialization: Each architecture in a population is represented by the architecture parameter, ?, which is sampled from a uniform distribution on the interval [0, 1).</p><p>Search Process: The search process on the CIFAR-10 is divided into three stages as was done in <ref type="bibr" target="#b16">[17]</ref> <ref type="bibr" target="#b12">[13]</ref>. In stage 1, we perform the search process for a cell block on CIFAR-10 by using four different seeds; this can be thought of as the search stage of the algorithm. In stage 2, the best architecture found in each trial of stage 1 is evaluated by retraining a larger network created using the same cell blocks discovered in stage 1 for 600 epochs from scratch on CIFAR-10. Next, we choose the best performing architecture among the four trials, making it the selection stage of the algorithm. In stage 3, we evaluate the best architecture found from stage 2 by training the network from scratch with ten different seeds for 600 epochs. This stage can be considered as the evaluation stage of the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Search on CIFAR-10:</head><p>Dataset: CIFAR-10 <ref type="bibr" target="#b10">[11]</ref> has 50,000 training images and 10,000 testing images with a fixed resolution of 32x32. During the architecture search, the training images are divided into two subsets of size 25,000 each, out of which the first subset is used for training the one shot model and the other subset is the validation data, which is used for calculating the fitness of each architecture in the popula- <ref type="table">Table 1</ref>: Comparison of EvNAS with other NAS methods on CIFAR-10 and CIFAR-100 datasets. The first block presents the performance of the hand-crafted architecture. The second block presents the performance of other NAS methods, the third block presents the performance of our method and the last block presents the performance of our ablation study. All the architecture search were performed using cutout. ? indicates that the result was reported in <ref type="bibr">[</ref> Search Space: We follow the setup given in DARTS <ref type="bibr" target="#b16">[17]</ref>. The one shot model is created by stacking normal cell with reduction cell inserted at 1/3 and 2/3 of the total depth of the model. Each cell has two input nodes, four intermediate node and one output node resulting in 14 edges among them. The operations considered for the cells are as follows: 3x3 and 5x5 dilated separable convolutions, 3x3 and 5x5 separable convolutions, 3x3 max pooling, 3x3 average pooling, skip connect and zero. Thus, each architecture is represented by two 14x8 matrices one for normal cell and one for reduction cell.</p><p>Training Settings: The training setting mainly follows the setup proposed by DARTS <ref type="bibr" target="#b16">[17]</ref>. Because of the high memory requirements of the one shot model, a smaller network, called proxy network <ref type="bibr" target="#b12">[13]</ref>, with 8 stacked cells and 16 initial channels is used during the architecture search process, i.e. stage 1. For deriving the discrete architecture, arch dis , each node in the discrete architecture is connected to two nodes among the previous nodes selected via the top-2 operations according to the architecture parameter ?. During the search process, we use SGD for training the one shot model with a batch size of 64, initial learn-ing rate of 0.025, momentum of 0.9, and weight decay of 3 ? 10 ?4 . The learning rate is annealed down to 0.001 by using the cosine annealing schedule without any restart during the search process. For our evolutionary algorithm, we use a population size of 50 in each generation, 0.1 as the mutation rate and 10 architectures are chosen randomly during the tournament selection. The search process runs for 50 generations on a single GPU, NVIDIA 2080 Ti, and takes 4.4 days to complete stage 1. Number of generations was chosen to match the number of epochs in DARTS <ref type="bibr" target="#b16">[17]</ref>. Population size was chosen based on the experiments where we ran our method for population size of 20, 30, 50 with tournament size chosen as one-fifth of the population size, as shown in <ref type="figure" target="#fig_1">Figure 2(b)</ref>. We did not go beyond 50 population size as we wanted to have search time similar to that of DARTS. Mutation rate was chosen based on the experiments where we ran our method for mutation rate of 0.05, 0.1, 0.15, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>(c). All our architecture search in <ref type="table">Table 1</ref> are done with cutout <ref type="bibr" target="#b4">[5]</ref>.</p><p>Architecture Evaluation: A larger network, called proxyless network <ref type="bibr" target="#b12">[13]</ref>, with 20 stacked cells and 36 initial channels is used during the selection and evaluation stage. Following DARTS <ref type="bibr" target="#b16">[17]</ref>, the proxyless network is trained with a batch size of 96, weight decay of 0.0003, cutout <ref type="bibr" target="#b4">[5]</ref>, <ref type="table">Table 2</ref>: Comparison of our method with other image classifiers on ImageNet in mobile setting. The first block presents the performance of the hand-crafted architecture. The second block presents the performance of other NAS methods and the last block presents the performance of our method. auxiliary tower with 0.4 as its weights, and path dropout probability of 0.2 for 600 epochs. The same setting is used to train and evaluate the proxyless network on the CIFAR-100 dataset <ref type="bibr" target="#b10">[11]</ref>.</p><p>Search Results and Transferability to CIFAR-100: We perform the architecture search on CIFAR-10 three times with different random number seeds and their results are provided in <ref type="table">Table 1</ref> as EvNAS-A, EvNAS-B and EvNAS-C, which are then transferred to CIFAR-100. The cells discovered during EvNAS-A are shown in <ref type="figure" target="#fig_2">Figure 4</ref> and those discovered by EvNAS-B and EvNAS-C are given in the supplementary. EvNAS-A evaluates 10K architectures during the search time and achieves the average test error of 2.47?0.06 and 16.37 on CIFAR-10 and CIFAR-100 respectively with search time significantly less than the previous evolution based methods. EENA <ref type="bibr" target="#b28">[29]</ref> found a competitive architecture in lesser search time than EvNAS using evolution but EvNAS was able to achieve better result on both CIFAR-10 and CIFAR-100 with fewer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Architecture Transferability to ImageNet:</head><p>Architecture Evaluation: The architecture discovered in the search process on CIFAR-10 is then used to train a network on the ImageNet dataset <ref type="bibr" target="#b3">[4]</ref> with 14 cells and 48 initial channels in the mobile setting, where the size of the input images is 224 x 224 and the number of multiply-add operations in the model is restricted to less than 600M. We follow the training settings used by PDARTS <ref type="bibr" target="#b1">[2]</ref>. The net-work is trained from scratch with a batch size of 1024 on 8 NVIDIA V100 GPUs.</p><p>ImageNet Results: The results of the evaluation on the ImageNet dataset are provided in <ref type="table">Table 2</ref>. The result shows that the cell discovered by EvNAS on CIFAR-10 can be successfully transferred to the ImageNet, achieving a top-5 error of 7.4%. Notably, EvNAS is able to achieve better result than previous state-of-the-art evolution based methods AmoebaNet <ref type="bibr" target="#b20">[21]</ref>, FairNAS <ref type="bibr" target="#b2">[3]</ref> while using significantly less computational resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>To discover the effect of the decoded architecture parameter,?, and the crossover and mutation operations during the search process, we conduct more architecture searches: without decoded architecture parameter,?, with crossover only, with mutation only and without crossover and mutation. The search results are provided in <ref type="table">Table 1</ref>.</p><p>Without Crossover and Mutation: Here, a population of 50 architectures are randomly changed after every generation and in the last generation, the architecture with the best performance on the validation set is chosen as the best found architecture. Thus, the search process only evaluates only 200 architectures to come up with the best architecture. The architecture found (listed as EvNAS-Rand in <ref type="table">Table 1</ref>) achieves an average error of 2.84 ? 0.08, as the search behaves as a random search and shows similar results to those reported in <ref type="bibr" target="#b12">[13]</ref>.</p><p>Without Decoded Architecture Parameter,?: Here, we conduct three architecture searches where a population of 50 architectures are modified through both crossover and mutation operations without using the decoded architecture parameter,?, (i) during the training (EvNAS-NDT), (ii) during the fitness evaluation of each individual architecture in the population (EvNAS-NDF) and (iii) during both training and fitness evaluation (EvNAS-ND). The architecture found (listed in <ref type="table">Table 1</ref>) in EvNAS-NDT performs better than that of EvNAS-NDF which shows that the decoded architecture parameter,?, is more important during the fitness estimation step than during the training step. Also, the architecture found in EvNAS-ND performs slightly better than that of the random search because of the direction component introduced by the crossover operation. The improvement is due to the fact that when using architecture parameter, ?, it allows a varying amount of contribution from other architectures during the fitness estimation of a particular architecture from the one shot model, resulting in very noisy fitness estimate. But when using the decoded architecture parameter,?, it assigns higher weight to the current architecture while giving equally small weights to other architectures.</p><p>With Mutation Only: Here, a population of 50 architectures are modified only through a mutation operation with 0.1 as the mutation rate while using the decoded architecture parameter. The architecture found (listed as EvNAS-Mut in <ref type="table">Table 1</ref>) performs slightly better than that of the random search even though mutation is a random process. This improvement can be attributed to elitism, which does not let the algorithm forget the best architecture learned thus far.</p><p>With Crossover Only: Here, a population of 50 architectures are modified only through a crossover operation only while using the decoded architecture parameter. The architecture found (listed as EvNAS-Cross in <ref type="table">Table 1</ref>) performs slightly better than that of the random search. This improvement can be attributed to the selection pressure <ref type="bibr" target="#b5">[6]</ref> introduced because of the tournament selection, which guides the search towards the better architecture solution.</p><p>The improvements in both EvNAS-Mut and EvNAS-Cross are not much as compared to the EvNAS-Rand because of the fact that we are using a partially trained network for evaluating the architectures on the validation set which provides a noisy estimate of their fitness/performance. The ablation study shows that the decoded architecture parameter?, mutation and crossover operations play an equally important role in the search process while the decoded architecture parameter,?, plays more important role during the fitness estimation. All the cells discovered in the ablation study are provided in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Discussion on Evolutionary Search vs Gradient Based Search</head><p>The gradient based methods are highly dependent on the search space and they tend to overfit to operations that lead to faster gradient descent which is the skip-connect operation due to its parameter-less nature leading to higher number of skip-connect in the final discovered cell <ref type="bibr" target="#b27">[28]</ref>. PDARTS <ref type="bibr" target="#b1">[2]</ref> uses a regularization method to restrict the number of skip-connect to a specific number in the final normal cell for the search space used in the original DARTS paper. PDARTS emperically found that the optimal number of skip-connect in the normal cell is 2, which is a search space dependent value and the optimal number may not be 2 if the search space is changed. This reduces the search space resulting in faster search time as compared to the original DARTS which is a gradient based method without any regularization applied to the search space. Notice that without such regularization to restrict the number of skip-connect to 2, the gradient based methods, e.g. DARTS, only provides similar search time but much worse performance than ours due to the overfitting problem. By contrast, EvNAS does not have to worry about the overfitting problem due to its stochastic nature and so it is not dependent on the search space. EvNAS arrives at this optimal solution without any regularization being applied to the search space as can be seen in the discovered normal cells in <ref type="figure" target="#fig_2">Figure 4</ref>(a) and all the figures in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future Directions</head><p>We propose an efficient method of applying a simple genetic algorithm to the neural architecture search problem with both parameter sharing among the individual architectures and weight inheritance using a one shot model, resulting in decreased computational requirements as compared to other evolutionary search methods. A decoding method for the architecture parameter was used to improve the fitness estimation of a partially trained individual architecture from the one shot model. The proposed crossover along with the tournament selection provides a direction to an otherwise directionless random search. The proposed algorithm was able to significantly reduce the search time of evolution based architecture search while achieving better results on CIFAR-10, CIFAR-100 and ImageNet dataset than previous evolutionary algorithms. A possible future direction to improve the performance of the algorithm is by making an age factor to be a part of the architecture, which makes sure that the old generation architectures do not die after one generation and can compete against the newer generation architectures. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The process of decoding the architecture parameter, ?. Better viewed in color mode. Here, we consider three operations in the operation space. (a) One shot model and its representation with arrows between the nodes representing all the operations in the search space, (b) Discrete architecture, arch dis , derived from ?, (c) Decoded architecture,?, created using arch dis . The thickness of the arrow is proportional to the weight given to an operation.2.A cell in the one shot model is represented by the parameter, ? called architecture parameter, which represents the weights of the different operations op(.) in the operation space O (i.e. search space of NAS) between a pair of nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>(a) Accuracy vs Decoded architecture value (b) Accuracy vs Population size (c) Accuracy vs mutation rate</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Discovered cell using EvNAS-A (a) Normal Cell (b) Reduction Cell nament selection, a certain number of architectures are randomly selected from the current population. The top-2 most fit architectures from the selected group become parent1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Illustration of crossover operation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :Figure 3 :Figure 4 :Figure 5 :Figure 6 :Figure 7 :Figure 8 :</head><label>2345678</label><figDesc>Discovered cell in EvNAS-C (a) Normal Cell (b) Reduction Cell. Discovered cell using random search (EvNAS-Rand) (a) Normal Cell (b) Reduction Cell. Discovered cell using EvNAS without decoding architecture? during both training and fitness evaluation (EvNAS-ND) (a) Normal Cell (b) Reduction Cell. Discovered cell using EvNAS without decoding architecture? during fitness evaluation (EvNAS-NDF) (a) Normal Cell (b) Reduction Cell. Discovered cell using EvNAS without decoding architecture? during training (EvNAS-NDT) (a) Normal Cell (b) Reduction Cell. Discovered cell using EvNAS with mutation only (EvNAS-Mut) (a) Normal Cell (b) Reduction Cell. Discovered cell using EvNAS with crossover only (EvNAS-Cross) (a) Normal Cell (b) Reduction Cell.</figDesc></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Abstract</head><p>Here, we present the summary of the algorithm and all the discovered cells from EvNAS-B, EvNAS-C and the ablation study which are shown in <ref type="figure">Figure 1</ref> to   </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Understanding and simplifying one-shot architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Progressive differentiable architecture search: Bridging the depth gap between search and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1294" to="1303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Fairnas: Rethinking evaluation fairness of weight sharing neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jixiang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.01845</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet: a large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Introduction to evolutionary computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">E</forename><surname>Agoston E Eiben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">53</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Hendrik</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.05377</idno>
		<title level="m">Neural architecture search: A survey</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Random search and reproducibility for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hierarchical representations for efficient architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chrisantha</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055</idno>
		<title level="m">Darts: Differentiable architecture search</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Evolutionary multi-objective surrogate-assisted neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyanmoy</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Banzhaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishnu Naresh</forename><surname>Boddeti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="35" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural architecture optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renqian</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7816" to="7827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameters sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melody</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno>PMLR. 1</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>Jennifer Dy and Andreas Krause</editor>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsm?ssan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Large-scale evolution of image classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><forename type="middle">Leon</forename><surname>Suematsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kurakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A hypercube-based encoding for evolving large-scale neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>D&amp;apos;ambrosio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gauci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial life</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="185" to="212" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Evolving neural networks through augmenting topologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risto</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miikkulainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Genetic cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1379" to="1388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">SNAS: stochastic neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sirui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Understanding and robustifying differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arber</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yassine</forename><surname>Marrakchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">EENA: efficient evolution of neural architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhulin</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanguang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erhu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
