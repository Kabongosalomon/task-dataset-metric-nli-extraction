<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RH-Net: Improving Neural Relation Extraction via Reinforcement Learning and Hierarchical Relational Searching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Ning</forename><surname>Wang</surname></persName>
							<email>lygwjn@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RH-Net: Improving Neural Relation Extraction via Reinforcement Learning and Hierarchical Relational Searching</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Distant supervision (DS) aims to generate largescale heuristic labeling corpus, which is widely used for neural relation extraction currently. However, it heavily suffers from noisy labeling and long-tail distributions problem. Many advanced approaches usually separately address two problems, which ignore their mutual interactions. In this paper, we propose a novel framework named RH-Net, which utilizes Reinforcement learning and Hierarchical relational searching module to improve relation extraction. We leverage reinforcement learning to instruct the model to select highquality instances. We then propose the hierarchical relational searching module to share the semantics from correlative instances between data-rich and data-poor classes. During the iterative process, the two modules keep interacting to alleviate the noisy and long-tail problem simultaneously. Extensive experiments on widely used NYT data set clearly show that our method significant improvements over state-of-the-art baselines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Relation extraction (RE) is a preliminary task in natural language processing (NLP), which aims to capture the relation between two target entities. Recently, RE based on conventional supervised learning has made a great success. However, it heavily relies on human annotations.</p><p>In order to obtain large-scale training corpus, distant supervision relation extraction (DSRE) <ref type="bibr" target="#b6">[Mintz et al., 2009</ref>] was proposed to generate heuristic labeling data by aligning entity pairs in raw text. It assumes that if two target entities have a semantic relation in KG, all the raw text containing the two entities can be labeled as this relation class. However, this solution makes an over-strong assumption and inevitably brings in massive wrong labeling data. For example, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>, given a fact (Obama, born in, US.) from existing KG, DS will regard all sentences with two linked entities Obama and US. express the relation born in. In consequence, only the first sentence is correct, but actually the second expresses the relation president of while the third cannot find the pre-defined relation. Additionally, according <ref type="bibr">(Obama, born in, US.)</ref> Aligned Sentences</p><p>Real Label  to a series works <ref type="bibr" target="#b4">[Li et al., 2020;</ref><ref type="bibr">Xu and Barbosa, 2019;</ref><ref type="bibr" target="#b2">Han et al., 2018b;</ref>, DS always suffers from long-tail distribution problem. We analyze that there are two main reasons: 1) existing knowledge bases are far from completion and they contain the overlapping problem, 2) the number of noisy labeling sentences in some of the relation labels is larger than correct data, which causes the semantics or data insufficient. Inevitably, the first factor relies on the quality of KG, which is fixed before alignment with plain text. Therefore, we only devote to find the target solution corresponding to the second factor. By intuition, if there're a lot of noisy sentences under a relation class, fewer high-quality sentences can be sufficiently used to train the model, which results in the long-tail. In other words, To improve the relation extraction, the noisy labeling and long-tail problem should be considered simultaneously. Recently, most approaches have been presented to solve the noisy labeling problem <ref type="bibr" target="#b3">[Hoffmann et al., 2011;</ref><ref type="bibr" target="#b7">Zeng et al., 2015;</ref><ref type="bibr">Jat et al., 2018;</ref><ref type="bibr" target="#b4">Ji et al., 2017;</ref><ref type="bibr" target="#b1">Feng et al., 2018;</ref><ref type="bibr" target="#b6">Qin et al., 2018b;</ref><ref type="bibr" target="#b6">Qin et al., 2018a;</ref><ref type="bibr" target="#b8">Zeng et al., 2018]</ref> and long-tail problems <ref type="bibr">[Vashishth et al., 2018;</ref><ref type="bibr" target="#b4">Li et al., 2020;</ref><ref type="bibr">Xu and Barbosa, 2019;</ref><ref type="bibr" target="#b2">Han et al., 2018b;</ref>. Despite the success and popularity of these methods, little works handle both two problems simultaneously, which ignore the mutual interactions.</p><p>In this paper, in order to jointly solve two problems, we propose a novel framework named RH-Net, which incorporates Reinforcement learning and Hierarchical relational searching module. At first, we leverage reinforcement learn-ing to select high-quality data. Concretely, given an original bag 1 , the agent splits its into the correct set and noisy set, and we train the downstream module only on the correct set. This idea is motivated by the previous work <ref type="bibr" target="#b1">[Feng et al., 2018]</ref>, but the difference is that we enhance the agent by integrating pretrained implicit relation information. For the second problem, we regard that the semantics of data-rich can be shared with similar data-poor relations. For example, the data-rich relation /people/person/place of birth in NYT corpus can represent a four-layers tree, from top to down are root, /people, /people/person and /people/person/place of birth, respectively, where root is virtual node, /people and /people/person are sub-relations. When given a data-poor relation people/person/religion, it can be integrated with related instances at the layer of root, /people, and /people/person. In contrast to <ref type="bibr" target="#b2">[Han et al., 2018b]</ref> and , we view RE as a tree search task from the root to the leaf node. During the search processing, we leverage the gating mechanism to save and combine the semantics of related instances at the current node, and calculate the score of each candidate child nodes and choose the maximum one. The two main components joint training at each iterative stage to capture the interactions. The contributions of this paper are as follows:</p><p>? We are the first to transform the relation extraction into a tree search task. We propose the hierarchical relational searching strategy to share the correlated instance semantics at each node. ? We propose a novel framework RH-Net, which is capable of simultaneously solving the noisy labeling and long-tail problem. At the iterative training stage, our method takes advantage of the mutual interactions between them. ? Extensive experiments on the NYT data set demonstrate that if we consider both two problems, the proposed method outperforms state-of-the-art baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Distant supervision (DS) <ref type="bibr" target="#b6">[Mintz et al., 2009</ref>] was proposed to automatically label large-scale corpus to overcome the timeconsuming and human-intensive problem, which is one of the popular methods for semi-supervised relation extraction. However, it suffers from noisy labeling and long-tail distribution problems, which both cause by the over-strong heuristic assumption. Some recent researches solve these problems by employing multi-instance learning (MIL) for bag-level classification <ref type="bibr" target="#b6">[Riedel et al., 2010;</ref><ref type="bibr" target="#b3">Hoffmann et al., 2011]</ref>. Inspired by MIL, <ref type="bibr">[Jat et al., 2018;</ref><ref type="bibr" target="#b5">Lin et al., 2016;</ref><ref type="bibr" target="#b7">Zeng et al., 2015;</ref><ref type="bibr" target="#b4">Ji et al., 2017]</ref> propose sentence-level attention, which can make the model focus on the high-quality sentence and reduce the influence of noise. Other works <ref type="bibr" target="#b7">[Yuan et al., 2019]</ref> denoise by extra bag-level attention to capture the correlation semantics between sentence and bag. Apart from MIL, recent studies have found a novel way to directly select correct data. <ref type="bibr" target="#b1">[Feng et al., 2018]</ref> is the first to utilize RL for RE.</p><p>The instance selector (agent) is modeled as a binary-classifier, where 1 represents select action and 0 denotes remove action. The relation classifier is trained on the selected set and returns a reward through validation loss to the instance selector.  and <ref type="bibr" target="#b6">[Qin et al., 2018b]</ref> improved RL by using Q-network. In addition, <ref type="bibr" target="#b6">[Qin et al., 2018a;</ref><ref type="bibr" target="#b1">Han et al., 2018a]</ref> leverage generative adversarial network (GAN) to filter noisy data by iterative training generator and discriminator.</p><p>Additionally, some recent researches start to focus on the long-tail problem. For example, <ref type="bibr">[Vashishth et al., 2018;</ref><ref type="bibr" target="#b4">Li et al., 2020;</ref><ref type="bibr">Xu and Barbosa, 2019]</ref> utilize side information to realize semantics enhancement. <ref type="bibr" target="#b0">[Beltagy et al., 2019]</ref> make data argumentation, such as entity type information, implicit or explicit relation-aware knowledge, etc. <ref type="bibr" target="#b2">Han et al., 2018b]</ref> leverage hierarchical attention to transfer data-rich information to data-poor class at the tail of the distribution, which succeeds in overcoming the long-tail problem without explicit external data.</p><p>However, they only focus on one problem and ignore the communication between noisy labeling and long-tail distribution problem. To fill the gap in this part, we incorporate the reinforcement learning and hierarchical relational searching strategies to alleviate both noisy labeling and long-tail problem, and train them iteratively to make full use of the mutual interactions.</p><p>3 Methodology <ref type="figure">Figure 2</ref> illustrates our RH-Net, which consists of three main modules, including sentence encoder, instance detector, and hierarchical relational searching module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sentence Encoder</head><p>We use PCNN <ref type="bibr" target="#b7">[Zeng et al., 2015]</ref> to represent the sentence into a low-dimension vector. Given an input of sentence: X = [w 1 , w 2 , ..., w n ], where w i ? R dw+2?dp is the ith word vector consists of d w -dimension pre-trained word embedding by <ref type="bibr">GloVe [Pennington et al., 2014]</ref> and d p dimension position embedding. We then use CNN with K different ldimension filters to encode sentence by:</p><formula xml:id="formula_0">L = CN N (X) (1) where L = [L (1) , L (2) , ..., L (K) ] ? R K?(n?l+1)</formula><p>. The piecewise max pooling vector of the j-filter can be calculated by :</p><formula xml:id="formula_1">c j = [max(L (j) 0:p1 ); max(L (j) p1:p2 ); max(L (j) p2:n )]<label>(2)</label></formula><p>where p 1 , p 2 is the position of two entities h B , t B . [?; ?] is the concatenate operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Instance Detector</head><p>We propose the instance detector module based on RL to automatically split the original bag into the correct and noisy set. We follow <ref type="bibr" target="#b1">[Feng et al., 2018]</ref> to define the state, action, and reward function.</p><p>State. RL can be abstracted as the Markov decision process (MDP) of iterative interaction between the agent  <ref type="figure">Figure 2</ref>: The architecture of our proposed RH-Net. The left is the instance detector, which aims to select high-quality data-driven by RL. The right is the hierarchical relational searching module, which views relation extraction as a top-to-down search processing. and the environment. In this paper, we regard the selection of a bag as an episode, and define the state embedding s t consists of: 1) the average vector of selected sentences from correct setx ? R dc , 2) the last state embedding s t?1 ? R ds , 3) the current sentence embedding c t ? R dc and 4) the implicit relation information r * ? R dr . Obviously, different from <ref type="bibr" target="#b1">[Feng et al., 2018;</ref><ref type="bibr" target="#b6">Qin et al., 2018b]</ref>, we integrate the pre-trained implicit relation information to make enhancement. Formally: Action. At each time t, the instance detector takes an action to decide whether to select for correct set or remove for noisy set. It can be viewed as a binary-classifier refers to the policy ? ? (a i |s t ):</p><formula xml:id="formula_2">s t = [tanh(W q [s t?1 ; c t ; r * ]);x t ] (3) where W q ? R (ds?dc)?(ds+dc+dr) is the trainable matrix. The implicit relation r * = t ? h,</formula><formula xml:id="formula_3">? ? (a i |s t ) = a i ?(W p s t ) + (1 ? a i )(1 ? ?(W p s t )) (4)</formula><p>where ?(?) is the sigmoid function, W p is the training matrix. a i ? {0, 1} is the action space, where 1 denotes select action and 0 denotes remove action.</p><p>Reward. Generally, the reward function is used to evaluate the utility of agent. We follow <ref type="bibr" target="#b1">[Feng et al., 2018]</ref> to design a novel reward function. We assume that the model has a terminal reward when it finishes all the selection:</p><formula xml:id="formula_4">R(B) = 1 M cre Sj ?Bcre logp(r B |S j )<label>(5)</label></formula><p>where M cre is the number of sentences in correct set B cre . Note that, we accumulate the log-aware probability of each sentence p(r B |S i ) in each subset to represent the occurrence probability of ground truth. We train this module by <ref type="bibr">REIN-FORCE algorithm [Williams, 1992]</ref> and following the same settings by <ref type="bibr" target="#b1">[Feng et al., 2018]</ref>. At last, we obtain the correct set B cre and corresponding bag-level embeddingx.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Hierarchical Relational Searching (HRS)</head><p>After denoising, we propose this module to extract the semantics relation. We introduce this module from three aspects:</p><p>The construction of hierarchical relation tree. Given an original relation label r ? R, it can be represented as a path from root (layer4) to the leaf (layer1), and the node at layer k denotes r k (k ? {1, 2, 3, 4}). In addition, we suppose that r k is the sibling node of r k . The child nodes set of r k , r k represents N k (r) and N k (r), respectively, where N k (r) is the complement set of N k (r). Therefore, we have r k?1 , r k?1 ? N k (r), r k?1 / ? N k (r), where r k?1 is the true path at layer k ? 1, r k?1 is the negative but share the same parent node, r k?1 is the negative but not share the same parent node.</p><p>As shown in <ref type="figure">Figure 2</ref>, each node consists of sub-relation embedding and memory cell embedding. The original relation embedding at layer 1 is pre-trained by <ref type="bibr">TransE [Fan et al., 2014]</ref>, and then we recursively calculate the embedding of each sub-relation from layer 2 to 4. Formally:</p><formula xml:id="formula_5">r k = 1 |N k (r)| r k?1 ?N k (r) r k?1<label>(6)</label></formula><p>where r k ? R dr is the sub-relation embedding of node r k , |N k (r)| denotes the number of child nodes. The memory cell embedding aims to preserve the semantics of instances 2 , which initialed as zeros vector C k (r) = 0 ? R d cell .</p><p>The Top-to-down search processing with gating mechanism. Given a bag B and the bag-level embeddingx which outputs from instance detector. HRS aims to search a path from the root to the leaf node, which can be also viewed as a multi-branch classification between two adjacent layers. Specifically, we first obtain the fusion of implicit relation information and bag-level embedding</p><formula xml:id="formula_6">G = tanh(W G [x; r * ] + b G ), where r * = t ? h denotes implicit relation, W G ? R d cell ?dc and b G ? R d cell is the trainable parameters. G ? R d cell</formula><p>is the fusion information of one bag. Suppose that the bag at node r k , inspired by GRUs and LSTMs, we use an input gate i k (r) to selective save this fusion information to update the memory cell C old k (r) to C new k (r) at the node r k :</p><formula xml:id="formula_7">i k (r) = ?(W i,k [x; C k (r)] + b i,k )<label>(7)</label></formula><p>C new k (r) = i k (r) ? G + (1 ? i k (r)) ? C old k (r) (8) We then use an output gate o k (r) to extract the mixed semantics from memory cell at the node r k :</p><formula xml:id="formula_8">o k (r) = ?(W o,k [x; C new k (r)] + b o,k ) (9) Z k (r) = o k (r) ? C new k (r) + (1 ? o k (r)) ? G (10) where W i,k , W o,k , b i,k , b o,<label>k</label></formula><p>are the trainable matrices and bias at the layer k, ?(?) is the sigmoid function, [?; ?] is the concatenate operation. Z k (r) is the mixed semantics of bag B at the node r k , we can calculate the score of each next branch to child node r k?1 ? N k (r), and choose the maximum one r * k?1 as the next node.</p><formula xml:id="formula_9">f (Z k (r), r k?1 ) = sof tmax(Z k (r)W f,k r T k?1 ) (11) r * k?1 = arg max r k?1 f (Z k (r), r k?1 )<label>(12)</label></formula><p>where W f,k is the matrix of score function f (?) at layer k. The hierarchical weighted ranking loss. We find that cross-entropy and hierarchical metric learning <ref type="bibr" target="#b6">[Verma et al., 2012]</ref> cannot be directly used, because 1) each node has a different number of branches, 2) when training one node, parameters of the other nodes will change which results in local optimum or divergence, 3) each layer or node has different influence degree on the loss.</p><p>Thus, we propose hierarchical weighted ranking loss. Firstly, we use the ranking loss to replace conventional crossentropy loss. In other words, we use the opposite of the score as the loss function, and train to maximize the score of the correct path. Secondly, we additionally perform negative sampling from two aspects: 1) r k?1 ? N k (r) and 2) r k?1 ? N k (r). Therefore, the loss function at the layer k(k ? {2, 3, 4}) can be defined as follows:</p><formula xml:id="formula_10">L k (B, r) = r k?1 ?N k (r) ||f (Z k (r), r k?1 ) + ? ? f (Z k (r), r k?1 )|| + + r k?1 ?N k (r) ||f (Z k (r), r k?1 ) + ? ? f (Z k (r), r k?1 )|| + (13) where ? ? [0, 1] is the margin hyper-parameter, || ? || + is the hinge function.</formula><p>At last, We heuristically design weighted value by considering that the model should pay more attention to the node which is near to root or has too many child nodes:</p><formula xml:id="formula_11">? k (r) = |N k (r)| + k ? 1 4 j=2 (|N j (r)| + j ? 1)<label>(14)</label></formula><p>The final loss of this module defined as:</p><formula xml:id="formula_12">L(B, r) = 4 k=2 ? k (r)L k (B, r)<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">The training strategy</head><p>In order to accelerate the training and make the model stable, we follow previous work <ref type="bibr" target="#b1">[Feng et al., 2018;</ref><ref type="bibr" target="#b6">Qin et al., 2018b</ref>] to pre-train at first. Concretely, we pre-train sentence encoder to obtain sentence-level embedding, and then pretrain the instance detector by computing the reward function. The pre-train stage of RL is crucial for our experiment. At the joint training stage, the parameters of the sentence encoder are fixed. We first train the instance detector and obtain the correct set. Then, we train the hierarchical relational searching module on the correct set. Specifically, the instance detector selects a few high-quality sentences under the guidance of the reward function, and then feeds into the hierarchical relational searching module, their semantics can be associated with other samples via a gating mechanism. The topto-down search processing aims to simulate the prediction of long-tail relations, and the result can then be used as a new reward function to update the first module. The whole process is executed iteratively. In this manner, our proposed RH-Net fully considers the interaction between the two problems, so as to further improve the performance of relation extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Evaluation Metrics</head><p>We evaluate the proposed framework on widely used Distant Supervision (DS) dataset <ref type="bibr">NYT [Riedel et al., 2010]</ref>. The dataset 3 has 52 semantic relations and a special NA label which means that no relation between entity pair. The training set contains 522611 sentences, 281270 entity pairs, and 18252 relational facts. The testing set contains 172448 sentences, 96678 entity pairs, and 1950 relational facts.</p><p>To fairly compare with some baselines 4 , we follow <ref type="bibr" target="#b5">[Lin et al., 2016]</ref> to evaluate our method in the held-out evaluation and manual evaluation. The held-out evaluation aims to compare the predicted relational fact from the test data with the facts in Freebase, but it does not consider the efficiency of predicting NA class. The manual evaluation is performed to avoid the influence of the noisy testing data by manually checking the efficiency. We select precision-recall (P-R) curve, P@N, and Hits@K metrics to report the results of the experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiment Settings</head><p>In sentence encoder, we use the same hyper-parameters as previous works <ref type="bibr" target="#b7">[Zeng et al., 2015]</ref>. The word embedding size d w = 50, The position embedding size d p = 5. The filters K = 230 and the window size l is set to 3. The implicit relation and memory cell embedding dimension d r = d cell = 50. The batch size is 64. The learning rate was 0.02, 0.01 at the pre-training and joint training stage, respectively. We employ a dropout strategy with a probability of 0.5. The small constant ? = 0.5. We pre-train sentence encoder and instance detector for 5 epochs. The joint training iteration number L is 30. We apply Adam [Kingma and Ba, 2014] method to optimize parameters both at the pre-training and iterative training stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison Methods</head><p>We use held-out evaluation to compare our model RH-Net with several baselines.</p><p>? Feature-based methods utilize traditional feature engineering and machine learning, such as Mintz <ref type="bibr" target="#b6">[Mintz et al., 2009]</ref> MultiR leverages MIL to reduce the noise and handle the overlapping problem by proposed probabilistic graphical module. MIML utilizes multiinstance multi-label method for extracting semantics relation between two entities.</p><p>? Soft-strategy methods leverage sentence-level attention mechanism to reduce the influence of noisy, including BGW-A [Jat et al., 2018], CNN +ATT and PCNN +ATT <ref type="bibr" target="#b5">[Lin et al., 2016]</ref>. BGWA is a bidirectional GRU based RE model with piecewise max pooling. CNN +ATT is a basic CNN module with sentence-level attention mechanism. PCNN +ATT combines the sentencelevel attention with PCNN to capture structure information between two entities.</p><p>? Hard-strategy methods aim to filter noisy before RE, consisting of CNN +RL <ref type="bibr" target="#b1">[Feng et al., 2018]</ref> and PCNN +ATT +RL <ref type="bibr" target="#b6">[Qin et al., 2018b]</ref>. CNN +RL is a novel method to reduce noisy labeling data by RL, and achieves rewards from CNN to evaluate the RL. PCNN +ATT +RL also introduce the RL, but it redistributes noisy sentences into negative examples.</p><p>Note that, we find a new work , which is also improved by reinforcement learning belongs to hard-strategy methods, but they do not provide the code or corresponding experimental P-R curve raw result data. Therefore, we ignore this comparison.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Main Results</head><p>As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, we use the P-R curve to make a comparison without NA label 5 , where the x-axis denotes the recall and y-axis denotes the precision. The main results indicate that 1) both soft-strategy and hard-strategy based on deep learning methods outperform the feature-based methods, it means that the representation and generalization of traditional feature engineering unable to improve the performance.</p><p>2) The performance of the CNN-based method is worse than the PCNN-based, this is due to the factor that CNN ignores the entity structure information, while other methods consist of piecewise max-pooling can make reliable promotion. 3) We also find our proposed RH-Net outperforms all other baselines by a large margin, which demonstrates that the successful improvement of considering two problems and integrating two main solution modules to joint train. 4) We also train our method without reinforcement learning (denotes RH-Net(no RL)), we find that the result will be slightly worse than RH-Net with RL. It indicates that the noisy labeling data can disturb the model to learn semantics relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analyses</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">The results for long-tail relations</head><p>We also demonstrate the improvements for long-tail relations. We choose three attention-based models +ATT <ref type="bibr" target="#b5">[Lin et al., 2016]</ref>, +HATT <ref type="bibr" target="#b2">[Han et al., 2018b]</ref>   ? +ATT is the traditional sentence-level attention mechanism over instances, such as CNN+ATT and PCNN+ATT <ref type="bibr" target="#b5">[Lin et al., 2016]</ref>.</p><p>? +HATT is the hierarchical attention method over the instances, the difference is that it considers the hierarchical structure of semantic relation.</p><p>? +KATT is also an attention-based method, which utilizes knowledge base embedding (KBE) and graph neural network (GNN) to represent the hierarchical relational label.</p><p>To make a fair comparison, we follow the same evaluation strategy by them. Specifically, we obtain a subset from testing data in which all the relations have fewer than 100 or 200 instances, we leverage the macro Hits@K metric, which means that the accuracy of the golden relation in the top K candidate relations recommended by our model. In this experiment, we select K from {10, 15, 20}. Note that, the PCNN+RL+HRS denotes our proposed RH-Net, where +RL means using reinforcement learning and +HRS means using hierarchical relational searching. CNN+RL+HRS is the same as RH-Net, except for the encoder is CNN. As shown in <ref type="table" target="#tab_4">Table 1</ref>, it illustrates that: 1) The PCNN-based encoder is better than CNN, which indicates that the piecewise information is also useful for longtail prediction. 2) HRS module with both CNN and PCNN outperforms previous works, it verifies that the hierarchical tree processing is really better than simple attention. 3) If we use RL to filter the noisy data before relation extraction, despite obtaining a bit of improvement, it is still hard to extract the long-tail relations because of the reduction of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Study</head><p>We perform ablation experiments to validate the contributions of different components of our models. We report the P@N metric, which denotes the top N of precision. Specifically, we evaluate all the testing instances and achieve the corresponding sorted precision value at layer 1, and then we choose the N-th value as P@N. We remove the following settings:</p><p>? w/o IR is the method without implicit relation in instance detector or HRS.</p><p>? w/o GM is the method without gating mechanism, which calculates the score function by only the semantics of instance itself.</p><p>? w/o WL is the method without the weighted influence of different layers or nodes, it means that we replace the Equal 15 with a simple average operation.</p><p>As shown in <ref type="table" target="#tab_5">Table 2</ref>, we find that if we remove one of these components, the performance of both RH-Net will be worse. Specifically, 1) if we ignore the implicit relation, the average of P@N will reduce by 2.10%, owing to the agent missing some semantics of implicit relation information. 2) when we remove the gating mechanism, the average of P@N metric will reduce by more than 10%, it illustrates that the gating mechanism is vital for sharing the knowledge between related instances, it is also an important part of our method to deal with the long-tail problem. 3) when we ignore the weighted sum loss, the average of P@N value will be reduced by 7.30%, which means that the weighted sum loss of different layers or nodes makes positive contributions to stable training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Case Study</head><p>We further present some sentences in <ref type="table" target="#tab_7">Table 3</ref> for the case study. The text in bold represents the entity. The first two sentences which belong to a long-tail class, successfully selected by the instance detector and predicted by HRS. The third noisy sentence is removed for the noisy set and directly predicted as NA. Our method makes the wrong prediction on the last sentence, we analyze that the sample number of /business/company/advisors is too small to predict the third layer, but our HRS still performs well in the first two layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose a novel framework to alleviate both noisy labeling and long-tail problem. We apply RL to select the correct data and improve the RL by implicit relation information and a novel reward function that consider the contributions of both correct and noisy data. For the longtail problem, we newly transform the relation extraction into a tree searching task and share the semantics of related instances between data-rich classes at the head of distribution and data-poor classes at the tail. We also provide a hierarchical weighted loss function to train this module. Extensive experimental results on the NYT dataset show that our method outperforms state-of-the-art baselines. In the future, we will pay attention to the overlapping problem. We also decide to apply this proposed framework to the few-shot RE task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The example of sentence alignment from fact (Obama, born in, US.) by distance supervision. It shows that only the first is correct labeling data and others are noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>/iesl.cs.umass.edu/riedel/ecml/ 4 https://github.com/thunlp/OpenNRE Comparison with previous baselines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>In 1961, [Obama] h [was born in] r Hawaii, [US.] t born in</figDesc><table /><note>[Barack Obama] h takes the Oath of Office as the 44th [president of] r the [United State]t president of Last night, [Obama] h gave a radical speech at McCormick Place, Chicago, [US.]t unknown ... ...</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>DS data Sentence Encoder Instance Detector</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>Pretrain</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>reward</cell><cell>Sentence Embedding</cell><cell></cell><cell>Evaluate</cell><cell cols="2">implicit relation</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>root</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Input Gate</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(layer k)</cell><cell></cell><cell></cell></row><row><cell>Noisy Correct</cell><cell>Policy</cell><cell>Bag Embedding</cell><cell cols="2">Hierarchical Searching Relational</cell><cell>output Gate (layer k)</cell><cell>/people Hierarchical Similarity</cell><cell></cell><cell>...</cell><cell>...</cell></row><row><cell>action</cell><cell>implicit relation</cell><cell></cell><cell></cell><cell></cell><cell>/people/family</cell><cell>/prople/person</cell><cell cols="2">/people/ethnicity</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>...</cell><cell cols="2">/people/person/religion</cell><cell>...</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Top-to-down Searching</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>where t, h denotes the knowledge base embedding pre-trained byTransE [Fan et al.,  2014].</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>The marco accuracy of Hits@K on long-tail relations. +RL means using reinforcement learning for denoising, +HRS means using hierarchical relational searching module.</figDesc><table><row><cell cols="3">Training Instances</cell><cell></cell><cell>&lt;100</cell><cell>&lt;200</cell></row><row><cell cols="3">Hits@K(Marco)</cell><cell>10</cell><cell cols="2">15 20 10 15 20</cell></row><row><cell></cell><cell cols="2">+ATT</cell><cell cols="3">&lt;5.0 &lt;5.0 18.5 &lt;5.0 16.2 33.3</cell></row><row><cell></cell><cell cols="2">+HATT</cell><cell cols="3">5.6 31.5 57.4 22.7 43.9 65.1</cell></row><row><cell>CNN</cell><cell cols="2">+KATT</cell><cell cols="3">9.1 41.3 58.5 23.3 44.1 65.4</cell></row><row><cell></cell><cell cols="2">+HRS</cell><cell cols="3">9.5 40.2 59.6 23.9 47.0 66.6</cell></row><row><cell></cell><cell cols="5">+RL+HRS 11.3 41.5 60.1 25.0 47.1 66.9</cell></row><row><cell></cell><cell cols="2">+ATT</cell><cell cols="3">&lt;5.0 7.4 40.7 17.2 24.2 51.5</cell></row><row><cell></cell><cell cols="5">+HATT 29.6 51.9 61.1 41.4 60.6 68.2</cell></row><row><cell>PCNN</cell><cell cols="5">+KATT 35.3 62.4 65.1 43.2 61.3 69.2</cell></row><row><cell></cell><cell cols="2">HRS</cell><cell cols="3">36.8 64.0 68.8 44.8 62.0 71.5</cell></row><row><cell></cell><cell cols="5">+RL+HRS 36.6 64.1 68.9 44.5 62.3 71.7</cell></row><row><cell cols="2">Methods</cell><cell cols="4">Precision P@100 P@200 P@300</cell><cell>Avg.</cell></row><row><cell cols="2">RH-Net</cell><cell cols="2">90.00</cell><cell>84.50</cell><cell>79.33</cell><cell>84.61</cell></row><row><cell cols="2">w/o IR</cell><cell cols="2">88.11</cell><cell>81.93</cell><cell>77.50</cell><cell>82.51</cell></row><row><cell cols="2">w/o GM</cell><cell cols="2">82.19</cell><cell>74.00</cell><cell>63.33</cell><cell>73.17</cell></row><row><cell cols="2">w/o WL</cell><cell cols="2">83.26</cell><cell>77.33</cell><cell>71.35</cell><cell>77.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>The ablation results of RH-Net on NYT data set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Kuhn and his wife luisa relocated to ponte vedra beach , florida in 1990 , ... /location/location/contains /location/location/contains No a former military ruler , muhammadu buhari , also a northern muslim , is a leading candidate , ... /people/person/religion /people/person/religion No ... the american rights to jonathan littell's novel les bienveillantes, which became a publishing sensation in france, have been sold to harpercollins, ...</figDesc><table><row><cell>Sentences</cell><cell>Original label</cell><cell>Predicted label</cell><cell>Is noise?</cell></row><row><cell></cell><cell>/people/person/nationality</cell><cell>NA</cell><cell>Yes</cell></row><row><cell>... the annual meeting morphed into a three and a half hour cele-</cell><cell cols="2">/business/company/advisors /business/company/founders</cell><cell>No</cell></row><row><cell>bration of sanford i. weill , citigroup 's departing chairman</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">and +KATT [Zhang et</cell></row><row><cell></cell><cell>al., 2019]:</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Some sentences for case study.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Bag is the set of sentences which aligned with the same entity pair and labeled as the same relation class.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Instance is referred to a bag in this module</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We follow previous works to only preserve the precision and recall value of no-NA labels, because that the prediction of NA is useless.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Chang, and Fang Zheng. Transition-based knowledge graph embedding with relational mapping properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beltagy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PACLIC</title>
		<imprint>
			<biblScope unit="page" from="328" to="337" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>NAACL</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Denoising distant supervision for relation extraction via instance-level adversarial training</title>
		<idno type="arXiv">arXiv:1808.08013</idno>
		<idno>arXiv:1805.10959</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Reinforcement learning for relation classification from noisy data</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hierarchical relation extraction with coarse-to-fine grained attention</title>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2236" to="2245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improving distantly supervised relation extraction using word and entity based attention</title>
		<idno>abs/1804.06987</idno>
	</analytic>
	<monogr>
		<title level="m">Sharmistha Jat, Siddhesh Khandelwal, and Partha P. Talukdar</title>
		<imprint>
			<publisher>CoRR</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
	<note>Knowledge-based weak supervision for information extraction of overlapping relations</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Self-attention enhanced selective gate with entity-aware embedding for distantly supervised relation extraction</title>
		<idno type="arXiv">arXiv:1412.6980</idno>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<meeting><address><addrLine>Ba</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3060</biblScope>
			<biblScope unit="page" from="8269" to="8276" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>AAAI</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural relation extraction with selective attention over instances</title>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2124" to="2133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mintz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<editor>Shikhar Vashishth, Rishabh Joshi, Sai Suman Prayaga, Chiranjib Bhattacharyya, and Partha Talukdar</editor>
		<imprint>
			<publisher>Xu and Barbosa</publisher>
			<date type="published" when="1992" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="3201" to="3206" />
		</imprint>
	</monogr>
	<note>NAACL</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction via piecewise convolutional neural networks</title>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<editor>Llu?s M?rquez, Chris Callison-Burch, Jian Su, Daniele Pighin, and Yuval Marton</editor>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1753" to="1762" />
		</imprint>
	</monogr>
	<note>Cross-relation cross-bag attention for distantly-supervised relation extraction</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long-tail relation extraction via knowledge graph embeddings and graph convolution networks</title>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3016" to="3025" />
		</imprint>
	</monogr>
	<note>AAAI</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
