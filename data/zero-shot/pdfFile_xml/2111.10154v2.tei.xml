<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Small Changes Make Big Differences: Improving Multi-turn Response Selection in Dialogue Systems via Fine-Grained Contrastive Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Machine Intelligence</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Xu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft STCA</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huang</forename><surname>Hu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft STCA</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sha</surname></persName>
							<email>2lei.sha@cs.ox.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Machine Intelligence</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
							<email>djiang@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft STCA</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Small Changes Make Big Differences: Improving Multi-turn Response Selection in Dialogue Systems via Fine-Grained Contrastive Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Retrieve-based dialogue response selection aims to find a proper response from a candidate set given a multi-turn context. Pre-trained language models (PLMs) based methods have yielded significant improvements on this task. The sequence representation plays a key role in the learning of matching degree between the dialogue context and the response. However, we observe that different context-response pairs sharing the same context always have a greater similarity in the sequence representations calculated by PLMs, which makes it hard to distinguish positive responses from negative ones. Motivated by this, we propose a novel Fine-Grained Contrastive (FGC) learning method for the response selection task based on PLMs. This FGC learning strategy helps PLMs to generate more distinguishable matching representations of each dialogue at fine grains, and further make better predictions on choosing positive responses. Empirical studies on two benchmark datasets demonstrate that the proposed FGC learning method can generally and significantly improve the model performance of existing PLMbased matching models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Building an intelligent conversational agent that can naturally and consistently converse with humans has drawn considerable attention in the field of natural language processing (NLP). So far, there are two kinds of approaches for implementing a dialogue system: generation-based <ref type="bibr" target="#b19">(Serban et al. 2016;</ref><ref type="bibr" target="#b24">Vinyals and Le 2015)</ref> and retrieval-based methods <ref type="bibr" target="#b16">(Lowe et al. 2015;</ref><ref type="bibr" target="#b34">Wu et al. 2017;</ref><ref type="bibr" target="#b41">Zhou et al. 2018)</ref>. In this work, we focus on the problem of multi-turn response selection for retrieval-based dialogue systems.</p><p>Multi-turn response selection is the task of predicting the most proper response using a retrieval model by measuring the matching degree between a multi-turn dialogue context and a set of response candidates. Most recently, pre-trained language models (PLMs) have achieved substantial performance improvements in multi-turn response selection <ref type="bibr" target="#b17">(Lu et al. 2020;</ref><ref type="bibr" target="#b5">Gu et al. 2020;</ref><ref type="bibr" target="#b11">Humeau et al. 2020)</ref>. PLM-based models take a pair of context and response as a whole and <ref type="figure">Figure 1</ref>: Although a standard contrastive learning can provide better uniformity on the vector space, it is still hard to distinguish between positive (yellow) and negative (blue) dialogues with the same context (in the same shape, e.g., star).</p><p>Despite the success of PLM-based matching models and their various variants, recent studies reveal that the contextualized word and sentence representations in PLM-based models are anisotropic, i.e., representations occupy a narrow cone in the vector space, which largely limits the expressiveness of representations <ref type="bibr" target="#b2">(Ethayarajh 2019;</ref><ref type="bibr" target="#b13">Li et al. 2020)</ref>. Contrastive learning provides a way to solve this problem, which uniforms representations over a hypersphere space as pointed out by <ref type="bibr" target="#b28">Wang and Isola (2020)</ref>. Although employing the standard constrative learning method <ref type="bibr" target="#b3">Fang and Xie 2020)</ref> with in-batch negative sampling enhances the uniformity of representations, we further observe by experiments (Section 6.3) that the matching representation of two dialogues with the same context but different responses are too similar, as is shown in <ref type="figure">Figure 1</ref>. This is mainly due to the following two reasons: (1) The overlap of the same context tokens in different context-response pairs makes the matching representation highly similar since the representation is aggregated from all tokens in the contextresponse pair.</p><p>(2) The in-batch negative samples are highly likely to be different in both context and response. This phenomenon makes the representations less distinguishable and makes it hard to tell dialogues with positive responses from negative ones.</p><p>To address the aforementioned issues, we propose a Fine-Grained Contrastive learning (FGC) approach to fine-tune matching representations for the response selection task. FGC introduces the connections between each dialogue pair with the same context and different responses into the training process with contrastive learning. In contrast to the standard contrastive learning method, which takes every other dialogue as negative examples, our proposed FGC takes context and response as separate parts and focuses on distinguishing between positive and negative dialogues with the same context. During fine-grained contrastive learning, each dialogue is converted into an augmented dialogue via rulebased transformation on the response utterance. Each dialogue is asked to be close to its augmentation, while the one with a positive response should be far away from the one with a negative response. FGC works totally in a selfsupervised way that no additional supervision is required besides the classification label used for response selection training.</p><p>We conduct experiments on two response selection benchmarks: the Ubuntu Dialogue Corpus <ref type="bibr" target="#b16">(Lowe et al. 2015)</ref> and the Douban Corpus <ref type="bibr" target="#b34">(Wu et al. 2017</ref>). These two datasets have a large variety of both topics and languages. Moreover, our proposed learning framework is independent of the choice of PLMs-based matching models. Therefore, for a comprehensive evaluation, we test FGC with five representative PLMs-based matching models, including the state-of-the-art one brought by self-supervised learning. Our empirical results demonstrate that FGC is able to consistently improve PLMs by up to 3.2% absolute improvement with an average of 1.7% absolute improvement in terms of R 10 @1. Besides, We also compare our method with standard-contrastive-learning-enhanced PLMs, which demonstrates the effectiveness of our proposed fine-grained contrastive objective.</p><p>In summary, our contributions in the paper are three-fold:</p><p>? We propose FGC, a novel fine-grained contrastive learning method, which helps generate better representations of dialogues and improves the response selection task. ? FGC shows good generality of effectiveness with various pre-trained language models for enhancing performance. ? Experimental results on two benchmark datasets demonstrate that FGC can significantly improve the performance of various strong PLM-based matching models, including state-of-the-art ones.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Multi-Turn Response Selection</head><p>Earlier studies on retrieval-based response selection focused on single-turn response selection <ref type="bibr" target="#b25">(Wang et al. 2013;</ref><ref type="bibr" target="#b10">Hu et al. 2015;</ref><ref type="bibr" target="#b27">Wang et al. 2015)</ref>. Recently, researchers have paid more attention to the multi-turn response selection. The dual LSTM model <ref type="bibr" target="#b16">(Lowe et al. 2015)</ref> was proposed to match the dialog context and response using a dual RNN-based architecture. <ref type="bibr" target="#b40">Zhou et al. (2016)</ref> proposed the multi-view model that encodes dialogue context and response both on the word-level and utterance-level. However, these methods ignore relationships among utterances by simply concatenating the utterances together or converting the whole context to a vector. To alleviate this, <ref type="bibr" target="#b34">Wu et al. (2017)</ref> proposed the sequential matching network that each utterance in the context first interacts with the response candidate, and then the matching features are aggregated according to the sequential order of multi-turn context. With the rise of the selfattention mechanism <ref type="bibr" target="#b23">(Vaswani et al. 2017)</ref>, some studies <ref type="bibr" target="#b41">(Zhou et al. 2018;</ref><ref type="bibr" target="#b21">Tao et al. 2019a</ref>) explored how to enhance representations with it. Besides, <ref type="bibr" target="#b37">Yuan et al. (2019)</ref> recently revealed that previous methods construct dialogue contextual representation using too much context, which damages the matching effect. They proposed a multi-hop selector to select the relevant utterances in the dialogue context for response matching.</p><p>Most recently, pre-trained language models (e.g., BERT <ref type="bibr" target="#b1">(Devlin et al. 2019)</ref> and RoBERTa ) have shown an impressive performance in the response selection. The post-training method, which helps transfer the representations of BERT from the general domain to the dialogue domain, was proposed by <ref type="bibr">Whang et al. (2020)</ref> and obtained state-of-the-art results. Subsequent researches <ref type="bibr" target="#b5">(Gu et al. 2020;</ref><ref type="bibr" target="#b17">Lu et al. 2020)</ref> focused on incorporating speaker information into BERT and showed its effectiveness in multi-turn response selection. Further, self-supervised learning contributed to the success of pre-trained language models was also applied in several NLP downstream tasks, such as summarization <ref type="bibr" target="#b26">(Wang et al. 2019</ref>) and the response generation <ref type="bibr" target="#b39">(Zhao, Xu, and Wu 2020)</ref>. In the task of response selection, <ref type="bibr" target="#b32">Whang et al. (2021a)</ref> and <ref type="bibr" target="#b36">Xu et al. (2021)</ref> indicated that incorporating well-designed self-supervised tasks according to the characteristics of the dialogue data into BERT finetuning can help with the multi-turn response selection. <ref type="bibr" target="#b8">Han et al. (2021)</ref> proposed a fine-grained post-training method for enhancing the pre-trained language model, while the post-training process is computationally expensive than finetuning a classification model. <ref type="bibr" target="#b20">Su et al. (2020)</ref> proposed a hierarchical curriculum learning framework for improving response selection with PLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Contrastive Learning for NLP</head><p>There have been several investigations for contrastive loss formulations for deep neural models, primarily in the computer vision domain. <ref type="bibr" target="#b18">Oord, Li, and Vinyals (2018)</ref> proposed a framework for contrastive learning to learn visual representations based on contrastive predictive coding, which predicts the features in latent space by using powerful autoregressive models. <ref type="bibr" target="#b12">Khosla et al. (2020)</ref> investigated supervised contrastive learning, allowing to leverage label information effectively. Following this trend, some researchers verified the effectiveness of constructive learning in specific NLP tasks. <ref type="bibr" target="#b3">Fang and Xie (2020)</ref> proposed pretraining language representation models with a contrastive self-supervised learning objective at the sentence level, outperforming previous methods on a subset of GLUE tasks. <ref type="bibr" target="#b7">Gunel et al. (2021)</ref> combined the cross-entropy with a supervised contrastive learning objective, showing improvements over fine-tuning RoBERTa-Large on multiple datasets of the GLUE benchmark. Our work differs from previous works in that we do not directly make contrast on one dialogue with all the other dialogues, as the granularity of negative samples constructed using this approach is too coarse to provide sufficient discrimination with the positive ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task Formalization</head><p>The response selection task is to select the best candidate to respond a given multi-turn dialogue context from a pool of candidate responses. Suppose that we have a dataset</p><formula xml:id="formula_0">D = {c i , r i , y i } N i=1 , where c i = {u 1 i , ? ? ? , u ni i</formula><p>} is a multiturn dialogue context with n i turns, r i denotes a candidate response, and y i ? {0, 1} denotes a label with y i = 1 indicating r i a proper response for c i and otherwise y i = 0. Our goal is to estimate a matching model y = f (?, ?) from D. For any given context-response pair (c, r), f (c, r) returns a score that reflects the matching degree between c and r.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pre-trained Language Model for Response Selection</head><p>As a trend in these years, pre-trained language models, e.g., BERT <ref type="bibr" target="#b1">(Devlin et al. 2019)</ref>, have been widely studied and adapted into numerous NLP tasks, showing several state-ofthe-art results. Dialogue response selection is one of them. Applying a pre-trained language model into response selection usually involves two steps. The first step is to make domain-adaptive post-training, which continues to train a standard pre-trained language model with a domain-specific corpus. This step helps to transfer the original pre-trained language model into the target domain.</p><p>The second step is to fine-tune the post-trained model with the response selection task. Given a context c = {u 1 , ? ? ? , u m } where u t is the t-th turn of the dialog context, and a response r, the model is asked to predict a score? to represent the matching degree between c and r. To achieve this, a special token [EOT] is added at the end of each turn to distinguish them in the context c. Utterances from both the context c and response r are concatenated with separator [EOT] and [SEP] between them. Taking x as input, BERT returns a sequence of vectors with the same length as x. The output of the first place s <ref type="bibr">[CLS]</ref> is an aggregated representation vector that holds the information of interaction between context c and response r. A relevance score? is computed based on s <ref type="bibr">[CLS]</ref> and optimized through a binary classification loss.?</p><formula xml:id="formula_1">= ?(W sel s [CLS] + b) L sel = ? (y log? + (1 ? y) log(1 ??)) ,<label>(1)</label></formula><p>where W sel and b are parameters 4 Methodology</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview</head><p>In this paper, we propose the Fine-Grained Contrastive Learning method (FGC) for learning PLMs-based matching models. It consists of two complementary contrastive objectives: (1) an instance-view contrastive objective (IVC); and (2) a category-view contrastive objective (CVC). <ref type="figure">Figure 2</ref> demonstrates the joint effects of the two contrastive objectives on the space of matching representations. The IVC objective pushes away dialogue instances with the same context and different responses, making the model easier to distinguish between positive and negative responses. However, only pushing the examples with the same context away increases the risk of instances with different contexts getting closer in the representation space. As a remedy, the CVC objective further pulls all context-response pairs into two distinguishable clusters in the matching space according to whether the pair is positive or not. These two objectives are introduced in 4.3 and 4.4 respectively. For simplicity, we take BERT as an example in the following sections in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Dialogue Data Augmentation</head><p>Data augmentation takes an important role in contrastive learning <ref type="bibr" target="#b42">(Zoph et al. 2020;</ref><ref type="bibr" target="#b9">Ho and Vasconcelos 2020)</ref>. Similar to standard contrastive learning (e.g., CERT), the first step of FGC is to create augmentations for every contextresponse pair. Given a context-response pair, we take an augmentation method on the response to generate an augmented response. The context and augmented response pair form the augmentation of the original context-response pair. In order to fine-grained control the difference between a dialogue and its corresponding augmentation and easily perform augmentation on various languages, a fully unsupervised rule-based utterance augmentation method is adopted for utterance augmentation. Inspired by <ref type="bibr" target="#b29">(Wei and Zou 2019)</ref>, we adopt three types of augmentation operations:</p><p>? Random deletion: Each token in the utterance is randomly and independently deleted with a probability p del . ? Random swaping: Each token in the utterance is randomly swapped with another token in the utterance with a probability p swap . ? Synonym replacing: Randomly replace a non-stop-word token to one of its synonyms with a probability p syn .</p><p>Given a response utterance r and an augmentation strength p ? [0, 1], we randomly pick out one of these three augmentation methods and then apply the augmentation on the utterance with the probability being p. After augmentation, the response r is converted into another augmented responser. The augmentation strength p is a hyper-parameter that controls how much difference is there between r andr.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Instance-View Contrastive Objective</head><p>The instance-view contrastive (IVC) objective aims at introducing more discrepancy between a pair of dialogues with the same context and positive/negative responses. Feeding</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IVC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vector Space</head><p>Vector Space CVC Vector Space <ref type="figure">Figure 2</ref>: FGC contains two objectives, i.e., IVC and CVC. IVC pushes away dialogues with the same context but different responses (icons in the same shape), while dialogues that belong to different categories may still be similar. CVC further solves this problem by pulling all dialogues into two distinguishable clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Negative Examples Positive Examples</head><formula xml:id="formula_2">+ + BERT ? + ? + BERT - - BERT ? - ? - BERT Minimize Minimize Maximize</formula><p>Rule-based Utterance Augmentation Module a dialogue into BERT, BERT helps to make internal interactions by attention mechanism and generate latent vectors representing the dialogue. The output vector of the [CLS] position s <ref type="bibr">[CLS]</ref> stands for a aggregated sequence representation of both context and response. We also take this vector as the dialogue matching representation used for contrastive learning. Moreover, we apply another projection layer to convert s <ref type="bibr">[CLS]</ref> into a smaller vector z. This projection is made through an MLP with one hidden layer. Through this projection, each coherent dialogue with positive responses (c i , r i +) is transformed into a projection vector z i +, and each incoherent dialogue (c i , r i ?) is transformed into z i ?. The augmentations of the positive and negative dialogues are also converted into two vectors, i.e.,z i + andz i ?. Here + and ? indicates the item belongs to the positive class or the negative class, and the bar indicates this item comes from an augmented example.</p><p>As illustrated by <ref type="bibr" target="#b2">Ethayarajh (2019)</ref> and <ref type="bibr" target="#b13">Li et al. (2020)</ref>, the embedding vectors of different utterances are distributed in a narrow cone of the vector space, showing less distin-guishability. This phenomenon is even worse when two utterances are semantically similar, e.g., two dialogues sharing the same context. Thus, we leverage the IVC objective on these projection vectors z to distinguish between positive and negative responses given the same context. IVC objective regards the projection vector z as a representation of response r given context c. This loss is applied on the projection vector z, which helps to maximize the similarity between a response with its augmentation given the same context, as well as minimize the similarity between each positive response and negative response pair. The maximum and minimum are achieved as a set of pair-wise comparisons, i.e.,</p><formula xml:id="formula_3">?i sim(z i +,z i +) &gt; sim(z i +, z i ?), sim(z i +,z i ?) sim(z i +, z i ?), sim(z i +,z i ?) ?i sim(z i ?,z? i ) &gt; sim(z i ?, z i +), sim(z i ?,z i +) sim(z i ?, z i +), sim(z i ?,z i +).</formula><p>(2)</p><p>Here we use the NT-Xent Loss  to model the similarities of projection vectors. By writing this pairwise comparison into a loss function, the IVC loss is formulated as l(z,z) = ? log exp(sim(z,z)/? )</p><formula xml:id="formula_4">z k =z exp(sim(z, z k )/? ) L ivc = N i=1 (l (z i +,z i +) + l (z i ?,z i ?)) ,<label>(3)</label></formula><p>where ? &gt; 0 is an adjustable scalar temperature parameter that controls the separation of positive and negative classes; z k ranges from {z+,z+, z?,z?}; and N is the total number of dialogues. Notice that the IVC objective aims to separate the representation of positive and negative responses given the same context, so that we do not take all other in-batch examples as negative examples in the same way as in standard contrastive learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Category-View Contrastive Objective</head><p>The IVC objective ensures a high difference between dialogues with the same context, while it cannot guarantee that the learned representations are suitable for classification. The representations of a positive dialogue may be close to the representation of another negative dialogue with a different context, as is shown in <ref type="figure">Figure 2</ref>. Thus, we introduce another category-view contrastive (CVC) objective into model training. The category-view contrastive objective aims at bunching dialogues that belong to the same category into a cluster and separate these two clusters.</p><p>The CVC objective is applied between dialogues from the two classes. It captures the similarity of projection vectors of the same class and contrasts them with projection vectors from the other class, i.e.,</p><formula xml:id="formula_5">?i, j, k, l sim(z i +, z j +) &gt; sim(z k +, z l ?) ?i, j, k, l sim(z i ?, z j ?) &gt; sim(z k +, z l ?).<label>(4)</label></formula><p>This category-view contrastive loss works with a batch of representation vectors of size 2N , where the number of both positive examples and negative examples is N . Denote {z 1 , z 2 , ? ? ? , z 2N ?1 , z 2N } to be all representation vectors in a batch, where {z 1 , z 2 , ? ? ? , z N } are representation vectors for positive dialogues and their augmentations, and {z N +1 , z N +2 , ? ? ? , z 2N } are representation vectors for negative dialogues and their representations. The CVC objective works as an additional restriction to punish the high similarity between positive-negative pairs and low similarity within all positive and negative dialogues. The following formulas give this loss:</p><formula xml:id="formula_6">l(z i , z j ) = log exp(z i ? z j /? ) i =r exp(z i ? z r /? ) L cvc = ? 1 N ? 1 2N i=1 i =j 1? i=?j l(z i , z j )<label>(5)</label></formula><p>.</p><p>Finally, the BERT model is fine-tuned with the standard response selection loss L sel and both IVC and CVC loss. A weighted summation is computed as</p><formula xml:id="formula_7">L = L sel + ?(L ivc + L cvc ),<label>(6)</label></formula><p>where ? is a hyper-parameter that controls the balance between response selection loss and contrastive loss. The model is optimized by minimizing the overall loss value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset</head><p>? Ubuntu Dialogue Corpus V1 The Ubuntu Dialogue Corpus V1 <ref type="bibr" target="#b16">(Lowe et al. 2015</ref>) is a domain-specific multi-turn conversation dataset. Conversations in this dataset are dumped from the multi-party chat room whose topic is the Ubuntu operating system. We conducted experiments on a preprocessed data released by <ref type="bibr" target="#b35">Xu et al. (2019)</ref>, in which numbers, URLs, and system paths are masked by placeholders. Negative responses for each dialogue are randomly sampled from other dialogues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? Douban Corpus</head><p>The Douban Corpus <ref type="bibr" target="#b34">(Wu et al. 2017</ref>) is a Chinese dataset collected from an online social network website named Douban. Douban Corpus is an open-domain conversation corpus, whose topic is much wider than that of Ubuntu Corpus.</p><p>The statistics of these two datasets are shown in <ref type="table">Table  1</ref>. These two datasets vary greatly in both language and topic. Following previous works, we take R 10 @ks as evaluation metrics, which measures the probability of having the positive response in the top k ranked responses. We take k = {1, 2, 5} for model evaluation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baseline Methods</head><p>We introduce FGC into several open-sourced PLM-based models, including BERT and ELECTRA. We also test the effectiveness of FGC on variants of BERT model, including BERT-small (H=4, L=4, H=512), BERT with domainadaptive post training named BERT-DPT <ref type="bibr">(Whang et al. 2020)</ref>, and BERT with self-supervised tasks named BERT-UMS <ref type="bibr" target="#b33">(Whang et al. 2021b)</ref>. Several non-PLM-based models are also compared with our proposed FGC. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Implementation Details</head><p>All models are implemented based on Pytorch and Huggingface's implementation. Each PLM model is trained for 5 epochs with a learning rate beginning from 3e-6 to 0 with a linear learning rate decay. Our model is trained with 8 Nvidia Tesla A100 GPUs, which have 40GB of memory for each of them. For more training details, please refer to Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Experimental Results</head><p>The comparison between PLMs and FGC-enhanced PLMs is shown in <ref type="table" target="#tab_2">Table 2</ref>. It can be seen from the table that all PLMbased methods outperform non-PLM-based methods. By adding our proposed FGC into PLM-based models, the performance of all models is significantly improved. The maximum improvement of a standard-sized BERT for the two datasets are 1.9% and 3.2% respectively in terms of R 10 @1.</p><p>The average performance improvement also achieves 1.1% and 2.2%. Besides, our proposed method can also enhance the current state-of-the-art method BERT-UMS by 1.1% and 0.8% on two datasets in terms of R 10 @1. In addition to a standard-sized BERT model, we also find an absolute gain of 0.9% by adding FGC on the BERT-Small model, which is about 10? smaller than a standard one. The success of these two datasets demonstrates the effectiveness of our proposed Models Ubuntu Douban R 10 @1 R 10 @2 R 10 @5 MAP MRR P@1 R 10 @1 R 10 @2 R 10 @5 non-PLM-based methods Multi-View <ref type="bibr" target="#b40">(Zhou et al. 2016)</ref> 0  FGC across different models, languages, and dialogue topics on multi-turn response selection. FGC separates representation vectors of dialogues into different latent spaces according to their type of relevance between contexts and responses. On the one hand, IVC helps distinguish between positive and negative responses given the same context. On the other hand, CVC separates representations of dialogues from two categories so that these representations can have better distinguishability. As a result, the matching representation of context-response pairs for positive and negative responses are forced to stay away from each other. This better representation ensures higher accuracy in selecting the positive response given a candidate set of responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Closer Analysis</head><p>We conduct closer analysis with BERT-DPT since combining post-training and fine-tuning is the most popular manner of applying BERT for down-streaming tasks. The Ubuntu Corpus is used in the following analysis.  <ref type="table">Table 3</ref>: Ablation Analysis on the Ubuntu corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Ablation Studies</head><p>As we add two contrastive learning objectives into training for response selection, we test the gain of each objective.</p><p>The results are shown in <ref type="table">Table 3</ref>. It can be observed from the table that both IVC and CVC can enhance the performance on response selection, with an absolute improvement of 1.4% and 0.4% respectively in terms of R 10 @1. By applying these two contrastive objectives, we obtain an absolute improvement of 1.9% based on the post-trained BERT model. Both of the two contrastive objectives share the same purpose of separating the representation of dialogues with positive and negative responses, and thus there is a performance overlap by adding these two objectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Sensitive Analysis</head><p>Temperature Temperature ? works as a hyperparameter that controls the punishment on the degree of separation of positive and negative classes. A smaller ? gives more power to pull away dialogues from different classes. We test how this hyperparameter can influence the response selection performance. We test ? in the range of {0.1, 0.5, 1} on FGC and the results are shown in <ref type="table" target="#tab_5">Table 4</ref>. FGC achieves the best performance when ? is set to be 0.5, while the performance drops given a smaller or a bigger ? . A suitable ? can provide a proper differentiation that is neither too strong nor too weak, keeping a balance between contrastive and response selection objectives.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Discussion</head><p>Compare with Standard Contrastive Learning The main difference between our proposed FGC and standard contrastive learning (e.g., CERT <ref type="bibr" target="#b3">(Fang and Xie 2020)</ref> and SimCSE <ref type="bibr" target="#b4">(Gao, Yao, and Chen 2021)</ref>) is that we only take dialogues with the same context but different responses as negative examples, instead of using in-batch examples as negative ones. We compare FGC with those methods, whose results are shown in <ref type="table" target="#tab_8">Table 6</ref>. Standard contrastive learning can bring less gain (or even harm) on the response selection task, while contrastive learning with fine-grained negative examples leads to a significant gain on this task.   is the average similarity between all positive dialogues and negative dialogues. As can be seen from <ref type="table" target="#tab_9">Table 7</ref>, both similarities are lowered from a positive value indicating positive correlation into a negative value indicating negative correlation by adding FGC. By introducing better distinguishability into dialogue representations, our proposed FGC helps to make better response predictions effectively. Though these two similarities can also be lowered by adding IVC alone, the category similarity is not small enough to separate the two categories well. This shortcoming is compensated by further applying CVC as an additional training objective. Besides, CVC alone can neither provide a sufficiently low level of instance-level similarity that separates dialogues with the same context.</p><p>Effect of Data Augmentation Alone Data augmentation, working as a kind of data noise, shows a positive effect on training models with robustness in natural language processing. One may concern that can data augmentation alone help with the response selection task. We conducted experiments with data augmentation alone, i.e., no contrastive learning strategy is included. The results are shown in <ref type="table" target="#tab_10">Table 8</ref>. It can be observed from the table that data augmentation alone cannot enhance the model but even harm the accuracy significantly. Data augmentation methods should work with finegrained contrastive learning to make positive effects for the multi-turn response selection task.</p><p>Ubuntu Douban BERT-DPT 0.862 0.290 +Aug 0.837 (-2.5%) 0.278 (-1.2%) BERT-UMS 0.875 0.318 +Aug 0.851 (-2.4%) 0.292 (-2.6%) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we propose FGC, a fine-grained contrastive learning method, which helps to improve the multi-turn response selection task with PLM-based models. FGC consists of an instance-view contrastive (IVC) objective that helps to differentiate positive response and negative response with the same context, and a category-view contrastive (CVC) objective that separate positive dialogues and negative dialogues into two distinguishable clusters. Experiments and analysis on two benchmark datasets and five PLM-based models demonstrates the effectiveness of FGC to significantly improve the performance of multi-turn dialogue response selection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>An overview of IVC learning. The input is a dialogue context c and a pair of positive and negative responses (r+, r?). Both responses are augmented with a rule-based utterance augmentation model to form a new pair (r+,r?).We concatenate the context c with four responses and fed them into the BERT encoder, which outputs a projection vector z for each dialogue. IVC aims to maximize the dissimilarity of z between positive examples and negative examples, as well as maintains high cohesion within positive and negative cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Evaluation results on the two data sets. Numbers in bold indicate that the PLM-based models using FGC outperforms the original models with a significance level p-value &lt; 0.05.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Influence of temperature ? in FGC.Utterance Augmentation Strength Utterance augmentation plays an important role in contrastive learning. A dialogue with a context and a positive response is drawn closer to its augmentation while pushed far away from the dialogue with the same context but a negative response. The strength of utterance augmentation decides the boundary of each cluster. We conduct experiments to test how augmentation strength can influence response selection accuracy. We range the augmentation strength p from {0.1, 0.2, 0.5}, and the testing results are shown inTable 5. It achieves the best performance when p equals 0.2. Augmentation strength being either too large or too small may harm the clustering. On the one hand, a too-large p brings too much noise into the clustering process, which blurs the boundary between positive and negative examples. On the other hand, a too-small p cannot provide enough variation to the utterance, which harms the generalization of identifying positive and negative responses.</figDesc><table><row><cell cols="4">Augment Strength p R 10 @1 R 10 @2 R 10 @5</cell></row><row><cell>BERT-DPT</cell><cell>0.862</cell><cell>0.935</cell><cell>0.987</cell></row><row><cell>+ FGC (p=0.1)</cell><cell>0.874</cell><cell>0.938</cell><cell>0.989</cell></row><row><cell>+ FGC (p=0.2)</cell><cell>0.881</cell><cell>0.944</cell><cell>0.990</cell></row><row><cell>+ FGC (p=0.5)</cell><cell>0.872</cell><cell>0.935</cell><cell>0.990</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Influence of utterance augmentation strength p in FGC.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Influence of utterance augmentation strength p in FGC. between Dialogues The goal of FGC is to enlarge distances between dialogue examples with the same context and different responses. To estimate how effective this target is achieved, we compute two average cosine similarities: (1) instance-level similarity, which is the average similarity between dialogue pairs with the same context but different responses; and (2) category-level similarity, which</figDesc><table><row><cell>Similarity</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Similarity Analysis on the Ubuntu corpus.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Model performance with data augmentation alone.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">A pre-trained Chinese BERT-Small is not available, thus we do not conduct experiments on it.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://huggingface.co/</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A More Implementation Details Domain-adaptive Post-training For domain-adaptive post-training, we take the same hyper-parameter settings as BERT-DPT <ref type="bibr">(Whang et al. 2020)</ref>. Concretely, the maximum length of input dialogue is set to be 512. A full dialogue is randomly cut into a shorted token sequence with a probability of 10%. A masked language model loss and a next sentence prediction loss is optimized jointly during post-training. For the masked language model training, we masked each token with a probability of 15%. The posttraining process traverses all the dialogues for 10 iterations, and the words that are masked during each iteration are independently sampled.</p><p>Fine-tuning for Response Selection The model is finetuned with the response selection task. The projection layer for transforming [CLS] vectors into projection vectors z is an MLP with one hidden layer with hidden size being 256. For dialogues longer than 512 (i.e. the maximum length supported by BERT), we discard the beginning of its context while keeps a complete response, as the latter part of the dialogue context may have stronger relevance with the response. We take an AdamW optimizer <ref type="bibr" target="#b15">(Loshchilov and Hutter 2019)</ref> with linear learning rate decay for fine-tuning. The initial learning rate is 3 * 10 ?5 , and gradually decreases to 0 within 5 epochs. The ? for controlling the balance between response selection loss and contrastive loss is set to be 1.</p><p>All pre-trained language model checkpoints are downloaded from huggingface 3 , with their names as the keys except for BERT-Small. For the BERT-Small model, the pretrained model checkpoint is downloaded with model name "prajjwal1/bert-small". Each model is trained by 3 times, and the best results among them are reported.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>: Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ethayarajh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="55" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Cert: Contrastive self-supervised learning for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08821</idno>
		<title level="m">SimCSE: Simple Contrastive Learning of Sentence Embeddings</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Speaker-aware bert for multi-turn response selection in retrieval-based chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2041" to="2044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Interactive matching network for multi-turn response selection in retrievalbased chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2321" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gunel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Finegrained Post-training for Improving Retrieval-based Dialogue Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1549" to="1558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<title level="m">Contrastive learning with adversarial examples. NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Convolutional neural network architectures for matching natural language sentences. NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Poly-encoders: Transformer architectures and pretraining strategies for fast and accurate multi-sentence scoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Humeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-A</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Supervised contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On the Sentence Embeddings from Pre-trained Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9119" to="9130" />
		</imprint>
	</monogr>
	<note>Online: Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Roberta: A robustly optimized bert pretraining approach. ICLR</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving contextual language models for response retrieval in multiturn conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SigIR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1805" to="1808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Building end-to-end dialogue systems using generative hierarchical neural network models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Dialogue response selection with hierarchical curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Collier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.14756</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-representation fusion network for multi-turn response selection in retrieval-based chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="267" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">One Time of Interaction May Not Be Enough: Go Deep with an Interaction-over-Interaction Network for Response Selection in Dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
	<note>: Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Attention is all you need. NIPS</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05869</idno>
		<title level="m">A neural conversational model</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A Dataset for Research on Short-Text Conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="935" to="945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Self-Supervised Learning for Contextualized Extractive Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2221" to="2227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">Syntax-based deep matching of short texts. IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Understanding contrastive representation learning through alignment and uniformity on the hypersphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9929" to="9939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6382" to="6388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lim</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An Effective Domain Adaptive Post-Training Method for BERT in Response Selection</title>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Do Response Selection Models Really Know What&apos;s Next? Utterance Manipulation Strategies for Multi-turn Response Selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Do Response Selection Models Really Know What&apos;s Next? Utterance Manipulation Strategies for Multi-turn Response Selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sequential Matching Network: A New Architecture for Multiturn Response Selection in Retrieval-Based Chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Long Papers; Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="496" to="505" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">BERT Post-Training for Review Reading Comprehension and Aspectbased Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2324" to="2335" />
		</imprint>
	</monogr>
	<note>: Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Learning an Effective Context-Response Matching Model with Self-Supervised Tasks for Retrieval-based Dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-hop Selector Network for Multiturn Response Selection in Retrieval-based Chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="111" to="120" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Modeling Multi-turn Conversation with Deep Utterance Aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3740" to="3752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning a Simple and Effective Model for Multi-turn Response Generation with Auxiliary Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3472" to="3483" />
		</imprint>
	</monogr>
	<note>Online: Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-view Response Selection for Human-Computer Conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="372" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1118" to="1127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Rethinking pre-training and selftraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
