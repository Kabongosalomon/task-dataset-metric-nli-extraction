<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Subcategory-aware Convolutional Neural Networks for Object Proposals and Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
							<email>yuxiang@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
							<email>wongun@nec-labs.com</email>
							<affiliation key="aff1">
								<orgName type="institution">NEC Laboratories America, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
							<email>linyuanqing@baidu.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Baidu, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Subcategory-aware Convolutional Neural Networks for Object Proposals and Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In Convolutional Neural Network (CNN)-based object detection methods, region proposal becomes a bottleneck when objects exhibit significant scale variation, occlusion or truncation. In addition, these methods mainly focus on 2D object detection and cannot estimate detailed properties of objects. In this paper, we propose subcategory-aware CNNs for object detection. We introduce a novel region proposal network that uses subcategory information to guide the proposal generating process, and a new detection network for joint detection and subcategory classification. By using subcategories related to object pose, we achieve stateof-the-art performance on both detection and pose estimation on commonly used benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Convolutional Neural Networks (CNNs) have become dominating in solving different recognition problems recently. CNNs are powerful due to their capability in both representation and learning. With millions of weights in the contemporary CNNs, they are able to learn much richer representations from data. In object detection, we have witnessed the performance boost when CNNs <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b28">28]</ref> are applied to commonly used benchmarks such as PASCAL VOC <ref type="bibr" target="#b8">[9]</ref> and ImageNet <ref type="bibr" target="#b26">[26]</ref>.</p><p>However, there are two main limitations of the state-ofthe-art CNN-based object detection methods <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b25">25]</ref>. First, they rely on region proposal methods <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b0">1]</ref> to generate object candidates, which are often based on lowlevel image features such as superpixels or edges. Although these methods work very well on PASCAL VOC <ref type="bibr" target="#b8">[9]</ref> and ImageNet <ref type="bibr" target="#b26">[26]</ref>, however, when it comes to the KITTI dataset for autonomous driving <ref type="bibr" target="#b14">[14]</ref> where objects have large scale variation, occlusion and truncation, these region proposal methods perform very poor as observed in our experiments. Recently, the Region Proposal Network (RPN) in <ref type="bibr" target="#b25">[25]</ref> is able to improve over the traditional region proposal methods. However, it still cannot efficiently handle the scale change of object, occlusion and truncation. Second, the existing CNN-based object detection methods mainly focus on 2D object detection with bounding boxes. As a result, they are not able to estimate detailed information about objects such as 2D segmentation boundary, 3D pose or occlusion relationship between objects, while these information is critical for various applications such as autonomous driving, robotics and augmented reality. In this work, we explore subcategory information, which is widely used in traditional object detection <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b36">36]</ref>, to tackle the aforementioned two limitations in CNN-based object detection. For region proposal generation, we introduce a new CNN architecture that uses subcategory detections as object candidates. For detection, we modify the network in Fast R-CNN <ref type="bibr" target="#b15">[15]</ref> for joint detection and subcategory classification. <ref type="figure" target="#fig_0">Fig. 1</ref> illustrates our object detection framework. The concept of subcategory is general here. A subcategory can be objects with similar properties or attributes such as 2D appearance, 3D pose or 3D shape. By associating object attributes to subcategories, we are able to estimate these attributes (e.g., 2D segmentation boundary or 3D pose) by conducting subcategory classification.</p><p>Specifically, motivated by the traditional detection methods that train a template or a detector for each subcategory, we introduce a subcategory convolutional (conv) layer in our Region Proposal Network (RPN), where each filter in the conv layer is trained discriminatively for subcategory detection. The subcategory conv layer outputs heat maps about the presence of certain subcategories at a specific location and scale. Using these heat maps, our RPN is able to output confident subcategory detections as proposals. For classifying region proposals and refining their locations, we introduce a new object detection network by injecting subcategory information into the network proposed in Fast R-CNN <ref type="bibr" target="#b15">[15]</ref>. Our detection network is able to perform object detection and subcategory classification jointly. By using 3D Voxel Patterns (3DVPs) <ref type="bibr" target="#b36">[36]</ref> as subcategories, our method is able to jointly detect the object, estimate its 3D pose, segment its boundary and estimate its occluded or truncated regions. In addition, in both our RPN and our detection CNN, we use image pyramids as input, and we introduce a new feature extrapolating layer to efficiently compute conv features in multiple scales. In this way, our method is able to detect objects with large scale variations.</p><p>We conduct experiments on the KITTI dataset <ref type="bibr" target="#b14">[14]</ref>, the PASCAL3D+ dataset <ref type="bibr" target="#b37">[37]</ref> and the PASCAL VOC 2007 dataset <ref type="bibr" target="#b9">[10]</ref>. Comparisons with the state-of-the-art methods on these benchmarks demonstrate the advantages of our subcategory-aware CNNs for object recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Subcategory in Object Detection. Subcategory has been widely utilized to facilitate object detection, and different methods of discovering object subcategories have been proposed. In DPM <ref type="bibr" target="#b10">[11]</ref>, subcategories are discovered by clustering objects according to the aspect ratio of their bounding boxes. <ref type="bibr" target="#b17">[17]</ref> performs clustering according to the viewpoint of the object to discover subcategories. Visual subcategories are constructed by clustering in the appearance space of object <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7]</ref>. 3DVP <ref type="bibr" target="#b36">[36]</ref> performs clustering in the 3D voxel space according to the visibility of the voxels. Unlike previous works, we utilize subcategory to improve CNN-based detection, and our framework is general to employ different types of object subcategories.</p><p>CNN-based Object Detection. We can categorize the state-of-the-art CNN-based object detection methods into two classes: one-stage detection and two-stage detection. In one-stage detection, such as the Overfeat <ref type="bibr" target="#b27">[27]</ref> framework, a CNN directly processes an input image, and outputs object detections. In two-stage detection, such as R-CNNs <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b25">25]</ref>, region proposals are first generated from an input image, where different region proposal methods can be employed <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b0">1]</ref>. Then these region proposals are fed into a CNN for classification and location refinement. It is debatable which detection paradigm is better. We adopt the two-stage detection framework in this work, and consider the region proposal process to be the coarse detection step in coarse-to-fine detection <ref type="bibr" target="#b34">[34]</ref>. We propose a novel RPN motivated by <ref type="bibr" target="#b25">[25]</ref> and demonstrate its advantages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Subcategory-aware RPN</head><p>Ideally, we want to have a region proposal approach that can cover objects in an input image with as few proposals as possible. Since objects in images appear at different locations and scales, region proposal itself is a challenging problem. Recently, <ref type="bibr" target="#b25">[25]</ref> proposed to tackle the region proposal problem with CNNs, demonstrating the advantages of using CNNs over traditional approaches for region proposal. In this section, we describe our subcategory-aware Region Proposal Network (RPN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network Architecture</head><p>We introduce a novel network architecture for generating object proposals from images. The architecture is inspired by the traditional sliding-window-based object detectors, such as the Aggregated Channel Feature (ACF) detector <ref type="bibr" target="#b7">[8]</ref> and the Deformable Part Model (DPM) <ref type="bibr" target="#b10">[11]</ref>. <ref type="figure" target="#fig_1">Fig. 2</ref> illustrates the architecture of our region proposal network. i) To handle different scales of objects, we input into our RPN an image pyramid. This pyramid is processed by several convolutional (conv) and max pooling layers to extract the conv feature maps, with one conv feature map for each scale. ii) In order to speed up the computation of conv features on image pyramids, we introduce the feature extrapolating layer, which generates feature maps for scales that are not covered by the image pyramid via extrapolation. iii) After computing the extrapolated conv feature maps, we specifically design a conv layer for object subcategory detection, where each filter in the conv layer corresponds to an object subcategory. We train these filters to make sure they fire on correct locations and scales of objects in the corresponding subcategories during the network training. The subcategory conv layer outputs a heat map for each scale, where each value in the heat map indicates the confidence of an object in the corresponding location, scale and subcategory. v) Using the subcategory heat maps, we design a RoI generating layer that generates object candidates (RoIs) by thresholding the heat maps. vi) The RoIs are used in a RoI pooling layer <ref type="bibr" target="#b15">[15]</ref> to pool conv features from the extrapolated conv feature maps. vii) Finally, our RPN terminates at two sibling layers: one that outputs softmax probability estimates over object subcategories, and the other layer that refines the RoI location with a bounding box regressor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature Extrapolating Layer</head><p>In our RPN, we use fixed-size conv filters in the subcategory conv layer to localize objects (e.g., 5 ? 5 conv filters). In order to handle different scales of objects, we resort to image pyramids. An image pyramid consists of images with different resolutions obtained by rescaling the original image according to different sampled scales. After constructing the image pyramid for an input image, multiresolution conv feature maps can be computed by applying  several conv layers and max pooling layers to each image in the pyramid <ref type="figure" target="#fig_1">(Fig. 2</ref>). If we perform convolution on every scale explicitly, it is computationally expensive, especially when a finely-sampled image pyramid is needed as in the region proposal process. In <ref type="bibr" target="#b7">[8]</ref>, Doll?r et al. demonstrate that multi-resolution image features can be approximated by extrapolation from nearby scales rather than being computed explicitly. Inspired by their work, we introduce a feature extrapolating layer to accelerate the computation of conv features on an image pyramid. Specifically, a feature extrapolating layer takes as input N feature maps that are supplied by the last conv layer for feature extraction, where N equals to the number of scales in the input image pyramid. Each feature map is a multidimensional array of size H ? W ? C, with H rows, W columns, and C channels. The width and height of the feature map corresponds to the largest scale in the image pyramid, where images in smaller scales are padded with zeros in order to generate feature maps with the same size. The feature extrapolating layer constructs feature maps at intermediate scales by extrapolating features from the nearest scales among the N scales using bilinear interpolation. Suppose we add M intermediate scales between every ith scale and (i + 1)th scale, i = 1, . . . , N ? 1. The output of the feature extrapolating layer is N = (N ? 1)M + N feature maps, each with size H ? W ? C. Since extrapolating a multi-dimensional array is much faster than computing a conv feature map explicitly, the feature extrapolating layer speeds up the feature computation with less memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Subcategory Conv Layer</head><p>After computing the conv feature maps, we design a subcategory conv layer for subcategory detection. Motivated by the traditional object detection methods that train a classifier or a template for each subcategory <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b36">36]</ref>, we train a conv filter in the subcategory conv layer to detect a specific subcategory. Suppose there are K subcategories to be considered. Then, the subcategory conv layer consists of K + 1 conv filters with one additional conv filter for a special "background" category. For multi-class detection (e.g., car, pedestrian, cyclist, etc.), the K subcategories are the aggregation of all the subcategories from all the classes. These conv filters operate on the extrapolated conv feature maps and output heat maps that indicate the confidences of the presence of objects in the input image. We use fixed-size conv filters in this layer (e.g., 5 ? 5 ? C conv filters), which are trained to fire on specific scales in the feature pyramid. Sec. 3.5 explains how we back-propagate errors from the loss layer to train these subcategory conv filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">RoI Generating Layer</head><p>The RoI generating layer takes as input N heat maps and outputs a set of region proposals (RoIs), where N is the number of scales in the feature pyramid after extrapolation. Each heat map is a multi-dimensional array of size H?W ?K for K subcategories (i.e., for RoI generating, we ignore the "background" channel in the heat map). The RoI generating layer first converts each heat map into a H ? W 2D array by performing max operation over the channels for subcategory. Then, it thresholds the 2D heat map to generate RoIs. In this way, we measure the objectness of a region by aggregating information from subcategories. Different generating strategies are used in testing and training.</p><p>In testing, each location (x, y) in a heat map with a score larger than a predefined threshold is used to generate RoIs. First, a canonical bounding box is centered on (x, y). The width and height of the box are the same as those of the conv filters (e.g., 5?5) in the subcategory conv layer, which have an aspect ratio one. Second, a number of boxes centered on (x, y) with the same areas as the canonical box (e.g., 25) but with different aspect ratios are generated. Finally, the RoI generating layer rescales the generated boxes according to the scale of the heat map, so as to cover objects in different scales and aspect ratios.</p><p>In training, the RoI generating layer outputs hard positive RoIs and hard negative RoIs for training the subcategory conv filters, given a budget on batch size in stochastic gradient descent. First, we use the same procedure as described in testing to generate a number of bounding boxes for each location in each heat map. Second, according to the ground truth bounding boxes of objects in a training image, we compute the intersection over union (IoU) overlap between the generated boxes and the ground truth boxes. Bounding boxes with IoU overlap larger/smaller than some threshold (e.g., 0.5) are considered to be positive/negative. Finally, given the number of RoIs to be generated for each training image R (i.e., batch size divided by the number of images in a batch), the RoI generating layer outputs R ? ?  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Network Training</head><p>After generating RoIs, we apply the RoI pooling layer proposed in <ref type="bibr" target="#b15">[15]</ref> to pool conv features for each RoI. Then the pooled conv features are used for two tasks: subcategory classification and bounding box regression. As illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>, our RPN has two sibling output layers. The first layer outputs a discrete probability distribution p = (p 0 , . . . , p K ), over K + 1 subcategories, which is computed by applying a softmax function over the K +1 outputs of the subcategory conv layer. The second layer outputs bounding box regression offsets t k = (t k x , t k y , t k w , t k h ), k = 0, 1, . . . , K for K object classes (K K). We parameterize t k as in <ref type="bibr" target="#b16">[16]</ref>, which specifies a scale-invariant translation and log-space width/height shift relative to a RoI.</p><p>We employ a multi-task loss as in <ref type="bibr" target="#b15">[15]</ref> to train our RPN for subcategory classification and bounding box regression:</p><formula xml:id="formula_0">L(p, k * , k * , t, t * ) = L subcls (p, k * ) + ?[k * ? 1]L loc (t, t * ),</formula><p>(1) where k * and k * are the truth subcategory label and the true class label respectively, L subcls (p, k * ) = ? log p k * is the standard cross-entropy loss, t * = (t * x , t * y , t * w , t * h ) is the true bounding box regression targets for class k * , and t = (t x , t y , t w , t h ) is the prediction for class k * . We use the smoothed L 1 loss defined in <ref type="bibr" target="#b15">[15]</ref> for the bounding box regression loss L loc (t, t * ). The indicator function [k * ? 1] indicates that bounding box regression is ignored if the RoI is background (i.e., k * = 0). ? is a predefined weight to balance the two losses.</p><p>In training, derivatives from the loss function are backpropagated (see red arrows in <ref type="figure" target="#fig_1">Fig. 2</ref>). The two subcategory conv layers in our RPN share their weights. These weights/conv filters are updated according to the derivatives from the softmax loss function for subcategory classification, so we are able to train these filters for subcategory detection. There is no derivative flow in computing heat maps using the subcategory conv layer and in the RoI generat-ing layer. Finally, our RPN generates confident subcategory detections as region proposals. <ref type="figure" target="#fig_2">Fig. 3</ref> illustrates the architecture of our detection network. The network is constructed based on the Fast R-CNN detection network <ref type="bibr" target="#b15">[15]</ref> with a number of improvements. i) We use image pyramids to handle the scale variation of objects. After the last conv layer for feature extraction, we add the feature extrapolating layer to increase the number of scales in the conv feature pyramid. ii) Given the region proposals generated from our RPN, we employ a RoI pooling layer to pool conv features for each RoI. Each RoI is mapped to a scale in the conv feature pyramid such that smaller RoIs pool features from larger scales. iii) The pooled conv features are fed into three fully connected (FC) layers, where the last FC layer is designed for subcategory classification. For K subcategories, the "subcategory FC" layer outputs a K + 1 dimensional vector with one additional dimension for the background class. We consider the output, named RoI feature vector, to be an embedding in the subcategory space. iv) Finally, the network terminates at three output layers. The first output layer applies a softmax function directly on the output of the "subcategory FC" layer for subcategory classification. The other two output layers operate on the RoI feature vector and apply FC layers for object class classification and bounding box regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Network Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Network Training</head><p>We train our object detection network with a multi-task loss for joint object class classification, subcategory classi-fication and bounding box regression:</p><formula xml:id="formula_1">L(p, k * , p , k * , t, t * ) = (2) L subcls (p, k * ) + ? 1 L cls (p , k * ) + ? 2 [k * ? 1]L loc (t, t * ),</formula><p>where p = (p 0 , . . . , p K ) is a probability distribution over K + 1 subcategories, p = (p 0 , . . . , p K ) is a probability distribution over K + 1 object classes, k * and k * are the truth subcategory label and the true class label respectively, t and t * are the predicted vector and the true vector for bounding box regression respectively, and ? 1 and ? 2 are predefined weights to balance the losses of different tasks. L subcls (p, k * ) = ? log p k * and L cls (p , k * ) = ? log p k * are the standard cross-entropy loss, and L loc (t, t * ) is the smoothed L 1 loss as in our RPN. In back-propagation training, derivatives for the multi-task loss are back-propagated to the previous layers. Red arrows in <ref type="figure" target="#fig_2">Fig. 3</ref> indicate the route of the derivative flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Settings</head><p>Datasets. We evaluate our object detection framework on the KITTI detection benchmark <ref type="bibr" target="#b14">[14]</ref>, the PASCAL3D+ dataset <ref type="bibr" target="#b37">[37]</ref> and the PASCAL VOC 2007 dataset <ref type="bibr" target="#b9">[10]</ref>. i) The KITTI dataset consists of video frames from autonomous driving scenes, with 7,481 images for training and 7,518 images for testing. Car, pedestrian and cyclist are evaluated for object detection. Since the ground truth annotations of the KITTI test set are not released, we split the KITTI training images into a train set and a validation set for analyses as in <ref type="bibr" target="#b36">[36]</ref>. ii) The PASCAL3D+ dataset augments 12 rigid categories in the PASCAL VOC 2012 <ref type="bibr" target="#b8">[9]</ref> with 3D annotations. Each object in the 12 categories is registered with a 3D CAD model. The train set of PASCAL VOC 2012 is used for training (5,717 images), while the val set is used for testing (5,823 images). iii) The PASCAL VOC 2007 dataset <ref type="bibr" target="#b9">[10]</ref> contains 5,011 training images and 4,952 testing images on 20 categories. Evaluation Metrics. On KITTI, we evaluate our detection framework at three levels of difficulty as suggested by <ref type="bibr" target="#b12">[13]</ref>, i.e., easy, moderate and hard, where the difficulty is measured by the minimal scale of object to be considered and the occlusion and truncation of the object. Average Precision (AP) <ref type="bibr" target="#b8">[9]</ref> is used to measure the detection performance, where 70%, 50%, and 50% overlap thresholds are adopted by the KITTI benchmark for car, pedestrian and cyclist respectively. To evaluate joint detection and orientation estimation on KITTI, <ref type="bibr" target="#b14">[14]</ref> introduces Average Orientation Similarity (AOS), which evaluates the orientation similarity between detections and ground truths at different detection recalls. <ref type="bibr" target="#b36">[36]</ref> introduces Average Segmentation Accuracy (ASA) for joint detection and segmentation, and Average Location Precision (ALP) for joint detection and 3D location similar to AOS. We also use these metrics here. On PASCAL3D+ and PASCAL VOC 2007, the standard AP with 50% overlap ratio is adopted to evaluate object detection. For joint detection and pose estimation, we use the Average Viewpoint Precision (AVP) suggested by <ref type="bibr" target="#b37">[37]</ref>, where a detection is considered to be a true positive if its location and viewpoint are both correct. Subcategories. We experiment with both 2D subcategories and 3D subcategories. For 2D subcategories, we cluster objects using 2D image features (i.e., aggregated channel features from <ref type="bibr" target="#b7">[8]</ref>). Only bounding box annotations are needed for 2D subcategories. When additional annotations are available, we can obtain 3D subcategories. We adopt the 3D Voxel Pattern (3DVP) representation <ref type="bibr" target="#b36">[36]</ref> for rigid objects (i.e., car in KITTI and the 12 categories in PAS-CAL3D+), which jointly models object pose, occlusion and truncation in the clustering process. Each 3DVP is considered to be a subcategory. For pedestrian and cyclist in KITTI, we perform clustering according to the object orientation, and each cluster is considered to be a subcategory. In this way, by subcategory classification, we can transfer the meta data carried by 3DVPs (3D pose, segmentation boundary and occluded regions) to the detected object.</p><p>For validation on KITTI (3,682 images for training, 3,799 images for testing), we use 173 subcategories (125 3DVPs for car, 24 poses for pedestrian and cyclist each), while for testing on KITTI (7,481 images for training, 7,518 images for testing), we use 275 subcategories (227 3DVPs for car, 24 poses for pedestrian and cyclist each). 3DVPs are discovered with affinity propagation clustering <ref type="bibr" target="#b11">[12]</ref>, which automatically discovers the number of clusters from the data. For PASCAL3D+, 337 3DVPs are discovered among the 12 categories. For PASCAL VOC 2007, we use 240 2D subcategories, with 12 for each class. Correspondingly, the output number of the subcategory conv layer in our RPN and that of the subcategory FC layer in our detection network equal to the number of subcategory plus one. Region Proposal Network Hyper-parameters. In our RPN, we use 5 scales for KITTI in the input image pyramid (0.25, 0.5, 1.0, 2.0, 3.0) and 4 scales for PASCAL (0.25, 0.5, 1.0, 2.0) (both PASCAL3D+ and PASCAL VOC 2007), where each number indicates the rescaling factor with respect to the original image size. Objects in PASCAL have smaller scale variation compared to objects in KITTI. Adding larger scales for PASCAL only results in marginal improvement but significantly increases the computation. The feature extrapolating layer extrapolates 4 scales with equal intervals between every two input scales, so the final conv feature pyramid has 21 scales for KITTI and 16 scales for PASCAL. In the RoI generating layer, each location in a heat map generates 7 boxes with 7 different aspect ratios (3.0, 2.0, 1.5, 1.0, 0.75, 0.5, 0.25) for KITTI and  <ref type="bibr" target="#b15">[15]</ref> for region proposal and detection. Fine-tuning Pre-trained Networks. Our framework is implemented in Caffe <ref type="bibr" target="#b18">[18]</ref>. We initialize the conv layers for feature extraction in both networks and the two FC layers before subcategory FC layer in the detection network with pre-trained networks on ImageNet <ref type="bibr" target="#b26">[26]</ref>. On KITTI, we experiment with the AlexNet <ref type="bibr" target="#b19">[19]</ref>, the VGG16 network <ref type="bibr" target="#b28">[28]</ref> and the GoogleNet <ref type="bibr" target="#b30">[30]</ref>. On PASCAL, we fine-tune the VGG16 network <ref type="bibr" target="#b28">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Analysis on KITTI Validation Set</head><p>Region Proposal Evalutaion on Recall. We evaluate the detection recall of our RPN and compare it with the stateof-the-art methods in <ref type="table">Table 1</ref>  In this experiment, we directly measure the detection and orientation estimation performance using different region proposals. <ref type="table">Table 2</ref> presents the detection and orientation estimation results using RPN in Faster R-CNN <ref type="bibr" target="#b25">[25]</ref> and the RPN we propose, while keeping the detection network the same as described in Sec. 4. We compare our RPN with two variations of the RPN in Faster R-CNN. For the first model, the RPN and the detection network are trained independently to each other ("unshared"). For the second model, the RPN and the detection network share their conv layers for feature extraction in order to save computation on convolution ("shared"). The sharing is achieved by the four-step alternating optimization training algorithm described in <ref type="bibr" target="#b25">[25]</ref>. By comparing the two models in <ref type="table">Table 2</ref>, we find that sharing conv layers hurts the performance on car and pedestrian, but improves the performance on cyclist. Car and pedestrian have much more training examples available than cyclist. With enough training data, the RPN and the detection network trained independently can develop conv features suitable for its own task. In this case, shared conv features degrade the performance. However, when the training data is insufficient, sharing conv features can help. In <ref type="table">Table 2</ref>, by using region proposals from our RPN, we achieve better performance on detection and orientation estimation across all the three categories. The experimental results demonstrate the advantages of our RPN. We also tried to share the conv layers in our RPN and our detection network. However, since the architecture of our RPN after the conv layers for feature extraction is quite different from that of the detection network, we found that the training cannot converge, which verifies our observation that the RPN and the detection network have developed their own KITTI PASCAL3D+ <ref type="figure">Figure 4</ref>. Examples of detections from our method. Detections with score larger than 0.5 on KITTI and 0.7 on PASCAL3D+ are shown.  conv features that are suitable for its own task. Detection Network Evalutaion. In <ref type="table" target="#tab_5">Table 3</ref>, we first show that our RPN achieves significantly better performance than the RPN in <ref type="bibr" target="#b25">[25]</ref> when the two RPNs are used with Fast R-CNN <ref type="bibr" target="#b15">[15]</ref> on the KITTI validation set respectively. Then, we use region proposals from our RPN and compare different variations of the network architecture for detection. i) "Ours w/o Pose" indicates using 2D subcategories from clustering on 2D appearances of objects without using additional pose information. As we can see, our method still outperforms Fatser R-CNN <ref type="bibr" target="#b25">[25]</ref> in this case. ii) By using pose information to obtain subcategories, our detection network is also able to estimate the orientation of the object. "Ours w/o Extra" refers to a network without feature extrapolating. By augmenting the network with the feature extrapolating layer, our full model ("Ours Full" in <ref type="table" target="#tab_5">Table 3</ref>) further boosts the performance, except for a minor drop on orientation estimation of pedestrian. Evaluation on 2D Segmentation and 3D Localization. 3DVPs enable us to transfer the meta data to the detect objects, so our method is able to segment the boundary of object. In addition, after detecting the objects and estimating their 3D poses, we can back-project them into 3D using the   <ref type="bibr" target="#b36">[36]</ref> on the KITTI validation set. We have significantly improve the segmentation accuracy and 3D location accuracy when the 2-meter threshold is used (i.e., a detection within 2 meters from the ground truth location is considered to be correct). Surprisingly, <ref type="bibr" target="#b36">[36]</ref> obtains better 3D localization accuracy with the 1-meter threshold, which indicates that more detections from <ref type="bibr" target="#b36">[36]</ref> are within the 1-meter distance from the ground truth.   our method on the KITTI test set by submitting our results to <ref type="bibr" target="#b12">[13]</ref>. <ref type="table" target="#tab_7">Table 5</ref> presents the detection and orientation estimation results on the three categories, where we compare our method (SubCNN) with different methods evaluated on KITTI. We have experimented fine-tuning both the VGG16 network and the GoogleNet for the detection network. Our method ranks on top among all the published methods. The experimental results demonstrate the ability of our CNNs in using subcategory information for detection and orientation estimation. <ref type="figure">Fig. 4</ref> presents some examples of our detection and 3D localization results on KITTI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">KITTI Test Set Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Evaluation on PASCAL3D+ and PASCAL VOC</head><p>We also evaluate our detection framework on the 12 categories in PASCAL3D+. <ref type="table" target="#tab_9">Table 6</ref> presents the detection results in AP and the joint detection and pose estimation results in AVP. After generating region proposals from our RPN, we experiment with our detection networks with and without feature extrapolation. First, in terms of detection, our method improves over R-CNN <ref type="bibr" target="#b16">[16]</ref> on all 12 categories. Second, in terms of join detection and pose estimation, our method significantly outperforms two state-of-the-art methods: VDPM <ref type="bibr" target="#b37">[37]</ref> and DPM-VOC+VP <ref type="bibr" target="#b24">[24]</ref>. Third, feature extrapolation helps both detection and pose estimation on PASCAL3D+. It is worth mentioning that PASCAL3D+ has much fewer training examples in each subcategory compared to KITTI. Our pose estimation performance is limited by the number of training examples available in PAS-CAL3D+. We also note that the two recent methods <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b29">29]</ref> achieve very appealing pose estimation results on PAS-CAL3D+. However, both of them utilize additional training images (ImageNet images in <ref type="bibr" target="#b32">[32]</ref> and synthetic images in <ref type="bibr" target="#b29">[29]</ref>) and conduct detection and pose estimation with separate CNNs, where a CNN is specifically designed for pose estimation. Our method is capable of simultaneous object detection and viewpoint estimation even in the presence of limited training examples per viewpoint. <ref type="figure">Fig. 4</ref> shows some detection results from our method. We again transfer segmentation masks of 3DVPs to the detected objects according to the subcategory classification results. Please see supplementary material for more examples.</p><p>To demonstrate that our method also works on datasets with bounding box annotations only, we have conducted experiments on the PASCAL VOC 2007 dataset, where subcategories are obtained by clustering on image features. In table 7, we compare with Fast R-CNN <ref type="bibr" target="#b15">[15]</ref> and Faster R-CNN <ref type="bibr" target="#b25">[25]</ref>. We have achieved comparable performance to the state-of-the-arts. Region proposal on PASCAL VOC is relatively easy compared to KITTI. So we do not see much improvement with our RPN on PASCAL VOC 2007.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we explore how subcategory information can be exploited in CNN-based object detection. We have proposed a novel region proposal network, and a novel object detection network, where we explicitly employ subcategory information to improve region proposal generation, object detection and object pose estimation. Our subcategory-aware CNNs can also handle the scale variation of objects using image pyramids in an efficient way. We have conducted extensive experiments on the KITTI detection benchmark, the PASCAL3D+ dataset and PASCAL VOC 2007 dataset. Our method achieves the state-of-the-art results on these benchmarks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Overview of our object detection framework. By exploiting subcategory information, we propose a new CNN architecture for region proposal and a new object detection network for joint detection and subcategory classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Architecture of our region proposal network. Red arrows indicate the route of derivatives in back-propagation training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Architecture of our object detection network. Red arrows indicate the route of derivatives in back-propagation training. hard positives (i.e., R ? ? positive bounding boxes with lowest scores in the heat maps) and R ? (1 ? ?) hard negatives (i.e., R?(1??) negative bounding boxes with highest scores in the heat maps), where ? ? (0, 1) is the percentage of positive examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Comparison of detection networks on KITTI val set.</figDesc><table><row><cell>Methods</cell><cell>Easy Moderate</cell><cell>Hard</cell><cell>Easy Moderate</cell><cell>Hard</cell><cell>Easy Moderate</cell><cell>Hard</cell></row><row><cell></cell><cell cols="6">Detection &amp; Segmentation (ASA) Detection &amp; 3D Loc.&lt;2m (ALP) Detection &amp; 3D Loc.&lt;1m (ALP)</cell></row><row><cell cols="2">DPM [11] 38.09 29.42</cell><cell>22.65</cell><cell>40.21 29.02</cell><cell>22.36</cell><cell>24.44 18.04</cell><cell>14.13</cell></row><row><cell cols="2">3DVP [36] 65.73 54.60</cell><cell>45.62</cell><cell>66.56 51.52</cell><cell>42.39</cell><cell>45.61 34.28</cell><cell>27.72</cell></row><row><cell>Ours</cell><cell>73.64 66.22</cell><cell>56.34</cell><cell>70.52 56.20</cell><cell>47.03</cell><cell>39.28 31.04</cell><cell>25.96</cell></row><row><cell cols="7">Table 4. 2D segmentation and 3D location of car on KITTI val set.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>Comparison between different methods on KITTI test set. camera parameters provided in KITTI, so as to evaluate the 3D localization performance. In table 4, we compare our method on 2D segmentation and 3D localization of car with DPM [11] and 3DVP</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>To compare with the state-of-the-art methods on the KITTI detection benchmark, we train our RPN and detection network with all the KITTI training data, and then test</figDesc><table><row><cell>Methods</cell><cell>aeroplane</cell><cell>bicycle</cell><cell>boat</cell><cell>bottle</cell><cell>bus</cell><cell>car</cell><cell>chair</cell><cell>diningtable</cell><cell>motorbike</cell><cell>sofa</cell><cell>train</cell><cell>tvmonitor</cell><cell>Average</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Object Detection (AP)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DPM [11]</cell><cell>42.2</cell><cell>49.6</cell><cell>6.0</cell><cell>20.0</cell><cell>54.1</cell><cell>38.3</cell><cell>15.0</cell><cell>9.0</cell><cell>33.1</cell><cell>18.9</cell><cell>36.4</cell><cell>33.2</cell><cell>29.6</cell></row><row><cell>R-CNN [16]</cell><cell>72.4</cell><cell>68.7</cell><cell>34.0</cell><cell>-</cell><cell>73.0</cell><cell>62.3</cell><cell>33.0</cell><cell>35.2</cell><cell>70.7</cell><cell>49.6</cell><cell>70.1</cell><cell>57.2</cell><cell>56.9</cell></row><row><cell>Ours w/o Extra</cell><cell>76.3</cell><cell>73.4</cell><cell>43.4</cell><cell>44.7</cell><cell>74.5</cell><cell>63.3</cell><cell>35.4</cell><cell>32.4</cell><cell>74.9</cell><cell>51.9</cell><cell>74.1</cell><cell>60.9</cell><cell>58.8</cell></row><row><cell>Ours Full</cell><cell>76.5</cell><cell>74.0</cell><cell>42.4</cell><cell>47.0</cell><cell>74.5</cell><cell>64.7</cell><cell>38.5</cell><cell>38.6</cell><cell>76.7</cell><cell>55.1</cell><cell>74.8</cell><cell>65.3</cell><cell>60.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Joint Object Detection and Pose Estimation (4 Views AVP)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VDPM [37]</cell><cell>34.6</cell><cell>41.7</cell><cell>1.5</cell><cell>-</cell><cell>26.1</cell><cell>20.2</cell><cell>6.8</cell><cell>3.1</cell><cell>30.4</cell><cell>5.1</cell><cell>10.7</cell><cell>34.7</cell><cell>19.5</cell></row><row><cell>DPM-VOC+VP [24]</cell><cell>39.4</cell><cell>43.9</cell><cell>0.3</cell><cell>-</cell><cell>49.1</cell><cell>37.6</cell><cell>6.1</cell><cell>3.0</cell><cell>32.2</cell><cell>11.8</cell><cell>12.5</cell><cell>33.2</cell><cell>24.5</cell></row><row><cell>Ours w/o Extra</cell><cell>62.3</cell><cell>56.6</cell><cell>18.0</cell><cell>-</cell><cell>62.0</cell><cell>40.9</cell><cell>19.3</cell><cell>14.9</cell><cell>62.3</cell><cell>44.1</cell><cell>58.1</cell><cell>58.5</cell><cell>45.2</cell></row><row><cell>Ours Full</cell><cell>61.4</cell><cell>60.4</cell><cell>21.1</cell><cell>-</cell><cell>63.0</cell><cell>48.7</cell><cell>23.8</cell><cell>17.4</cell><cell>60.7</cell><cell>47.8</cell><cell>55.9</cell><cell>62.3</cell><cell>47.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Joint Object Detection and Pose Estimation (8 Views AVP)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VDPM [37]</cell><cell>23.4</cell><cell>36.5</cell><cell>1.0</cell><cell>-</cell><cell>35.5</cell><cell>23.5</cell><cell>5.8</cell><cell>3.6</cell><cell>25.1</cell><cell>12.5</cell><cell>10.9</cell><cell>27.4</cell><cell>18.7</cell></row><row><cell>DPM-VOC+VP [24]</cell><cell>29.7</cell><cell>42.6</cell><cell>0.4</cell><cell>-</cell><cell>39.5</cell><cell>36.8</cell><cell>9.4</cell><cell>2.6</cell><cell>32.9</cell><cell>11.0</cell><cell>10.3</cell><cell>28.6</cell><cell>22.2</cell></row><row><cell>Ours w/o Extra</cell><cell>45.9</cell><cell>25.5</cell><cell>11.1</cell><cell>-</cell><cell>37.7</cell><cell>34.6</cell><cell>15.2</cell><cell>7.4</cell><cell>37.1</cell><cell>33.0</cell><cell>42.5</cell><cell>24.3</cell><cell>28.6</cell></row><row><cell>Ours Full</cell><cell>48.8</cell><cell>36.3</cell><cell>16.4</cell><cell>-</cell><cell>39.8</cell><cell>37.2</cell><cell>19.1</cell><cell>13.2</cell><cell>37.0</cell><cell>32.1</cell><cell>44.4</cell><cell>26.9</cell><cell>31.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Joint Object Detection and Pose Estimation (16 Views AVP)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VDPM [37]</cell><cell>15.4</cell><cell>18.4</cell><cell>0.5</cell><cell>-</cell><cell>46.9</cell><cell>18.1</cell><cell>6.0</cell><cell>2.2</cell><cell>16.1</cell><cell>10.0</cell><cell>22.1</cell><cell>16.3</cell><cell>15.6</cell></row><row><cell>DPM-VOC+VP [24]</cell><cell>17.0</cell><cell>24.7</cell><cell>1.0</cell><cell>-</cell><cell>49.0</cell><cell>30.1</cell><cell>6.6</cell><cell>3.0</cell><cell>17.2</cell><cell>7.7</cell><cell>20.4</cell><cell>20.2</cell><cell>17.9</cell></row><row><cell>Ours w/o Extra</cell><cell>23.3</cell><cell>19.2</cell><cell>8.4</cell><cell>-</cell><cell>52.6</cell><cell>27.0</cell><cell>9.9</cell><cell>5.1</cell><cell>23.6</cell><cell>20.9</cell><cell>27.4</cell><cell>27.9</cell><cell>22.3</cell></row><row><cell>Ours Full</cell><cell>28.0</cell><cell>23.7</cell><cell>10.7</cell><cell>-</cell><cell>50.8</cell><cell>31.4</cell><cell>14.3</cell><cell>9.4</cell><cell>23.4</cell><cell>19.5</cell><cell>30.7</cell><cell>27.8</cell><cell>24.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Joint Object Detection and Pose Estimation (24 Views AVP)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VDPM [37]</cell><cell>8.0</cell><cell>14.3</cell><cell>0.3</cell><cell>-</cell><cell>39.2</cell><cell>13.7</cell><cell>4.4</cell><cell>3.6</cell><cell>10.1</cell><cell>8.2</cell><cell>20.0</cell><cell>11.2</cell><cell>12.1</cell></row><row><cell>DPM-VOC+VP [24]</cell><cell>10.6</cell><cell>16.7</cell><cell>2.2</cell><cell>-</cell><cell>43.5</cell><cell>25.4</cell><cell>4.4</cell><cell>2.3</cell><cell>11.3</cell><cell>4.9</cell><cell>22.4</cell><cell>14.4</cell><cell>14.4</cell></row><row><cell>Ours w/o Extra</cell><cell>18.9</cell><cell>10.5</cell><cell>6.7</cell><cell>-</cell><cell>34.3</cell><cell>23.3</cell><cell>8.3</cell><cell>6.5</cell><cell>20.6</cell><cell>17.5</cell><cell>33.8</cell><cell>17.0</cell><cell>17.9</cell></row><row><cell>Ours Full</cell><cell>20.7</cell><cell>16.4</cell><cell>7.9</cell><cell>-</cell><cell>34.6</cell><cell>24.6</cell><cell>9.4</cell><cell>7.6</cell><cell>19.9</cell><cell>20.0</cell><cell>32.7</cell><cell>18.2</cell><cell>19.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 .</head><label>6</label><figDesc>AP/AVP Comparison between different methods on the PASCAL3D+ dataset.</figDesc><table><row><cell>mAP</cell><cell>aero</cell><cell>bike</cell><cell>bird</cell><cell>boat</cell><cell>bottle</cell><cell>bus</cell><cell>car</cell><cell>cat</cell><cell>chair</cell><cell>cow</cell><cell>table</cell><cell>dog</cell><cell>horse</cell><cell>mbike</cell><cell>person</cell><cell>plant</cell><cell>sheep</cell><cell>sofa</cell><cell>train</cell><cell>tv</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 .</head><label>7</label><figDesc>AP comparison between Fast R-CNN<ref type="bibr" target="#b15">[15]</ref>, Faster R-CNN<ref type="bibr" target="#b25">[25]</ref> and our method on PASCAL VOC 2007 dataset.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">. Subcategory-aware Detection Network After the region proposal process, CNNs are utilized to classify these proposals and refine their locations<ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b25">25]</ref>. Since region proposal significantly reduces the search space, more powerful CNNs can be used in the detection step, which usually contain several fully connected layers with high dimensions. In this section, we introduce our subcategory-aware object detection network for joint detection and subcategory classification.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. We acknowledge the support of Nissan grant 1188371-1-UDARQ and MURI grant 1186514-1-TBCJE.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="328" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A unified multi-scale deep convolutional neural network for fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3d object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Enriching visual knowledge bases via object discovery and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2035" to="2042" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">How important are deformable parts in the deformable parts model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EC-CVW</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="31" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning everything about anything: Webly-supervised visual concept learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3270" to="3277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast feature pyramids for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1532" to="1545" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html.1" />
		<title level="m">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Clustering by passing messages between data points. science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dueck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">315</biblScope>
			<biblScope unit="page" from="972" to="976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Kitti object detection benchmark</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Accessed: 2016-03-13</title>
		<ptr target="http://www.cvlibs.net/datasets/kitti/evalobject.php" />
		<imprint>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Discriminative mixture-of-templates for viewpoint classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="408" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Integrating context and occlusion for car detection by hierarchical and-or model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="652" to="667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ensemble of exemplar-svms for object detection and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to detect vehicles by clustering appearance patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ohn-Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2511" to="2521" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Occlusion patterns for object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pepik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3286" to="3293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-view and 3d deformable part models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pepik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2232" to="2245" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">ImageNet Large Scale Visual Recognition Challenge. IJCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Render for cnn: Viewpoint estimation in images using cnns trained with rendered 3d model views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2686" to="2694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep learning strong parts for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1904" to="1912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Viewpoints and keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1510" to="1519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Regionlets for generic object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Data-driven 3d voxel patterns for object category recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Beyond pascal: A benchmark for 3d object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="75" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Filtered channel features for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1751" to="1760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="391" to="405" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
