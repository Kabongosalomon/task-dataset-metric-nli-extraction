<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AlephBERT: A Hebrew Large Pre-Trained Language Model to Start-off your Hebrew NLP Application With</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Seker</surname></persName>
							<email>aseker00@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Bar-Ilan University</orgName>
								<address>
									<settlement>Ramat-Gan</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elron</forename><surname>Bandel</surname></persName>
							<email>elronbandel@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Bar-Ilan University</orgName>
								<address>
									<settlement>Ramat-Gan</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Bareket</surname></persName>
							<email>dbareket@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Bar-Ilan University</orgName>
								<address>
									<settlement>Ramat-Gan</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Brusilovsky</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Bar-Ilan University</orgName>
								<address>
									<settlement>Ramat-Gan</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaked</forename><surname>Refael</surname></persName>
							<email>shakedgreenfeld@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Bar-Ilan University</orgName>
								<address>
									<settlement>Ramat-Gan</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reut</forename><surname>Greenfeld</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Bar-Ilan University</orgName>
								<address>
									<settlement>Ramat-Gan</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tsarfaty</surname></persName>
							<email>reut.tsarfaty@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Bar-Ilan University</orgName>
								<address>
									<settlement>Ramat-Gan</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AlephBERT: A Hebrew Large Pre-Trained Language Model to Start-off your Hebrew NLP Application With</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large Pre-trained Language Models (PLMs) have become ubiquitous in the development of language understanding technology and lie at the heart of many artificial intelligence advances. While advances reported for English using PLMs are unprecedented, reported advances using PLMs in Hebrew are few and far between. The problem is twofold. First, Hebrew resources available for training NLP models are not at the same order of magnitude as their English counterparts. Second, there are no accepted tasks and benchmarks to evaluate the progress of Hebrew PLMs on. In this work we aim to remedy both aspects. First, we present AlephBERT, a large pre-trained language model for Modern Hebrew, which is trained on larger vocabulary and a larger dataset than any Hebrew PLM before. Second, using AlephBERT we present new stateof-the-art results on multiple Hebrew tasks and benchmarks, including: Segmentation, Partof-Speech Tagging, full Morphological Tagging, Named-Entity Recognition and Sentiment Analysis. We make our AlephBERT model publicly available, providing a single point of entry for the development of Hebrew NLP applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Contextualized word representations, provided by models such as BERT <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> and RoBERTa <ref type="bibr" target="#b8">(Liu et al., 2019)</ref>, were shown in recent years to be critical for obtaining state-of-the-art performance on a wide range of Natural Language Processing (NLP) tasks -such as syntactic and semantic parsing, question answering, natural language inference, text summarization, natural language generation, and more. These contextualized word representations are obtained by pre-training a large language model on massive quantities of unlabeled data, aiming to maximize a simple yet effective objective of masked word prediction.</p><p>While advances reported for English using such models are unprecedented, in Hebrew previously reported results using BERT-based models are far from impressive. Specifically, the BERT-based Hebrew section of multilingual-BERT <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> (henceforth, mBERT), did not provide a similar boost in performance to what is observed for the English section of mBERT. In fact, for several reported tasks, the mBERT model results are on a par with pre-neural models, or neural models based on non-contextialized embeddings ). An additional Hebrew BERT-based model, HeBERT <ref type="bibr" target="#b5">(Chriqui and Yahav, 2021)</ref>, has been released, yet there is no reported evidence on performance improvements on key component of the Hebrew NLP pipelinewhich includes, at the very least: morphological segmentation, full morphological tagging, and full (token/morpheme-based) named entity recognition.</p><p>In this work we present AlephBERT, a Hebrew pre-trained language model, larger and more effective than any Hebrew PLM before. Using Aleph-BERT we show substantial improvements on all essential tasks in the Hebrew NLP pipeline, tasks tailored to fit a morphologically-rich language, including: Segmentation, Part-of-Speech Tagging, full morphological tagging, Named Entity Recognition and Sentiment Analysis. Since previous Hebrew NLP studies used varied corpora and annotation schemes, we confirm our results on all existing Hebrew benchmarks and variants. For morphology and POS tagging, we test on both the Hebrew section of the SPMRL shared task <ref type="bibr" target="#b13">(Seddah et al., 2013)</ref>, and the Hebrew UD corpus <ref type="bibr" target="#b12">(Sadde et al., 2018)</ref>. For Named Entity recognition, we test on both the corpus of Ben <ref type="bibr">Mordecai and Elhadad (2005)</ref> and that of . For sentiment analysis we test on the facebook corpus of <ref type="bibr" target="#b0">Amram et al. (2018)</ref>, as well as a newer (fixed) variant of this benchmark.</p><p>We make our pre-trained model publicly available 1 and additionally we deliver an online demo 2 allowing to qualitatively compare the maskprediction capacity of different PLMs available for Hebrew. In the near future we will release the complete AlephBERT-geared pipeline we developed, containing the aforementioned tasks, as means for evaluating and comparing future Hebrew PLMs, and as a starting point for developing further downstream applications and tasks. We also plan to showcase AlephBERT's capacities on downstream language understanding tasks such as: Information Extraction, Text Summarization, Reading Comprehension, and more. As future research, we are pursuing a plan to investigate the effect of different word decomposition algorithms and input representation variants on the different tasks in the Pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Challenge</head><p>This paper presents a case study for PLM development for a morphologically-rich and resource-poor language. Specifically, we address Modern Hebrew, a Semitic, morphologically-rich language, that is long known to be notoriously hard to parse.</p><p>The challenges posed to automatically processing Hebrew texts and obtaining good accuracies on downstream tasks stem from (at least) two main factors. The first is the internal-complexity of wordtokens, resulting from the rich morphology, complex orthography, and lack of diacritization in Hebrew written texts. Space-delimited tokens have non-transparent decomposition and are highly ambiguous, making even the simplest of the tasks in the pipeline very challenging . The second factor is the fact that Modern Hebrew, with only a few dozens of millions of native speakers, is often studied in resource-scarce settings.</p><p>The resource-scarce setting is problematic for PLM development in at least two ways. First, there are insufficient amounts of free unlabeled text for pre-training. To wit, the Hebrew Wikipedia that was the source for training multilingual BERT is of orders of magnitude smaller than the English Wikipedia (See <ref type="table">Table 1</ref>   Previous studies on various tasks on Hebrew data do exist, each relying on disparate data sources, with varied evaluation metrics and annotation schemes even for the same task. To investigate Hebrew PLMs and probe their ability to capture linguistic structure, we introduce and evaluate Hebrew PLMs on the full set of tasks, sentence-based , token-based and morpheme-based tasks, including specific task variants and evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Model</head><p>Data. The PLM nicknamed AlephBERT is trained on a larger dataset and a larger vocabulary than any Hebrew BERT instantiation before. Data statistics are provided in <ref type="table" target="#tab_1">Table 2</ref>. Specifically, we employ the following datasets for pre-training:</p><p>? Oscar: A deduplicated Hebrew portion of the OSCAR corpus, which is "extracted from Common Crawl via language classification, filtering and cleaning" (Ortiz <ref type="bibr" target="#b11">Su?rez et al., 2020)</ref>.</p><p>? Twitter: Texts of Hebrew tweets collected between 2014-09-28 and 2018-03-07. We slightly cleaned up the texts by removing retweet signals "RT:", user mentions (e.g. "@username"), and URLs.</p><p>? Wikipedia: The texts in all of Hebrew Wikipedia, 4 extracted using <ref type="bibr" target="#b1">Attardi (2015)</ref> This corpus is available on our github. 5</p><p>One of the most important factors driving the success of PLMs in other languages is the availability of enormous amounts of text to learn from. The Hebrew portions of Oscar and Wikipedia provides us with a training set size which is an order of magnitude smaller compared with resource-savvy languages, as shown in <ref type="table">Table 1</ref>. In order to build a strong PLM we need a considerable boost in the amount of text that the PLM can learn from, which in our case comes form massive amounts of tweets added to the training set. The textual utterances provided by the Twitter sample API tend be short and diverge from valid syntax and canonical language use for the most part. And while the free form language expressed in tweets might differ significantly from the text found in Oscar and Wikipedia, the sheer volume of tweets helps us close the resource gap substantially. Combining all resources together we have tweets comprising the lion's share of sentences in our dataset (72%).</p><p>Training We used the Transformers training framework of Huggingface <ref type="bibr" target="#b17">(Wolf et al., 2020)</ref> and trained two different models -a small model with 6 hidden layers learned from the Oscar portion of our dataset, and a base model with 12 hidden layers which was trained on the entire dataset. The processing units used in both the small and base Aleph-BERT models are wordpieces generated by training BERT tokenizers over the respective datasets with a vocabulary size of 52K in both cases. Traditionally, BERT models are optimized with an objective function optimized using both masked token prediction as well as next sentence prediction losses. Following the work on RoBERTa <ref type="bibr" target="#b8">(Liu et al., 2019)</ref> we employ masked-token prediction loss only in our training objective. Incidentally our choice of dataset also forces us to ignore next sentence prediction because a large portion of our data comprises of tweets which are unrelated and independent of each other (we did not attempt to reconstruct the discourse threads of retweets and replies). For more training details see the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Goal We set out to pre-train Hebrew PLMs and evaluate them empirically on a range of Hebrew NLP tasks. We evaluated the two AlephBERT variants (small and base) on the different tasks, in order to empirically gauge the effect of model size and data size on the quality of the language model. In addition, we compared the performance of our models to existing Hebrew BERT-based instantiations (mBERT <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> and HeBERT <ref type="bibr" target="#b5">(Chriqui and Yahav, 2021)</ref>). We evaluated the PLMs on all key tasks of the Hebrew NLP pipeline.</p><p>Benchmarks We evaluate our BERT-based models on various Hebrew NLP tasks using the following benchmarks:</p><p>? Word Segmentation, Part-of-Speech Tagging, Full Morphological Tagging:</p><p>-The Hebrew Section of the SPMRL Task <ref type="bibr" target="#b13">(Seddah et al., 2013</ref>) -The Hebrew Section of the UD 6 treebanks collection <ref type="bibr" target="#b12">(Sadde et al., 2018)</ref> ? Named Entity Recognition:</p><p>-Token-based NER evaluation based on the corpus of Ben-Mordecai and Elhadad (Ben Mordecai and Elhadad, 2005) -Token-based and Morpheme-based NER evaluation based on the Named Entities and MOrphology (henceforth NEMO) corpus  ? Sentiment Analysis:</p><p>-Sentiment Analysis evaluation based on the corpus of <ref type="bibr" target="#b0">Amram et al. (2018)</ref>. -Since the aforementioned corpus is reported to be leaking (shared material between test and train), we provide a cleaned up version and evaluate on the updated split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Tasks and Modeling Strategies</head><p>A key question when assessing BERT-based PLM performance for Hebrew concerns how to develop models for the different levels of granularity. Here we briefly sketch our modeling strategies, starting with the easiest (classification) tasks and continuing to the more challenging setups, involving the use of PLMs to predict the tokens' internal structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Sentence-Based Modeling</head><p>Sentiment Analysis The first task we report on is a simple sentence classification task, classifying the sentiment of a given sentence to one of three values: negative, positive, neutral. We trained and evaluated BERT-based sentence classification on two variants of the Hebrew Sentiment dataset of <ref type="bibr" target="#b0">Amram et al. (2018)</ref>. The first variant is the original sentiment dataset of <ref type="bibr" target="#b0">Amram et al. (2018)</ref> with an additional split to create a dev set (the original paper had only train and test split, and the test set remains the same). The dev set contains 10% of the train data which leaves us with a split of 70-10-20.</p><p>Unfortunately, the original dataset of Amram et al. had a significant data leakage between the splits, with duplicates in the data samples. After removing the duplicates out of the original 12,804 sentences, we are left with a dataset of size 8,465. <ref type="bibr">7</ref> We fine-tuned all the models for 15 epochs with the default Huggingface <ref type="bibr" target="#b17">(Wolf et al., 2020)</ref> parameters on 5 different seeds. We report per-comment accuracy, and take the mean of these 5 runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Token-Based Modeling</head><p>Named Entity Recognition For the NER task, we initially assume a token-based sequence labeling model. The input comprises of the sequence of tokens in the sentence, and the output contains BIOES tags indicating entity spans. The tokenbased model is a simple fine-tuned model using the Transformer's token-classification script of <ref type="bibr" target="#b17">Wolf et al. (2020)</ref>. We evaluate this model on two corpora. The first is the corpus by Ben Mordecai and Elhadad <ref type="formula">(2005)</ref>, henceforth, the BMC corpus. The BMC corpus annotates entities at Token-level. This means that a Hebrew token containing both a preposition and an entity mention will not deliver the entity-mention boundaries. The BMC contains 3294 sentences and 4600 entities, and has seven different entity categories (DATE, LOC, MONEY, ORG, PER, PER-CENT, TIME). To remain compatible with the original work we train and test the models on the 3 different splits as in . <ref type="bibr">8</ref> For the BMC corpus we report token-based F1 scores on the detected entity mentions.</p><p>The second corpus is an extension of the SPMRL dataset with Named Entities annotation, also marked by BIOSE tags, respecting the precise (token-internal) morphological boundaries of NEs (henceforth, NEMO, standing for Named Entities and MOrphology) . This corpus provides both a token-based and a 7 https://github.com/OnlpLab/ Hebrew-Sentiment-Data 8 https://github.com/OnlpLab/ HebrewResources/tree/master/BMCNER morpheme-based annotation of the entities, where the latter contains the accurate (token-internal) entity boundaries. The NEMO corpus has nine categories (ANG, DUC, EVE, FAC, GPE, LOC, ORG, PER, WOA). It contains 6220 sentences and 7713 entities, and we used the standard SPMRL Train-Dev-Test, as in  The models were trained over 15 epochs and no hyper parameter tuning. For the BMC we used 3 different seeds for each split set, leading to overall nine different training rounds, and for the NEMO set we used the average mean of five different seeds. For both benchmarks we report token-based F1 scores on the detected entity mentions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Morpheme-Based Modeling</head><p>Modern Hebrew is a Semitic language with rich morphology and complex orthography. As a result, the basic processing units in the language are typically smaller than a given token's span. To probe AlephBERT's capacity to accurately predict such token-internal linguistic structure, we test our models on four tasks that require knowledge of the internal morphology of the raw tokens:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? Segmentation</head><p>Input: A Hebrew sentence containing raw space-delimited tokens Output: A sequence of morphological segments representing basic processing units. 9</p><p>? Part-of-Speech Tagging Input: A Hebrew sentence containing raw space-delimited tokens Output: Segmentation of the tokens to basic processing units as above, where each segment is tagged with its single disambiguated part-of-speech tag.</p><p>? Morphological Tagging Input: A Hebrew sentence containing raw space-delimited tokens Output: Segmentation of the tokens to basic processing units as above, where each segment is tagged with a single POS tag and a set of morphological features. 10 9 These units comply with the 2-level representation of tokens defined by UD, where each basic unit corresponds to a single POS tag. https://universaldependencies. org/u/overview/tokenization.html 10 Equivalent to the AllTags evaluation metric defined in the CoNLL18 shared task.</p><p>https: //universaldependencies.org/conll18/ results-alltags.html</p><p>? Morpheme-Based NER Input: A Hebrew sentence containing raw space-delimited tokens Output: Segmentation of the tokens to basic processing as above where segment is tagged with a BIOSE tags indicating entity spans, along with the entity-type label.</p><p>An illustration of these tasks is given in <ref type="table" target="#tab_3">Table 3</ref>. As opposed to fine-tuning the PLM model parameters, as done in sentence-based and tokenbased classification tasks, segmented morphemes are not readily available in the BERT representation. In order to provide proper segmentation and labeling for the four aforementioned tasks we developed a model designated to produce the morphological segments of each token in context. The morphological segmentation model which we designed is composed of a PLM responsible for transforming input tokens into contextualized embedded vectors, which we then feed into a charbased seq2seq module that extracts the output segments. The seq2seq module is composed of an encoder implemented as a simple char-based BiL-STM, and a decoder implemented as a char-based LSTM generating the output character symbols, or a space symbol signalling the end of a morphological segment. We train the model for 15 epochs, optimizing next-character prediction loss function.</p><p>For the other tasks, involving both segmentation and labeling we deploy an MTL (multi-task learning) setup. That is, when generating an endof-segment symbol, the model then predicts task labels which can be one or more of the following: POS-tag, NER-tag, morphological features. In order to guide the training to learn we optimize the combined segmentation and label prediction loss values. Currently we simply add together the loss values, but we note that as a future improvement it is likely that assigning different weights to the different loss values could prove to be beneficial. All of these morphological labeling models are trained for 15 epochs and evaluated on both the UD <ref type="bibr" target="#b12">(Sadde et al., 2018)</ref> and SPMRL data <ref type="bibr" target="#b13">(Seddah et al., 2013)</ref>.</p><p>In addition, we design another setup for running the various morphological labeling tasks in which we first segment the text (using the abovementioned segmentation model) and then perform fine-tuning with a token classification attention head directly applied to the PLM (similar to the way we fine-tune the PLM for the token-based NER task described in the previous section). In this pipeline setup we utilize the PLM twice; as part of the segmentation model to generate segments, which we then feed directly into the PLM (augmented with a token classification head) which is fine-tuned for the specific labeling task. We acknowledge the fact that we are fine-tuning the PLM using morphological segments even though it was originally pre-trained without any knowledge of sub-token units. But, as we shall see shortly, this seemingly unintuitive strategy performs surprisingly well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>Sentence-Based Tasks The Sentiment analysis experimental results are provided in <ref type="table" target="#tab_6">Table 5</ref>. As can be seen, all BERT-based models substantially outperform the original CNN Baseline reported by <ref type="bibr" target="#b0">Amram et al. (2018)</ref>. Interestingly, both AlephBERT-small and AlephBERT-base outperform all BERT-based variants, with BERT-base setting new SOTA results on the new (fixed) dataset.</p><p>Token-Based Tasks For our two NER benchmarks, we report the NER F1 scores on the tokenbased fine-tuned model in <ref type="table" target="#tab_4">Table 4</ref>.</p><p>Here, although we see noticeable improvements for the mBERT and HeBert variants over the current SOTA, the most significant increase is in the AlephBERT-base model. We also see a substantial difference between the AlephBERT-small and AlephBERT-base models, with the latter providing a new SOTA results on these both data sets. Crucially, this holds for the token-based evaluation metrics (as defined in ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Morpheme-Based Tasks</head><p>As a particular novelty of this work, we report BERT-based results on subtoken (segment-level) information. Specifically, we evaluate segmentation F1, POS F1, Morphological Features F1 and morphem-base NER F1, compared against the disambiguated labeled segments. In all cases we use raw space-delimited tokens as input, letting the BERT-based models perform both the segmentation and labeling. <ref type="table" target="#tab_7">Table 6</ref> presents the segmentation, POS tags, and morphological tags F1 for the SPMRL dataset, all evaluated at the granularity of morphological segments. We report the aligned multiset F1 Scores as in previous work on Hebrew <ref type="bibr" target="#b10">(More et al., 2019)</ref>.</p><p>We see that segmentation results for all BERTbased models are similar, and they are already at   With respect to all BERT-based variants, we see an improvement for AlephBERT on all other alternatives, but on a small scale. That said, we do notice a repeating trend that places AlephBERT-base as the best model for all of our morphological tasks, indicating that the improvement provided by the depth of the model and a larger dataset does also improve the ability to capture token-internal structure. These trends are replicated on the UD Hebrew corpus, for two different evaluation metrics -the Aligned MultiSet F1 Scores as in previous work on Hebrew <ref type="bibr" target="#b10">(More et al., 2019)</ref>, , and the Aligned F1 scores metrics in the UD shared task <ref type="bibr" target="#b18">(Zeman et al., 2018)</ref> -as reported in <ref type="table" target="#tab_8">Tables 7 and 8</ref> respectively. AlephBERT obtains the best results for all tasks, even if not by a large margin.</p><p>Morpheme-Based NER Earlier in this section we considered NER as a token-based task that sim-ply requires fine-tuning on the token labels. However, this setup is not accurate enough and less useful for downstream tasks, since the exact entity boundaries are often token internal . We hence also report here morpheme-based NER evaluation, respecting the exact boundaries of the Entity mentions. To obtain morpheme-based labeled-span of Named Entities as discussed above we could either employ a pipeline, first predicting segmentation and then applying a fine tuned labeling model directly on the segments, or we can use the MTL model and predict NER labels while performing the segmentation. <ref type="table" target="#tab_10">Table 9</ref> presents segmentation and NER results for three different scenarios: (i) a pipeline assuming gold segmentation (ii) a pipeline assuming the best predicted segmentation (as predicted above) (iii) obtaining the segmentation and NER labels jointly in the MTL setup.</p><p>As our results indicate, AlephBERT-base consistently scores highest in both pipeline (oracle and predicted) and multi-task setups. Looking at the Pipeline-Predicted scores, there is a clear correlation between a higher segmentation quality of a PLM and its ability to produce better NER results. Moreover, the differences in NER scores between the models are considerable (unlike the subtle differences in segmentation, POS and morphological features scores) and draw our attention to the relationship between the size of the PLM, the size of the pre-training data and the quality of the final NER models. Specifically, HeBERT and AlephBERT-small were pre-trained with similar datasets -HeBERT with Oscar and Wikipedia, AlephBERT-small with Oscar only (the Wikipedia portion is order of magnitude smaller compared with Oscar) and comparable vocabulary sizes (heBERT with 30K and AlephBERT-small with 52K). However we notice that HeBERT, with its 12 hidden layers, performs significantly better        for the Pipeline (Oracle), Pipeline (Predicted) and a Hybrid (almost-joint) Scenarios, respectively. compared to AlephBERT-small which is composed of only 6 hidden layers. It thus appears that semantic information is learned in those deeper layers which helps in both learning to discriminate entities and improve the overall morphological segmentation capacity.</p><p>In addition, comparing HeBERT to AlephBERTbase we point to the fact that they are both modeled with the same 12 hidden layer architecture, the only differences between them are in the size of their vocabularies (30K vs 52K respectively) and the size of the training data (Oscar-Wikipedia vs Oscar-Wikipedia-Tweets). The improvements exhibited by AlephBERT-base, compared to HeBERT, suggests that it is a result of the large amounts of training data and larger vocabulary available in our setup. By exposing AlephBERT-base to an amount of text which order of magnitude larger we increased its NER capacity.</p><p>Finally, our NER experiments suggest that a pipeline composed of our near-to-perfect morphological segmentation model followed by AlephBERT-base augmented with a token classification head is the best strategy for generating morphologically-aware NER labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Qualitative Assessment</head><p>To allow for qualitative assessment of the PLMs, we deliver an online demo where one can compare the masked-word prediction capacities of the different models, and get the impression of the strengths and weaknesses. Our demo, available at https:// nlp.biu.ac.il/?elronbandel/alephbert/, offers friendly graphical interface that allows one to mask an item in a running Hebrew text and obtain the top-N list of alternatives predicted by each of the models. The demo allows to explore the predictions of our models both at token level and at sub-token level, masking individual word-pieces.</p><p>Note that the AlephBERT family of models is still under development, and we will add new model variants as we proceed. Stay tuned!</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>Modern Hebrew, a morphologically rich and resource-scarce language, has for long suffered from a gap in the resources available for NLP applications, and lower level of empirical results than observed in other, resource-rich languages. This work provides the first step in remedying the situation, by making available a large Hebrew PLM, nicknamed AlephBERT, with larger vocabulary and larger training set than any Hebrew PLM before, and with clear evidence as to its empirical advantages. Our AlephBERT-base model obtains stateof-the-art results on the tasks of segmentation, Part of Speech Tagging, Named Entity Recognition, and Sentiment Analysis. We outperform both general multilingual PLMs (mBERT) as well as language specific instantiations (HeBERT). More importantly, using the new AlephBERT models we are now gaining similar benefits as achieved in high resource languages from PLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Acknowledgements</head><p>We are enormously grateful to Roee Aharoni from Google and Yoav Goldberg from Bar Ilan University for technical advise during the project. No less importantly, we are indebted to Roee Aharoni for coining the brand-name AlephBERT. The research reported in this paper is funded by an individual grant by the Israel Science Foundation (ISF grant #1739/26) and a starting grant by the European Research Council (ERC-StG Grant #677352), for which we are grateful.</p><p>AlephBERT-base AlephBERT-small HeBERT mBERT-cased max position <ref type="table" target="#tab_1">embeddings  512  512  512  512  num attention heads  12  12  12  12  num hidden layers  12  6  12  12  vocab size  52K  52K  30K</ref> 120K ? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A AlephBERT Training Details</head><p>For reference and to make our work reproducible we specify here the main steps taken and parameters used during training of AlephBERT. We utilized the Huggingface Transformers framework with most of the default training parameter values. <ref type="table">Table-</ref>10 lists all of the training parameters that we have manually specified in our code. We also list the values used by the other models. Training our AlephBERT-base model using the entire dataset proved to be technically challenging due to the model size and data size. With the naive approach training the entire dataset without splitting it into chunks did not utilize the full processing capacity of the GPUs and would have taken several weeks to complete. To overcome this issue we followed the advice to split the dataset into chunks based on the number of tokens in a sentence. The first chunk consisted of 70M senetences with 32 or less tokens. By limiting the maximum number tokens we consequently limit the size of the training matrices used by this chunk which consequently allowed for significantly increasing the batch size which resulted in dramatically shorter training time -these 70M sentences took only 2.5 days to complete 5 epochs. The second chunk consisted of sentences having between 32 and 64 tokens, the third chunk between 64 and 128 and the final last chunk all sentences with more than 128 tokens. We trained each chunk for 5 epochs with setting the learning rate to 1e-4. Once we went over the entire dataset we trained for another 5 epochs with a learning rate set to 5e-5 for a total of 10 epochs. We trained our base model over the entire dataset for 10 epochs on a NVidia DGX server with 8 V100 GPUs which took 8 days. The small model was trained over 10 epochs using 4 GTX 2080ti GPUs for 5 days in total.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Data Statistics for AlephBERT's training sets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Illustration of Evaluated Token and Morpheme-Based Downstream Tasks. The input is the two-word input phrase " ????? ?"????? (to the White House). Sequence and Hebrew text goes from right to left.</figDesc><table><row><cell></cell><cell>NEMO BMC</cell></row><row><cell>Previous SOTA</cell><cell>77.75 85.22</cell></row><row><cell>mBERT</cell><cell>79.07 87.77</cell></row><row><cell>HeBERT</cell><cell>81.48 89.41</cell></row><row><cell>AlephBERT-small</cell><cell>78.69 89.07</cell></row><row><cell>AlephBERT-base</cell><cell>84.91 91.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Token-Based NER Results on the NEMO and</cell></row><row><cell>the Ben-Mordecai Corpora. Previous SOTA on both</cell></row><row><cell>corpora has been reported by the NEMO models of</cell></row><row><cell>Bareket and Tsarfaty (2020).</cell></row><row><cell>the higher range of 97-98 F1 scores, which are</cell></row><row><cell>hard to improve further. 11 For POS tagging and</cell></row><row><cell>morphological features, all BERT-based models</cell></row><row><cell>significantly outperform the previous SOTA pro-</cell></row><row><cell>vided by (Seker and Tsarfaty, 2020) (referred to</cell></row><row><cell>as PtrNet) for POS tags and (More et al., 2019)</cell></row><row><cell>(referred to as YAP) for morphological features.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Sentiment Analysis Scores on the Facebook Corpus. Previous SOTA is reported by<ref type="bibr" target="#b0">Amram et al. (2018)</ref>.</figDesc><table><row><cell></cell><cell cols="3">Segmentation F1 POS F1 Morphological Features F1</cell></row><row><cell>Previous SOTA</cell><cell>NA</cell><cell>90.49</cell><cell>85.98</cell></row><row><cell>mBERT-morph</cell><cell>97.36</cell><cell>93.37</cell><cell>89.36</cell></row><row><cell>HeBERT-morph</cell><cell>97.97</cell><cell>94.61</cell><cell>90.93</cell></row><row><cell>AlephBERT-small-morph</cell><cell>97.71</cell><cell>94.11</cell><cell>90.56</cell></row><row><cell>AlephBERT-base-morph</cell><cell>98.10</cell><cell>94.90</cell><cell>91.41</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell cols="4">: Morpheme-Based Aligned MultiSet (mset) Results on the SPMRL Corpus. Previous SOTA is as reported</cell></row><row><cell cols="4">by (Seker and Tsarfaty, 2020) (POS) and (More et al., 2019) (morphological features)</cell></row><row><cell></cell><cell cols="3">Segmentation F1 POS F1 Morphological Features F1</cell></row><row><cell>Previous SOTA</cell><cell>NA</cell><cell>94.02</cell><cell>NA</cell></row><row><cell>mBERT-morph</cell><cell>97.70</cell><cell>94.76</cell><cell>90.98</cell></row><row><cell>HeBERT-morph</cell><cell>98.05</cell><cell>96.07</cell><cell>92.53</cell></row><row><cell>AlephBERT-small-morph</cell><cell>97.86</cell><cell>95.58</cell><cell>92.06</cell></row><row><cell>AlephBERT-base-morph</cell><cell>98.20</cell><cell>96.20</cell><cell>93.05</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell cols="4">: Morpheme-Based Aligned MultiSet (mset) Results on the UD Corpus. Previous SOTA is as reprted by</cell></row><row><cell>(Seker and Tsarfaty, 2020) (POS)</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Segmentation F1 POS F1 Morphological Features F1</cell></row><row><cell>Previous SOTA</cell><cell>96.03</cell><cell>93.75</cell><cell>91.24</cell></row><row><cell>mBERT-morph</cell><cell>97.17</cell><cell>94.27</cell><cell>90.51</cell></row><row><cell>HeBERT-morph</cell><cell>97.54</cell><cell>95.60</cell><cell>92.15</cell></row><row><cell>AlephBERT-small-morph</cell><cell>97.31</cell><cell>95.13</cell><cell>91.65</cell></row><row><cell>AlephBERT-base-morph</cell><cell>97.70</cell><cell>95.84</cell><cell>92.71</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8</head><label>8</label><figDesc>NEMO) 100.00 79.10 95.15 69.52 97.05 77.11 mBERT 100.00 77.92 97.68 72.72 97.24 72.97</figDesc><table><row><cell>: Morpheme-Based Aligned (CoNLL shared task) Results on the UD Corpus. Previous SOTA is as reported</cell></row><row><cell>by Minh Van Nguyen and Nguyen (2021)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Morpheme-Based NER Evaluation on the NEMO Corpus. Previous SOTA is as reported by</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Huggingface BERT Configurations Comparison. ?Only 2450 vocabulary entries contain Hebrew letters</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Dump: hewiki-20200201-pages-articles.xml.bz2 5 https://github.com/OnlpLab/AlephBERT/ blob/main/data/wikipedia/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://universaldependencies.org</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">Some of these errors are due to annotation errors, or truly ambiguous cases.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Representations and architectures in neural sentiment analysis for morphologically rich languages: A case study from modern hebrew</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Amram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reut</forename><surname>Tsarfaty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-08-20" />
			<biblScope unit="page" from="2242" to="2252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giusepppe</forename><surname>Attardi</surname></persName>
		</author>
		<ptr target="https://github.com/attardi/wikiextractor" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural modeling for named entities and morphology (nemo?2)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Bareket</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reut</forename><surname>Tsarfaty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/2007.15620</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Hebrew named entity recognition</title>
		<editor>Naama Ben Mordecai and Michael Elhadad</editor>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Hebert -&amp; hebemo: a hebrew bert model and a tool for polarity analysis and emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avihay</forename><surname>Chriqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inbal</forename><surname>Yahav</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Getting the ##life out of living: How adequate are word-pieces for modelling complex morphology?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stav</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reut</forename><surname>Tsarfaty</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.sigmorphon-1.24</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology</title>
		<meeting>the 17th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology</meeting>
		<imprint>
			<date type="published" when="2020-07-10" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="204" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<title level="m">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Trankit: A lightweight transformer-based toolkit for multilingual natural language processing</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations</title>
		<editor>Amir Pouran Ben Veyseh Minh Van Nguyen, Viet Lai and Thien Huu Nguyen</editor>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Joint transition-based models for morpho-syntactic parsing: Parsing strategies for mrls and a case study from modern hebrew</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>More</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Seker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Basmova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reut</forename><surname>Tsarfaty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="33" to="48" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A monolingual approach to contextualized word embeddings for mid-resource languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro Javier Ortiz</forename><surname>Su?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Romary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beno?t</forename><surname>Sagot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1703" to="1714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The hebrew universal dependency treebank: Past present and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoval</forename><surname>Sadde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Seker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reut</forename><surname>Tsarfaty</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/w18-6016</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Universal Dependencies</title>
		<meeting>the Second Workshop on Universal Dependencies<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-11-01" />
			<biblScope unit="page" from="133" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Wolfgang Seeker, Yannick Versley, Veronika Vincze, Marcin Wolinski, Alina Wr?blewska, and?ric Villemonte de la Clergerie</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djam?</forename><surname>Seddah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reut</forename><surname>Tsarfaty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>K?bler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename><surname>Candito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinho</forename><forename type="middle">D</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich?rd</forename><surname>Farkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iakes</forename><surname>Goenaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Koldo Gojenola Galletebeitia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spence</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Habash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Kuhlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Przepi?rkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages</title>
		<meeting>the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10-18" />
			<biblScope unit="page" from="146" to="182" />
		</imprint>
	</monogr>
	<note>Overview of the SPMRL 2013 shared task: A cross-framework evaluation of parsing morphologically rich languages</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A pointer network architecture for joint morphological segmentation and tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Seker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reut</forename><surname>Tsarfaty</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.391</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4368" to="4378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">From SPMRL to NMRL: what did we learn (and unlearn) in a decade of parsing morphologically-rich languages (mrls)?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reut</forename><surname>Tsarfaty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Bareket</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stav</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Seker</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.660</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-07-05" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="7396" to="7408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">What&apos;s wrong with hebrew nlp? and how to make it right</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reut</forename><surname>Tsarfaty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoval</forename><surname>Sadde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stav</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Seker</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-3044</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-03" />
			<biblScope unit="page" from="259" to="264" />
		</imprint>
	</monogr>
	<note>-System Demonstrations</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">CoNLL 2018 shared task: Multilingual parsing from raw text to Universal Dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Haji?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K18-2001</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
		<meeting>the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
