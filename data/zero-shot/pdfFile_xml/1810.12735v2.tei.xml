<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spoken Language Understanding on the Edge</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-10-02">2 Oct 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaa</forename><surname>Saade</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Snips, Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Coucke</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Snips, Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Caulier</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Snips, Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Dureau</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Snips, Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Ball</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Snips, Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Th?odore</forename><surname>Bluche</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Snips, Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Leroy</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Snips, Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Doumouro</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Snips, Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Gisselbrecht</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Snips, Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Caltagirone</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Snips, Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Snips, Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ma?l</forename><surname>Primet</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Snips, Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Spoken Language Understanding on the Edge</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-10-02">2 Oct 2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider the problem of performing Spoken Language Understanding (SLU) on small devices typical of IoT applications. Our contribution is two-fold. First, we outline the design of an embedded, private-by-design SLU system and show that it has performance on-par with cloud-based commercial solutions. Second, we release the datasets used in our experiments in the interest of reproducibility and in the hope that they can prove useful to the community.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Spoken Language Understanding (SLU) is the task of extracting meaning from a spoken utterance. Over the last years, thanks in part to steady improvements brought by deep learning approaches to Automatic Speech Recognition (ASR) <ref type="bibr" target="#b17">[18]</ref>, voice interfaces implementing SLU have greatly evolved from spotting limited and predetermined keywords to understanding arbitrary formulations of a given intention, and are becoming ubiquitous in connected devices. Most current solutions however offload their processing to the cloud, where computationally demanding engines can be deployed. As an example, the ASR engine achieving human parity in <ref type="bibr" target="#b17">[18]</ref> is a combination of several neural networks, each containing several hundreds of millions of parameters, and large-vocabulary language models made of several millions of n-grams. The size of these models, along with the computational resources necessary to run them in real-time, make them unfit for deployment on small devices. Running SLU on the edge (i.e. embedding the engine directly on the device without resorting to the cloud) however offers several advantages. First, on-device processing removes the need to send speech, or other personal data to third-party servers, therefore guaranteeing a high level of privacy. In particular, we show in Section 3.1 how an embedded SLU model can be personalized on device using user data. Additional benefits include a reduction in latency and offline capabilities <ref type="bibr" target="#b13">[14]</ref>. In this paper, we describe the Snips Voice Platform, a SLU system that runs directly on device, therefore offering all the advantages of edge computing, and has performance on-par with commercial, cloud-based solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Outline and main results</head><p>A typical SLU system has three main components. First, an Acoustic Model (AM) maps a spoken utterance to a sequence of probabilities over phones (units of speech). Second, a Language Model (LM) maps the output of the AM to a likely text sentence. These first two components constitue the ASR system. Third, a Natural Language Understanding (NLU) engine extracts from the sentence the intent of the user (e.g. querying the weather forecast) and the slots qualifying her query (e.g. a city in the case of a weather forecast query). Our main contribution is to outline the design of an embedded SLU system that achieves performances on-par with cloud-based solutions, and is efficient enough to run in real time on IoT devices as small as the Raspberry Pi 3, with 1GB of RAM and 1.4GHz CPU. This is achieved by optimizing a trade-off between accuracy and computational efficiency when designing the AM, and by contextualizing the LM and NLU components in order both to reduce their size and increase their in-domain accuracy. While the AM is trained once per language, the subsequent SLU components are use-case dependent. We have also released publicly 1 the datasets used for the experiments of Section 5 in the hope that they can be useful to the research community. The NLU component of the Snips Voice Platform is open source 2 . Our SLU models can be trained through a web console, at no cost for non-commercial use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Relation to previous work</head><p>Recent interest in mobile speech recognition has lead to new work on ASR model compression <ref type="bibr" target="#b7">[8]</ref>. In this work, personal data is incorporated dynamically in the language model using a class-based model similar to the one we introduce in the following. The authors however do not study the performance of their system in terms of SLU performance but rather on a large-vocabulary speech recognition task. We rather introduce contextualized models assessed through end-to-end SLU metrics, which are arguably a better proxy for user experience <ref type="bibr" target="#b15">[16]</ref>. Another line of work is interested in embedded speech commands, leveraging small models that can understand a small range of predefined commands, usually limited to one or two words <ref type="bibr" target="#b16">[17]</ref>. These approaches however cannot handle the variety of natural language interactions addressed in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Acoustic modeling</head><p>Our AM is designed so as to optimize a trade-off between accuracy and computational efficiency. We use training datasets consisting of a few thousand hours of audio data with corresponding transcripts. Noisy, far-field conditions with reverberation are simulated by augmenting the data with thousands of virtual rooms with random microphone and speaker locations. We train deep neural AMs using the Kaldi toolkit <ref type="bibr" target="#b11">[12]</ref>. Our typical architectures have 7 layers (and one output layer), predict ? 1600 biphone senones, and are trained with the lattice-free Maximum Mutual Information criterion <ref type="bibr" target="#b12">[13]</ref>, using natural gradient descent with a learning rate of 0.0005. By varying the number of neurons of each layer of the AM, we obtain models of different sizes with different computational requirements (see <ref type="table" target="#tab_0">Table 1</ref>). The AM is chosen to offer near state-of-the-art performance, while running in real time with acceptable memory requirements dependent on the target hardware. In <ref type="table">Table 2</ref>, we assess the accuracy of the various architectures on a standard large-vocabulary speech recognition task with the LibriSpeech dataset <ref type="bibr" target="#b10">[11]</ref> using the accompanying LM (refered to as tegmed in Kaldi). In the following, we consider the nn256 model which is close to nn512 in accuracy while being six times smaller, and runs in real time on a Raspberry Pi 3. We show in the following how to compensate this loss in accuracy by contextualizing the subsequent components of the SLU pipeline to a certain domain, e.g. by restricting the vocabulary and the variety of the queries that should be modeled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Language modeling</head><p>The mapping from the output of the acoustic model to likely word sequences is done via a Viterbi search in a weighted Finite State Transducer (wFST) <ref type="bibr" target="#b9">[10]</ref>, called ASR decoding graph in the following. Formally, the decoding graph may be written as the composition of four wFSTs,</p><formula xml:id="formula_0">H * C * L * G ,<label>(1)</label></formula><p>where * denotes transducer composition, H represents Hidden Markov Models (HMMs) modeling context-dependent phones, C represents the context-dependency, L is the lexicon and G is the LM, typically a bigram or a trigram model represented as a wFST. The compositions are carried out right to left, with determinization and minimization operations <ref type="bibr" target="#b9">[10]</ref> applied at each step to optimize decoding. We refer the interested reader to <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12]</ref> and references therein for background on wF-STs and their use in speech recognition. In the following, we focus on the construction of the G transducer, encoding the LM, from a domain-specific dataset.  <ref type="table">Table 2</ref>: Word error rates (%) achieved with neural networks of different sizes on the splits of the LibriSpeech dataset <ref type="bibr" target="#b10">[11]</ref>. KALDI denotes the performance of the reference Kaldi recipe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Language model adaptation</head><p>Our LM is adapted to understand arbitrary formulations of a finite set of intents described in a dataset. Generalization to unseen queries is enabled by using both a statistical n-gram LM <ref type="bibr" target="#b5">[6]</ref> which allows to mix parts of the training queries to create new ones, and class-based language modeling <ref type="bibr" target="#b2">[3]</ref> to swap slot values. More precisely, we start by building patterns abstracting the queries of the dataset by replacing all occurrences of each slot by a symbol. For example, the query "Play some music by (The Rolling Stones)[artist]" is abstracted to "Play some music by ARTIST". An n-gram model is then trained on the resulting set of patterns, converted to a wFST called G p <ref type="bibr" target="#b9">[10]</ref>. Next, for each slot s i where i ? [1, n] and n is the number of slots, an acceptor G si is defined to encode the values the slot can take. G si can either encode an n-gram model trained on a gazetteer (i.e. a list of possible values), or a generative grammar exhaustively describing the construction of any slot value (e.g. for numbers or dates). Denoting wFST replacement as "Replace", we have <ref type="bibr" target="#b4">[5]</ref> </p><formula xml:id="formula_1">G = Replace(G p , {G si , ?i ? [1, n]}) ,<label>(2)</label></formula><p>The resulting SLU system is contextualized, and supported on a domain-specific vocabulary. As a result, while a sufficient amount of specific training data may guarantee sampling the important words which allow to discriminate between different intents, it will in general prove unable to correctly sample filler words from general spoken language. In order to fix this and detect out of vocabulary words (OOV), we use an approach based on so-called confusion networks <ref type="bibr" target="#b18">[19]</ref> to represent decoded words along with their posterior probability. We finally tag decoded words as unknown if their posterior probability is lower than some threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dynamic language model</head><p>On small devices, computing the decoding graph (1) can result in a prohibitively large wFST for larger assistants. For this reason, we build a dynamic language model by precomputing HCL and G, and composing them lazily <ref type="bibr" target="#b1">[2]</ref>. The states and transitions of the decoding graph are thus computed on demand during inference, notably speeding up the building of the LM. Additionally, employing lazy composition allows to break the decoding graph into two pieces, with sizes typically much smaller than the equivalent, statically-composed HCLG. When using a dynamic LM, a better composition algorithm must be used in order to keep the decoding fast enough. We use composition filters <ref type="bibr" target="#b1">[2]</ref> such as look-ahead filters followed by label reachability filters with weights and labels pushing, allowing to discard inaccessible and costly decoding hypotheses early in the decoding. Crucially, we ensure that the lexicon verifies the so-called C1P property (i.e. each symbol has a unique pronunciation <ref type="bibr" target="#b0">[1]</ref>) by associating a unique symbol for each pair (word, pronunciation). Finally, the Replace operation of Equation <ref type="formula" target="#formula_1">(2)</ref> is performed upon loading the model from disk. This allows to further break the decoding graph into smaller distinct pieces: the HCL transducer mapping the output of the acoustic model to words, the query language model G p , and the slots' language models {G si , ?i ? [1, n]}.</p><p>Breaking down the LM into smaller, separate parts makes it possible to efficiently update it. In particular, performing on-device injection of new values in the LM becomes straightforward, enabling users to customize their embedded SLU engine. For instance, if we consider an assistant dedicated to making phone calls ("call (Jane Doe)[contact]"), the user's list of contacts could be added to the values of the slot "contact" without this sensitive data ever leaving the device. To do so, the new words and their pronunciations are first added to the HCL transducer, using an embedded Grapheme to Phoneme engine (G2P) to compute the missing pronunciations. The new slot values are then added to the corresponding slot wFST G si by updating the counts of the n-grams. The time required for the complete slot value injection procedure ranges from a few seconds for small assistants, to a few dozen seconds for larger assistants supporting a vocabulary comprising tens of thousands of words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Natural language understanding</head><p>The NLU component performs intent classification followed by slot filling. The former is implemented with a logistic regression trained on the queries from every intent. The latter consists in several linear-chain Conditional Random Fields (CRFs) <ref type="bibr" target="#b6">[7]</ref>, each of them trained for a specific intent. While CRFs are a standard approach for slot filling <ref type="bibr" target="#b14">[15]</ref>, we note that more computationally demanding approaches based on deep learning models have been recently proposed <ref type="bibr" target="#b8">[9]</ref>. Our experiments showed that these approaches do not yield any significant gain in accuracy in the typical training size regime of custom voice assistants (a few hundred queries). Data sparsity is addressed by integrating features based on precomputed word clusters, obtained by clustering word embeddings computed on a large independent corpus, effectively reducing the vocabulary size from typically 50K words to a few hundred clusters. Finally, gazetteer features are used, based on parsers built from the slot values provided in the training data. Consistently with the n-gram slot models G si in the LM (see Section 3.1), these parsers can match partial slot values. When injecting personal user data (see Section 3.1), these gazetteer parsers are augmented accordingly to cover the new slot values. This NLU component is open source and has been benchmarked and proven to be competitive against various commercial solutions <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Numerical Results</head><p>In this section, we present an end-to-end evaluation of both our SLU system and a cloud-based commercial solution, on two domains of increasing complexity posing different challenges. In the interest of reproducibility, the datasets used in the following are publicly available (see Section 1.1). The trained SLU models can be obtained through the Snips web console at no cost for noncommercial use. In our comparison with Google's cloud services, we used the service's built-in slots and features whenever possible in the interest of fairness. For all experiments, we fix our threshold for OOV detection to 0.2, the pattern transducer G p is a bigram model, while the G si corresponding to the gazetteer-based slots are trigrams (see Section 3.1 for definitions of these quantities).</p><p>Experimental setting. Our datasets contain up to a few thousand text queries with their supervision, i.e. intent and slots, collected using an in-house data generation pipeline described in <ref type="bibr" target="#b3">[4]</ref>. We then crowdsource the recording of these sentences and collect one spoken utterance for each text query in the dataset. Far-field datasets are created by playing these utterances with a neutral speaker and record them using a microphone array positioned at a distance of 2 meters. The aim of a SLU system is then, given one such spoken utterance, to predict the ground-true intent (intent classification) and slots. We measure the performance of both our SLU system and Google's cloud services in terms of F1-score on intent classification, and percentage of perfectly parsed utterances, such that both intent and slots are recovered.</p><p>Small assistant. We first consider a small assistant typical of smart home use cases, the "Smart-Lights" assistant, comprising 6 intents allowing to turn on or off the light, or change its brightness or color. It has a vocabulary size of approximately 400 words, and depends on three slots (room,   <ref type="table">Table 4</ref>: Music assistants: percentage of perfectly parsed utterances of the form "I want to listen to #ARTIST". The tiers are created using a ranking of 10k artists according to their stream counts on Spotify: Tier 1 corresponds to artists with rank between 1 and 1,000, tier 2 have ranking between 4,500 and 5,500 and tier 3 between 9,000 and 10,000. The Snips SLU system is trained on a complete music assistant handling several interactions with a smart speaker (see text). The results labeled "Google" correspond to replacing the Snips ASR component by Google's Speech Recognition API.</p><p>brightness and color). <ref type="table" target="#tab_2">Table 3</ref> shows that we reach an accuracy similar to a commercial, cloud-based solution. Our SLU system for this assistant has a total size of 15.1MB and runs in real time on a Raspberry Pi 3.</p><p>Large assistant. We then turn to a large and complex assistant allowing to control a smart speaker through playback control (volume control, track navigation, etc), but also play music from large libraries of artists, tracks, and albums. In addition to the English version of the assistant, we also consider a French version which presents the additional difficulty of handling the pronunciations of many English words in French. We compute cross-language pronunciations for these words using a statistical English G2P, and then mapping their phonemes to the closest ones in the French phonology. The vocabulary of the resulting English music assistant contains more than 65k words, corresponding to 178k pronunciations, while the French assistant has more than 70k words, with 390k pronunciations. These assistants are the largest we consider, with a total size on disk of 80MB for the English version, and 112MB for the French version. They run in real time on a Raspberry Pi 3. We test these assistants on utterances of the form "play some music by #ARTIST", where we sample "#ARTIST" from a publicly available list of the most streamed artists on Spotify (released together with the dataset). This experiment is representative of the difficulty of the SLU task, and additionally allows to estimate the performance of ASR systems as a function of the popularity of artists. To this end, we consider two sets of experiments. In the first, we perform inference using a full Snips SLU engine and compute the fraction of correctly parsed utterances. In a second experiment, we replace Snips ASR by Google's Speech Recognition API. We find (see <ref type="table">Table 4</ref>) that the performance of cloud-based, general-purpose solutions such as Google's ASR decay rapidly with the ranking of the artist. By contrast, our class-based approach outlined in Section 3.1 assigns similar weights to all artists, resulting in more robust performance even for less popular artists. Additionally, in practice, our SLU system can incorporate user-specific tastes through value injection (see Section 3.2), e.g. by connecting privately to a user's favorite streaming service.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>SLU on the edge can achieve the accuracy of cloud-based solutions without compromising on user privacy while running in real time on small IoT devices. This is mainly done by optimizing a tradeoff between accuracy and computational efficiency when designing the AM and by contextualizing the LM and NLU components. Future work includes further optimization to run our models on microcontrollers and leveraging local speaker identification to improve the decoding accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Network architecture with corresponding layer sizes. TDNN refers to a Time-Delay layer with ReLU activation. LSTMP means Long Short-Term Memory with Projection layer. A projection layer size of N is denoted pN . The context, i.e. the number of relative frames seen by the layer at time t, is shown in parentheses: the recurrent connections skip 3 frames in LSTMP layers, and the TDNN layers consider inputs from various time steps.</figDesc><table><row><cell>Layer Type</cell><cell></cell><cell>nn256</cell><cell>nn512</cell><cell>nn768</cell></row><row><cell cols="2">TDNN(?2, ?1, 0, 1, 2)</cell><cell>256</cell><cell>512</cell><cell>768</cell></row><row><cell cols="2">2 ? TDNN(?1, 0, 1)</cell><cell>256</cell><cell>512</cell><cell>768</cell></row><row><cell cols="2">LSTMP(rec: -3)</cell><cell cols="3">256, p128 512, p256 768, p256</cell></row><row><cell cols="2">2 ? TDNN(?3, 0, 3)</cell><cell>256</cell><cell>512</cell><cell>768</cell></row><row><cell cols="2">LSTMP(rec: -3)</cell><cell cols="3">256, p128 512, p256 768, p256</cell></row><row><cell>Num. params</cell><cell></cell><cell>2.6M</cell><cell>8.7M</cell><cell>15.4M</cell></row><row><cell cols="5">Model dev-clean dev-other test-clean test-other</cell></row><row><cell>nn256</cell><cell>7.3</cell><cell>19.2</cell><cell>7.6</cell><cell>19.6</cell></row><row><cell>nn512</cell><cell>6.4</cell><cell>17.1</cell><cell>6.6</cell><cell>17.6</cell></row><row><cell>nn768</cell><cell>6.4</cell><cell>16.8</cell><cell>6.6</cell><cell>17.5</cell></row><row><cell>KALDI</cell><cell>3.9</cell><cell>10.2</cell><cell>4.2</cell><cell>10.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="6">: 'SmartLights" assistant: end-to-end generalization performance compared with Google's</cell></row><row><cell cols="6">Dialogflow cloud service on a 5-fold cross-validation experiment, in terms of F1-score in intent</cell></row><row><cell cols="6">classification and percentage of perfectly parsed utterances (both intent and slots are recovered).</cell></row><row><cell></cell><cell></cell><cell>Close field</cell><cell></cell><cell>Far field</cell><cell></cell></row><row><cell cols="2">Language Provider</cell><cell cols="2">Tier 1 Tier 2 Tier 3 Average</cell><cell cols="2">Tier 1 Tier 2 Tier 3 Average</cell></row><row><cell cols="2">English Snips</cell><cell>71.27 67.73 67.21</cell><cell>68.73</cell><cell>42.08 39.36 35.58</cell><cell>39.01</cell></row><row><cell></cell><cell>Google</cell><cell>68.78 37.90 36.74</cell><cell>47.81</cell><cell>58.82 28.85 27.21</cell><cell>38.29</cell></row><row><cell>French</cell><cell>Snips</cell><cell>78.20 74.14 73.06</cell><cell>75.13</cell><cell>57.49 53.56 53.89</cell><cell>54.98</cell></row><row><cell></cell><cell>Google</cell><cell>61.04 33.51 32.38</cell><cell>42.31</cell><cell>36.24 15.83 13.47</cell><cell>21.85</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="33">33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://research.snips.ai/datasets/spoken-language-understanding 2 https://github.com/snipsco/snips-nlu</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A generalized composition algorithm for weighted finite-state transducers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Schalkwyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Filters for efficient composition of weighted finitestate transducers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Schalkwyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Implementation and Application of Automata</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="28" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Class-based n-gram models of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter F Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent J Della</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenifer C</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Snips voice platform: an embedded spoken language understanding system for private-by-design voice interfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Coucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaa</forename><surname>Saade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Th?odore</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Caulier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Doumouro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Gisselbrecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Caltagirone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10190</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">How to add word classes to the kaldi speech recognition toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Horndasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Kaufhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elmar</forename><surname>N?th</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Text, Speech, and Dialogue</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="486" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Estimation of probabilities from sparse data for the language model component of a speech recognizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slava</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on acoustics, speech, and signal processing</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="400" to="401" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando Cn</forename><surname>Pereira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Personalized speech recognition on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Mcgraw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raziel</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Montse</forename><surname>Gonzalez Arenas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanishka</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Rybach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ouais</forename><surname>Alsharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ha?im</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gruenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?oise</forename><surname>Beaufays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5955" to="5959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Using recurrent neural networks for slot filling in spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gr?goire</forename><surname>Mesnil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokhan</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="530" to="539" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Weighted finite-state transducers in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="88" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Librispeech: an asr corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassil</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The kaldi speech recognition toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnab</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nagendra</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirko</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanmin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 2011 workshop on automatic speech recognition and understanding, number CONF. IEEE Signal Processing Society</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Purely sequence-trained neural networks for asr based on latticefree mmi</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijayaditya</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Galvez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pegah</forename><surname>Ghahremani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vimal</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2751" to="2755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The emergence of edge computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahadev</forename><surname>Satyanarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="39" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Discriminative models for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ninth International Conference on Spoken Language Processing</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Is word error rate a good indicator for spoken language understanding accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye-Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Workshop on Automatic Speech Recognition and Understanding</title>
		<imprint>
			<biblScope unit="issue">03EX721</biblScope>
			<biblScope unit="page" from="577" to="582" />
			<date type="published" when="2003" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03209</idno>
		<title level="m">Speech commands: A dataset for limited-vocabulary speech recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Achieving human parity in conversational speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasha</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.05256</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Minimum bayes risk decoding and system combination based on a recursion for edit distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haihua</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidia</forename><surname>Mangu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="802" to="828" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
