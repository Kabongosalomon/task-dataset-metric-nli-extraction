<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 1 Distantly-Supervised Long-Tailed Relation Extraction Using Constraint Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianming</forename><surname>Liang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Gaurav</forename><surname>Sharma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maozu</forename><surname>Guo</surname></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 1 Distantly-Supervised Long-Tailed Relation Extraction Using Constraint Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Relation Extraction</term>
					<term>Distant Supervision</term>
					<term>Multi-instance Learning</term>
					<term>Label Noise</term>
					<term>Long Tail</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Label noise and long-tailed distributions are two major challenges in distantly supervised relation extraction. Recent studies have shown great progress on denoising, but paid little attention to the problem of long-tailed relations. In this paper, we introduce a constraint graph to model the dependencies between relation labels. On top of that, we further propose a novel constraint graph-based relation extraction framework(CGRE) to handle the two challenges simultaneously. CGRE employs graph convolution networks to propagate information from data-rich relation nodes to data-poor relation nodes, and thus boosts the representation learning of long-tailed relations. To further improve the noise immunity, a constraint-aware attention module is designed in CGRE to integrate the constraint information. Extensive experimental results indicate that CGRE achieves significant improvements over the previous methods for both denoising and long-tailed relation extraction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>R ELATION extraction (RE), which aims to extract the semantic relations between two entities from unstructured text, is crucial for many natural language processing applications, such as knowledge graph completion <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, search engines <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> and question answering <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. Although conventional supervised approaches have been extensively researched, they are still limited by the scarcity of manually annotated data. Distantly supervised relation extraction (DSRE) <ref type="bibr" target="#b6">[7]</ref> is one of the most promising techniques to address this problem, because it can automatically generate large scale labeled data by aligning the entity pairs between text and knowledge bases (KBs). However, distant supervision suffers from two major challenges when used for RE.</p><p>The first challenge in DSRE is label noise, which is caused by the distant supervision assumption: if one entity pair has a relationship in existing KBs, then all sentences mentioning the entity pair express this relation. For example, due to the relational triple (Bill Gates, Founded, Microsoft), distant supervision will generate a noisy label Founded for the sentence "Bill Gates speaks at a conference held by Microsoft", although this sentence does not mention this relation at all.</p><p>In recent years, many efforts have been devoted to improving the robustness of RE models against label noise. The combination of multi-instance learning and attention mechanism is one of the most popular strategies to reduce the influence of label noise <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. This strategy extracts relations of entity pairs from sentence bags, with the objective of alleviating the sentence-level label noise. In addition, some novel strategies, such as reinforcement learning <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, adversarial training <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> and deep clustering <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> also show great potential for DSRE. However, these approaches are driven totally by the noisy labeling data, which may misguide the optimization of parameters and further hurt the reliability of models.</p><p>To address this problem, some researchers attempt to enrich the background knowledge of models by integrating external information. In general, the external information, e.g., entity descriptions <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, entity types <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref> and knowledge graphs <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b23">[24]</ref>, will be encoded as vector form, and then integrated to DSRE models by simple concatenation or attention mechanism. Compared with the above implicit knowledge, constraint rules are explicit and direct information that can effectively enhance the discernment of models in noisy instances. For example, the relation for the sentence "Bill Gates was 19 when he and Paul Allen started Microsoft" can not be Child of, since its head/tail entity must be a Person, while Microsoft is an Organization. Here, we define (Person, Child of, Person) as a constraint for Child of. However, directly removing the constraint-violating sentences from a dataset would result in loss of significant useful information (as demonstrated in Sec. 3.6.2). Hence, we explore a soft way to integrate the constraint information by attention mechanism in this paper.</p><p>The second challenge in DSRE is long-tailed relation extraction, however, which tends to be neglected as compared with the noisy labeling problem. In fact, real-world datasets of distant supervision always have a skewed distribution with a long tail, i.e., a small proportion of relations (a.k.a head relation) occupy most of the data, while most relations (a.k.a long-tailed relation) merely have a few training instances. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, more than 60% arXiv:2105.11225v4 [cs.CL] 17 May 2022 0 10 20 30 <ref type="bibr">40 50</ref> Sorted relation index relations are long-tailed with fewer than 100 instances in the popular New York Times (FB-NYT) dataset <ref type="bibr" target="#b24">[25]</ref>. Longtailed relation extraction is important for knowledge graph construction and completion. Unfortunately, even the stateof-the-art DSRE models are not able to handle long-tailed relations well. Hence, how to train a balanced relation extractor from unbalanced data becomes a serious problem in relation extraction. In general, long-tailed classfication tasks are addressed by re-sampling or re-weighting, nevertheless, long-tailed RE is special because the relation labels are semantically related rather than independent of each other. For example, if relation /location/country/capital holds, another relation /location/location/contains will holds as well. Therefore, a promising strategy for long-tailed RE is to mine latent relationships between relations by modeling the dependency paths. Once the dependency paths have been constructed, the relation labels are no longer independent of each other, and rich information can be propagated among the relations through these paths, which is crucial for the data-poor long-tailed relations. To accomplish this, <ref type="bibr" target="#b25">[26]</ref> proposed the relation hierarchical tree, which connect different relation labels according to hierarchical information in the relation names. For instance, relation /people/person/nationality and relation /people/person/religion have the same parent node /people/person in the relation hierarchical tree. On account of its effectiveness, most of the previous long-tailed RE models <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref> rely on the relation hierarchical tree. However, relation hierarchical trees suffer from several limitations:</p><p>(1) the construction of the relation hierarchical trees requires the relation names in hierarchical format, which conflicts with many existing RE datasets, such as SemEval-2010 Task8 <ref type="bibr" target="#b29">[30]</ref>, TACRED <ref type="bibr" target="#b30">[31]</ref>, and FewRel <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>;</p><p>(2) the sparsity of the hierarchy tree hinders the representation learning of some extremely long-tailed relations;</p><p>(3) the optimization of parameters is purely driven by the noisy training data, lacking enough effective supervision signals. Specifically, the key goal of selective attention mechanism is to automatically recognize the noisy sentences and then reduce their weights, but this is not guaranteed. Although pre-trained knowledge graph embeddings were utilized by <ref type="bibr" target="#b26">[27]</ref> to guide the learning of attention mechanism, they were simply used as initial embeddings of the relation labels.</p><p>To overcome the limitations of relation hierarchies, we explore the feasibility of utilizing another structure, constraint graph, to represent the intrinsic connection between relations. As shown in <ref type="figure">Figure 2</ref>, a constraint graph is a bipartite digraph <ref type="bibr" target="#b33">[34]</ref>, in which each directed edge connects a relation node to a type node or vice versa. As compared with entity-level knowledge graphs, constraint graphs provide more basic and general knowledge, more direct rule constraints, and a smaller vector space for knowledge mining and reasoning. Moreover, most knowledge bases (e.g. Freebase) preserve the entity type constraint information <ref type="bibr" target="#b34">[35]</ref>, which can be directly used for constraint graph construction. Even for datasets without relevant information, the constraint graph can be easily constructed according to the definition of each relation or the co-occurrence frequency of each relation with entity type pairs in the training set.</p><p>In our observations, there are at least three kinds of information in constraint graphs that are beneficial to DSRE: (1) type information. As shown in many previous works <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, entity types provide meaningful information for models to understand the entities, and play a crucial role in relation extraction; (2) constraint information. For example, it is obvious that the types of head and tail entities for relation /people/person/children are both Person. Such a constraint provides direct and effective prior information for DSRE models to recognize the noisy instances; (3) interactive information. As showed in <ref type="figure">Figure 2</ref>, relation nodes are connected indirectly through entity type nodes. Similar to the relation hierarchy-based methods, we can utilize message passing between nodes to transfer rich knowledge from head relations to long-tailed relations.</p><p>Based on the aforementioned motivations, we propose a novel constraint graph-based relation extraction framework (CGRE). Our framework consists of three main components: a sentence encoder used for encoding the corpus information, a graph encoder used for encoding the con-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classifier</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Entity Types Relations</head><p>Sentence 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence 2</head><p>Sentence 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Constraint-Aware Attention Module</head><p>Bag Output</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Constraint Graph</head><p>Graph Encoder ? 2 <ref type="figure" target="#fig_3">Fig. 3</ref>. Overview of the proposed constraint graph-based framework. The sentence encoder aims to extract the representations for the sentences in a bag, while the graph encoder aims to extract the representations for the relations and the entity types from the constraint graph. In the constraint-aware attention module, selective attention is applied after concatenation operations to aggregate the three representations into a bag representation, based on which the classifier predicts the relations mentioned in the sentence bag. straint graph information, and an attention module used for integrating different information from the two separate encoders. Specifically, we adopt graph convolution networks(GCNs) <ref type="bibr" target="#b38">[39]</ref> as the graph encoder to extract interactive information from the constraint graph. Intuitively, the neighborhood integration mechanism of GCNs can efficiently improve the representation learning of long-tailed relations by propagating rich information from popular nodes to rare nodes. In addition, different from the plain selective attention <ref type="bibr" target="#b7">[8]</ref>, which is mainly dependent on the semantic similarity between sentences and relation labels, our constraint-aware attention combines both the semantic information and constraint information. In our approach, the attention score of each instance depends on not only its semantic similarity with the relation representations, but also its compatibility with the entity type constraints. We summarize our main contributions as follows:</p><p>? We explore a novel knowledge structure, constraint graph, to model the intrinsic dependencies between relation labels. Compared with the popular relation hierarchical trees, our constraint graph shows better universality, directness and effectiveness. We propose CGRE, a constraint graph-based DSRE framework, to simultaneously address the challenges of label noise and long-tailed distributions. <ref type="bibr">?</ref> We conduct large-scale experiments for performance comparison and ablation studies to demonstrate that CGRE is highly effective for both denoising and long-tailed RE. ? We make our pre-processed datasets and source code publicly available at https://github.com/tmliang/CGRE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">FRAMEWORK</head><p>Starting with notations and definitions, we will introduce the construction process for a raw constraint graph, and then detail each component of our framework. As illustrated in <ref type="figure" target="#fig_3">Figure 3</ref>, our framework consists of three key modules:</p><p>? Sentence Encoder. Given a sentence with two mentioned entities, the sentence encoder is adopted to derive a sentence representation. ? Graph Encoder. Given a raw constraint graph G, we first transform it into an embedding matrix and then apply the graph encoder to extract the representations of relations and entity types. ? Constraint-Aware Attention. By combining outputs from the two seperate encoders, this module allocates an attention score for each instance in a bag, and then computes the bag representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Notations and Definitions</head><p>We define a constraint graph as a triple G = {T , R, C}, where T , R, C indicate the sets of entity types, relations, and constraints, respectively. Each constraint (t e1 r , r, t e2 r ) ? C indicates that for relation r, the type of head entity can be t e1 r ? T and the type of tail entity can be t e2 r ? T . Given a bag of sentences B = {s 1 , . . . , s ns } and a corresponding entity pair (e 1 , e 2 ), the objective of distantly supervised relation extraction is to predict the relation r i for the entity pair (e 1 , e 2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Constraint Graph Construction</head><p>Most knowledge bases provide the entity type constraint information. For example, in Freebase, these constraints are located in rdf-schema#domain and rdf-schema#range fields. However, the original constraints always involve thousands of entity types. To avoid over-parameterization, we merely use 18 coarse entity types 1 defined in OntoNotes 5.0 <ref type="bibr" target="#b39">[40]</ref>, and then remove the constraints containing unrelated types. Finally, we can obtain a raw constraint graph G that consists 1. We choose the 18 entity types of OntoNotes for two main reasons: of a relation set R, a type set T , and a constraint set C. Note that we use a special type Others for the entities that do not belong to the 18 types, thus the size of T is 19. Specially, the negative category NA (i.e., no relations between the two entities) is connected to all types, since its head/tail entities could belong to any type. We emphasize that the constraint graph is specific to an entire dataset, rather than a bag in the dataset. That means one public constraint graph is shared by all the bags in the dataset. Furthermore, compared with the commonly used knowledge graph, the scale of a constraint graph is rather tiny. For example, there are merely 72 nodes (which is the sum of relation number and type number) and 164 edges in the constraint graph for the FB-NYT dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Sentence Encoder</head><p>To encode the sentence information, we first adopt entityaware word embeddings <ref type="bibr" target="#b40">[41]</ref> to represent each word in a sentence, and then apply the Piecewise Convolutional Neural Network (PCNN) <ref type="bibr" target="#b41">[42]</ref> to derive the sentence representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Input Layer</head><p>The input layer aims to maps words into a distributed embedding space to capture their semantic and syntactic information. Given a sentence s = {w 1 , . . . , w l }, we transform each word w i into a d w -dimensional vector w i by a pre-trained embedding matrix. Then following <ref type="bibr" target="#b40">[41]</ref>, we represent the target entities e 1 and e 2 by their word vectors w e1 and w e2 . To incorporate the position information, we use two d p -dimensional vectors p e1 i and p e2 i to embed the relative distances between w i and the target entities, as used in <ref type="bibr" target="#b41">[42]</ref>. By concatenating, two types of word embeddings can be obtained as follows:</p><formula xml:id="formula_0">x p i = [w i ; p e1 i ; p e2 i ] ? R dw+2dp , x e i = [w i ; w e1 ; w e2 ] ? R 3dw ,<label>(1)</label></formula><p>where d w and d p are both pre-defined hyper-parameters. Finally, we apply the entity-aware word embedding to represent each word w i as follows:</p><p>A e = sigmoid (? ? (W e X e + b e )) ,</p><formula xml:id="formula_1">X p = tanh (W p X p + b p ) , X = A e X e + (1 ? A e ) X p ,<label>(2)</label></formula><p>where X p = {x p 1 , . . . , x p l }, X e = {x e 1 , . . . , x e l }, denotes element-wise product, W e and W p are weight matrixes, b e and b p are bias vectors, and ? is a smoothing coefficient hyper-parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Encoding Layer</head><p>The encoding layer aims to extract a high-dimensional representation from the input sequence. In consideration of simplicity and effectiveness, we employ PCNN <ref type="bibr" target="#b41">[42]</ref> as our feature extractor. Given an input sequence X = {x 1 , . . . , x l }, PCNN slides the convolutional kernels W k over X to capture the hidden representations as follows:</p><formula xml:id="formula_2">h i = W k x i?w+1:i ? R l 1 ? i ? m,<label>(3)</label></formula><p>where x i:j denotes the concatenating of x i to x j , and m is the number of kernels. Then, PCNN performs piecewise max-pooling over the hidden representations as follows:</p><formula xml:id="formula_3">q (1) i = max 1?j?l1 (h ij ) q (2) i = max l1+1?j?l2 (h ij ) 1 ? i ? m, q (3) i = max l2+1?j?l (h ij )<label>(4)</label></formula><p>where l 1 and l 2 are positions of the two target entities respectively. Then we can obtain the pooling result q i = {q</p><formula xml:id="formula_4">(1) i ; q (2) i , q<label>(3)</label></formula><p>i } of the i-th convolutional kernel. Finally, we concatenate all the pooling results q 1:m and then apply a non-linear function to produce the sentence representation s as follows:</p><formula xml:id="formula_5">s = ? (q 1:m ) ? R 3m ,<label>(5)</label></formula><p>where ?(?) is an activation function (e.g. RELU).</p><p>Through the above-mentioned sentence encoder, we can achieve the vector representation for each sentence in a bag.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Graph Encoder</head><p>To encode the information of the constraint graph, we first transform the raw graph into vector representations via the input layer, and then run GCNs over the input vectors to extract the interactive features of the nodes. Finally, the representations of entity types and relations can be obtained by dividing the node representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Input Layer</head><p>Given a raw constraint graph G = {T , R, C}, we denote the node set as V = T ? R. For each constraint (t e1 r , r, t e2 r ) ? C, we add (t e1 r , r) and (r, t e2 r ) into the edge set E. Then with the edge set, the adjacency matrix? ? R n?n (n = |V|) is defined as:</p><formula xml:id="formula_6">? ij = 1 if (v i , v j ) ? E, 0 otherwise.<label>(6)</label></formula><p>For each node v i ? V, we randomly initialize a d vdimensional embedding v (0)</p><p>i . Through the aforesaid process, the raw constraint graph is transformed into an embedding matrix</p><formula xml:id="formula_7">V (0) = {v (0) 1 , . . . , v (0)</formula><p>n } with an adjacency matrix?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Encoding Layer</head><p>In this study, we select a two-layer GCN to encode the graph information. GCNs are neural networks that operate directly on graph structures <ref type="bibr" target="#b38">[39]</ref>. With the neighborhood integration mechanism, GCNs can effectively promote information propagation in the graph. To aggregate the information of the node itself <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b42">[43]</ref>, we add the self-loops into E, which means? ii = 1. With the node embeddings V (0) and the adjacency matrix? as inputs, we apply GCNs to extract high-dimensional representations of nodes. The computation for node v i at the k-th layer in GCNs can be defined as:</p><formula xml:id="formula_8">v (k) i = ? ? ? n j=1? ij W (k) v (k?1) j + b (k) ? ? ,<label>(7)</label></formula><p>where W (k) and b (k) are respectively the weight matrix and the bias vector of the k-th layer, and ?(?) is an activation function (e.g. RELU). Finally, according to the category of each node, we divide the output representations V (2) ? R n?dn into relation representations R ? R nr?dn and type representations T ? R nt?dn .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Constraint-Aware Attention Module</head><p>Given a bag of sentences B = {s 1 , . . . , s ns } and a raw constraint graph G, we achieved the sentence representations S = {s 1 , . . . , s ns }, the relation representations R = {r 1 , . . . , r nr }, and the type representations T = {t 1 , . . . , t nt } by the two aforementioned encoders, where n s , n r , and n t are the numbers of sentences, relations, and types, respectively. To aggregate the information of the instances and the constraints, we first construct the instance representations and the constraint representations by concatenation operations, and then apply the selective attention over these to compute the final bag output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1">Instance Representation</head><p>For each sentence s i , we use NER tools 2 to recognize the types of its target entities. For the unrecognized entities, we assign the special type Others. After NER, the type representations t e1 and t e2 can be obtained by looking up in the type representation matrix T. Finally, the instance representation is achieved by concatenating the the representations of the sentence and its entity type representations as follows:</p><formula xml:id="formula_9">g i = [s i ; t e1 si ; t e2 si ] ? R 3d h 1 ? i ? n s .<label>(8)</label></formula><p>For example, given a sentence "Bill Gates was 19 when he and Paul Allen started Microsoft", we firstly use NER tools to recognize the types of the head entity Bill Gates and the tail entity Microsoft as Person and Organization respectively. Then the instance representation is achieved as g = [s; t per ; t org ], where s is the sentence representation, t per is the representation of Person, and t org is the representation of Organization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.2">Constraint Representation</head><p>Assume t e1 ri and t e2 ri are respectively the immediate predecessor and the immediate successor of relation r i in the constraint graph. Similarly, we can obtain the type representations t e1 ri and t e2 ri of r i by looking up in T. Note that some relations may have multiple immediate predecessors or successors, e.g. the tail entity type of /location/location/contains can be GPE (which denotes countries, cities, and states) or LOC (which denotes non-GPE locations such as mountains and rivers). For these cases, we take the average over the immediate predecessors or the immediate successors. Finally, the constraint representation is achieved by concatenating the representations of the relation and its entity type representations as</p><formula xml:id="formula_10">c i = [r i ; t e1 ri ; t e2 ri ] ? R 3d h 1 ? i ? n r .<label>(9)</label></formula><p>For example, assume that Person and Organization are respectively the unique immediate predecessor and the 2. To balance the efficiency and the accuracy, we select Flair <ref type="bibr" target="#b43">[44]</ref> as our NER tool. unique immediate successor of /business/person/company in the constraint graph, then the constraint representation of /business/person/company is achieved as c = [r; t per ; t org ], where r is the representation of /business/person/company, t per is the representation of Person, and t org is the representation of Organization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.3">Attention Layer</head><p>Different from the previous selective attention mechanisms that mainly depend on the semantic similarity between sentences and relations <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b44">[45]</ref>, our attention mechanism combines semantic information and constraint information to calculate the bag output. Intuitively, the similarity between the former parts (i.e., s and r) measures the semantic matching between the sentence s and the relation r, while that between the latter parts (i.e., [t e1 s ; t e2 s ] and [t e1 r ; t e2 r ]) measures agreement of the instance with the constraint of r. Given a bag of instances, the attention weight ? i of i-th instance for the corresponding relation r can be computed as follows:</p><formula xml:id="formula_11">e i = g i c r , ? i = exp (e i ) ns j=1 exp (e j ) ,<label>(10)</label></formula><p>where c r is the constraint representation of r. Then the bag representation is derived as the weighted sum of the sentence representations:</p><formula xml:id="formula_12">z r = ns i=1 ? i g i .<label>(11)</label></formula><p>Finally, we feed the bag representation z r into a softmax classifier to calculate the probability distribution over relation labels as follows:</p><formula xml:id="formula_13">P (r | B; G; ?) = softmax (Wz r + b) ,<label>(12)</label></formula><p>where ? is the set of model parameters, W is the weight of the classifier and b is the bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Optimization</head><p>We define the objective function using cross-entropy at the bag level as follows:</p><formula xml:id="formula_14">J(?) = ? 1 n n i=1 log P (r i | B i ; G; ?) ,<label>(13)</label></formula><p>where n is the number of bags and r i is the label of B i . Note that at the test stage, the ground-truth label r is unknown, thus all constraint representations are applied to calculate the posterior probabilities for the corresponding relation, and the relation with the highest probability is the prediction result <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>We use two popular benchmark datasets for our primary experiments:</p><p>? FB-NYT <ref type="bibr" target="#b24">[25]</ref> is generated by aligning Freebase facts with a New York Times corpus. As described in <ref type="bibr" target="#b45">[46]</ref> and <ref type="bibr" target="#b46">[47]</ref>, FB-NYT has been released in two main versions: the filtered version NYT-520K and the non-filtered version NYT-570K. The two versions are identical, except the training set of NYT-520K does not share any entity pairs with the test set. NYT-570K has been widely used in previous studies <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b25">[26]</ref>, while NYT-520K provides a more accurate evaluation for model's in-depth comprehension of the relation semantics rather than superficial memory of the KB facts during training <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b47">[48]</ref>. Therefore, we experiment with both the two versions of FB-NYT. ? GDS <ref type="bibr" target="#b48">[49]</ref> is constructed from human-annotated Google Relation Extraction corpus with additional instances from Internet documents, guaranteeing that each bag contains at least one sentence that expresses the bag label. The guarantee enhances the reliability for bag-level automatic evaluation, hence GDS has become a popular benchmark in recent DSRE studies based on multi-instance learning <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b49">[50]</ref>. We present the overall statistics for NYT-520K, NYT-570K and GDS in <ref type="table" target="#tab_1">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hyper-parameters Settings</head><p>For a fair comparison, most of the hyper-parameters are set identical to those in <ref type="bibr" target="#b7">[8]</ref>, and we mainly tune the hyperparameters of the graph encoder and the classifier. The word embeddings are initialized by the pre-trained word2vec released by OpenNRE 3 . All weight matrixes and position embeddings are initialized by Xavier initialization <ref type="bibr" target="#b50">[51]</ref>, and the bias vectors are all initialized to 0. To prevent overfitting, we apply dropout <ref type="bibr" target="#b51">[52]</ref> before the classifier layer. Table 2 lists all the hyper-parameters used in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Denoising Evaluation</head><p>To demonstrate the denoising performance of CGRE, we compare against five competitive DSRE models:  Note that the first four models (i.e., PCNN+ATT 3 , PCNN+HATT 4 , PCNN+BATT 5 and RESIDE 6 ) were originally applied on NYT-570K. Hence, their results on NYT-570K are extracted from the respective publications. For DSRE-VAE 7 , results on both NYT-520K and NYT-570K are reported in the original publication. Other results were obtained using the official source codes. We do not test DSRE-VAE (+KB) on the GDS dataset, since the KB of GDS is unavailable.</p><formula xml:id="formula_15">? PCNN+ATT [</formula><p>To further evaluate the effectiveness of entity-aware word embedding proposed by <ref type="bibr" target="#b40">[41]</ref>, we additionally build PCNN+ATT+ENT as a baseline by simply replacing the input layer of PCNN+ATT with the entity-aware word embedding layer (as described in Sec. 2.3.1).</p><p>Following the previous works <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b25">[26]</ref>, we adopt precision-recall (PR) curves shown in <ref type="figure">Figure 4</ref> to measure the overall performance of DSRE models in a noisy environment. For a ready comparison, we report the accuracy of top-N predictions (P@N) and the area under curve (AUC) of the PR curves in <ref type="table" target="#tab_4">Table 3</ref>.</p><p>Our observations on the denoising results shown in <ref type="figure">Figure 4</ref> and <ref type="table" target="#tab_4">Table 3</ref> are summarized as follows:</p><p>(1) CGRE vs. PCNN+ATTs. As a variant of PCNN+ATT, CGRE improves the performance of vanilla PCNN+ATT by a large margin. As observed in <ref type="table" target="#tab_4">Table 3</ref>, CGRE outperforms other PCNN+ATT variants, i.e. PCNN+HATT and PCNN+BATT, by at least 4.6%, 9.9% and 8.9% on NYT-520K, NYT-570K and GDS, respectively;</p><p>(2) CGRE vs. RESIDE. CGRE shows high efficiency and sufficiency in utilizing type information. Compared with RESIDE which uses 38 entity types 8 , CGRE merely uses 18 types but achieves AUC improvements of 6.2%, 10.4% and 2.7% on NYT-520K, NYT-570K and GDS, respectively. Furthermore, <ref type="figure">Figure 4</ref>(c) and  able improvements on GDS (whose training data is less than 3% of FB-NYT's);</p><p>(3) CGRE vs. DSRE-VAEs. On NYT-520K, CGRE outperforms the vanilla DSRE-VAE by 3.1% in AUC values, but underperforms DSRE-VAE (+KB) by 1.2%. We believe the performance gap between CGRE and DSRE-VAE (+KB) arises from the different external information they use. Seemingly, the KB priors (knowledge graph embeddings) in DSRE-VAE (+KB) contribute higher performance improvements than the constraint graph in CGRE. However, note that the scale of the former is several orders of magnitude larger than that of the latter. For the case of NYT-520K, the knowledge graph used by DSRE-VAE (+KB) contains 3,065,045 nodes and 24,717,676 edges, while the constraint graph used by CGRE merely contains 72 nodes and 164 edges (as mentioned in Sec 2.2). Nevertheless, CGRE can still achieve performance comparable with DSRE-VAE (+KB) on NYT-520K, and on NYT-570K, CGRE outperforms DSRE-VAE (+KB) by a large margin. These comparison results demonstrate the effectiveness and promise of our proposed Constraint Graph and CGRE. In view of the performance advantage of DSRE-VAE (+KB), integrating the KB priors into CGRE for further improvements is worthy of exploration in future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Long Tail Evaluation</head><p>To fully demonstrate the effectiveness of CGRE on long-tail RE, we compare CGRE with several competitive baselines, including two vanilla DSRE models PCNN+ATT <ref type="bibr" target="#b7">[8]</ref> and PCNN+ATT+ENT that ignore the long-tailed relations, and six state-of-the-art long-tailed RE models:</p><p>? PCNN+HATT <ref type="bibr" target="#b25">[26]</ref>: the first model to apply relation hierarchies for long-tailed RE; ? PCNN+KATT <ref type="bibr" target="#b26">[27]</ref>: a model that utilizes GCNs to encode relation hierarchies and applies knowledge graph embeddings for initialization; ? PA-TRP <ref type="bibr" target="#b27">[28]</ref>: a model that learns relation prototypes from unlabeled texts for embedding relation hierarchies; ? CoRA <ref type="bibr" target="#b28">[29]</ref>: a model that uses sentence embeddings to generate the representations for each node in relation hierarchies; ? ToHRE <ref type="bibr" target="#b53">[54]</ref>: a model that applies a top-down classification strategy to explore relation hierarchies; ? DPEN <ref type="bibr" target="#b54">[55]</ref>: a model that aggregates relations and entity types by stages to dynamically enhance the classifier. Note that the first five models are all based on the relation hierarchy.</p><p>Following the previous studies <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, we adopt Hit@K metric on NYT-570K to evaluate the performance on long-tailed relations. We extract the relations that have few than 100/200 training instances from the test set. Then Hits@K metric, which measures the probability that the true label falls in the top-K recommendations of the model, is applied over these long-tailed relations. From the results in <ref type="table" target="#tab_6">Table 4</ref>, we observe:</p><p>(1) effectiveness of modeling latent dependencies among relations. PCNN+ATT and PCNN+ATT+ENT give the worst results, showing the ineffectiveness of vanilla denoising models on long-tailed relations. By modeling the latent dependencies among relations, the performance improves significantly, but is still far from satisfactory. (2) advantages of the constraint graph over the relation hierarchical tree. Based on the constraint graph, CGRE significantly and consistently outperforms all the previous relation hierarchy-based models. This confirms the advantages of the constraint graph for modeling dependencies among relations, which could be further amplified via the GCN.</p><p>(3) superiority of CGRE over DPEN in utilizing type information. As compared with DPEN, which also utilizes entity types to transfer information across relations, CGRE achieves significant and consistent performance superiority for long-tailed RE. We believe this performance gap stems mainly from the distinction in how the methods model and utilize type information. DPEN models the connections between entity types and relations implicitly, while CGRE makes these explicit via the constraint graph. Also, DPEN aggregates the features of entity types and relations by stages with a specific dynamic parameter generator, while CGRE adopts GCNs to achieve this objective directly.</p><p>As described in the aforementioned observations, longtailed RE is still an intractable challenge even for the stateof-art DSRE models. A feasible solution is modeling intrinsic dependencies between the relations to transfer information from data-rich relations to data-poor relations, and the design of relation dependency structure is one of the key to modeling. With outstanding performance, the relation hierarchy seems to be the indispensable foundation of longtailed RE frameworks <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b53">[54]</ref>, however, it is not the only option. In this work, we confirm the constraint graph is also a novel and effective structure for modeling latent dependencies between relations. Further, we believe there are still many promising relation dependency structures deserving extensive exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Quantitative Analysis of Selective Attention Mechanisms</head><p>In this section, we quantitatively evaluate the effects of selective attention mechanism on DSRE, to demonstrate the performance of our designed constraint-aware attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">Performance of Selective Attention Mechanisms</head><p>We conduct top-N evaluations on the bags containing different number of sentences, to compare the performance of different selective attention mechanisms. Following the previous works <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b25">[26]</ref>, we randomly select one / two / all sentences for each test entity pair with more than two sentences to construct three new test sets, on which we calculate P@N for these methods.</p><p>We report the results on NYT-520K and NYT-570K in <ref type="table" target="#tab_6">Table 4</ref> and 5. For the bags of size one, these selective attention mechanisms are ineffective, since they operate at the sentence level. As the bag size increases, selective attention mechanisms provide increasing performance improvements of bag-level DSRE. Across different bag sizes, CGRE achieves performance comparable with DSRE-VAE (+KB), and significantly outperforms other selective attentionbased models. These results demonstrate the effectiveness of our constraint-aware attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">Evaluation with Different Noise Ratios</head><p>We further quantitatively evaluate the performance of the proposed constraint-aware attention mechanism on bags with different noise ratios, using the analytical framework proposed by <ref type="bibr" target="#b55">[56]</ref>.</p><p>This analytical framework requires label-clean test data for quantitative evaluation. In this work, instead of creating a new dataset from scratch, we experiment with the existing NYT-H dataset because it provides human-annotated test data. NYT-H consists of a training set containing 96,831 instances (14,012 entity pairs) and a test set containing 9,955 instances (3,548 entity pairs), involving 21 relations (excluding NA).</p><p>In this experiment, we adopt two evaluation metrics as follows:</p><p>? Attention Accuracy (AAcc). The key goal of the selective attention mechanism is to dynamically assign higher weights to valid sentences and lower weights to noisy sentences, and AAcc is designed for measuring this ability. Given a bag B i = {s i1 , . . . , s ins } containing both valid and noisy sentences and the attention weights {? i1 , . . . , ? ins } over the sentences, AAcc of this bag is formally defined as follows:</p><formula xml:id="formula_16">AAcc i = ns j=1 ns k=1 I (z ij ) I (1 ? z ik ) I (? ij &gt; ? ik ) ns j=1 I (z ij ) ns j=1 I (1 ? z ij ) where I(?)</formula><p>is an indicator function and z ij is a flag indicating whether the sentence s ij is valid. The numerator counts how many valid-noisy sentence pairs show higher attention weight on the valid sentence, and the denominator is the number of valid-noisy sentence pairs is the bag B i . Then AAcc of the whole test set is computed as</p><formula xml:id="formula_17">AAcc = n i=1</formula><p>AAcci n , where n is total number of bags. ? F1-Score. We adopt the F1-Score instead of AUC (which is used in <ref type="bibr" target="#b55">[56]</ref>) as the evaluation metric, since the test labels are human-annotated. Firstly, we calculate F1-Score on the whole test set as a baseline. Then we construct three test sets with zero / one / all valid sentences in each bag. Finally, we calculate F1-Scores on these three datasets. We emphasize that for the case of zero, a prediction equaling the bag relation is incorrect, since there are no sentences expressing the relation in the bag. We present the results in <ref type="table" target="#tab_9">Table 7</ref>, in which CGRE is renamed PCNN+CATT (constraint-aware attention), to  <ref type="bibr" target="#b21">[22]</ref> 68.0 67. <ref type="bibr" target="#b4">5</ref>    highlight the attention mechanisms used. From <ref type="table" target="#tab_9">Table 7</ref>, we observe that:</p><p>(1) PCNN+CATT achieves 4.2% AAcc improvements over PCNN+ATT, demonstrating that the stronger ability of CATT over ATT to recognize the noisy sentences and to dynamically reduce their attention weights.</p><p>(2) PCNN+CATT achieves 5.5%, 7.2%, 6.1% and 4.5% F1-Score improvements over PCNN+ATT, in the settings of Orig., Zero, One and All, respectively. Compared with the original selective attention mechanism <ref type="bibr" target="#b7">[8]</ref>, our designed constraint-aware attention mechanism shows stronger abilities to identify valid sentences in various noisy environments. Specially, all sentences in Zero are valid, so it is satisfactory that any sentence obtains higher attention weight. However, PCNN+CATT still achieve a significant advantage in Zero. The reason we speculate is that CATT tends to assign higher attention weights to the easily recognized valid sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Ablation Study</head><p>We perform ablation studies to further evaluate the effects and contributions of different components of CGRE. Note that all ablation experiments are conducted on NYT-520K, for a more accurate evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.1">Effect of Backbone Models</head><p>We evaluate the effects of different backbone models on the performance of CGRE. We experiment with three widely used sentence encoders CNN <ref type="bibr" target="#b56">[57]</ref>, PCNN <ref type="bibr" target="#b41">[42]</ref> and BERT <ref type="bibr" target="#b57">[58]</ref>, and three representative graph encoders GCN <ref type="bibr" target="#b38">[39]</ref>, GAT <ref type="bibr" target="#b58">[59]</ref> and SAGE <ref type="bibr" target="#b59">[60]</ref>. Compared with GCN, GAT dynamically assigns different attention weights to different neighbors in aggregation computation, while SAGE generates embeddings by sampling and aggregating neighboring features instead of training individual embeddings for each node. Note that the entity-aware word embedding <ref type="bibr" target="#b40">[41]</ref> is only applied in CNN and PCNN, since BERT has integrated word embeddings.</p><p>We report the experimental results in <ref type="table" target="#tab_10">Table 8</ref>, from which we observe that:</p><p>(1) compared with the baselines without the constraint graph, any combination achieves significant and consistent improvements on performance of denoising and long-tailed RE. This strongly suggests the effectiveness of the proposed constraint graph;</p><p>(2) among the sentence encoders, BERT shows huge advantages on overall performance against noise and top-5 hit rate on long-tailed relations. However, CNN and PCNN also show competitive performance on the top-10 hit rate;</p><p>(3) among the graph encoders, GAT achieves the highest anti-noise performance, independently of the sentence  <ref type="bibr" target="#b44">[45]</ref>, which achieves an AUC value of 42.2% on NYT-520K.</p><p>Although BERT and GAT show significant superiority in this ablation experiment, we still utilize PCNN and GCN as the sentence and graph encoders of vanilla CGRE, for their high efficiency and acceptable accuracy. We retain the choices of other backbone models as the variants of CGRE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.2">Effect of the Constraint Graph</head><p>As effectiveness of the constraint graph has been confirmed in Sec. 3.6.1, we further explore the effects of different information (i.e., type information and constraint information) in the constraint graph. We briefly denote the CGRE without the constraint graph as Base, which is equivalent to PCNN+ATT+ENT. To evaluate the effect of type information, we build Base+Type, which concatenates the sentence representations with the corresponding type representations, and then transforms the dimension of the concatenating representation to match that of the relation representation for attention computation. To evaluate the effect of constraint information, we build Base+Const, which is identical to Base, except that instances violating constraints are removed during training and predicted as NA with 100% probability during testing.</p><p>We present the results of AUC and macro accuracy of Hits@K in <ref type="table" target="#tab_11">Table 9</ref>, from which we observe that:</p><p>(1) both the external information of entity types and relation constraints provide significant improvements on denoising and long-tailed RE;</p><p>(2) improvements on long-tailed RE from type information and constraint information are comparable. However, type information shows significant advantages (with 2.4% AUC improvement over constraint information) on overall performance against noise;</p><p>(3) compared with directly integrating the type information or the hard constraint strategy, our proposed CGRE organically combines these two kinds of information to achieve significant improvements in terms of denoising and long-tailed RE.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.3">Effect of GCNs</head><p>To investigate the effect of the integration mechanism of GCNs on knowledge transfer between relations, we evaluate the long-tailed RE performance of GCNs with different layer numbers and different output options. We denote CGRE without GCN as w/o GCN, in which the input embeddings V (0) are directly output, and denote CGRE with a k-layer GCN as k-Layer. For a 2-layer GCN, we design 3 different output options:</p><p>? Opt-1 directly take the representations of last layer, i.e. V <ref type="bibr" target="#b1">(2)</ref> , for output; ? Opt-2 concatenates the representations of last 2 layers, i.e.</p><p>[V <ref type="bibr" target="#b0">(1)</ref> ; V (2) ], for output; ? Opt-3 concatenates the representations of last 3 layers, i.e.</p><p>[V (0) ; V (1) ; V <ref type="bibr" target="#b1">(2)</ref> ], for output. To ensure dimensional matching, the concatenated vectors are transformed by a linear layer before output.</p><p>From the results shown in <ref type="figure" target="#fig_4">Figure 5</ref>, we observe that:</p><p>(1) GCN-based variants consistently outperform CGRE w/o GCN. This confirms the intuition that the neighborhood integration mechanism of GCN could effectively propagate rich information from the head relations to the longtailed relations.</p><p>(2) due to over-smoothing, the increase of number of GCN layers may hurt the performance. As shown in <ref type="figure" target="#fig_4">Fig.  5(a)</ref>, the 2-layer GCN achieve the best performance in our experiments;</p><p>(3) <ref type="figure" target="#fig_4">Fig. 5(b)</ref> shows a huge advantage of Opt-2 on longtailed RE. We speculate the reason is that concatenation with the low-layer integrated representations could enhance the information propagation and feature learning, while concatenation with the non-integrated embeddings may impedes the knowledge transfer among relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.4">Effect of Entity-aware Word Embedding</head><p>The entity-aware word embedding proposed by <ref type="bibr" target="#b40">[41]</ref> shows huge advantages on NYT-570K, and has been applied in several recent works <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b53">[54]</ref>. In this study, we comprehensively evaluate the performance of this module. As shown in <ref type="figure">Fig. 4</ref> and <ref type="table" target="#tab_4">Table 3</ref>, its advantages on NYT-520K and GDS are not as huge as that on NYT-570K, which contains many overlapping entity pairs. This suggests that the overall improvements provided by entity-aware word embedding are mainly due to rote memorization of the entity pairs occurring in the training set. However, the positive effectiveness of the module should not be entirely denied, since it provides 0.8% and 5.0% AUC improvements on NYT-520K and GDS, respectively. Hence, we still retain this module in vanilla CGRE for its slight improvements and plug-and-play property.</p><p>We further investigate the effect of the smoothing coefficient ?, which balances the contribution of entity information and word information. We experiment with ? ? [0, 20], and present the AUC trend in <ref type="figure" target="#fig_5">Fig 6.</ref> It is clear that the selection of ? has a significant impact on the overall performance. We observe that the two peaks occur at ? = 3 and 17, while the trough occurs at ? = 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Case Study</head><p>We present some examples to demonstrate how our constraint-aware attention combines the semantic and constraint information. As shown in <ref type="table" target="#tab_1">Table 10</ref>, the incorrect sentences with constraint violation (s 3 and s 6 ) or semantic inconsistency (s 2 and s 5 ) are assigned lower scores, while the correct sentences (s 1 and s 4 ) score higher. With the constraint-aware attention mechanism, CGRE can effectively recognize the valid instances from noisy data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head><p>To automatically obtain large scale labeled training data, <ref type="bibr" target="#b6">[7]</ref> proposed distant supervision. However, the relation labels collected through distant supervision are not only noisy but also extremely unbalanced. Therefore, denoising and longtailed RE become two major challenges in DSRE. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Denoising in DSRE.</head><p>Multi-instance learning (MIL), aiming to extract relations of an entity pair from a sentence bag instead of a single sentence, is a popular approach to alleviate the noisy labeling problem <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b61">[62]</ref>. Under the MIL framework, various denoising methods are proposed, including selective attention mechanism <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>, external information integration <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b21">[22]</ref> and robust context encoders <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b44">[45]</ref>. Different from the bag-level prediction of MIL, some studies <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b62">[63]</ref> attempt to predict relations at sentence level by selecting trustable instances from training data. In this work, we mainly focus on the bag-level prediction. There are two main differences between CGRE and the above MIL methods: (1) rather than treat each relation label separately, CGRE attempts to mine the latent dependencies between relations; (2) the attention mechanism of CGRE integrates the prior constraint information, which is effective for improving the noise immunity of RE models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Long-tailed relation extraction in DSRE.</head><p>[64] develops an RE system, which can automatically learn the rules for long-tailed relations from Web. To generate fewer but more precise rules, <ref type="bibr" target="#b64">[65]</ref> applies explanation-based learning to improve the RE system. However, the above methods merely handle each relation in isolation, regardless of the implicit associations between relations. Therefore, some recent works <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b53">[54]</ref> attempt to mine the semantic dependencies between relations from the relation hierarchical tree. To achieve the same objective without the relation hierarchical tree, <ref type="bibr" target="#b54">[55]</ref> proposed DPEN 9 , utilizing entity types to implicitly transfer information across different relations. Although these works achieve significant improvement for long-tailed relations, they are still far from satisfactory.</p><p>Different from relation hierarchies that mainly contain the hierarchical information between labels, the proposed constraint graph involves not only the implicit connection information between labels, but also the explicit prior information of constraints. By integrating both kinds of in-9. Main differences between DPEN and CGRE are detailed in Sec. <ref type="bibr">3.4.</ref> formation, our CGRE can effectively handle the noisy and long-tailed labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Type constraint information in DSRE.</head><p>Type information of entities showed great potential on relation extraction <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>. Some works <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> explored the utilization of fine-grained entity type information to improve the robustness of DSRE models. However, these methods merely focus on the semantic information of types, i.e., they simply use type information as concatenating features to enrich the sentence semantic representation, but neglect the explicit constraint information, which can be effective for recognizing the noisy instances. On the contrary, <ref type="bibr" target="#b34">[35]</ref> applies probabilistic soft logic to encode the type constraint information for denoising, but ignores the semantic information of entity types.</p><p>As compared with the above methods, CGRE can take full advantage of type information. With the constraintaware attention mechanism, CGRE can combine the semantic and constraint information of entity types. Moreover, in CGRE, the entity types are used as bridges between the relations, enabling message passing between relation labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION AND FUTUTE WORK</head><p>In this paper, we aim to address both the challenges of label noise and long-tailed relations in DSRE. We introduce a novel relation dependency structure, constraint graph, to model the dependencies between relations, and then propose a novel relation extraction framework, CGRE, to integrate the information from the constraint graph. Experimental results on two popular benchmark datasets have shown that:</p><p>(1) the constraint graph can effectively express the latent semantic dependencies between relation labels;</p><p>(2) our framework can take full advantage of the information in the constraint graph, significantly and consistently outperforming the competitive baselines in terms of denoising and long-tailed RE.</p><p>We believe our exploration for constraint graph shall shed light on future research directions, especially for longtailed relation extraction.</p><p>Specifically, we plan to explore:</p><p>(1) structural extensions of the constraint graph. For example, it would be promising to further improve the performance of long-tailed RE by combining the constraint graph and the relation hierarchical tree;</p><p>(2) effective approaches to strengthen the constraint graph. As demonstrated in <ref type="bibr" target="#b26">[27]</ref> and <ref type="bibr" target="#b46">[47]</ref>, KB priors are effective external information for improving DSRE. It is therefore desirable to integrate KB priors into the constraint graph for further performance improvements.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Instance number distribution of 52 positive relations in FB-NYT dataset. Relations with more than 100 instances are allocated to head relation, while the remaining relations belong to long-tailed relation. Note that the vertical axis uses a logarithmic scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( 1 )</head><label>1</label><figDesc>CoNLL-2003 and OntoNotes are the two most popular named-entity recognition (NER) benchmarks. Hence, there exists a large number of CoNLL-2003/OntoNotes-based NER tools, which can be directly used in the instance representation construction (Sec. 2.5.1); (2) the 18 types of OntoNotes are more fine-grained than the 4 of CoNLL-2003.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3 (</head><label>3</label><figDesc>(a) Performance of GCNs with different layer numbers. b) Performance of GCNs with different output options.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Performance on the long-tailed relations with different number of training instances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>AUC for different smoothing coefficient ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1</head><label>1</label><figDesc>Statistics for NYT-520K, NYT-570K and GDS datasets, where Ins. and Ent. stand for instances and entity pairs respectively.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Training</cell><cell>Test</cell><cell></cell><cell># Relation</cell></row><row><cell></cell><cell># Ins.</cell><cell># Ent.</cell><cell># Ins.</cell><cell># Ent.</cell><cell></cell></row><row><cell cols="5">NYT-520K 523,312 280,275 172,448 96,678 NYT-570K 570,088 291,699</cell><cell>53</cell></row><row><cell>GDS</cell><cell>13,161</cell><cell>7,580</cell><cell>5,663</cell><cell>3,247</cell><cell>5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2</head><label>2</label><figDesc>Hyper-parameters settings for NYT-520K, NYT-570K and GDS.</figDesc><table><row><cell>Component</cell><cell>Parameters</cell><cell cols="3">NYT-520K NYT-570K GDS</cell></row><row><cell></cell><cell>filter num.</cell><cell>230</cell><cell>230</cell><cell>230</cell></row><row><cell>Sentence Encoder</cell><cell>window size word size position size</cell><cell>3 50 5</cell><cell>3 50 5</cell><cell>3 50 5</cell></row><row><cell></cell><cell>coefficient ?</cell><cell>17</cell><cell>20</cell><cell>17</cell></row><row><cell>Graph Encoder</cell><cell>emb. size hidden size output size</cell><cell>100 750 1150</cell><cell>700 950 690</cell><cell>150 900 150</cell></row><row><cell>Classifier</cell><cell>input size</cell><cell>650</cell><cell>690</cell><cell>300</cell></row><row><cell></cell><cell>batch size</cell><cell>160</cell><cell>160</cell><cell>160</cell></row><row><cell>Optimization</cell><cell>learning rate</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell></row><row><cell></cell><cell>dropout rate</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>also demonstrate the significance of utilizing type information in situations with scarce training data, as CGRE and RESIDE show consider-PR curves for different models on NYT-520K, NYT-570K and GDS. Note that DSRE-VAE indicates the vanilla DSRE-VAE, and DSRE-VAE (+KB) indicates the the DSRE-VAE incorporating KB priors.</figDesc><table><row><cell></cell><cell>1.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.00</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CGRE (Ours)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CGRE (Ours)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.95</cell><cell></cell><cell></cell><cell></cell><cell cols="2">DSRE-VAE</cell><cell></cell><cell>0.95</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">DSRE-VAE</cell><cell></cell><cell>0.95</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">DSRE-VAE (+KB)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">DSRE-VAE (+KB)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.90</cell><cell></cell><cell></cell><cell></cell><cell cols="3">PCNN+ATT+ENT</cell><cell>0.90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">PCNN+ATT+ENT</cell><cell>0.90</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Precision</cell><cell>0.75 0.80 0.85</cell><cell></cell><cell></cell><cell></cell><cell cols="2">PCNN+BATT PCNN+HATT RESIDE PCNN+ATT</cell><cell>Precision</cell><cell>0.75 0.80 0.85</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">PCNN+BATT PCNN+HATT RESIDE PCNN+ATT</cell><cell>Precision</cell><cell>0.80 0.85 0.75</cell><cell cols="2">CGRE (Ours) DSRE-VAE PCNN+ATT+ENT</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.70</cell><cell>PCNN+BATT PCNN+HATT</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.65</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.65</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.65</cell><cell>RESIDE</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PCNN+ATT</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.00 0.60</cell><cell>0.05</cell><cell>0.10</cell><cell>0.15</cell><cell>0.20</cell><cell>0.25</cell><cell>0.30</cell><cell>0.00 0.60</cell><cell>0.05</cell><cell>0.10</cell><cell>0.15</cell><cell>0.20</cell><cell>0.25</cell><cell>0.30</cell><cell>0.35</cell><cell>0.40</cell><cell>0.0 0.60</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Recall</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Recall</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Recall</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">(a) NYT-520K</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">(b) NYT-570K</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(c) GDS</cell><cell></cell><cell></cell></row><row><cell cols="2">Fig. 4.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>4. https://github.com/thunlp/HNRE 5. https://github.com/ZhixiuYe/Intra-Bag-and-Inter-Bag- Attentions 6. https://github.com/malllabiisc/RESIDE 7. https://github.com/fenchri/dsre-vae 8. RESIDE integates 38 entity types defined in the first hierarchy of FIGER [53].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 3 (</head><label>3</label><figDesc>%) P@N and AUC values of different models on NYT-520K, NYT-570K and GDS.</figDesc><table><row><cell>Model</cell><cell></cell><cell cols="2">NYT-520K</cell><cell></cell><cell></cell><cell cols="2">NYT-570K</cell><cell></cell><cell></cell><cell>GDS</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="12">P@100 P@200 P@300 AUC P@100 P@200 P@300 AUC P@100 P@200 P@300 AUC</cell></row><row><cell>PCNN+ATT [8]</cell><cell>81.6</cell><cell>73.0</cell><cell>66.9</cell><cell>34.7</cell><cell>72.9</cell><cell>71.5</cell><cell>69.6</cell><cell>38.4</cell><cell>96.4</cell><cell>93.3</cell><cell>91.5</cell><cell>79.9</cell></row><row><cell>PCNN+HATT [26]</cell><cell>77.5</cell><cell>74.2</cell><cell>69.9</cell><cell>37.1</cell><cell>85.0</cell><cell>81.5</cell><cell>77.5</cell><cell>41.9</cell><cell>94.0</cell><cell>92.9</cell><cell>92.4</cell><cell>81.6</cell></row><row><cell>PCNN+BATT [10]</cell><cell>76.9</cell><cell>75.4</cell><cell>72.9</cell><cell>35.2</cell><cell>86.5</cell><cell>84.7</cell><cell>79.4</cell><cell>42.0</cell><cell>96.3</cell><cell>94.7</cell><cell>93.1</cell><cell>80.2</cell></row><row><cell>RESIDE [22]</cell><cell>69.3</cell><cell>67.5</cell><cell>63.4</cell><cell>35.5</cell><cell>81.4</cell><cell>74.9</cell><cell>73.8</cell><cell>41.5</cell><cell>98.4</cell><cell>96.4</cell><cell>95.2</cell><cell>86.8</cell></row><row><cell>DSRE-VAE [47]</cell><cell>74.0</cell><cell>74.5</cell><cell>71.6</cell><cell>38.6</cell><cell>80.0</cell><cell>76.0</cell><cell>75.6</cell><cell>44.6</cell><cell>96.9</cell><cell>96.7</cell><cell>96.3</cell><cell>87.6</cell></row><row><cell>DSRE-VAE (+KB) [47]</cell><cell>83.0</cell><cell>75.5</cell><cell>73.0</cell><cell>42.9</cell><cell>81.0</cell><cell>77.5</cell><cell>73.6</cell><cell>45.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PCNN+ATT+ENT</cell><cell>79.4</cell><cell>74.7</cell><cell>68.3</cell><cell>35.5</cell><cell>83.7</cell><cell>81.4</cell><cell>77.5</cell><cell>47.0</cell><cell>93.9</cell><cell>93.7</cell><cell>93.5</cell><cell>84.9</cell></row><row><cell>CGRE (Ours)</cell><cell>82.7</cell><cell>80.3</cell><cell>76.5</cell><cell>41.7</cell><cell>88.9</cell><cell>86.4</cell><cell>81.8</cell><cell>51.9</cell><cell>98.6</cell><cell>98.1</cell><cell>97.7</cell><cell>90.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 4 (</head><label>4</label><figDesc>%) Accuracy of Hits@K on relations with training instances fewer than 100/200.</figDesc><table><row><cell>Training Instances</cell><cell></cell><cell>&lt;100</cell><cell></cell><cell></cell><cell>&lt;200</cell><cell></cell></row><row><cell>Hits@K (Macro)</cell><cell>10</cell><cell>15</cell><cell>20</cell><cell>10</cell><cell>15</cell><cell>20</cell></row><row><cell>PCNN+ATT [8]</cell><cell>&lt;5.0</cell><cell>7.4</cell><cell cols="4">40.7 17.2 24.2 51.5</cell></row><row><cell>PCNN+ATT+ENT</cell><cell>22.2</cell><cell cols="5">33.3 61.1 36.4 45.5 68.2</cell></row><row><cell>PCNN+HATT [26]</cell><cell>29.6</cell><cell cols="5">51.9 61.1 41.4 60.6 68.2</cell></row><row><cell>PCNN+KATT [27]</cell><cell>35.3</cell><cell cols="5">62.4 65.1 43.2 61.3 69.2</cell></row><row><cell>PA-TRP [28]</cell><cell>63.9</cell><cell cols="5">70.3 72.2 66.7 72.3 73.8</cell></row><row><cell>CoRA [29]</cell><cell>59.7</cell><cell cols="5">63.9 73.6 65.4 69.0 77.4</cell></row><row><cell>ToHRE [54]</cell><cell>62.9</cell><cell cols="5">75.9 81.4 69.7 80.3 84.8</cell></row><row><cell>DPEN [55]</cell><cell>57.6</cell><cell cols="5">62.1 66.7 64.1 68.0 71.8</cell></row><row><cell>CGRE (Ours)</cell><cell>77.8</cell><cell cols="5">77.8 87.0 81.8 81.8 89.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 5 (</head><label>5</label><figDesc>%) P@N on NYT-520K with different bag sizes.</figDesc><table><row><cell>Bag Size</cell><cell></cell><cell></cell><cell>One</cell><cell></cell><cell></cell><cell></cell><cell>Two</cell><cell></cell><cell></cell><cell></cell><cell>All</cell><cell></cell></row><row><cell>P@N</cell><cell>100</cell><cell>200</cell><cell>300</cell><cell>Mean</cell><cell>100</cell><cell>200</cell><cell>300</cell><cell>Mean</cell><cell>100</cell><cell>200</cell><cell>300</cell><cell>Mean</cell></row><row><cell>PCNN+ATT [8]</cell><cell cols="3">74.0 65.0 61.3</cell><cell>66.8</cell><cell cols="3">78.0 69.0 65.7</cell><cell>70.9</cell><cell cols="3">83.0 69.5 67.0</cell><cell>73.2</cell></row><row><cell>PCNN+HATT [26]</cell><cell cols="3">75.0 70.0 66.0</cell><cell>70.3</cell><cell cols="3">76.0 73.5 67.7</cell><cell>72.4</cell><cell cols="3">77.0 73.5 69.0</cell><cell>73.2</cell></row><row><cell>PCNN+BATT [10]</cell><cell cols="3">74.0 65.0 63.7</cell><cell>67.6</cell><cell cols="3">81.0 72.0 67.4</cell><cell>72.6</cell><cell cols="3">86.0 75.5 68.3</cell><cell>76.6</cell></row><row><cell>RESIDE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 6 (</head><label>6</label><figDesc>%) P@N on NYT-570K with different bag sizes.</figDesc><table><row><cell>Bag Size</cell><cell></cell><cell></cell><cell>One</cell><cell></cell><cell></cell><cell></cell><cell>Two</cell><cell></cell><cell></cell><cell></cell><cell>All</cell><cell></cell></row><row><cell>P@N</cell><cell>100</cell><cell>200</cell><cell>300</cell><cell>Mean</cell><cell>100</cell><cell>200</cell><cell>300</cell><cell>Mean</cell><cell>100</cell><cell>200</cell><cell>300</cell><cell>Mean</cell></row><row><cell>PCNN+ATT [8]</cell><cell cols="3">73.3 69.2 60.8</cell><cell>67.8</cell><cell cols="3">77.2 71.6 66.1</cell><cell>71.6</cell><cell cols="3">76.2 73.1 67.4</cell><cell>72.2</cell></row><row><cell>PCNN+HATT [26]</cell><cell cols="3">84.0 76.0 69.7</cell><cell>76.6</cell><cell cols="3">85.0 76.0 72.7</cell><cell>77.9</cell><cell cols="3">88.0 79.5 75.3</cell><cell>80.9</cell></row><row><cell>PCNN+BATT [10]</cell><cell cols="3">86.8 77.6 73.9</cell><cell>79.4</cell><cell cols="3">91.2 79.2 75.4</cell><cell>81.9</cell><cell cols="3">91.8 84.0 78.7</cell><cell>84.8</cell></row><row><cell>RESIDE [22]</cell><cell cols="3">80.0 75.5 69.3</cell><cell>74.9</cell><cell cols="3">83.0 73.5 70.6</cell><cell>75.7</cell><cell cols="3">84.0 78.5 75.6</cell><cell>79.4</cell></row><row><cell>DSRE-VAE [47]</cell><cell cols="3">79.0 68.5 65.7</cell><cell>71.1</cell><cell cols="3">74.0 72.5 69.3</cell><cell>71.9</cell><cell cols="3">77.0 72.5 67.3</cell><cell>72.3</cell></row><row><cell cols="4">DSRE-VAE (+KB) [47] 79.0 72.0 65.7</cell><cell>72.2</cell><cell cols="3">81.0 74.0 66.7</cell><cell>73.9</cell><cell cols="3">88.0 74.0 70.7</cell><cell>77.6</cell></row><row><cell>PCNN+ATT+ENT</cell><cell cols="3">87.0 84.0 80.7</cell><cell>83.9</cell><cell cols="3">89.0 85.5 81.3</cell><cell>85.3</cell><cell cols="3">91.0 87.0 83.3</cell><cell>87.1</cell></row><row><cell>CGRE (Ours)</cell><cell cols="3">95.0 88.5 85.0</cell><cell>89.5</cell><cell cols="3">95.0 90.0 84.7</cell><cell>89.9</cell><cell cols="3">94.0 92.5 88.3</cell><cell>91.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 7 (</head><label>7</label><figDesc>%) AAcc and F1-Scores results on NYT-H, where Orig. indicates the original test set, Zero indicates the test set in which no sentences are valid, One indicates the test set in which only one sentence is valid in each bag, All indicates the test set in which all sentences are valid.</figDesc><table><row><cell>Model</cell><cell>AAcc</cell><cell></cell><cell cols="2">F1-Score</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Orig. Zero One</cell><cell>All</cell></row><row><cell>PCNN+ATT [8]</cell><cell>52.9</cell><cell>60.5</cell><cell>63.9</cell><cell>72.4</cell><cell>75.7</cell></row><row><cell>PCNN+CATT</cell><cell>57.3</cell><cell>66</cell><cell>71.1</cell><cell>78.5</cell><cell>80.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 8 (</head><label>8</label><figDesc>%) AUC and macro accuracy of Hits@K of different backbone models on NYT-520K, where None indicate the case without the constraint graph (e.g., CNN+None is equivalent to CNN+ATT+ENT), playing the baseline role, and &lt;100/&lt;200 indicates the long-tailed relations with training instances fewer than 100/200.</figDesc><table><row><cell cols="2">Encoder</cell><cell>&lt;100</cell><cell></cell><cell>&lt;200</cell><cell></cell><cell>AUC</cell></row><row><cell>Sent.</cell><cell cols="5">Graph Hits@5 Hits@10 Hits@5 Hits@10</cell><cell></cell></row><row><cell></cell><cell>None</cell><cell>7.5</cell><cell>20.0</cell><cell>19.2</cell><cell>32.4</cell><cell>35.0</cell></row><row><cell>CNN</cell><cell>GCN</cell><cell>17.5</cell><cell>40.0</cell><cell>31.2</cell><cell>50.0</cell><cell>38.2</cell></row><row><cell></cell><cell>GAT</cell><cell>27.5</cell><cell>40.0</cell><cell>39.6</cell><cell>50.0</cell><cell>38.7</cell></row><row><cell></cell><cell>SAGE</cell><cell>24.2</cell><cell>40.0</cell><cell>34.0</cell><cell>50.0</cell><cell>37.3</cell></row><row><cell></cell><cell>None</cell><cell>15.0</cell><cell>35.0</cell><cell>28.2</cell><cell>45.8</cell><cell>35.5</cell></row><row><cell>PCNN</cell><cell>GCN</cell><cell>19.2</cell><cell>40.0</cell><cell>32.6</cell><cell>50.0</cell><cell>41.7</cell></row><row><cell></cell><cell>GAT</cell><cell>23.3</cell><cell>40.0</cell><cell>35.2</cell><cell>50.0</cell><cell>41.8</cell></row><row><cell></cell><cell>SAGE</cell><cell>22.5</cell><cell>65.0</cell><cell>34.5</cell><cell>70.8</cell><cell>39.3</cell></row><row><cell></cell><cell>None</cell><cell>10.0</cell><cell>30.0</cell><cell>25.0</cell><cell>41.7</cell><cell>46.1</cell></row><row><cell>BERT</cell><cell>GCN</cell><cell>31.7</cell><cell>36.7</cell><cell>43.1</cell><cell>47.2</cell><cell>46.8</cell></row><row><cell></cell><cell>GAT</cell><cell>23.3</cell><cell>40.0</cell><cell>36.1</cell><cell>50.0</cell><cell>48.9</cell></row><row><cell></cell><cell>SAGE</cell><cell>25.0</cell><cell>45.0</cell><cell>36.6</cell><cell>54.2</cell><cell>47.2</cell></row><row><cell cols="7">encoder choices. In terms of long-tailed RE, GAT provides</cell></row><row><cell cols="7">the highest improvements for CNN and PCNN, while GCN</cell></row><row><cell cols="5">and SAGE show more benefits for BERT;</cell><cell></cell><cell></cell></row><row><cell cols="7">(4) the BERT-based CGRE variants achieve significant</cell></row><row><cell cols="7">improvements over the previous Transformer architectural</cell></row><row><cell cols="2">model, DISTRE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 9 (</head><label>9</label><figDesc>%) AUC and macro accuracy of Hits@K of different variants on NYT-520K.</figDesc><table><row><cell>Model</cell><cell>&lt;100</cell><cell></cell><cell>&lt;200</cell><cell></cell><cell>AUC</cell></row><row><cell></cell><cell cols="4">Hits@5 Hits@10 Hits@5 Hits@10</cell><cell></cell></row><row><cell>Base</cell><cell>5.0</cell><cell>5.0</cell><cell>15.3</cell><cell>20.8</cell><cell>34.7</cell></row><row><cell>Base+Type</cell><cell>&lt;5.0</cell><cell>27.8</cell><cell>16.2</cell><cell>40.9</cell><cell>39.4</cell></row><row><cell>Base+Const</cell><cell>12.5</cell><cell>25.0</cell><cell>26.2</cell><cell>37.5</cell><cell>37.0</cell></row><row><cell>CGRE</cell><cell>19.2</cell><cell>40.0</cell><cell>32.6</cell><cell>50.0</cell><cell>41.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 10</head><label>10</label><figDesc>Examples for case study. The column Satisfactory indicates whether the sentence satisfies the constraint, and column the Correct indicates whether the label is correct. The words in bold represent the target entities. In Entity Type, NORP implies nationalities, religious or political groups, LANG implies languages, and GPE implies countries, cities or states. With the city so influenced by chinese and japanese culture, asian cuisine is always an excellent bet. In the last several years, the site has housed a french restaurant, an asian buffet, and as of about three months ago a japanese restaurant...</figDesc><table><row><cell>ID</cell><cell>Sentence</cell><cell>Entity Type</cell><cell>Satisfactory</cell><cell>Correct</cell><cell>Score</cell></row><row><cell>s 1</cell><cell></cell><cell>(NORP, NORP)</cell><cell>Yes</cell><cell>Yes</cell><cell>0.967</cell></row><row><cell>s 2</cell><cell></cell><cell>(NORP, NORP)</cell><cell>Yes</cell><cell>No</cell><cell>0.009</cell></row><row><cell></cell><cell>"Three extremes", 125 minutes in japanese, korean, mandarin and</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>s 3</cell><cell>cantonese, this trilogy provides a sampler of short horror films from</cell><cell>(LANG, NORP)</cell><cell>No</cell><cell>No</cell><cell>0.003</cell></row><row><cell></cell><cell>high-profile asian directors.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Relation: /people/ethnicity/included in group</cell><cell></cell><cell cols="3">Constraint: (LANG, NORP)</cell></row><row><cell>s 4</cell><cell>Herzen was tireless in his crusade for freeing russia 's serfs and refor-ming russian society.</cell><cell>(NORP, GPE)</cell><cell>Yes</cell><cell>Yes</cell><cell>0.695</cell></row><row><cell>s 5</cell><cell>The russian debate on the treaty is subtly shifting, with new attention to the missiles russia will really need.</cell><cell>(NORP, GPE)</cell><cell>Yes</cell><cell>No</cell><cell>0.032</cell></row><row><cell>s 6</cell><cell>In st.petersburg, russia, summer advantage is giving teenagers early college credit for russian language classes.</cell><cell>(LANG, GPE)</cell><cell>No</cell><cell>No</cell><cell>0.007</cell></row><row><cell cols="2">Relation: /people/ethnicity/geographic distribution</cell><cell></cell><cell cols="3">Constraint: (NORP, GPE)</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Modeling relation paths for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding based on multi-view clustering framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="585" to="596" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generating query facets using knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="315" to="329" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Web people search via connection analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">V</forename><surname>Kalashnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nuray-Turan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1550" to="1565" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Answering natural language questions by subgraph matching over knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="824" to="837" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Few-shot complex knowledge base question answering via meta reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5827" to="5837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-IJCNLP</title>
		<meeting>ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural relation extraction with selective attention over instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2124" to="2133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction with linear attenuation simulation and non-iid relevance embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7418" to="7425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distant supervision relation extraction with intra-bag and inter-bag attentions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2810" to="2819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reinforcement learning for relation classification from noisy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5779" to="5786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adaptive graph convolutional networks with attention mechanism for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCNN</title>
		<meeting>IJCNN</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adversarial training for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bamman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1778" to="1783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dsgan: generative adversarial training for distant supervision relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="496" to="505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gan driven semi-distant supervision for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3026" to="3035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving performance of relation extraction algorithm via leveled adversarial pcnn and database expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Puspitaningrum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CITSM</title>
		<meeting>CITSM</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Noise-clustered distant supervision for relation extraction: A nonparametric bayesian perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1808" to="1813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Are noisy sentences useless for distant supervised relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI, 2020</title>
		<meeting>AAAI, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="8799" to="8806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction with sentence-level attention and entity descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3060</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving distantly-supervised relation extraction with joint label embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-IJCNLP</title>
		<meeting>EMNLP-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3812" to="3820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Exploring fine-grained entity type constraints for distantly supervised relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2107" to="2116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reside: improving distantly-supervised neural relation extraction using side information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Prayaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1257" to="1266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improving neural relation extraction with implicit mutual relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICDE</title>
		<meeting>ICDE</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1021" to="1032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Label-free distant supervision for relation extraction via knowledge graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2246" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECML-PKDD</title>
		<meeting>ECML-PKDD</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hierarchical relation extraction with coarse-to-fine grained attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2236" to="2245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Long-tail relation extraction via knowledge graph embeddings and graph convolution networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3016" to="3025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning relation prototype from unlabeled texts for long-tail relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improving long-tail relation extraction with collaborating relation-augmented attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1653" to="1664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">?</forename><surname>S?aghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pad?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Szpakowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions</title>
		<meeting>the Workshop on Semantic Evaluations: Recent Achievements and Future Directions</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="94" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Position-aware attention and supervised data improve slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="35" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fewrel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4803" to="4809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fewrel 2.0: Towards more challenging few-shot relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-IJCNLP</title>
		<meeting>EMNLP-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6251" to="6256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Digraphs: theory, algorithms and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">Z</forename><surname>Gutin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cooperative denoising for distantly supervised relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="426" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning from context or names? an empirical study on neural relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3661" to="3672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A frustratingly easy approach for entity and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="50" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A noise-aware method with type constraint pattern for neural relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Towards robust linguistic analysis using ontonotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bj?rkelund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Self-attention enhanced selective gate with entity-aware embedding for distantly supervised relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8269" to="8276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction via piecewise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1753" to="1762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Encoding sentences with graph convolutional networks for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1506" to="1515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Flair: an easy-to-use framework for state-of-the-art nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schweter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="54" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Fine-tuning pre-trained transformer language models to distantly supervised relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>H?bner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hennig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1388" to="1398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Structured minimally supervised learning for neural relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3057" to="3069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Distantly supervised relation extraction with sentence reconstruction and knowledge base priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Christopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL, 2021</title>
		<meeting>NAACL, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="11" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Robust neural relation extraction via multi-granularity noises reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3297" to="3310" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Improving distantly supervised relation extraction using word and entity based attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Talukdar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06987</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Knowing false negatives: An adversarial training method for distantly supervised relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP, 2021</title>
		<meeting>EMNLP, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="9661" to="9672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AISTATS</title>
		<meeting>AISTATS</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Fine-grained entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="94" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Tohre: A top-down classification strategy with hierarchical bag representation for distantly supervised relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1665" to="1676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A dynamic parameter enhanced network for distant supervised relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">197</biblScope>
			<biblScope unit="page">105912</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">How knowledge graph and attention help? a qualitative analysis into bag-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-IJCNLP</title>
		<meeting>ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4662" to="4671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><forename type="middle">C</forename><surname>Kenton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Knowledge-based weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Multi-instance multi-label learning for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-CoNLL</title>
		<meeting>EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Arnor: attention regularization based noise reduction for distant supervision relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1399" to="1408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Large-scale learning of relation-extraction rules with distant supervision from the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISWC</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="263" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Exploring long tail data in distantly supervised relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Language Understanding and Intelligent Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="514" to="522" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
