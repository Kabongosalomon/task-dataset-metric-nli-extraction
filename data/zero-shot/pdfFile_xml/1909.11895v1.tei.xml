<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint-task Self-supervised Learning for Temporal Correspondence</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueting</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Merced</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sifei</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shalini</forename><surname>De Mello</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Merced</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Joint-task Self-supervised Learning for Temporal Correspondence</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes to learn reliable dense correspondence from videos in a self-supervised manner. Our learning process integrates two highly related tasks: tracking large image regions and establishing fine-grained pixel-level associations between consecutive video frames. We exploit the synergy between both tasks through a shared inter-frame affinity matrix, which simultaneously models transitions between video frames at both the region-and pixel-levels. While region-level localization helps reduce ambiguities in fine-grained matching by narrowing down search regions; fine-grained matching provides bottom-up features to facilitate region-level localization. Our method outperforms the state-of-the-art self-supervised methods on a variety of visual correspondence tasks, including video-object and part-segmentation propagation, keypoint tracking, and object tracking. Our self-supervised method even surpasses the fully-supervised affinity feature representation obtained from a ResNet-18 pre-trained on the ImageNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning representations for visual correspondence is a fundamental problem that is closely related to a variety of vision tasks: correspondences between multi-view images relate 2D and 3D representations, and those between frames link static images to dynamic scenes. To learn correspondences across frames in a video, numerous methods have been developed from two perspectives: (a) learning region/object-level correspondences, via object tracking <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b47">48]</ref> or (b) learning pixel-level correspondences between multi-view images or frames, e.g., via stereo matching <ref type="bibr" target="#b33">[34]</ref> or optical flow estimation <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b30">31]</ref>.</p><p>However, most methods address one or the other problem and significantly less effort has been made to solve both of them together. The main reason is that methods designed to address either of them optimize different goals. Object tracking focuses on learning object representations that are invariant to viewpoint and deformation changes, while learning pixel-level correspondence focuses on modeling detailed changes within an object over time. Subsequently, the existing supervised methods for these two problems often use different annotations. For example, bounding boxes are annotated in real videos for object tracking <ref type="bibr" target="#b51">[52]</ref>; and pixel-wise associations are generated from synthesized data for optical flow estimation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10]</ref>. Datasets with annotations for both tasks are scarcely available and supervision, here, is a further bottleneck preventing us from connecting the two tasks.</p><p>In this paper, we demonstrate that these two tasks inherently require the same operation of learning an inter-frame transformation that associates the contents of two images. We show that the two tasks benefit greatly by modeling them jointly via a single transformation operation which can simultaneously match regions and pixels. To overcome the lack of data with annotations for both tasks we exploit self-supervision via the signals of (a) Temporal Coherency, which states that objects or scenes move smoothly and gradually over time; (b) Cycle Consistency, correct correspondences should ensure that pixels or regions match bi-directionally and (c) Energy Preservation, which <ref type="figure">Figure 1</ref>: Our method (c) compared against (a) region-level matching (e.g., object tracking), and (b) pixel-level matching, e.g., matching by colorization <ref type="bibr" target="#b43">[44]</ref>. We propose a joint-task framework which conducts region-level and fine-grained matching simultaneously and which are supported by a single inter-frame affinity matrix A. During training, the two tasks improve each other progressively. To illustrate this, we unroll two training iterations and illustrate the improvement with the red box and arrow.</p><p>preserves the energy of feature representations during transformations. Since all these supervisory signals naturally exist in videos and are task-agnostic, the transformation that we learn through them can generalize well to any video without restriction on domain or object category.</p><p>Our key idea is to learn a single affinity matrix for modeling all inter-frame transformations through a network that learns appropriate feature representations that model the affinity. We show that region localization and fine-grained matching can be carried out by sharing the affinity in a fully differentiable manner: the region localization module finds a pair of patches with matching parts in the two frames ( <ref type="figure">Figure 1</ref>, mid-top), and the fine-grained module reconstructs the color feature by transforming it between the patches <ref type="figure">(Figure 1</ref>, mid-bottom), all through the same affinity matrix. These two tasks symbiotically facilitate each other: the fine-grained matching module learns better feature representations that lead to an improved affinity matrix, which in turn generates better localization that reduces the search space and ambiguities for fine-grained matching <ref type="figure">(Figure 1</ref>, right).</p><p>The contributions of this work are summarized as: (a) A joint-task self-supervision network is introduced to find accurate correspondences at different levels across video frames. (b) A general inter-frame transformation is proposed to support both tasks and to satisfy various video constraintscoherency, cycle, and energy consistency. (c) Our method outperforms state-of-the-art methods on a variety of visual correspondence tasks, e.g., video instance and part segmentation, keypoints tracking, and object tracking. Our self-supervised method even surpasses the fully-supervised affinity feature representation obtained from a ResNet-18 pre-trained on the ImageNet <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Learning correspondence in time is widely explored in visual tracking <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b47">48]</ref> and optical flow estimation <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b15">16]</ref>. Existing models are mainly trained on large annotated datasets, which require significant efforts. To overcome the limit of annotations, numerous methods have been developed to learn correspondences in a self-supervised manner <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b43">44]</ref>. Our work establishes on learning correspondence with self-supervision, and we discuss the most related methods here.</p><p>Object-level correspondence. The goal of visual tracking is to determine a bounding box in each frame based on an annotated box in the reference image. Most methods belong to one of the two categories that use: (a) the tracking-by-detection framework <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b24">25]</ref>, which models tracking as detection applied independently to individual frames; or (b) the tracking-by-matching framework that models cross-frame relations and includes several early attempts, e.g., mean-shift trackers <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b53">54]</ref>, kernelized correlation filters (KCF) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b26">27]</ref>, and several works that model correlation filters as differentiable blocks <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b46">47]</ref>. Most of these methods use annotated bounding boxes <ref type="bibr" target="#b51">[52]</ref> in every frame of the videos to learn feature representations for tracking. Our work can be viewed as exploiting the tracking-by-matching framework in a self-supervised manner.</p><p>Fine-grained correspondence. Dense correspondence between video frames has been widely applied for optical flow and motion estimation <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b15">16]</ref>, where the goal is to track individual pixels. Most deep neural networks <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b39">40]</ref> are trained with the objective of regressing the groundtruth optical flow produced by synthetic datasets <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10]</ref>. In contrast to many classic methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b28">29]</ref> that model dense correspondence as a matching problem, direct regression of pixel offsets has limited  A pf is the affinity between p1 and f2, and App is that between p1 and p2. p2 is a differentiable crop from the frame f2. The maps lx and ly are the coordinates of pixels on a regular grid. All modules are differentiable, where the gradient flow is visualized via the red dashed arrows.</p><p>capability for frames containing dramatic appearance changes <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b38">39]</ref>, and suffers from problems related to domain shift when applied to real-world scenarios. Self-supervised learning. Recently, numerous approaches have been developed for correspondence learning via various self-supervised signals, including image <ref type="bibr" target="#b16">[17]</ref> or color transformation <ref type="bibr" target="#b43">[44]</ref> and cycle-consistency <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b44">45]</ref>. Self-supervised learning of correspondence in videos has been explored along the two different directions -for region-level localization <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b44">45]</ref> and for fine-grained pixellevel matching <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b22">23]</ref>. In <ref type="bibr" target="#b44">[45]</ref>, a correlation filter is learned to track regions via a cycle-consistency constraint, and no pixel-level correspondence is determined. <ref type="bibr" target="#b50">[51]</ref> develops patch-level tracking by modeling the similarity transformation of pixels within a fixed rectangular region. Conversely, several methods learn a matching network by transforming color/RGB information between adjacent frames <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b22">23]</ref>. As no region-level regularization is exploited, these approaches are less effective when color features are less distinctive (see <ref type="figure">Figure 1</ref>(b)). In contrast, our method learns object-level and pixel-level correspondence jointly across video frames in a self-supervised manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>Video frames are temporally coherent in nature. For a pair of adjacent frames, pixels in a later frame can be considered as being copied from some locations of an earlier one with slight appearance changes conforming to object motion. This "copy" operator can be expressed via a linear transformation with a matrix A, in which A ij = 1 denotes that the pixel j in the second frame is copied from pixel i in the first one. An approximation of A is the inter-frame affinity matrix <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b50">51]</ref>:</p><formula xml:id="formula_0">A ij = ?(f 1i , f 2j )<label>(1)</label></formula><p>where ? denotes some similarity function. Each entry A ij represents the similarity of subspace pixels i and j in the two frames f 1 ? R C?N1 and f 2 ? R C?N2 , where f ? R C?N is a vectorized feature map with C channels and N pixels. In this work, our goal is to learn the feature embedding f that optimally associates the contents of the two frames.</p><p>One free supervisory signal that we can utilize is color. To learn the inter-frame transformation in a self-supervised manner, we can slightly modify (1) to generate the affinity via features f learned only from gray-scale images. The learned affinity is then utilized to map the color channels from one frame to another <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b29">30]</ref>, while using the ground-truth color as the self-supervisory signal.</p><p>One strict assumption of this formulation is that the paired frames need to have the same contentsno new object or scene pixel should emerge over time. Hence, the existing methods <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b29">30]</ref> sample pairs of frames either uniformly, or randomly within a specified interval, e.g., 50 frames. However, it is difficult to determine a "perfect" interval as video contents may change sporadically. When transforming color from a reference frame to a target one, the objects/scene pixels in the target frame may not exist in the reference frame, thereby leading to wrong matches and an adverse effect on feature learning. Another issue is that a large portion of the video frames are "static", in which the sampled pair of frames are almost the same and cause the learned affinity to be an identity matrix.</p><p>We show that the above problems can be addressed by incorporating a region-level localization module. Given a pair of reference and target frames, we first randomly sample a patch in the reference frame and localize this patch in the target frame (see <ref type="figure" target="#fig_1">Figure 2</ref>). The inter-frame color transformation is then estimated between the paired patches. Both localization and color transformation are supported by a single affinity derived from a convolutional neural network (CNN) based on the fact that the affinity matrix can simultaneously track locations and transform features discussed in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Transforming Feature and Location via Affinity</head><p>We sample a pair of frames and denote the 1 st frame as the reference and the 2 nd one as the target. The CNN can be any effective model, e.g., ResNet-18 <ref type="bibr" target="#b12">[13]</ref> with the first 4 blocks that takes a gray-scale image as input. We compute the affinity and conduct the feature transformation and localization on the top layer of the CNN, with features that are one-eighth the size of the input image. This ensures the affinity matrix to be memory efficient and each pixel in the feature space to contain considerable local contextual information.</p><p>Transforming feature representations. We adopt the dot product for ? in (1) to compute the affinity, where each column can be interpreted as the similarity score between a point in the target frame to all points in the reference frame. For dense correspondence, the inter-frame affinity needs to be sparse to ensure one-to-one mapping. However, it is challenging to model a sparse matrix in a deep neural network. We relax this constraint and encourage the affinity matrix to be sparse by normalizing each column with the softmax function, so that the similarity score distribution can be peaky and only a few pixels with high similarity in the reference frame are matched to each point in the target frame:</p><formula xml:id="formula_1">Aij = exp(f 1i f2j) k exp(f 1k f2j) , ?i ? [1, N1], j ? [1, N2]<label>(2)</label></formula><p>where the variable definitions follow <ref type="bibr" target="#b0">(1)</ref>. The transformation is carried out as? 2 = c 1 A, where A ? R N1?N2 , and c i has the same number of entries as f i and can be features of the reference frame or any associated label, e.g., color, segmentation mask or keypoint heatmap.</p><p>Tracing pixel locations. We denote l j = (x j , y j ), l ? R 2?N as the vectorized location map for an image/feature with N pixels. Given a sparse affinity matrix, the location of an individual pixel can be traced from a reference frame to an adjacent target frame:</p><formula xml:id="formula_2">l 12 j = N 1 k=1 l 11 k A kj , ?j ? [1, N2]<label>(3)</label></formula><p>where l mn j represents the coordinate in frame m that transits to the j th pixel in frame n. Note that l nn (e.g., l 11 in (3)) usually represents a canonical grid as shown in <ref type="figure" target="#fig_2">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Region-level Localization</head><p>In the target frame, region-level localization aims to localize a patch randomly selected from the reference frame by predicting a bounding box (denoted as "bbox") on a region that shares matching parts with the selected patch. In other words, it is a differential region of interest (ROI) with learnable center and scale. We compute an N 1 ? N 2 affinity A pf according to <ref type="bibr" target="#b1">(2)</ref> between feature representations of the patch in the reference frame, and that of the whole target frame (see <ref type="figure" target="#fig_1">Figure 2</ref>(a)). Locating the center. To track the center position of the reference patch in the target frame, we first localize each individual pixel of the reference patch p 1 in the target frame f 2 , according to <ref type="bibr" target="#b2">(3)</ref>. As we obtain the set l 21 , with the same number of entries as p 1 , that collects the coordinates of the most similar pixels in f 2 , we can compute the average coordinate</p><formula xml:id="formula_3">C 21 = 1 N1 N1 i=1 l 21</formula><p>i of all the points, as the estimated new position of the reference patch. Scale modeling. For region-level tracking, the reference patch may undergo significant scale changes. Scale estimation in object tracking is challenging and existing methods mainly enumerate possible scales <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b44">45]</ref> and select the optimal one. In contrast, the scale can be estimated by our proposed model. We assume that the transformed locations l 21 are still distributed uniformly in a local rectangular region. By denoting w as the width of the new bounding box, the scale is estimated by:?</p><formula xml:id="formula_4">= 2 N1 N 1 i=1 xi ? C 21 (x) 1<label>(4)</label></formula><p>where the x i is the x-coordinate of the i th entry in the l <ref type="bibr" target="#b20">21</ref> . We note that (4) can be proved by using the analogous continuous space. Suppose there is a rectangle with scale (2w, 2h) and with its center located at the origin of a 2D coordinate plane. By integrating points inside of it, we have:</p><formula xml:id="formula_5">1 w w ?w x 1 dx = 2 w w 0 xdx = w<label>(5)</label></formula><p>This represents the average absolute distances w.r.t. the center when transforming to the discrete space. The estimation of height is conducted in the same manner.</p><p>Moving as a unit. An important assumption in the aforementioned ROI estimation in the target frame is that the pixels from the reference patch should move in unison -this is true in most videos, as an object or its parts typically move as one unit at the region level. We enforce this constraint with a concentration regularization <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b14">15]</ref> term on the transformed pixels, with a truncated loss to penalize these points from moving too far away from the center:</p><formula xml:id="formula_6">Lc = 0, l 12 j (x) ? C 12 (x) 1 ? w and l 12 j (y) ? C 12 (y) 1 ? h 1 N 2 N 2 j=1 l 12 j ? C 12 2 , otherwise<label>(6)</label></formula><p>This formulation encourages all the tracked pixels, originally from a patch, to be concentrated (see <ref type="figure" target="#fig_2">Figure 3</ref>) rather than being dispersed to other objects, which is likely to happen for methods that are based on pixel-wise matching only, e.g., when matching by color reconstruction, pixels of different objects having similar colors may match each other, as shown in <ref type="figure">Figure 1(b)</ref>. Fine-grained matching aims to reconstruct the color information of the located patch in the target frame, given the reference patch (see <ref type="figure">Figure 1)</ref>. We re-use the inter-frame affinity A pf by extracting a sub-affinity matrix A pp containing the columns corresponding to the located pixels in the target frame, and by using it for the color transformation described in the formulations in Section 3.1. To make the color feature compatible with the affinity matrix, we train an auto-encoder that learns to reconstruct an image in the Lab space faithfully (see the encoder E and the decoder D in <ref type="figure" target="#fig_1">Figure 2</ref>). This network also encodes global contextual information from color channels. We show that using the color feature instead of pixels significantly reduces the errors caused by reconstructing color directly in the image space <ref type="bibr" target="#b43">[44]</ref> (see <ref type="table" target="#tab_0">Table 1</ref>, ours vs. <ref type="bibr" target="#b43">[44]</ref>). In the following, we introduce self-supervisory signals as regularization for fine-grained matching. For brevity, we denote A as the sub-affinity, l and f as the vectorized coordinate and feature map, respectively, for the paired patches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Fine-grained Matching</head><p>Orthogonal regularization. Another important constraint, cycle-consistency, for the transformation of both location <ref type="bibr" target="#b50">[51]</ref> and feature <ref type="bibr" target="#b29">[30]</ref> is the orthogonal regularization. For a pair of patches, we encourage every pixel to fall into the same location after one cycle of forward and backward tracking, as shown in <ref type="figure" target="#fig_2">Figure 3</ref> (middle and right):</p><formula xml:id="formula_7">l 12 = l 11 A 1?2 ,l 11 =l 12 A 2?1<label>(7)</label></formula><p>Here we specifically add m ? n to denote affinity transforming from the frame m to n, i.e., A m?n = ?(f m , f n ). Similarly, the cycle-consistency can be applied to the feature space:</p><formula xml:id="formula_8">f 2 = f 1 A 1?2 ,f 1 =f 2 A 2?1<label>(8)</label></formula><p>We show that enforcing cycle-consistency is equivalent to regularizing A to be orthogonal: With <ref type="formula" target="#formula_7">(7)</ref> and <ref type="formula" target="#formula_8">(8)</ref>, it is easy to show that the optimal solution is achieved when A ?1 1?2 = A 2?1 . Inspired by recent style transfer methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">30]</ref>, the color energy represented by the Gram-matrix should be consistent such that f 1 f 1 = f 2 f 2 , which derives that A 1?2 = A 2?1 is the goal to reconstruct the color information. Thus, it is easy to show that regularizing A as orthogonal automatically satisfies the cycle constraint. In practice, we switch the role of reference and target to perform the transformation, as described in <ref type="formula" target="#formula_7">(7)</ref> and <ref type="bibr" target="#b7">(8)</ref>. We use the MSE loss between bothl 11 and l 11 ,f 1 and f 1 , and specifically replace A 2?1 with A 1?2 in Eq. (8) to enforce the regularization. Namely, the orthogonal regularization provides a concise mathematical formulation for many recent works <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b44">45]</ref> that exploit cycle-consistency in videos.</p><p>Concentration regularization. We additionally apply the concentration loss (i.e., Eq.(6) without the truncation) in local, non-overlapping 8 ? 8 grids of a feature map, to encourage local context or object parts to move as an entity over time. Unlike <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b37">38]</ref> where local patches are regularized by similarity transformation via a spatial transformation network <ref type="bibr" target="#b17">[18]</ref>, this local concentration loss is more flexible by allowing arbitrary deformations within each local grid.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We compare with state-of-the-art algorithms <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b50">51]</ref> on several tasks: instance mask propagation, pose keypoints tracking, human parts segmentation propagation and visual tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Network Architecture</head><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, our model consists of a region-level localization module and a fine-grained matching module that share a feature representation network (see <ref type="figure" target="#fig_1">Figure 2</ref>). We use the ResNet-18 <ref type="bibr" target="#b12">[13]</ref> as the network for fair comparisons with <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b50">51]</ref>. The patch randomly cropped from the reference frame is of 256 ? 256 pixels.</p><p>Training. We first train the auto-encoder in the matching module (the encoder "E" and decoder "D" in <ref type="figure" target="#fig_1">Figure 2</ref>) to reconstruct images in the Lab space using the MSCOCO <ref type="bibr" target="#b27">[28]</ref> dataset. We then fix it and train the feature representation network using the Kinetics dataset <ref type="bibr" target="#b20">[21]</ref>. For all experiments, we train our model from scratch without any level of pre-training or human annotations. The objectives include: (a) concentration loss (Section 3.2 and 3.3), (b) color reconstruction loss and (c) orthogonal regularization (Section 3.3). Involving the localization module from the beginning in the training process prevents the network from converging because poor localization makes matching impossible. Thus we first train our network using patches cropped at the same location with the same size in the reference and target frame respectively. Fine-grained matching is conducted between the two patches for 10 epochs. We then jointly train the localization and matching module for another 10 epochs.</p><p>Inference. In the inference stage, we directly apply the affinity learned to transform color feature representations, on different types of inputs, e.g., segmentation masks and keypoint maps. We use the same testing protocol as Wang et al. <ref type="bibr" target="#b50">[51]</ref> for all tasks. Similar to <ref type="bibr" target="#b50">[51]</ref>, we adopt a recurrent inference strategy by propagating the ground truth segmentation mask or keypoint heatmap from the first frame, as well as the predicted results from the preceding k frames onto the target frame. We average all k + 1 predictions to obtain the final propagated map (k is 1 for the VIP, and 7 for all the other tasks).</p><p>To compare with the ResNet-18 trained on the ImageNet with classification labels, we replace our learned network weights with it and leave other settings unchanged for fair comparisons.  <ref type="figure" target="#fig_5">Figure 6</ref>: Visualization of the ablation studies. Given a set of points in the reference frame (a), we visualize the results of propagating these points on to the target frame (b). "L", "C", "O" and "all" correspond to the localization modules, concentration or orthogonal regularization, or all of them (d-g). <ref type="figure" target="#fig_3">Figure 4</ref> (a) and <ref type="figure" target="#fig_4">Figure 5</ref> show the propagated instance masks and <ref type="table" target="#tab_0">Table 1</ref> lists quantitative results of all evaluated methods based on the Jacaard index J (IOU) and contour-based accuracy F. Our model performs favorably against the self-supervised state-of-the-art methods. Specifically, our model outperforms Wang et al. <ref type="bibr" target="#b50">[51]</ref> by 13.3% in J and 16.6% in F. and is even 6.9% better in J and 4.1% better in F than the ResNet-18 model <ref type="bibr" target="#b12">[13]</ref> trained on ImageNet <ref type="bibr" target="#b8">[9]</ref> with classification labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Instance Segmentation Mask Propagation on the DAVIS-2017 dataset</head><p>Furthermore, we demonstrate that by including the localization module during inference, our model can exclude noise from background pixels. Given the instance masks in the first frame, we obtain the bounding box w.r.t. the instance mask and first locate it in the target frame by our localization module. Then, we propagate the instance masks within the bounding box in the reference frame to the localized bounding box in the target frame using our matching module. Since the propagation is carried out within two bounding boxes instead of the entire frames, we can minimize noise introduced by background pixels as shown in <ref type="figure" target="#fig_4">Figure 5 (d)</ref> and (e). The quantitative evaluation of this improved model outperforms the model that does not include the localization module during inference. (see "Ours-track" vs. "Ours" in <ref type="table" target="#tab_0">Table 1</ref>)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Studies on the DAVIS-2017 Dataset</head><p>We carry out ablation studies to see the contributions of each term, as shown in <ref type="figure" target="#fig_5">Figure 6</ref> and <ref type="table" target="#tab_2">Table 2</ref>. Note that inference is conducted between a pair of full-size frames without localization.</p><p>Region-level Localization. Our model trained with the region-level localization module is able to place the individual points all within a reasonable local region ( <ref type="figure" target="#fig_5">Figure 6 (c)</ref>). We show that the model can accurately capture both region-level shifts (e.g., person moving forward), and subtle deformations (e.g., movement of body parts), while preserving the correct spatial relations among all the points. In contrast, the model trained without the localization module tends to model global matching, leading to less accurate preservation of the local spatial relationships among points, e.g., the red points in <ref type="figure" target="#fig_5">Figure 6</ref> (d) tend to cluster together as shown in the cyan circle. Consistent quantitative results can also be found in <ref type="table" target="#tab_2">Table 2</ref> (c), where the J and F measures drop 2.5% and 0.9%, respectively, when trained without the localization module. We also discover that the localization module should always be trained together with the concentration loss to satisfy the assumption in Section 3.2(Table 2(f)(g)).</p><p>Concentration regularization. The concentration regularization encourages locality during the transformation process, i.e. points within a neighbourhood in the reference frame stay together in the target frame. The model trained without it tends to introduce outliers, as shown in the cyan circle of <ref type="table" target="#tab_2">Table 2</ref>: Ablation studies. The minus sign "-" indicates training without the specific module or regularization. "L", "O" and "C" mean the localization module, orthogonal and concentration regularization, respectively. The last column ("(g) -all") shows results of a baseline model trained without any of "L", "O" or "C".   Orthogonal regularization. The orthogonal regularization term enforces points to match back to themselves after a cycle of forward and backward transformation. As shown in <ref type="figure" target="#fig_5">Figure 6</ref> (f), the model trained without the orthogonal regularization term is less effective in preserving local structures. The effectiveness of the orthogonal regularization is also validated quantitatively at <ref type="table" target="#tab_2">Table 2</ref> (e) and (f). We demonstrate that our model learns accurate correspondence by evaluating it on the J-HMDB dataset <ref type="bibr" target="#b18">[19]</ref>, which requires precise matching of points compared to the coarser propagation of masks. Given the 15 ground truth human pose keypoints in the first frame, we propagate them to the remaining frames. We quantitatively evaluate performance using the probability of correct keypoint (PCK) metric <ref type="bibr" target="#b55">[56]</ref>, which measures the ratio of joints that fall within a threshold distance from the ground truth joint locations. We show quantitative evaluations against the state-of-the-art methods in <ref type="table" target="#tab_5">Table 5</ref> and qualitative propagation results in <ref type="figure" target="#fig_3">Figure 4</ref>(b). Our model performs well versus all self-supervised methods <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b43">44]</ref> and notably achieves better results than ResNet-18 <ref type="bibr" target="#b12">[13]</ref> trained with classification labels <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Tracking Pose Keypoint Propagation on the J-HMDB Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Visual Tracking on the OTB Dataset</head><p>Other than the tasks that require dense matching, e.g., segmentation or keypoints propagation, the features learned by our model can be applied to object matching tasks such as visual tracking, because of its capability of localizing an object or a relatively global region. Without any fine-tuning, we directly integrate our network trained via self-supervision into a classic tracking framework <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b35">36]</ref> based on correlation filters, by replacing the Siamese network in <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b35">36]</ref> with our model, while keeping other parts in the tracking framework unchanged. Even without training with a correlation filter, our features are general and robust enough to achieve comparable performance on the OTB2015 dataset <ref type="bibr" target="#b51">[52]</ref> to methods trained with this filter <ref type="bibr" target="#b44">[45]</ref>, as shown in <ref type="table" target="#tab_3">Table 3</ref>. <ref type="figure" target="#fig_3">Figure 4(d)</ref> shows that our learned features are robust against occlusion (left), object scale, as well as illumination changes (right) and can track objects through a long sequence (hundreds of frames in the OTB2015 dataset).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Semantic and Instance Propagation on the VIP Dataset</head><p>We evaluate our method on the VIP dataset <ref type="bibr" target="#b57">[58]</ref>, which includes dense human parts segmentation masks on both the semantic and instance levels. We use the same settings as Wang et al. <ref type="bibr" target="#b50">[51]</ref> and resize the input frames to 560 ? 560. For the semantic propagation task, we propagate the semantic segmentation maps of human parts (e.g., arms and legs) and evaluate performance via the mean IoU metric. For the part instance propagation task, we propagate the instance-level segmentation of human parts (e.g., arms of the first person or legs of the second person) and evaluate performance via the mean average precision of the instance-level human parsing metric <ref type="bibr" target="#b25">[26]</ref>. <ref type="table" target="#tab_4">Table 4</ref> shows that our method performs favourably against all self-supervised methods and notably the ResNet-18 model trained on ImageNet with classification labels for both tasks. <ref type="figure" target="#fig_3">Figure 4</ref>(c) shows sample semantic segmentation propagation results. Interestingly, our model correctly propagates each part mask onto an unseen instance (the woman which does not appear in the first frame) in the second example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this work, we propose to learn correspondences across video frames in a self-supervised manner.</p><p>Our method jointly tackles region-level and pixel-level correspondence learning and allows them to facilitate each other through a shared inter-frame affinity matrix. Experimental results demonstrate the effectiveness of our approach versus the state-of-the-art self-supervised video correspondence learning methods, as well as supervised models such as the ResNet-18 trained on ImageNet with classification labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Texture Propagation</head><p>Propagations Input In <ref type="figure" target="#fig_6">Figure 7</ref>, we show results of texture propagation. Following Wang et al. <ref type="bibr" target="#b50">[51]</ref>, we overlay a texture map on the object in the first video frame, then propagate this texture map across the rest of the video frames. As shown in <ref type="figure" target="#fig_6">Figure 7</ref>, our model is able to preserve the texture well during propagation, this indicates that our model is able to find precise correspondences between video frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Instance Segmentation Propagation on DAVIS-2017</head><p>In <ref type="figure">Figure 8</ref>, we show more instance mask propagation results on the DAVIS-2017 dataset <ref type="bibr" target="#b34">[35]</ref>. Our model is resilient to rapid object shape and scale changes, e.g., the horse, the motorbike and the cart in <ref type="figure">Figure 8</ref>. In <ref type="table" target="#tab_6">Table 6</ref>, we demonstrate more comparisons with state-of-the-art methods. We use the full 480p images during inference for our model. For fair comparisons we test the model by Wang et al. <ref type="bibr" target="#b50">[51]</ref> with the resolution of 480p, in addition to the result reported using 400 ? 400 images.</p><p>In <ref type="figure">Figure 9</ref>, we visualize the process of including the localization module during inference. Given the instance mask of the first frame, we first propagate each point (marked as green) from the reference frame to the target frame by localizing a bbox on it before matching. Instead of directly applying the center as described in Section 3.2 in the paper, we refine the center at inference by applying the mean-shift algorithm, i.e.,</p><formula xml:id="formula_9">C t = N i=1 K(l i ? C t?1 )l i N i=1 K(l i ? C t?1 )<label>(9)</label></formula><p>where l i is the coordinate of the i th pixel, the C is the center of all l i at the t th iteration, and K(a ? b) = e a?b <ref type="bibr" target="#b1">2</ref> . Scale is estimated via Eq.(4) as well, see the bboxes in <ref type="figure">Figure 9</ref>. The green points in <ref type="figure">Figure 9</ref> illustrate the individually propagated points and the red bounding box indicates the estimated bounding box of an object in the target frame. We then propagate the instance segmentation mask within the bounding box in the reference frame to the bounding box in the target frame.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Main steps of proposed method. Blue grids represent the reference-patch p1's and target-frame f2's feature maps that are shared by the region-level localization (left box) and fine-grained matching (right box) modules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Concentration (left) and orthogonal (right) regularization. The dots denote pixels in feature space. The orange arrows show how they push the pixels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of the propagation results. (a) Instance mask propagation on the DAVIS-2017 [35] dataset. (b) Pose keypoints propagation on the J-HMDB [19] dataset. (c) Parts segmentation propagation on the VIP [58] dataset. (d) Visual tracking on the OTB2015 [52] dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative comparison with other methods. (a) Reference frame with instance masks. (b) Results by the ResNet-18 trained on ImageNet. (c) Results by Wang et al. [51]. (d) Ours (global matching). (e) Ours with localization during inference. (f) Target frame with ground truth instance masks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 (</head><label>6</label><figDesc>e).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Texture Propagation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Instance mask propagation results.PropagationsInput Visualization of the process of including the localization module during inference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Evaluation of instance segmentation propagation on the DAVIS-2017 dataset<ref type="bibr" target="#b34">[35]</ref>. A more comprehensive comparison can be found in the supplementary.</figDesc><table><row><cell cols="2">Model</cell><cell>Supervised</cell><cell>Dataset</cell><cell></cell><cell>J (Mean)</cell><cell>J (Recall)</cell><cell>F (Mean)</cell><cell>F (Recall)</cell></row><row><cell cols="2">SIFT Flow [29]</cell><cell>?</cell><cell>-</cell><cell></cell><cell>33.0</cell><cell>-</cell><cell>35.0</cell><cell>-</cell></row><row><cell cols="2">DeepCluster [6]</cell><cell>?</cell><cell cols="2">YFCC100M [42]</cell><cell>37.5</cell><cell>-</cell><cell>33.2</cell><cell>-</cell></row><row><cell cols="2">Transitive Inv [50]</cell><cell>?</cell><cell>-</cell><cell></cell><cell>32.0</cell><cell>-</cell><cell>26.8</cell><cell>-</cell></row><row><cell cols="2">Vondrick et al. [44]</cell><cell>?</cell><cell cols="2">Kinetics [21]</cell><cell>34.6</cell><cell>34.1</cell><cell>32.7</cell><cell>26.8</cell></row><row><cell cols="2">Wang et al. [51]</cell><cell>?</cell><cell cols="2">VLOG [11]</cell><cell>43.0</cell><cell>43.7</cell><cell>42.6</cell><cell>41.3</cell></row><row><cell></cell><cell>Ours</cell><cell>?</cell><cell cols="2">Kinetics [21]</cell><cell>56.3</cell><cell>65.0</cell><cell>59.2</cell><cell>64.1</cell></row><row><cell cols="2">Ours-track</cell><cell>?</cell><cell cols="2">Kinetics [21]</cell><cell>57.7</cell><cell>68.3</cell><cell>61.3</cell><cell>69.8</cell></row><row><cell cols="2">ResNet-18(3 blocks)</cell><cell></cell><cell cols="2">ImageNet [9]</cell><cell>49.4</cell><cell>52.9</cell><cell>55.1</cell><cell>56.6</cell></row><row><cell cols="2">ResNet-18(4 blocks)</cell><cell></cell><cell cols="2">ImageNet [9]</cell><cell>40.2</cell><cell>36.1</cell><cell>42.5</cell><cell>36.6</cell></row><row><cell cols="2">OSVOS [5]</cell><cell></cell><cell cols="2">ImageNet,DAVIS [35]</cell><cell>56.6</cell><cell>63.8</cell><cell>63.9</cell><cell>73.8</cell></row><row><cell>(a) Reference frame</cell><cell cols="2">(b) Target frame</cell><cell>(c) Ours</cell><cell>(d) w/o L</cell><cell cols="2">(e) w/o C</cell><cell>(f) w/o O</cell><cell>(g) w/o all</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>(b)(e) demonstrate the contribution of this concentration regularization term, e.g., compared to (b), the J in (e) decrease by 8% without this regularization term.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Tracking results on OTB2015<ref type="bibr" target="#b51">[52]</ref> </figDesc><table><row><cell>Model</cell><cell>Supervised</cell><cell>AUC score (%)</cell></row><row><cell>UDT [45]</cell><cell>?</cell><cell>59.4</cell></row><row><cell>Ours</cell><cell>?</cell><cell>59.2</cell></row><row><cell>ResNet-18</cell><cell></cell><cell>55.6</cell></row><row><cell>Fully Supervised [2]</cell><cell></cell><cell>58.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Segmentation propagation on VIP<ref type="bibr" target="#b57">[58]</ref>.</figDesc><table><row><cell>Model</cell><cell>Supervised</cell><cell>mIoU</cell><cell>AP r vol</cell></row><row><cell>DeepCluster. [6]</cell><cell>?</cell><cell>21.8</cell><cell>8.1</cell></row><row><cell>Wang et al. [51]</cell><cell>?</cell><cell>28.9</cell><cell>15.6</cell></row><row><cell>Ours</cell><cell>?</cell><cell>34.1</cell><cell>17.7</cell></row><row><cell>ResNet-18</cell><cell></cell><cell>31.8</cell><cell>12.6</cell></row><row><cell>Fully Supervised [37]</cell><cell></cell><cell>37.9</cell><cell>24.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Kepoints propagation on J-HMDB<ref type="bibr" target="#b18">[19]</ref>.</figDesc><table><row><cell>Model</cell><cell>Supervised</cell><cell>PCK@.1</cell><cell>PCK@.2</cell></row><row><cell>Vondrick et al. [44]</cell><cell>?</cell><cell>45.2</cell><cell>69.6</cell></row><row><cell>Wang et al. [51]</cell><cell>?</cell><cell>57.3</cell><cell>78.1</cell></row><row><cell>Ours</cell><cell>?</cell><cell>58.6</cell><cell>79.8</cell></row><row><cell>ResNet-18</cell><cell></cell><cell>53.8</cell><cell>74.6</cell></row><row><cell>Fully Supervised [55]</cell><cell></cell><cell>68.7</cell><cell>92.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Evaluation of instance segmentation propagation on the DAVIS-2017 dataset<ref type="bibr" target="#b34">[35]</ref>.</figDesc><table><row><cell>Model</cell><cell>Supervised</cell><cell>Dataset</cell><cell>J (Mean)</cell><cell>J (Recall)</cell><cell>F (Mean)</cell><cell>F (Recall)</cell></row><row><cell>SIFT Flow [29]</cell><cell>?</cell><cell>-</cell><cell>33.0</cell><cell>-</cell><cell>35.0</cell><cell>-</cell></row><row><cell>DeepCluster [6]</cell><cell>?</cell><cell>YFCC100M [42]</cell><cell>37.5</cell><cell>-</cell><cell>33.2</cell><cell>-</cell></row><row><cell>Transitive Inv [50]</cell><cell>?</cell><cell>-</cell><cell>32.0</cell><cell>-</cell><cell>26.8</cell><cell>-</cell></row><row><cell>Vondrick et al. [44]</cell><cell>?</cell><cell>Kinetics [21]</cell><cell>34.6</cell><cell>34.1</cell><cell>32.7</cell><cell>26.8</cell></row><row><cell>Wang et al. [51] (400 ? 400)</cell><cell>?</cell><cell>VLOG [11]</cell><cell>43.0</cell><cell>43.7</cell><cell>42.6</cell><cell>41.3</cell></row><row><cell>Wang et al. [51] (480p)</cell><cell>?</cell><cell>VLOG [11]</cell><cell>46.4</cell><cell>50.1</cell><cell>50.0</cell><cell>48.0</cell></row><row><cell>mgPFF [23]</cell><cell>?</cell><cell>-</cell><cell>42.2</cell><cell>41.8</cell><cell>46.9</cell><cell>44.4</cell></row><row><cell>Lai et al. [24]</cell><cell>?</cell><cell>Kinetics [21]</cell><cell>47.7</cell><cell>-</cell><cell>51.3</cell><cell>-</cell></row><row><cell>ours</cell><cell>?</cell><cell>Kinetics [21]</cell><cell>56.8</cell><cell>65.7</cell><cell>59.5</cell><cell>65.1</cell></row><row><cell>ours-track</cell><cell>?</cell><cell>Kinetics [21]</cell><cell>57.7</cell><cell>67.1</cell><cell>60.0</cell><cell>65.7</cell></row><row><cell>ResNet-18(3 blocks)</cell><cell></cell><cell>ImageNet [9]</cell><cell>49.4</cell><cell>52.9</cell><cell>55.1</cell><cell>56.6</cell></row><row><cell>ResNet-18(4 blocks)</cell><cell></cell><cell>ImageNet [9]</cell><cell>40.2</cell><cell>36.1</cell><cell>42.5</cell><cell>36.6</cell></row><row><cell>SiamMask [49]</cell><cell></cell><cell>YouTube-VOS [53]</cell><cell>54.3</cell><cell>62.8</cell><cell>58.5</cell><cell>67.5</cell></row><row><cell>OSVOS [5]</cell><cell></cell><cell>ImageNet,DAVIS [35]</cell><cell>56.6</cell><cell>63.8</cell><cell>63.9</cell><cell>73.8</cell></row><row><cell>Input</cell><cell></cell><cell></cell><cell>Propagations</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Implementation</head><p>We train our model using Adam <ref type="bibr" target="#b21">[22]</ref> as the optimizer with a learning rate of 10 ?4 for the warm-up and 0.5 ? 10 ?4 for the joint training of the localization and matching modules. We set the temperature in the softmax layer applied to the top layer CNN features to 1. For fair comparisons, we also use the k-NN propagation schema as Wang et al. <ref type="bibr" target="#b50">[51]</ref> and set k = 5 for all tasks.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">People-tracking-by-detection and people-detection-by-tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.09549</idno>
		<title level="m">Fully-convolutional siamese networks for object tracking</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Large displacement optical flow: descriptor matching in variational motion estimation. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Attentional correlation filter network for adaptive visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Demiris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Young</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Real-time tracking of non-rigid objects using mean shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>H?usser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Haz?rba?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">From lifestyle vlogs to everyday interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">High-speed tracking with kernelized correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scops: Self-supervised co-part segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sfnet: Learning object-aware semantic flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P B H J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neurips</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Towards understanding action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Tracking-learning-detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01693</idno>
		<title level="m">Multigrid predictive filter flow for unsupervised learning on videos</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Self-supervised learning for video correspondence flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00875</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">High performance visual tracking with siamese region proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.03612</idno>
		<title level="m">Holistic, instance-level human parsing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A scale adaptive kernel correlation filter tracker with feature integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Sift flow: Dense correspondence across scenes and its applications. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Switchable temporal propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">De</forename><surname>Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An iterative image registration technique with an application to stereo vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DARPA Image Understanding Workshop</title>
		<meeting><address><addrLine>Vancouver, British Columbia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1981" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hierarchical convolutional features for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Long-term correlation tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A multiple-baseline stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Dcfnet: Discriminant correlation filters network for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">X M Z W H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04057</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adaptive temporal encoding network for video instance-level human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G L L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM International Conference on Multimedia (ACM MM)</title>
		<meeting>of ACM International Conference on Multimedia (ACM MM)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">End-to-end weakly-supervised semantic alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Towards longer long-range motion trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Siamese instance search for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.01817</idno>
		<title level="m">Yfcc100m: The new data in multimedia research</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">End-to-end representation learning for correlation filter based tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Tracking emerges by colorizing videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unsupervised deep tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning a deep compact image representation for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neurips</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Dcfnet: Discriminant correlation filters network for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04057</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning attentions: residual attentional siamese network for high performance online visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Fast online object tracking and segmentation: A unifying approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Transitive invariance for self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning correspondence from the cycle-consistency of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">Object tracking benchmark. TPAMI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.03327</idno>
		<title level="m">Youtube-vos: A large-scale video object segmentation benchmark</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Efficient mean-shift tracking via a new similarity measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Duraiswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Efficient video object segmentation via network modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Articulated human detection with flexible mixtures of parts. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Unsupervised discovery of object landmarks as structural representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Adaptive temporal encoding network for video instance-level human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.00661</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
