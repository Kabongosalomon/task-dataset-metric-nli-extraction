<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Referring Image Matting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizhizi</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
							<email>jing.zhang1@sydney.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
							<email>dacheng.tao@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">JD Explore Academy</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Referring Image Matting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image matting refers to extracting the accurate foregrounds in the image. Current automatic methods tend to extract all the salient objects in the image indiscriminately. In this paper, we propose a new task named Referring Image Matting (RIM), referring to extracting the meticulous alpha matte of the specific object that can best match the given natural language description. However, prevalent visual grounding methods are all limited to the segmentation level, probably due to the lack of high-quality datasets for RIM. To fill the gap, we establish the first large-scale challenging dataset RefMatte by designing a comprehensive image composition and expression generation engine to produce synthetic images on top of current public high-quality matting foregrounds with flexible logics and re-labelled diverse attributes. RefMatte consists of 230 object categories, 47,500 images, 118,749 expression-region entities, and 474,996 expressions, which can be further extended easily in the future. Besides this, we also construct a realworld test set with manually generated phrase annotations consisting of 100 natural images to further evaluate the generalization of RIM models. We first define the task of RIM in two settings, i.e., prompt-based and expression-based, and then benchmark several representative methods together with specific model designs for image matting. The results provide empirical insights into the limitations of existing methods as well as possible solutions. We believe the new task RIM along with the RefMatte dataset will open new research directions in this area and facilitate future studies. The dataset and code will be made publicly available at https://github.com/JizhiziLi/RIM. Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Image matting refers to extracting the soft alpha matte of the foreground in natural images, which is beneficial for various downstream applications such as video conferences, advertisement production, and e-Commerce promotion <ref type="bibr" target="#b48">[49]</ref>. Typical matting methods can be divided into two groups: 1) the methods based on auxiliary inputs, e.g., trimap <ref type="bibr">[13; 1]</ref>, and 2) automatic matting methods that can extract the foreground without any human intervention <ref type="bibr">[15; 16; 14]</ref>. However, the former ones are not applicable for automatic application scenarios, and the latter ones are generally limited to specific object categories, e.g., human <ref type="bibr">[2; 48; 26]</ref>, animal <ref type="bibr" target="#b14">[15]</ref>, or all the salient objects <ref type="bibr">[50; 33]</ref>. It is still unexplored to carry out controllable image matting on arbitrary objects, i.e., extracting the alpha matte of the specific object that can best match the given natural language description.</p><p>Language-driven tasks such as referring expression segmentation (RES) <ref type="bibr" target="#b45">[46]</ref>, referring image segmentation (RIS) <ref type="bibr">[8; 45; 20]</ref>, visual question answering (VQA) <ref type="bibr" target="#b3">[4]</ref>, and referring expression comprehension (REC) <ref type="bibr" target="#b23">[24]</ref>, have been widely explored. Great progress in the areas has been made based on many datasets like ReferIt <ref type="bibr" target="#b9">[10]</ref>, Google RefExp <ref type="bibr" target="#b26">[27]</ref>, RefCOCO <ref type="bibr" target="#b46">[47]</ref>, VGPhraseCut <ref type="bibr" target="#b39">[40]</ref>, and Cops-Ref <ref type="bibr" target="#b2">[3]</ref>. For example, RES methods aim to segment arbitrary object indicated by the natural language description. Nevertheless, the obtained masks are limited to the segmentation level without fine details, due  to the low-resolution images and coarse mask annotations in the datasets. Therefore, it is impossible for them to be used in the scenarios that require a meticulous alpha matte of the foreground object.</p><p>To fill this gap, we propose a new task named Referring Image Matting (RIM) in this paper. RIM refers to extracting the specific foreground object in the image that best matches the given natural language description together with a meticulous high-quality alpha matte. Different from the tasks that were solved by the two kinds above-mentioned matting methods, RIM aims for controllable image matting on arbitrary objects in the image indicated by language description. It is of practical significance in industrial application domains and opens up new research directions in academia community. To facilitate the study of RIM, we establish the first dataset named RefMatte, which consists of 230 object categories, 47,500 images, and 118,749 expression-region entities together with the corresponding high-quality alpha mattes and 474,996 expressions. Specifically, to build up this dataset, we first revisit a lot of prevalent public matting datasets like AM-2k <ref type="bibr" target="#b14">[15]</ref>, P3M-10k <ref type="bibr" target="#b13">[14]</ref>, AIM-500 <ref type="bibr" target="#b15">[16]</ref>, SIM <ref type="bibr" target="#b36">[37]</ref> and manually label the category of every object (a.k.a. entity) carefully. We also adopt multiple deep learning-based pre-trained models <ref type="bibr">[21; 41]</ref> to generate various attributes for each entity, such as gender, age, and clothes type of human. Then, we design a comprehensive composition and expression generation engine to generate the synthetic images with reasonable absolute and relative positions considering other foreground objects. Finally, we present several expression logic forms to generate varying language descriptions by making use of the rich visual attributes. In addition, we propose a real-world test set RefMatte-RW100 with 100 images containing diverse objects and human-annotated expressions, which is used to evaluate the generalization ability of RIM methods. Some examples are shown in <ref type="figure" target="#fig_0">Figure 1</ref> and <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>To provide fair and comprehensive evaluation of the state-of-the-art methods in relevant tasks, we benchmark them on RefMatte under two different settings, i.e., the prompt-based setting and expression-based setting, depending on the form of language descriptions. Since the representative methods are particularly designed for segmentation tasks <ref type="bibr">[23; 9]</ref>, there are still gaps when directly applying them on the RIM task. To address this issue, we propose two strategies to customize them for RIM, i.e., 1) we carefully design a light-weight matting head on top of CLIPSeg named CLIPMat to generate high-quality alpha matte results while keeping its end-to-end trainable pipeline; and 2) we provide several separate coarse map-based matting methods to serve as post refiners to further improve the segmentation/matting results. Extensive experimental results 1) show the value of the proposed RefMatte dataset for the research of the RIM task, 2) identify the important role of the form of language descriptions; and 3) validate the effectiveness of the proposed customization strategies. <ref type="figure" target="#fig_0">Figure 1</ref> and <ref type="figure" target="#fig_1">Figure 2</ref> show some results of the proposed CLIPMat given prompt and expression.</p><p>The main contribution of this study is three-fold. 1) We define a new task named RIM, aiming at identifying and extracting the alpha matte of the specific foreground object that best matches the given natural language description; 2) We establish the first large-scale dataset RefMatte, consisting of 47,500 images and 118,749 expression-region entities with high-quality alpha mattes and flowery expressions; 3) We benchmark representative state-of-the-art methods with two customization strategies for RIM on RefMatte under two different settings and gain some useful insights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Image matting Image matting is a fundamental vision task that is essential for various downstream applications. Previous image matting methods are divided into two groups depending on whether or not they use auxiliary user inputs. In the first group, the methods use a three-class trimap <ref type="bibr">[43; 18]</ref>, or a background image from the same scene <ref type="bibr" target="#b18">[19]</ref>, or a coarse map <ref type="bibr" target="#b47">[48]</ref> as the auxiliary input to provide guidance for estimating the alpha matte. In the second group, the methods <ref type="bibr">[33; 50; 2; 15; 16; 11; 14]</ref> extract the foreground objects automatically without any manual efforts involved. Recently, there are also some works making effort on controlling the matting process by determining which objects can be extracted. For example, Xu et al. propose a matting method <ref type="bibr" target="#b41">[42]</ref> to extract the foreground human and all related objects automatically for human-object interaction. Sun et al. propose to extract each foreground human instance separately rather than extracting all of them indiscriminately <ref type="bibr" target="#b37">[38]</ref>. However, it is still unexplored for controllable image matting by using natural language description as guidance to extract specific foreground object that best matches the input text, although it is an efficient and flexible way for the matting model to interact with a human. In this paper, we fill this gap by proposing the RIM task, RefMatte dataset, and effective solutions.</p><p>Matting datasets Many matting datasets have been proposed to advance the progress in the image matting area. Typical matting datasets contain high-resolution images belonging to some specific object categories that have lots of details like hair, accessories, fur, and net as well as transparent objects. For example, the matting datasets proposed by Xu et al. <ref type="bibr" target="#b42">[43]</ref>, Qiao et al <ref type="bibr" target="#b32">[33]</ref>, Sun et al. <ref type="bibr" target="#b36">[37]</ref>, and Li et al. <ref type="bibr" target="#b15">[16]</ref> contain many different categories of objects including human, animal, car, plastic bag and plant. Besides, there are some other matting datasets focusing on a specific category of object, e.g., humans in P3M-10K <ref type="bibr" target="#b13">[14]</ref> and animals in AM-2K <ref type="bibr" target="#b14">[15]</ref>. In addition to the foreground objects, background images are also useful for generating abundant composite images. For example, Li et al. <ref type="bibr" target="#b14">[15]</ref> propose a large-scale background dataset containing 20k high-resolution and diverse background images, which are helpful to reduce the domain gap between composite images and natural images. All the above datasets have open licenses and can be used to further construct customized matting datasets, e.g., the proposed RefMatte dataset.</p><p>Vision-language tasks and methods Vision-Language tasks such as RIS <ref type="bibr" target="#b7">[8]</ref>, RES <ref type="bibr" target="#b45">[46]</ref>, REC <ref type="bibr" target="#b24">[25]</ref>, text-driven manipulation <ref type="bibr" target="#b29">[30]</ref>, and text-to-image generation <ref type="bibr">[32; 31; 35]</ref>, have been widely explored and are useful for lots of applications like interactive image editing. Among them, RIS aims to segment the target object given language expression is most related to our work. RIS methods can be divided into single-stage ones <ref type="bibr">[20; 17; 23; 39; 36]</ref> and two-stage ones <ref type="bibr">[9; 46; 7; 22]</ref>. The former ones directly train a segmentation network by leveraging the pre-trained models like CLIP <ref type="bibr" target="#b33">[34]</ref>, while the latter ones perform region proposal and segmentation sequentially. However, they usually generate low-resolution and coarse object masks due to the task setting (i.e., for segmentation rather than matting) and the lack of high-quality annotations (e.g., alpha mattes) <ref type="bibr">[10; 27; 47; 40]</ref>. By contrast, we propose the RIM task along with the RefMatte dataset to facilitate the research of generating alpha matte of the foreground object described by natural language. Moreover, we customize the above RIS methods for RIM, benchmark their performances on RefMatte, and discuss the limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The RefMatte dataset</head><p>In this section, we present the overview pipeline of constructing RefMatte (Sec. 3.1 and Sec. 3.2) along with the task settings (Sec. 3.3) and statistics of the dataset (Sec. <ref type="bibr">3.5)</ref>. Some examples from RefMatte are shown in <ref type="figure">Figure.</ref> 3. Furthermore, we also construct a real-world test set consisting of 100 natural images with manually labelled annotations of flowery language descriptions (Sec. 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preparation of matting entities</head><p>To prepare enough high-quality matting entities to help construct our RefMatte dataset, we revisit currently available matting datasets to filter out foregrounds that satisfy our requirements. We then manually label the categories for all the candidate entities and annotate their attributes leveraging multiple deep learning-based pre-trained models <ref type="bibr">[21; 41]</ref>. We present the details as follows.</p><p>Pre-processing and filtering Due to the nature of the image matting task, all the candidate entities should be in high-resolution, with clear and fine details in the alpha matte. Moreover, the data should be publicly available with open licences and without privacy concerns to facilitate future research. With regard to these requirements, we adopt all the foreground images from AM-2k <ref type="bibr" target="#b14">[15]</ref>, P3M-10k <ref type="bibr" target="#b13">[14]</ref>, and AIM-500 <ref type="bibr" target="#b15">[16]</ref>. Specifically, for P3M-10k, we filter out the images with more than two adhesive foreground instances to make sure each entity is related to only one foreground instance. For other available datasets like SIM <ref type="bibr" target="#b36">[37]</ref>, DIM <ref type="bibr" target="#b42">[43]</ref>, and HATT <ref type="bibr" target="#b32">[33]</ref>, we filter out those foreground images with identifiable faces in human instances. We also filter out those foreground images in low-resolution or having low-quality alpha mattes. The final number of the entities is 13,187 in total, whose details could be found in the supplemental material. For the background images used in the following composition step, we choose all the images in BG-20k <ref type="bibr" target="#b14">[15]</ref>.</p><p>Annotate the category names of entities Since previous automatic matting methods tend to extract out all the salient foreground objects from the image, they do not provide the specific (category) name for each entity. However, for the RIM task, we need the entity name to describe it. Following <ref type="bibr" target="#b28">[29]</ref>, we label the entry-level category name for each entity, which stands for the most common used name for the specific entity by people. Here, we adopt a semi-automatic strategy. Specifically, we use the Mask RCNN detector <ref type="bibr" target="#b4">[5]</ref> with a ResNet-50-FPN <ref type="bibr" target="#b5">[6]</ref> backbone from <ref type="bibr" target="#b40">[41]</ref> to automatically detect and label the category names for each foreground instance and then manually check and correct them. In total, we have 230 categories in RefMatte. Furthermore, we adopt WordNet <ref type="bibr" target="#b27">[28]</ref> to generate synonyms for each category name to enhance the diversity. We manually check the synonyms and replace some of them with more reasonable ones. The details are presented in the supplemental material.</p><p>Annotate the attributes of entities To ensure all the entities having rich visual properties to support forming abundant expressions, we annotate all entities with several attributes such as color for all entities; gender, age, and clothes type for the human entities. We also adopt a semi-automatic strategy to generate such attributes. For generating color, we cluster all the pixel values of the foreground image, find out the most frequent value, and match it with the specific color in webcolors. For gender and age, we adopt the pre-trained models provided by Levi et al. in <ref type="bibr" target="#b11">[12]</ref>, and follow the common sense to define the age group based on the predicted ages. For clothes type, we adopt the pre-trained model provided by Liu et al. in <ref type="bibr" target="#b20">[21]</ref>. Furthermore, motivated by the categorization of foreground types in <ref type="bibr" target="#b15">[16]</ref>, we add the attributes of salient or not as well as transparent or not for all entities, since such attributes are also important in the image matting task. In summary, we have at least 3 attributes for each entity and 6 attributes for human entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Image composition and expression generation</head><p>Based on the collected matting entities in the previous section, we propose an image composition engine and expression generation engine to construct our RefMatte dataset. How to arrange different entities to form reasonable composite images while generating semantically clear, grammatically correct, abundant and fancy expressions to describe the entities in these composite images is the key of constructing RefMatte, which is also challenging. To this end, we define six types of position relationships for arranging different entities in a composite image and leverage diverse logic forms to produce appropriate expressions. We present the details as follows.</p><p>Image composition engine To keep the high resolution of entities while arranging them with reasonable position relationships, we adopt two or three entities for each composite image. We define six types of position relationships as: left, right, top, bottom, in front of, and behind. For each relationship, we generate the foreground images by <ref type="bibr" target="#b12">[13]</ref> and composite them with the background image from BG-20k <ref type="bibr" target="#b14">[15]</ref> via alpha blending. Specifically, for the relationships of left, right, top, and bottom, we ensure there are no occlusions in the foreground instances to preserve their details. For the relationships of in front of and behind, we simulate occlusions between the foreground instances by adjusting their relative positions. We prepare a bag of candidate words to denote each relationship. The details are provided in the supplemental material. Some examples are shown in <ref type="figure" target="#fig_2">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Expression generation engine</head><p>To provide abundant expressions for the entities in the composite images, we define three types of expressions for each entity from the aspect of different logic forms defined as follows, where &lt;att i &gt; stands for the attribute, &lt;obj i &gt; stands for the category name, and &lt;rel i &gt; stands for the relationship between the reference entity and the related one:</p><p>1. Basic expression This is the expression that describes the target entity with as many attributes as one can, e.g, the/a &lt;att 0 &gt; &lt;att 1 &gt;...&lt;obj 0 &gt; or the/a &lt;obj 0 &gt; which/that is &lt;att 0 &gt; &lt;att 1 &gt;, and &lt;att 2 &gt;. For example, as shown in <ref type="figure" target="#fig_2">Figure 3</ref>(a), the basic expression for the flower entity is 'the lightpink and salient flower'; 2. Absolute position expression This is the expression that describes the target entity with many attributes and its absolute position in the image, e.g., the/a &lt;att 0 &gt; &lt;att 1 &gt;...&lt;obj 0 &gt; &lt;rel 0 &gt; the photo/image/picture or the/a &lt;obj 0 &gt; which/that is &lt;att 0 &gt; &lt;att 1 &gt; &lt;rel 0 &gt; the photo/image/picture. For example, as shown in <ref type="figure" target="#fig_2">Figure 3</ref>(a), the absolute position expression for the flower is 'the plant which is lightpink and salient at the rightmost edge of the picture'; 3. Relative position expression This is the expression that describes the target entity with many attributes and its relative position with another entity, e.g., the/a &lt;att 0 &gt; &lt;att 1 &gt;...&lt;obj 0 &gt; &lt;rel 0 &gt; the/a &lt;att 2 &gt; &lt;att 3 &gt;...&lt;obj 1 &gt; or the/a &lt;obj 0 &gt; which/that is &lt;att 0 &gt; &lt;att 1 &gt; &lt;rel 0 &gt; the/a &lt;obj 1 &gt; which/that is &lt;att 2 &gt; &lt;att 3 &gt;. For example, as shown in <ref type="figure" target="#fig_2">Figure 3</ref>(a), the relative position expression for the flower is 'the flower which is lightpink at the right side of the cat which is dimgray and non-transparent'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dataset split and task settings</head><p>We have 13,187 matting entities in total and split out <ref type="bibr" target="#b10">11</ref> Task settings To benchmark RIM methods given different forms of language descriptions, we set up two settings in RefMatte. We present their details as follows:</p><p>1. Prompt-based setting The text description in this setting is a prompt, which is the entry-level category name of the entity, e.g., the prompt in <ref type="figure" target="#fig_2">Figure 3</ref> is flower, human, and alpaca, respectively; 2. Expression-based setting The text description in this setting is the generated expression in previous section, chosen from the basic expressions, absolute position expressions, and relative position expressions. Some examples can also be seen from <ref type="figure" target="#fig_2">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Real-world test set</head><p>Since RefMatte is built upon composites images, there may exist a domain gap between them and real-world images. To investigate the generalization ability of RIM models trained on it to real-world images, we further establish a real-world test set name RefMatte-RW100, which consists of 100 realworld high-resolution images with 2 to 3 entities in each image. We then annotate their expressions following the same three settings in Sec. 3.2. In addition, we add a free expression to the annotations. For the high-quality alpha matte labels, we generate them using image editing softwares, e.g., Adobe Photoshop and GIMP. Some examples from RefMatte-RW100 are shown in <ref type="figure" target="#fig_1">Figure 2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Statistics of the RefMatte dataset and RefMatte-RW100 test set</head><p>We calculate the statistics of the RefMatte dataset and RefMatte-RW100 test set in <ref type="table" target="#tab_1">Table 1</ref>. For the prompt-based setting, since the text description is the entry-level category name, we remove the images with multiple entities belonging to the same category to avoid ambiguous reasoning. Consequently, we have 30,391 images in the training set and 1,602 images in the test set in this setting. The number of alpah mattes, text descriptions, categories, attributes, and relationships are shown in the following columns, respectively. The average text length in the prompt-based setting is about 1, since there is usually a single word for each category, while it is much larger in the expression-based setting, i.e., about 16.8 in RefMatte and 12 in RefMatte-RW100. We also generate the wordcloud of the prompts, attributes and relationships in RefMatte in <ref type="figure" target="#fig_3">Figure 4</ref>.</p><p>As can be seen, the dataset has a large portion of human and animals since they are very common in the image matting task. The most frequent attributes in RefMatte are male, gray, transparent, and salient, while the relationship words are more balanced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment settings</head><p>Implementation details Since RIM is a totally new task that has not been explored before, we choose to benchmark state-of-the-art methods from relevant tasks including RIS and RES. Specifically, we benchmark two typical methods CLIPSeg <ref type="bibr" target="#b22">[23]</ref> and MDETR <ref type="bibr" target="#b8">[9]</ref> that represent two main streams of RIS and RES tasks. We use the code provided by the authors and made slight modifications to customize them for the RIM task as described later. We train them on the RefMatte training set with the weights pre-trained on VGPhraseCut <ref type="bibr" target="#b39">[40]</ref>. We resize the image to 512 ? 512 during training. The models are trained on NVIDIA A100 GPUs with the batch size and learning rate set following the original papers. For the expression-based setting, we randomly choose one expression from the three types introduced in Sec. 3.2 during training. During test, we also randomly choose one expression for each image. In addition, we report the performance of different methods per expression type.</p><p>Customization strategies Since there is a task discrepancy between RIM and RIS/RES, we find that the results of directly applying RIS/RES methods to RIM are not promising, e.g., the details of alpha mattes are not clear. To address the issue, we propose two strategies to customize them for RIM:</p><p>1. Adding matting head We design light-weight matting heads on top of existing models to generate high-quality alpha mattes while keeping the end-to-end trainable pipeline. Specifically, we design a light version of the matting decoder in GFM <ref type="bibr" target="#b14">[15]</ref> on top of CLIPSeg <ref type="bibr" target="#b22">[23]</ref>, termed CLIPMat; 2. Using matting refiner We adopt separate coarse map-based matting methods as the post refiners to further improve the segmentation/matting results of above-mentioned methods. Specifically, we train GFM <ref type="bibr" target="#b14">[15]</ref> and P3M <ref type="bibr" target="#b13">[14]</ref> with the input of image and coarse map as the matting refiner.</p><p>Evaluation metrics Following the common practice in previous matting methods <ref type="bibr">[43; 15; 14]</ref>, we use the sum of absolute differences (SAD), mean squared error (MSE), and mean absolute difference (MAD) to evaluate the performance of different methods, which are averaged over all the entities in the test set. Moreover, we calculate the average SAD, MSE, and MAD for the entities in each image, and average them over the test set, denoted as SAD-E, MSE-E, MAD-E, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Prompt-based setting</head><p>We evaluate MDETR <ref type="bibr" target="#b8">[9]</ref>, CLIPSeg <ref type="bibr" target="#b22">[23]</ref> and CLIPMat on the prompt-based setting of RefMatte test set, and show the quantitative results in <ref type="table" target="#tab_2">Table 2</ref>. As can be seen, our customized method CLIPMat performs the best compared with MDETR and CLIPSeg, no matter whether the matting refiners are used or not , validating the effectiveness of adding a matting head to customize CLIPSeg for the RIM task. Besides, using either of the two matting refiners can further improve the performance of the three methods. Specifically, P3M <ref type="bibr" target="#b13">[14]</ref> performs slightly better than GFM <ref type="bibr" target="#b14">[15]</ref> due to its effective design for promoting interactions between the decoders and the effective encoder <ref type="bibr" target="#b43">[44]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Expression-based setting</head><p>We also evaluate these three methods under the expression-based setting on the RefMatte test set and RefMatte-RW100, and show the quantitative results in <ref type="table" target="#tab_3">Table 3</ref>. CLIPMat again performs the best on the RefMatte test set owing to its good ability to preserve more details. When testing on RefMatte-RW100, single-stage methods like CLIPSeg and CLIPMat fall behind the two-stage one, i.e., MDETR, probably due to the better ability of the detector of MDETR in understanding cross-modal semantics. Nevertheless, it is noteworthy that the results under the expression-based setting are much worse than those on the prompt-based setting in <ref type="table" target="#tab_2">Table 2</ref>, implying that it is more difficult to infer the correct foreground entity from the long text sentence. Besides, the results on RefMatte-RW100 are worse than those on RefMatte test set, since there is a domain gap between synthetic images and real-world ones as also shown in <ref type="bibr" target="#b14">[15]</ref>. More efforts could be made to reduce the domain gap and improve the generalization ability of RIM models trained on synthetic images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Impact of input texts</head><p>Impact of prompt templates To investigate the impact of the input forms of prompts, we evaluate the performance of different prompt templates. In addition to the traditional template used in <ref type="bibr" target="#b33">[34]</ref>, we add more templates that are specifically designed for the image matting task, e.g., the foreground/mask/alpha matte of &lt;entity name&gt;. The results are shown in <ref type="table" target="#tab_4">Table 4</ref>. We can see that the performance of CLIPMat given different prompts varies a lot, where the basic &lt;entity name&gt; template delivers the best result while the mask of &lt;entity name&gt; leads to the worst. We suspect it is because other forms of prompts beyond &lt;entity name&gt; have not been seen during training. More efforts on prompt augmentation could be explored in future work.</p><p>Impact of different expressions Since we have introduced different types of expressions in our task, it is interesting to see the influence of each type on the matting performance. As shown in <ref type="table" target="#tab_5">Table 5</ref>, we test the best performed model CLIPMat on the RefMatte test set and the model MDETR on RefMatte-RW100. As can be seen, the absolute position expression is more informative (or easy to understand) than others, leading to the best-performance on both the synthetic test set as well as the real-world one. The performance could be degraded by about 2 or 4 times when using the relative position expression, probably due to the fact that it is more difficult to identify the correct entity when many attributes words and the name of other entities are involved in the expression. Besides, the free expressions which are labeled by human annotators following their preferred style, lead to the worst results, mainly due to the significant diversity of logic, grammar, and words used to describe the entities. More efforts could be made to study the most effective expressions that matter in automatic applications as well as improving the generalization ability of RIM models to deal with diverse expressions in human-machine interaction applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Limitation and discussion</head><p>Based on RefMatte, we benchmark representative methods for the RIM task and obtain useful empirical insights. Despite the large scale of RefMatte and the effectiveness of the proposed baseline  model CLIPMat and post refinement strategy, there are still rooms for improvement. First, more effective end-to-end single-stage model could be further studied since two-stage methods may suffer from the wrong predictions in the first stage as demonstrated in previous matting works <ref type="bibr">[15; 14]</ref>. Second, it also matters to reduce the synthetic-to-real domain gap like RSSN in <ref type="bibr" target="#b14">[15]</ref> to improve the generalization of RIM models trained on synthetic images. Third, More efforts could be made to improve the cross-modal understanding ability to deal with complex and diverse expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose a novel task named referring image matting (RIM) and establish a largescale dataset RefMatte. We customize existing representative methods in relevant tasks for RIM and benchmark their performance through extensive experiments on RefMatte. The results provide useful insights on model design, impact of text descriptions, and domain gap between synthetic and real images. We hope that the RefMatte dataset could serve as a test bed to facilitate future study.</p><p>Social impact The study of RIM can benefit many practical applications such as interactive image editing and human-machine interaction. RefMatte could facilitate the research in this area. However, the synthetic-to-real domain gap may lead to limited generalization to real-world images. Besides, specific attention should be paid to fairness and privacy-preserving during annotating the expressions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Some examples from our RefMatte test set (top) and the results of our CLIPMat given prompt and expression input (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Some examples from our RefMatte-RW100 test set (top) and the results of our CLIP-Mat given expression input (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Some examples from our RefMatte dataset. The first row shows the composite images with different foreground instances while the second row shows the natural language descriptions corresponding to the specific foreground instances indicated by the green dots.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The wordcloud of the prompts (a), attributes (b), and relationships (c) in RefMatte.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>,799 for constructing the training set and 1,388 for the test set. For the training/test split, we try to keep the original split in the source matting datasets except for putting all the long-tailed categories to the training set. However, the categories for the training set and test set are not balanced since most of the entities belong to the human or animal categories. Specifically, among the 11,799 entities in the training set, there are 9,186 human, 1,800 animals and 813 objects. In the test set with 1,388 entities, there are 977 human, 200 animals and 211 objects. To balance the categories, we duplicate the entities to achieve a ratio of 5:1:1 for human:animal:object. Consequently, we have 10,550 human, 2,110 animals, and 2,110 objects in the training set, and 1,055 human, 211 animals and 211 objects in the test set.To generate images for RefMatte, we pick 5 human, 1 animal and 1 object from the training or test split as one group and feed them into the image composition engine. For each group in either train or randomly choose entities and relationships from all candidates to form another 2,800 training images and 390 test images. Finally, we have 45,000 composite images in the training set and 2,500 images in the test set.</figDesc><table /><note>test split, we generate 20 images to form the training set, and 10 images to form the test set. The ratio of left/right:top/bottom:in front of/behind relationships is set to 7:2:1. The number of entities in each image is set to 2 or 3. And for the relationships of front of/behind, we always choose 2 entities to keep the high resolution of each entity. After this process, we have 42,200 training images and 2,110 test images. To further enhance the diversity for the combination of entities, we</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of RefMatte and RefMatte-RW100 in regarding to the number of images, alpha mattes, text descriptions, categories, attributes, relationship words, and average length of texts.</figDesc><table><row><cell>Dataset</cell><cell>Setting</cell><cell>Split</cell><cell>Images Num.</cell><cell>Mattes Num.</cell><cell>Text Num.</cell><cell>Categories Num.</cell><cell>Attrs. Num.</cell><cell>Rels. Num.</cell><cell>Texts Length.</cell></row><row><cell>RefMatte</cell><cell>Prompt Expression</cell><cell cols="4">train 30,391 test 1,602 train 45,000 112,506 449,624 77,849 77,849 4,085 4,085 test 2,500 6,243 24,972</cell><cell>230 66 230 66</cell><cell>--132 102</cell><cell>--31 31</cell><cell>1.06 1.04 16.86 16.80</cell></row><row><cell cols="3">RefMatte-RW100 Expression test</cell><cell>100</cell><cell>221</cell><cell>884</cell><cell>29</cell><cell>135</cell><cell>34</cell><cell>12.01</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on the RefMatte test set under the prompt-based setting.</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell>Refiner</cell><cell>SAD</cell><cell>MSE</cell><cell>MAD</cell><cell cols="2">SAD-E MSE-E MAD-E</cell></row><row><cell></cell><cell></cell><cell>-</cell><cell cols="3">32.27 0.0137 0.0183</cell><cell>33.52</cell><cell>0.0141</cell><cell>0.0190</cell></row><row><cell></cell><cell>MDETR [9]</cell><cell cols="4">GFM [15] 28.15 0.0126 0.0160</cell><cell>29.09</cell><cell>0.0129</cell><cell>0.0165</cell></row><row><cell></cell><cell></cell><cell cols="4">P3M [14] 27.33 0.0123 0.0155</cell><cell>28.22</cell><cell>0.0126</cell><cell>0.0160</cell></row><row><cell>RefMatte test set</cell><cell>CLIPSeg [23]</cell><cell cols="4">-GFM [15] 12.70 0.0045 0.0072 17.75 0.0064 0.0101 P3M [14] 12.17 0.0042 0.0069</cell><cell>18.69 13.34 12.75</cell><cell>0.0067 0.0047 0.0044</cell><cell>0.0106 0.0076 0.0073</cell></row><row><cell></cell><cell></cell><cell>-</cell><cell cols="3">12.95 0.0045 0.0074</cell><cell>13.54</cell><cell>0.0047</cell><cell>0.0077</cell></row><row><cell></cell><cell>CLIPMat</cell><cell cols="4">GFM [15] 11.45 0.0039 0.0065</cell><cell>11.95</cell><cell>0.0040</cell><cell>0.0068</cell></row><row><cell></cell><cell></cell><cell cols="4">P3M [14] 11.05 0.0037 0.0063</cell><cell>11.52</cell><cell>0.0038</cell><cell>0.0066</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results on the RefMatte test set and RefMatte-RW100 under the expression-based setting.</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell>Refiner</cell><cell>SAD</cell><cell>MSE</cell><cell cols="2">MAD SAD-E MSE-E MAD-E</cell></row><row><cell></cell><cell></cell><cell>-</cell><cell cols="3">84.70 0.0434 0.0482</cell><cell>90.45</cell><cell>0.0463</cell><cell>0.0515</cell></row><row><cell></cell><cell>MDETR [9]</cell><cell cols="4">GFM [15] 81.65 0.0429 0.0464</cell><cell>87.14</cell><cell>0.0459</cell><cell>0.0496</cell></row><row><cell></cell><cell></cell><cell>P3M [14]</cell><cell cols="3">80.48 0.0424 0.0458</cell><cell>85.83</cell><cell>0.0452</cell><cell>0.0488</cell></row><row><cell>RefMatte test set</cell><cell>CLIPSeg [23]</cell><cell cols="4">-GFM [15] 64.80 0.0343 0.0369 69.13 0.0358 0.0394 P3M [14] 64.48 0.0341 0.0367</cell><cell>73.53 68.94 68.56</cell><cell>0.0381 0.0365 0.0364</cell><cell>0.0419 0.0393 0.0391</cell></row><row><cell></cell><cell></cell><cell>-</cell><cell cols="3">57.08 0.0296 0.0323</cell><cell>60.76</cell><cell>0.0315</cell><cell>0.0344</cell></row><row><cell></cell><cell>CLIPMat</cell><cell cols="4">GFM [15] 55.67 0.0290 0.0315</cell><cell>59.28</cell><cell>0.0310</cell><cell>0.0336</cell></row><row><cell></cell><cell></cell><cell>P3M [14]</cell><cell cols="3">55.38 0.0289 0.0313</cell><cell>58.97</cell><cell>0.0308</cell><cell>0.0334</cell></row><row><cell></cell><cell></cell><cell>-</cell><cell cols="4">131.58 0.0675 0.0751 136.59 0.0700</cell><cell>0.0779</cell></row><row><cell></cell><cell>MDETR [9]</cell><cell cols="5">GFM [15] 127.25 0.0673 0.0725 132.26 0.0699</cell><cell>0.0753</cell></row><row><cell></cell><cell></cell><cell cols="5">P3M [14] 125.78 0.0669 0.0717 130.72 0.0697</cell><cell>0.0744</cell></row><row><cell>RefMatte -RW100</cell><cell>CLIPSeg [23]</cell><cell cols="5">-GFM [15] 207.49 0.1171 0.1198 217.26 0.1226 211.86 0.1178 0.1222 222.37 0.1236 P3M [14] 207.04 0.1166 0.1195 216.93 0.1222</cell><cell>0.1282 0.1254 0.1252</cell></row><row><cell></cell><cell></cell><cell>-</cell><cell cols="4">219.06 0.1228 0.1255 225.53 0.1265</cell><cell>0.1293</cell></row><row><cell></cell><cell>CLIPMat</cell><cell cols="5">GFM [15] 217.88 0.1220 0.1248 224.39 0.1257</cell><cell>0.1286</cell></row><row><cell></cell><cell></cell><cell cols="5">P3M [14] 220.17 0.1232 0.1261 226.36 0.1268</cell><cell>0.1297</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Results of CLIPMat with different prompt templates on the RefMatte test set under the prompt-based setting. &lt;en&gt;: &lt;entity name&gt;.</figDesc><table><row><cell>prompt template</cell><cell>SAD</cell><cell>MSE</cell><cell>MAD</cell></row><row><cell>&lt;en&gt;</cell><cell cols="3">12.95 0.0045 0.0074</cell></row><row><cell>a photo of a &lt;en&gt;</cell><cell cols="3">13.87 0.0050 0.0079</cell></row><row><cell>a photograph of a &lt;en&gt;</cell><cell cols="3">14.58 0.0054 0.0083</cell></row><row><cell>an image of a &lt;en&gt;</cell><cell cols="3">13.69 0.0049 0.0078</cell></row><row><cell cols="4">the foreground of the &lt;en&gt; 18.16 0.0075 0.0104</cell></row><row><cell>the mask of the &lt;en&gt;</cell><cell cols="3">53.28 0.02699 0.0305</cell></row><row><cell cols="4">the alpha matte of the &lt;en&gt; 22.77 0.0102 0.0131</cell></row><row><cell>to extract the &lt;en&gt;</cell><cell cols="3">14.04 0.0050 0.0080</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Expression-based RIM results on Ref-Matte and RefMatte-RW100. BE: basic expression. APE: absolute position expression. RPE: relative position expression. FE: free expression.</figDesc><table><row><cell>Dataset</cell><cell>Expression</cell><cell>SAD</cell><cell>MSE</cell><cell>MAD</cell></row><row><cell>RefMatte test set</cell><cell>BE APE RPE</cell><cell cols="3">64.83 0.0341 0.0368 17.49 0.0074 0.0100 81.40 0.0432 0.0461</cell></row><row><cell>RefMatte -RW100</cell><cell>BE APE RPE</cell><cell cols="3">114.39 0.0058 0.0065 72.58 0.0355 0.0417 135.97 0.0698 0.0774</cell></row><row><cell></cell><cell>FE</cell><cell cols="3">144.80 0.0748 0.0828</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Disentangled image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8819" to="8828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic human matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<title level="m">Cops-ref: A new dataset and task on compositional referring expression comprehension. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10083" to="10092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vqs: Linking segmentations to questions and answers for supervised attention in vqa and question-focused semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1811" to="1820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Modeling relationships in referential expressions with compositional modular networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4418" to="4427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Referring image segmentation via cross-modal progressive comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10485" to="10494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mdetr-modulated detection for end-to-end multi-modal understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1780" to="1790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Referitgame: Referring to objects in photographs of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="787" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Modnet: Real-time trimap-free portrait matting via objective decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Age and gender classification using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="34" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A closed-form solution to natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="228" to="242" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Privacy-preserving portrait matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3501" to="3509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bridging composite and real: towards end-to-end deep image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep automatic natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21</title>
		<editor>Z.-H. Zhou</editor>
		<meeting>the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21</meeting>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>Main Track</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Referring image segmentation via recurrent refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5745" to="5753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Natural image matting via guided contextual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11450" to="11457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ryabtsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.07810</idno>
		<title level="m">Real-time high-resolution background matting</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recurrent multimodal interaction for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1280" to="1289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mmfashion: An open-source toolbox for visual fashion analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia 2021, Open Source Software Competition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improving referring expression grounding with cross-modal attention-guided erasing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1950" to="1959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Prompt-based multi-modal image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>L?ddecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10003</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-task collaborative network for joint referring expression comprehension and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF Conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10034" to="10043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-task collaborative network for joint referring expression comprehension and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10031" to="10040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.16828</idno>
		<title level="m">Rethinking portrait matting with privacy preserving</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Generation and comprehension of unambiguous object descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Wordnet: A lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">From large scale image categorization to entry-level categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2768" to="2775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Styleclip: Text-driven manipulation of stylegan imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Patashnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2065" to="2074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learn, imagine and create: Text-to-image generation from prior knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mirrorgan: Learning text-to-image generation by redescription</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1505" to="1514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attention-guided hierarchical structure aggregation for image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>abs/2102.12092</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Denseclip: Language-guided dense prediction with context-aware prompting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<idno>abs/2112.01518</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Semantic image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11120" to="11129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Human instance matting via mutual guidance and multi-instance refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.10767</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cris</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.15174</idno>
		<title level="m">Clip-driven referring image segmentation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Phrasecut: Language-based image segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10216" to="10225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron2</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Virtual multi-modality self-supervised foreground matting for human-object interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="438" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2970" to="2979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Vitae: Vision transformer advanced by exploring intrinsic inductive bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Cross-modal self-attention network for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10494" to="10503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Mattnet: Modular attention network for referring expression comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1307" to="1315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Modeling context in referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Poirson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Mask guided matting via progressive refinement network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="1154" to="1163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Empowering things with intelligence: a survey of the progress, challenges, and opportunities in artificial intelligence of things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet of Things Journal</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="7789" to="7817" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A late fusion cnn for digital matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7469" to="7478" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
