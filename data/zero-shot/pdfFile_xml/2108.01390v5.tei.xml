<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Tencent Youtu Lab</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijie</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengdan</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kekai</forename><surname>Sheng</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Dong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Tencent Youtu Lab</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqing</forename><surname>Zhang</surname></persName>
							<email>zhang-lq@cs.sjtu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
							<email>changsheng.xu@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Sun</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Vision transformers (ViTs) have recently received explosive popularity, but the huge computational cost is still a severe issue. Since the computation complexity of ViT is quadratic with respect to the input sequence length, a mainstream paradigm for computation reduction is to reduce the number of tokens. Existing designs include structured spatial compression that uses a progressive shrinking pyramid to reduce the computations of large feature maps, and unstructured token pruning that dynamically drops redundant tokens. However, the limitation of existing token pruning lies in two folds: 1) the incomplete spatial structure caused by pruning is not compatible with structured spatial compression that is commonly used in modern deep-narrow transformers; 2) it usually requires a time-consuming pre-training procedure. To tackle the limitations and expand the applicable scenario of token pruning, we present Evo-ViT, a self-motivated slow-fast token evolution approach for vision transformers. Specifically, we conduct unstructured instance-wise token selection by taking advantage of the simple and effective global class attention that is native to vision transformers. Then, we propose to update the selected informative tokens and uninformative tokens with different computation paths, namely, slow-fast updating. Since slow-fast updating mechanism maintains the spatial structure and information flow, Evo-ViT can accelerate vanilla transformers of both flat and deep-narrow structures from the very beginning of the training process. Experimental results demonstrate that our method significantly reduces the computational cost of vision transformers while maintaining comparable performance on image classification. For example, our method accelerates DeiT-S by over 60% throughput while only sacrificing 0.4% top-1 accuracy on ImageNet-1K, outperforming current token pruning methods on both accuracy and efficiency. Code is available at https://github.com/YifanXu74/Evo-ViT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head> <ref type="figure">Figure 1</ref><p>: An illustration of technique pipelines for computation reduction via tokens. The dash lines denote iterative training. The first branch: the pipeline of unstructured token pruning <ref type="bibr" target="#b14">(Rao et al. 2021;</ref><ref type="bibr" target="#b15">Tang et al. 2021</ref>) based on pretrained models. The second branch: the pipeline of structured compression <ref type="bibr" target="#b8">(Graham et al. 2021)</ref>. The third branch: our proposed pipeline that performs unstructured updating while suitable for structured compressed models. networks (CNNs) lack, especially the property of modeling long-range dependencies. However, dense modeling of long-range dependencies among image tokens brings computation inefficiency, because images contain large regions of low-level texture and uninformative background.</p><p>Existing methods follow two pipelines to address the inefficiency problem of modeling long-range dependencies among tokens in ViT as shown in the above two pathways of <ref type="figure">Fig. 1</ref>. The first pipeline, as shown in the second pathway, is to perform structured compression based on local spatial prior, such as local linear projection <ref type="bibr" target="#b19">(Wang et al. 2021a)</ref>, convolutional projection <ref type="bibr" target="#b10">(Heo et al. 2021)</ref>, and shift windows . Most modern transformers with deep-narrow structures are within this pipeline. However, the structured compressed models treat the informative object tokens and uninformative background tokens with the same priority. Thus, token pruning, the second pipeline, proposes to identify and drop the uninformative tokens in an unstructured way. <ref type="bibr" target="#b15">(Tang et al. 2021)</ref> improves the efficiency of a pre-trained transformer network by developing a topdown layer-by-layer token slimming approach that can identify and remove redundant tokens based on the reconstruction error of the pre-trained network. The trained pruning mask is fixed for all instances. <ref type="bibr" target="#b14">(Rao et al. 2021)</ref> proposes to accelerate a pre-trained transformer network by remov-arXiv:2108.01390v5 [cs.CV] 6 Dec 2021 original interpretability class attention (layer 5) class attention (layer 10) <ref type="figure">Figure 2</ref>: Visualization of class attention in DeiT-T. The interpretability comes from <ref type="bibr" target="#b2">(Chefer, Gur, and Wolf 2021)</ref>.</p><p>ing redundant tokens hierarchically, and explores an datadependent down-sampling strategy via self-distillation. Despite of the significant acceleration, these unstructured token pruning methods are restricted in two folds due to their incomplete spatial structure and information flow, namely, the inapplicability on structured compressed transformers and inability to train from scratch. In this paper, as shown in the third pathway of <ref type="figure">Fig. 1</ref>, we propose to handle the inefficiency problem in a dynamic data-dependent way from the very beginning of the training process while suitable for structured compression methods. We denote uninformative tokens that contribute little to the final prediction but bring computational cost when bridging redundant long-range dependencies as placeholder tokens. Different from structured compression that reduces local spatial redundancy in <ref type="bibr" target="#b19">(Wang et al. 2021a;</ref><ref type="bibr" target="#b8">Graham et al. 2021)</ref>, we propose to distinguish the informative tokens from the placeholder tokens for each instance in an unstructured and dynamic way, and update the two types of tokens with different computation paths. Instead of searching for redundancy and pruning in a pre-trained network such as <ref type="bibr" target="#b15">(Tang et al. 2021;</ref><ref type="bibr" target="#b14">Rao et al. 2021)</ref>, by preserving placeholder tokens, the complete spatial structure and information flow can be maintained. In this way, our method can be a generic plugin in most ViTs of both flat and deep-narrow structures from the very beginning of training.</p><p>Concretely, Evo-ViT, a self-motivated slow-fast token evolution approach for dynamic ViTs is proposed in this work. "Self-motivated" means that transformers can naturally distinguish informative tokens from placeholder tokens for each instance, since they have insights into global dependencies among image tokens. Without loss of generality, we take DeiT  in <ref type="figure">Fig. 2</ref> as example. We find that the class token of DeiT-T estimates importance of each token for dependency modeling and final classification. Especially in deeper layers (e.g., layer 10), the class token usually augments informative tokens with higher attention scores and has a sparse attention response, which is quite consistent to the visualization result provided by <ref type="bibr" target="#b2">(Chefer, Gur, and Wolf 2021)</ref> for transformer interpretability. In shallow layers (e.g., layer 5), the attention of the class token is relatively scattered but mainly focused on informative regions. Thus, taking advantage of class tokens, informative tokens and placeholer tokens are determined. The preserved placeholer tokens ensure complete information flow in shallow layers of a transformer for modeling accuracy. After the two kinds of tokens are determined, they are updated in a slow-fast approach. Specifically, the placeholder tokens are summarized to a representative token that is evolved via the full transformer encoder simultaneously with the informative tokens in a slow and elaborate way. Then, the evolved representative token is exploited to fast update the placeholder tokens for more representative features.</p><p>We evaluate the effectiveness of the proposed Evo-ViT method on two kinds of baseline models, namely, transformers of flat structures such as DeiT ) and transformers of deep-narrow structures such as LeViT (Graham et al. 2021) on ImageNet <ref type="bibr" target="#b5">(Deng et al. 2009</ref>) dataset. Our self-motivated slow-fast token evolution method allows the DeiT model to improve inference throughput by 40%-60% and further accelerates the state-of-the-art efficient transformer LeViT while maintaining comparable performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Vision Transformer. Recently, a series of transformer models <ref type="bibr" target="#b9">(Han et al. 2020;</ref><ref type="bibr" target="#b21">Xu et al. 2022</ref>) are proposed to solve various computer vision tasks. The transformer has achieved promising success in image classification <ref type="bibr" target="#b6">(Dosovitskiy et al. 2021;</ref><ref type="bibr" target="#b17">Touvron et al. 2021;</ref><ref type="bibr" target="#b4">D'Ascoli et al. 2021)</ref>, object detection <ref type="bibr" target="#b1">(Carion et al. 2020;</ref><ref type="bibr" target="#b13">Liu et al. 2021;</ref><ref type="bibr" target="#b26">Zhu et al. 2020</ref>) and instance segmentation <ref type="bibr" target="#b7">(Duke et al. 2021;</ref><ref type="bibr" target="#b24">Zheng et al. 2021</ref>) due to its significant capability of modeling long-range dependencies. Vision Transformer (ViT) <ref type="bibr" target="#b6">(Dosovitskiy et al. 2021</ref>) is among the pioneering works that achieve state-of-the-art performance with largescale pre-training. DeiT ) manages to tackle the data-inefficiency problem in ViT by simply adjusting training strategies and adding an additional token along with the class token for knowledge distillation. To achieve better accuracy-speed trade-offs for general dense prediction, recent works <ref type="bibr" target="#b8">Graham et al. 2021;</ref><ref type="bibr" target="#b19">Wang et al. 2021a</ref>) design transformers of deep-narrow structures by adopting sub-sampling operation (e.g., strided down-sampling, local average pooling, convolutional sampling) to reduce the number of tokens in intermediate layers.</p><p>Redundancy Reduction. Transformers take high computational cost because the multi-head self-attention (MSA) requires quadratic time complexity and the feed forward network (FFN) increases the dimension of latent features. The existing acceleration methods for transformers can be mainly categorized into sparse attention mechanism, knowledge distillation <ref type="bibr" target="#b15">(Sanh et al. 2019)</ref>, and pruning. The sparse attention mechanism includes, for example, low rank factorization <ref type="bibr" target="#b18">Wang et al. 2020)</ref>, fixed local patterns , and learnable patterns <ref type="bibr" target="#b16">(Tay et al. 2020;</ref><ref type="bibr" target="#b0">Beltagy, Peters, and Cohan 2020)</ref>. <ref type="bibr" target="#b23">(Yue et al. 2021)</ref> handles the inefficiency problem by sparse input patch sampling. <ref type="bibr" target="#b9">(He et al. 2020)</ref> proposes to add an evolved global attention to the attention matrix in each layer for a better residual mechanism. Motivated by this work, we propose the evolved global class attention to guide the token selection in each layer. The closest paradigm to this work is token pruning. <ref type="bibr" target="#b15">(Tang et al. 2021</ref>) presents a top-down layer-bylayer patch slimming algorithm to reduce the computational cost in pre-trained vision transformers. The patch slimming scheme is conducted under a careful control of the feature reconstruction error, so that the pruned transformer network can maintain the original performance with lower computational cost. <ref type="bibr" target="#b14">(Rao et al. 2021</ref>) devises a lightweight prediction module to estimate the importance score of each token given the current features of a pre-trained transformer. The module is plugged into different layers to prune placeholder tokens in a unstructured way and is supervised by a distillation loss based on the predictions of the original pre-trained transformer. Different from these pruning works, we proposed to preserve the placeholder tokens, and update the informative tokens and placeholder tokens with different computation paths; thus our method can achieve better performance and be suitable for various transformers due to the complete spatial structure. In addition, the complete information flow allows us to accelerate transformers with scratch training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preliminaries</head><p>ViT <ref type="bibr" target="#b6">(Dosovitskiy et al. 2021</ref>) proposes a simple tokenization strategy that handles images by reshaping them into flattened sequential patches and linearly projecting each patch into latent embedding. An extra class token (CLS) is added to the sequence and serves as the global image representation. Moreover, since self-attention in the transformer encoder is position-agnostic and vision applications highly require position information, ViT adds position embedding into each token, including the CLS token. Afterwards, all tokens are passed through stacked transformer encoders and the CLS token is used for final classification.</p><p>The transformer is composed of a series of stacked encoders where each encoder consists of two modules, namely, a multi-head self-attention (MSA) module and a feed forward network (FFN) module. The FFN module contains two linear transformations with an activation function. The residual connections are employed around both MSA and FFN modules, followed by layer normalization (LN). Given the input x 0 of ViT, the processing of the k-th encoder can be mathematically expressed as</p><formula xml:id="formula_0">x 0 = x cls | x patch + x pos , y k = x k?1 + M SA(LN (x k?1 )), x k = y k + F F N (LN (y k )),<label>(1)</label></formula><p>where x cls ? R 1?C and x patch ? R N ?C are CLS and patch tokens respectively and x pos ? R (1+N )?C denotes the position embedding. N and C are the number of patch tokens and the dimension of the embedding.</p><p>Specifically, a self-attention (SA) module projects the input sequences into query, key, value vectors (i.e., Q, K, V ? R (1+N )?C ) using three learnable linear mapping W Q , W K and W V . Then, a weighted sum over all values in the sequence is computed through:  MSA is an extension of SA. It splits queries, keys, and values for h times and performs the attention function in parallel, then linearly projects their concatenated outputs. It is worth noting that one very different design of ViT from CNNs is the CLS token. The CLS token interacts with patch tokens at each encoder and summarizes all the patch tokens for the final representation. We denote the similarity scores between the CLS token and patch tokens as class attention A cls , formulated as:</p><formula xml:id="formula_1">SA(Q, K, V ) = Sof tmax( Q ? K T ? C )V.<label>(2)</label></formula><formula xml:id="formula_2">A cls = Sof tmax( q cls ? K T ? C ),<label>(3)</label></formula><p>where q cls is the query vector of the CLS token. Computational complexity. In ViT, the computational cost of the MSA and FFN modules are O(4N C 2 + 2N 2 C) and O(8N C 2 ), respectively. For pruning methods <ref type="bibr" target="#b14">(Rao et al. 2021;</ref><ref type="bibr" target="#b15">Tang et al. 2021)</ref>, by pruning ?% tokens, at least ?% FLOPS in the FFN and MSA modules can be reduced. Our method can achieve the same efficiency while suitable for scratch training and versatile downstream applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology Overview</head><p>In this paper, we aim to handle the inefficiency modeling issue in each input instance from the very beginning of the training process of a versatile transformer. As shown in <ref type="figure" target="#fig_0">Fig 3,</ref> the pipeline of Evo-ViT mainly contains two parts: the structure preserving token selection module and the slowfast token updating module. In the structure preserving token selection module, the informative tokens and the placeholder tokens are determined by the evolved global class attention, so that they can be updated in different manners in the following slow-fast token updating module. Specifically, the placeholder tokens are summarized and updated by a representative token. The long-term dependencies and feature richness of the representative token and the informative tokens are evolved via the MSA and FFN modules. We first elaborate on the proposed structure preserving token selection module. Then, we introduce how to update the informative tokens and the placeholder tokens in a slowfast approach. Finally, the training details, such as the loss and other training strategies, are introduced. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structure preserving token selection</head><p>In this work, we propose to preserve all the tokens and dynamically distinguish informative tokens and placeholder tokens for complete information flow. The reason is that it is not trivial to prune tokens in shallow and middle layers of a vision transformer, especially in the beginning of the training process. We explain this problem in both inter-layer and intra-layer ways. First, shallow and middle layers usually present fast growing capability of feature representation. Pruning tokens brings severe information loss. Following Refiner , we use centered kernel alignment (CKA) similarity <ref type="bibr" target="#b12">(Kornblith et al. 2019)</ref> to measure similarity of the intermediate token features in each layer and the final CLS token, assuming that the final CLS token is strongly correlated with classification. As shown in <ref type="figure" target="#fig_1">Fig. 4(a)</ref>, the token features of DeiT-T keep evolving fast when the model goes deeper and the final CLS token feature is quite different from token features in shallow layers. It indicates that the representations in shallow or middle layers are insufficiently encoded, which makes token pruning quite difficult. Second, tokens have low correlation with each other in the shallow layers. We evaluate the Pearson correlation coefficient (PCC) among different patch token queries with respect to the network depth in the DeiT-S model to show redundancy. As shown in <ref type="figure" target="#fig_1">Fig. 4(b)</ref>, the lower correlation with larger variance in the shallow layers also proves the difficulty to distinguish redundancy in shallow features.</p><p>The attention weight is the easiest and most popular approach <ref type="bibr" target="#b0">(Abnar and Zuidema 2020;</ref><ref type="bibr" target="#b20">Wang et al. 2021b</ref>) to interpret a model's decisions and to gain insights about the propagation of information among tokens. The class attention weight described in Eqn. 3 reflects the information collection and broadcast processes of the CLS token. We find that our proposed evolved global class attention is able to be a simple measure to help dynamically distinguish informative tokens and placeholder tokens in a vision transformer. In <ref type="figure" target="#fig_1">Fig. 4(a)</ref>, the distinguished informative tokens have high CKA correlation with the final CLS token, while the placeholder tokens have low CKA correlation. As shown in <ref type="figure">Fig. 2</ref>, the global class attention is able to focus on the object tokens, which is consistent to the visualization results of <ref type="bibr" target="#b2">(Chefer, Gur, and Wolf 2021)</ref>. In the following part of this section, detailed introduction of our structure preserving token selection method is provided.</p><p>As discussed in Preliminaries Section, the class attention A cls is calculated by Eqn. 3. We select k tokens whose scores in the class attention are among the top k as the informative tokens. The remaining N ? k tokens are recognized as placeholder tokens that contain less information. Different from token pruning, the placeholder tokens are kept and fast-updated rather than dropped.</p><p>For better capability of capturing the underlying information among tokens in different layers, we propose a global class attention that augments the class attention by evolving it across layers as shown in <ref type="figure" target="#fig_0">Fig. 3</ref>. Specifically, a residual connection between class attention of different layers is designed to facilitate the attention information flow with some regularization effects. Mathematically,</p><formula xml:id="formula_3">A k cls,g = ? ? A k?1 cls,g + (1 ? ?) ? A k cls ,<label>(4)</label></formula><p>where A k cls,g is the global class attention in the k-th layer, and A k cls is the class attention in the k-th layer. We use A k cls,g for the token selection in the (k+1)-th layer for stability and efficiency. For each layer with token selection, only the global class attention scores of the selected informative tokens are updated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Slow-fast token updating</head><p>Once the informative tokens and the placeholder tokens are determined by the global class attention, we propose to update tokens in a slow-fast way instead of harshly dropping placeholder tokens as <ref type="bibr" target="#b15">(Tang et al. 2021;</ref><ref type="bibr" target="#b14">Rao et al. 2021)</ref>. As shown in <ref type="figure" target="#fig_0">Fig. 3</ref>, informative tokens are carefully evolved via MSA and FFN modules, while placeholder tokens are coarsely summarized and updated via a representative token. We introduce our slow-fast token updating strategy mathematically as follows.</p><p>For N patch tokens x patch , we first split them into k informative tokens x inf ? R k?C and N ? k placeholder tokens x ph ? R (N ?k)?C by the above-mentioned token selection strategy. Then, the placeholder tokens x ph are aggregated into a representative token x rep ? R 1?C , as follows: rep for skip connections, which can be denoted by:</p><formula xml:id="formula_4">x rep = ? agg (x ph ),<label>(5)</label></formula><formula xml:id="formula_5">x (1) inf , x (1) rep = M SA(x inf , x rep ), x inf ?x inf + x (1) inf , x rep ? x rep + x (1) rep , x (2) inf , x (2) rep = F F N (x inf , x rep ), x inf ? x inf + x (2) inf .<label>(6)</label></formula><p>Thus, the informative tokens x inf and the representative token x rep are updated in a slow and elaborate way.</p><p>Finally, the placeholder tokens x ph are updated in a fast way by the residuals of x rep :</p><formula xml:id="formula_6">x ph ? x ph + ? exp (x (1) rep ) + ? exp (x (2) rep ),<label>(7)</label></formula><p>where ? exp : R 1?C ? R (N ?k)?C denotes an expanding function, such as simple copy in our method. It is worth noting that the placeholder tokens are fast updated by the residuals of x rep rather than the output features. In fact, the fast updating serves as a skip connection for the placeholder tokens. By utilizing residuals, we can ensure the output features of the slow updating and fast updating modules within the same order of magnitude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Strategies</head><p>Layer-to-stage training schedule. Our proposed token selection mechanism becomes increasingly stable and consistent as the training process. <ref type="figure" target="#fig_3">Fig. 5</ref> shows that the token selection results of a well-trained transformer turn to be consistent across different layers; thereby indicating that the transformer tends to augment informative tokens with computing resource as much as possible, namely the full transformer networks. Thus, we propose a layer-to-stage training strategy for further efficiency. Specifically, we conduct the token selection and slow-fast token updating layer by layer at the first 200 training epochs. During the remaining 100 epochs, we only conduct token selection at the beginning of each stage, and then slow-fast updating is normally performed in each layer. For transformers with flat structure such as DeiT, we manually arrange four layers as one stage. Assisted CLS token loss. Although many state-of-the-art vision transformers <ref type="bibr" target="#b19">(Wang et al. 2021a;</ref><ref type="bibr" target="#b8">Graham et al. 2021)</ref> remove the CLS token and use the final average pooled features for classification, it is not difficult to add a CLS token in their models for our token selection strategy. We empirically find that the ability of distinguishing two types of tokens of the CLS token as illustrated in <ref type="figure">Fig. 2</ref> is kept in these models even without supervision on the CLS token. For better stability, we calculate classification losses based on the CLS token together with the final average pooled features during training. Mathematically,</p><formula xml:id="formula_7">y cls ,? = m(x cls , x patch ), L = ?(? cls , y) + ?(Avg(?), y),<label>(8)</label></formula><p>where y is the ground-truth of x cls and x patch ; m denotes the transformer model; ? is the classification metric function, usually realized by the cross-entropy loss. During inference, the final average pooled features are used for classification and the CLS token is only used for token selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Setup</head><p>In this section, we demonstrate the superiority of the proposed Evo-ViT approach through extensive experiments on the ImageNet-1k <ref type="bibr" target="#b5">(Deng et al. 2009</ref>) classification dataset.</p><p>To demonstrate the generalization of our method, we conduct experiments on vision transformers of both flat and deep-narrow structures, i.e., DeiT ) and LeViT <ref type="bibr" target="#b8">(Graham et al. 2021)</ref>. Following <ref type="bibr" target="#b8">(Graham et al. 2021)</ref>, we train LeViT with distillation and without batch normalization fusion. We apply the position embedding in <ref type="bibr" target="#b19">(Wang et al. 2021a)</ref> to LeViT for better efficiency. For overall comparisons with the state-of-the-art methods <ref type="bibr" target="#b14">(Rao et al. 2021;</ref><ref type="bibr" target="#b15">Tang et al. 2021;</ref><ref type="bibr" target="#b3">Chen et al. 2021;</ref><ref type="bibr">Pan et al. 2021)</ref>, we conduct the token selection and slow-fast token updating from the fifth layer of DeiT and the third layer (excluding the convolution layers) of LeViT, respectively. The selection ratios of informative tokens in all selected layers of both DeiT and LeViT are set to 0.5. The global CLS attention trade-off ? in Eqn. 4 are set to 0.5 for all layers. For fair comparisons, all the models are trained for 300 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main Results</head><p>Comparisons with existing pruning methods. In <ref type="table" target="#tab_1">Table 1</ref>, we compare our method with existing token pruning methods <ref type="bibr" target="#b14">(Rao et al. 2021;</ref><ref type="bibr">Pan et al. 2021;</ref><ref type="bibr" target="#b15">Tang et al. 2021;</ref><ref type="bibr" target="#b3">Chen et al. 2021</ref>). Since token pruning methods are unable to recover the 2D structure and are usually designed for transformers with flat structures, we comprehensively conduct the comparisons based on DeiT ) on Im-ageNet dataset. We report the top-1 accuracy and throughput for performance evaluation. The throughput is measured on a single NVIDIA V100 GPU with batch size fixed to 256, which is the same as the setting of DeiT. Results indicate that our method outperforms previous token pruning methods on both accuracy and efficiency. Our method accelerates Comparisons with state-of-the-art ViTs. Owing to the preserved placeholder tokens, our method guarantees the spatial structure that is indispensable for most existing modern ViT networks. Thus, we further apply our method to the stateof-the-art efficient transformer LeViT <ref type="bibr" target="#b8">(Graham et al. 2021)</ref>, which presents a deep-narrow architecture. As shown in Table 2, our method can further accelerate the deep-narrow transformer such as LeViT. We have observed larger accuracy degradation of our method on LeViT than on DeiT. The reason lies that the deeper layers of LeViT have few tokens and therefore have less redundancy due to the shrinking pyramid structure. With dense input, such as the image resolution of 384?384, our method accelerates LeViT with less accuracy degradation and more acceleration ratio, which indicates the effectiveness of our method on dense input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Analysis</head><p>Effectiveness of each module. To evaluate the effectiveness of each sub-method, we add the following improvements step by step in Tab. 3 on transformers of both flat and deepnarrow structures, namely DeiT and LeViT: i) Naive selection. Simply drop the placeholder tokens based on the original class attention in each layer; ii) Structure preservation.</p><p>Preserve the placeholder tokens but not fast update them; iii) Global attention. Utilize the proposed global class attention  instead of vanilla class attention for token selection; iv) Fast updating. Augment the preserved placeholder tokens with fast updating; v) Layer-to-stage. Apply the proposed layerto-stage training strategy to further accelerate inference.</p><p>Results on DeiT indicate that our structure preserving strategy further improves the selection performance due to its capacity of preserving complete information flow. The evolved global class attention enhances the consistency of token selection across layers and achieves better performance. The fast updating strategy has less effect on DeiT than on LeViT. We claim that the performance of DeiT turns to be saturated based on structure preservation and global class attention, while LeViT still has some space for improvement. LeViT exploits spatial pooling for token reduction, which makes unstructured token reduction in each stage more difficult. By using the fast updating strategy, it is possible to collect some extra cues from placeholder tokens for accurate and augmented feature representations. We also evaluate the layer-to-stage strategy. Results indicate that it further accelerates inference while maintaining accuracy. Different Token Selection Strategy. We compare our global-attention-based token selection strategy with several common token selection strategies and sub-sampling methods in Tab. 4 to evaluate the effectiveness of our method. All token selection strategies are conducted under our structure preserving strategy without layer-to-stage training schedule. The token selection strategies include: randomly selecting the informative tokens (random selection); Utilizing the class attention of the last layer for selection in all layers via twice inference (last class attention); taking the column mean of the attention matrix as the score of each token as proposed in <ref type="bibr" target="#b11">(Kim et al. 2021</ref>   <ref type="figure">Figure 6</ref>: Different architecture of the accelerated DeiT-S via our method. We start our token selection from the fifth layer.</p><p>tention outperforms the other selection strategies and common sub-sampling methods on both accuracy and efficiency. We have observed obvious performance degradation with last class attention, although the attention in deeper layers is more focused on objects in <ref type="figure">Fig. 2</ref>. A possible reason is that the networks require some background information to assist classification, while restricting all layers to only focus on objects during the entire training process leads to underfitting on the background features. Visualization. We visualize the token selection in <ref type="figure" target="#fig_3">Fig. 5</ref> to demonstrate performance of our method during both training and testing stages. The visualized models in the left and middle three columns are trained without the layer-tostage training strategy. The left three columns demonstrate results on different layers of a well-trained DeiT-T model. Results show that our token selection method mainly focuses on objects instead of backgrounds, thereby indicating that our method can effectively discriminate the informative tokens from placeholder tokens. The selection results tend to be consistent across layers, which proves the feasibility of our layer-to-stage training strategy. Another interesting finding is that some missed tokens in the shallow layers are retrieved in the deep layers owing to our structure preserving strategy. Take the baseball images as an example, tokens of the bat are gradually picked up as the layer goes deeper. This phenomenon is more obvious under our layer-to-stage training strategy in the right three columns. We also investigate how the token selection evolves during the training stage in the middle three columns. Results demonstrate that some informative tokens, such as the fish tail, are determined as placeholder tokens at the early epochs. With more training epochs, our method gradually turns to be stable for discriminative token selection. Consistent keeping ratio. We set different keeping ratio of tokens in each layer to investigate the best acceleration architecture of Evo-ViT. The keeping ratio determines how many tokens are kept as informative tokens. Previous token pruning works <ref type="bibr" target="#b14">(Rao et al. 2021;</ref><ref type="bibr" target="#b15">Tang et al. 2021</ref>) present a gradual shrinking architecture, in which more tokens are recognized as placeholder tokens in deeper layers. They are restricted in this type of architecture due to direct pruning. Our method allows more flexible token selection owing to the structure preserving slow-fast token evolution. As shown in <ref type="figure">Fig. 6</ref>, we maintain the sum of the number of placeholder tokens in all layers and adjust the keeping ratio in each layer. Results demonstrate that the best performance is reached with a consistent keeping ratio across all layers. We explain the reason as follows. In the above visualization, we find that the token selection results tend to be consistent across layers, indicating that the transformer tends to augment informative tokens with computing resource as much as possible. In <ref type="figure">Fig. 6</ref>, at most 50% tokens are passed through the full transformer network when the keeping ratios in all layers are set to 0.5, thereby augmenting the most number of informative tokens with the best computing resource, namely, the full transformer network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>In this work, we investigate the efficiency of vision transformers by developing a self-motivated slow-fast token evolution (Evo-ViT) method. We propose the structure preserving token selection and slow-fast updating strategies to solve the limitation of token pruning on modern structured compressed transformers and scatch training. Extensive experiments on two popular ViT architectures, i.e., DeiT and LeViT, indicate that our Evo-ViT approach is able to accelerate various transformers significantly while maintaining comparable classification performance. As for future work, an interesting and worthwhile direction is to extend our method to downstream tasks, such as object detection and instance segmentation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>The overall diagram of the proposed method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Two folds that illustrate the difficulty of pruning the shallow layers. (a) The CKA similarity between the final CLS token and token features in each layer. (b) The Pearson correlation coefficient of the token features in each layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>where ? agg : R (N ?k)?C ? R 1?C denotes an aggregating function, such as weighted sum or transposed linear projection<ref type="bibr" target="#b16">(Tolstikhin et al. 2021</ref>). Here we use weighted sum based on the corresponding global attention score in Eqn. 4.Then, both the informative tokens x inf and the representative token x rep are fed into MSA and FFN modules, and their residuals are recorded as x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Token selection results on DeiT-T. The left, middle, and right three columns denote the selection results on a welltrained Evo-ViT, the fifth layer at different training epochs, and Evo-ViT with the proposed layer-to-stage strategy, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Global CLS attention Global Class Attention Evolution Global Class Attention Evolution Vanilla Transformer Block CLS attention FFN Vanilla Transformer Block</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>Slow Updating</cell><cell></cell></row><row><cell>0.05 0.2</cell><cell></cell><cell></cell><cell>MSA</cell><cell>+</cell></row><row><cell>0.06</cell><cell></cell><cell></cell></row><row><cell>0.05</cell><cell></cell><cell></cell></row><row><cell>0.2</cell><cell></cell><cell></cell></row><row><cell>0.04</cell><cell></cell><cell></cell></row><row><cell>0.1</cell><cell></cell><cell></cell></row><row><cell>0.3</cell><cell></cell><cell>+</cell></row><row><cell>Token Selection</cell><cell>Fast Updating</cell><cell>+ + +</cell></row><row><cell></cell><cell>CLS token</cell><cell>placeholder token</cell><cell>selected informative token</cell></row><row><cell></cell><cell>representative token</cell><cell></cell><cell>global class attention</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison with existing token pruning methods on DeiT. The image resolution is 224 ? 224 unless specified. denotes that the image resolution is 384 ? 384.</figDesc><table><row><cell>Method</cell><cell cols="3">Top-1 Acc. Throughput (%) (img/s) (%)</cell></row><row><cell cols="2">DeiT-T</cell><cell></cell><cell></cell></row><row><cell>Baseline (Touvron et al. 2021)</cell><cell>72.2</cell><cell>2536</cell><cell>-</cell></row><row><cell>PS-ViT (Tang et al. 2021)</cell><cell>72.0</cell><cell cols="2">3563 40.5</cell></row><row><cell>DynamicViT (Rao et al. 2021)</cell><cell>71.2</cell><cell cols="2">3890 53.4</cell></row><row><cell>SViTE (Chen et al. 2021)</cell><cell>70.1</cell><cell cols="2">2836 11.8</cell></row><row><cell>Evo-ViT (ours)</cell><cell>72.0</cell><cell cols="2">4027 58.8</cell></row><row><cell cols="2">DeiT-S</cell><cell></cell><cell></cell></row><row><cell>Baseline (Touvron et al. 2021)</cell><cell>79.8</cell><cell>940</cell><cell>-</cell></row><row><cell>PS-ViT (Tang et al. 2021)</cell><cell>79.4</cell><cell cols="2">1308 43.6</cell></row><row><cell>DynamicViT (Rao et al. 2021)</cell><cell>79.3</cell><cell cols="2">1479 57.3</cell></row><row><cell>SViTE (Chen et al. 2021)</cell><cell>79.2</cell><cell cols="2">1215 29.3</cell></row><row><cell>IA-RED 2 (Pan et al. 2021)</cell><cell>79.1</cell><cell cols="2">1360 44.7</cell></row><row><cell>Evo-ViT (ours)</cell><cell>79.4</cell><cell cols="2">1510 60.6</cell></row><row><cell cols="2">DeiT-B</cell><cell></cell><cell></cell></row><row><cell>Baseline (Touvron et al. 2021)</cell><cell>81.8</cell><cell>299</cell><cell>-</cell></row><row><cell>Baseline  *  (Touvron et al. 2021)</cell><cell>82.8</cell><cell>87</cell><cell>-</cell></row><row><cell>PS-ViT (Tang et al. 2021)</cell><cell>81.5</cell><cell>445</cell><cell>48.8</cell></row><row><cell>DynamicViT (Rao et al. 2021)</cell><cell>80.8</cell><cell>454</cell><cell>51.8</cell></row><row><cell>SViTE (Chen et al. 2021)</cell><cell>82.2</cell><cell>421</cell><cell>40.8</cell></row><row><cell>IA-RED 2 (Pan et al. 2021)</cell><cell>80.9</cell><cell>453</cell><cell>42.9</cell></row><row><cell>IA-RED 2 *  (Pan et al. 2021)</cell><cell>81.9</cell><cell>129</cell><cell>51.5</cell></row><row><cell>Evo-ViT (ours)</cell><cell>81.3</cell><cell>462</cell><cell>54.5</cell></row><row><cell>Evo-ViT  *  (ours)</cell><cell>82.0</cell><cell>139</cell><cell>59.8</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison with state-of-the-art vision transformers. The input image resolution is 224?224 unless specified. denotes that the image resolution is 384 ? 384.</figDesc><table><row><cell>Model</cell><cell>Param (M)</cell><cell>Throughput (img/s)</cell><cell>Top-1 Acc. (%)</cell></row><row><cell>LeViT-128S</cell><cell>7.8</cell><cell>8755</cell><cell>74.5</cell></row><row><cell>LeViT-128</cell><cell>9.2</cell><cell>6109</cell><cell>76.2</cell></row><row><cell>LeViT-192</cell><cell>10.9</cell><cell>4705</cell><cell>78.4</cell></row><row><cell>PVTv2-B1</cell><cell>14.0</cell><cell>1225</cell><cell>78.7</cell></row><row><cell>CoaT-Lite Tiny</cell><cell>5.7</cell><cell>1083</cell><cell>76.6</cell></row><row><cell>PiT-Ti</cell><cell>4.9</cell><cell>3030</cell><cell>73.0</cell></row><row><cell>Evo-LeViT-128S</cell><cell>7.8</cell><cell>10135</cell><cell>73.0</cell></row><row><cell>Evo-LeViT-128</cell><cell>9.2</cell><cell>8323</cell><cell>74.4</cell></row><row><cell>Evo-LeViT-192</cell><cell>11.0</cell><cell>6148</cell><cell>76.8</cell></row><row><cell>LeViT-256</cell><cell>18.9</cell><cell>3357</cell><cell>80.1</cell></row><row><cell>LeViT-256  *</cell><cell>19.0</cell><cell>906</cell><cell>81.8</cell></row><row><cell>PVTv2-B2</cell><cell>25.4</cell><cell>687</cell><cell>82.0</cell></row><row><cell>PiT-S</cell><cell>23.5</cell><cell>1266</cell><cell>80.9</cell></row><row><cell>Swin-T</cell><cell>29.4</cell><cell>755</cell><cell>81.3</cell></row><row><cell>CoaT-Lite Small</cell><cell>20.0</cell><cell>550</cell><cell>81.9</cell></row><row><cell>Evo-LeViT-256</cell><cell>19.0</cell><cell>4277</cell><cell>78.8</cell></row><row><cell>Evo-LeViT-256  *</cell><cell>19.2</cell><cell>1285</cell><cell>81.1</cell></row><row><cell>LeViT-384</cell><cell>39.1</cell><cell>1838</cell><cell>81.6</cell></row><row><cell>LeViT-384  *</cell><cell>39.2</cell><cell>523</cell><cell>82.8</cell></row><row><cell>PVTv2-B3</cell><cell>45.2</cell><cell>457</cell><cell>83.2</cell></row><row><cell>PiT-B</cell><cell>73.8</cell><cell>348</cell><cell>82.0</cell></row><row><cell>Evo-LeViT-384</cell><cell>39.3</cell><cell>2412</cell><cell>80.7</cell></row><row><cell>Evo-LeViT-384  *</cell><cell>39.6</cell><cell>712</cell><cell>82.2</cell></row><row><cell cols="4">the inference throughput by over 60% with negligible accu-</cell></row><row><cell cols="2">racy drop (-0.4%) on DeiT-S.</cell><cell></cell><cell></cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Method ablation on DeiT and LeViT.</figDesc><table><row><cell></cell><cell></cell><cell>DeiT-T</cell><cell cols="2">LeViT 128S</cell></row><row><cell>Strategy</cell><cell cols="4">Acc. Throughput Acc. Throughput</cell></row><row><cell></cell><cell>(%)</cell><cell>(img/s)</cell><cell>(%)</cell><cell>(img/s)</cell></row><row><cell>baseline</cell><cell>72.2</cell><cell>2536</cell><cell>74.5</cell><cell>8755</cell></row><row><cell>+ naive selection</cell><cell>70.8</cell><cell>3824</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">+ structure preservation 71.6</cell><cell>3802</cell><cell>72.1</cell><cell>9892</cell></row><row><cell>+ global attention</cell><cell>72.0</cell><cell>3730</cell><cell>72.5</cell><cell>9452</cell></row><row><cell>+ fast updating</cell><cell>72.0</cell><cell>3610</cell><cell>73.0</cell><cell>9360</cell></row><row><cell>+ layer-to-stage</cell><cell>72.0</cell><cell>4027</cell><cell>73.0</cell><cell>10135</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Different token selection strategies on DeiT-T. We conduct all sub-sampling methods at the seventh layer and conduct token selection strategies from the fifth layer.</figDesc><table><row><cell>Method</cell><cell>Acc. (%)</cell><cell>Throughput (img/s)</cell></row><row><cell>average pooling</cell><cell>69.5</cell><cell>3703</cell></row><row><cell>max pooling</cell><cell>69.8</cell><cell>3698</cell></row><row><cell>convolution</cell><cell>70.2</cell><cell>3688</cell></row><row><cell>random selection</cell><cell>66.4</cell><cell>3760</cell></row><row><cell>last class attention</cell><cell>69.7</cell><cell>1694</cell></row><row><cell>attention column mean</cell><cell>71.2</cell><cell>3596</cell></row><row><cell>global class attention</cell><cell>72.0</cell><cell>3730</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>) (attention column mean). Results in Tab. 4 indicate that our evolved global class at-</figDesc><table><row><cell></cell><cell></cell><cell>Testing phase</cell><cell></cell><cell></cell><cell>Training phase</cell><cell></cell><cell cols="2">Layer-to-stage</cell></row><row><cell>Original</cell><cell>Layer 5</cell><cell>Layer 9</cell><cell>Layer 11</cell><cell>Epoch 10</cell><cell>Epoch 50</cell><cell>Epoch 100</cell><cell>Stage 2</cell><cell>Stage 3</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Quantifying attention flow in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuidema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00928</idno>
		<idno>arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Longformer: The long-document transformer</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Transformer interpretability beyond attention visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04533</idno>
		<title level="m">Chasing Sparsity in Vision Transformers: An End-to-End Exploration</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>D&amp;apos;ascoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Leavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Biroli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sagun</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<title level="m">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sstvos: Sparse spatiotemporal transformers for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Aarabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">LeViT: a Vision Transformer in ConvNet&apos;s Clothing for Faster Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12556</idno>
		<idno>arXiv:2012.11747</idno>
		<title level="m">Realformer: Transformer likes residual attention</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>A survey on visual transformer</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rethinking Spatial Dimensions of Vision Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thorsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hassoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00910</idno>
		<title level="m">Learned Token Pruning for Transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Similarity of neural network representations revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">IA-RED 2 : Interpretability-Aware Redundancy Reduction for Vision Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.12620</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision<address><addrLine>Pan, B</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Swin transformer: Hierarchical vision transformer using shifted windows</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.02034</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dis-tilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01108</idno>
		<idno>arXiv:2106.02852</idno>
	</analytic>
	<monogr>
		<title level="m">Patch Slimming for Efficient Vision Transformers</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Synthesizer: Rethinking self-attention in transformer models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-C</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00743</idno>
		<idno>arXiv:2105.01601</idno>
	</analytic>
	<monogr>
		<title level="m">Mlp-mixer: An all-mlp architecture for vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<title level="m">Linformer: Self-attention with linear complexity</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Evolving attention with residual convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12895</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Co-Scale Conv-Attentional Image Transformers</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Transformers in computational visual media: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">E</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Vision transformer with progressive sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03714</idno>
		<title level="m">Refiner: Refining Self-attention for Vision Transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
