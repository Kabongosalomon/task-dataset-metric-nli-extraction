<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Wav2Seq: Pre-training Speech-to-Text Encoder-Decoder Models Using Pseudo Languages</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ASAPP Inc. Carnegie Mellon University ? Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwangyoun</forename><surname>Kim</surname></persName>
							<email>kkim@asapp.com</email>
							<affiliation key="aff0">
								<orgName type="institution">ASAPP Inc. Carnegie Mellon University ? Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ASAPP Inc. Carnegie Mellon University ? Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyu</forename><surname>Han</surname></persName>
							<email>khan@asapp.com</email>
							<affiliation key="aff0">
								<orgName type="institution">ASAPP Inc. Carnegie Mellon University ? Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
							<email>rmcdonald@asapp.com</email>
							<affiliation key="aff0">
								<orgName type="institution">ASAPP Inc. Carnegie Mellon University ? Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
							<email>kweinberger@asapp.com</email>
							<affiliation key="aff0">
								<orgName type="institution">ASAPP Inc. Carnegie Mellon University ? Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ASAPP Inc. Carnegie Mellon University ? Cornell University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Wav2Seq: Pre-training Speech-to-Text Encoder-Decoder Models Using Pseudo Languages</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce Wav2Seq, the first self-supervised approach to pre-train both parts of encoder-decoder models for speech data. We induce a pseudo language as a compact discrete representation, and formulate a self-supervised pseudo speech recognition task -transcribing audio inputs into pseudo subword sequences. This process stands on its own, or can be applied as low-cost second-stage pre-training. We experiment with automatic speech recognition (ASR), spoken named entity recognition, and speech-to-text translation. We set new state-of-the-art results for end-to-end spoken named entity recognition, and show consistent improvements on 20 language pairs for speech-to-text translation, even when competing methods use additional text data for training. On ASR, our approach enables encoderdecoder methods to benefit from pre-training for all parts of the network, and shows comparable performance to highly optimized recent methods.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Self-supervised pre-trained models have recently become a core part of speech models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b61">62]</ref>, leading to impressive performance on a wide variety of speech tasks <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b65">66]</ref>. This trend mirrors recent success in natural language processing [NLP; <ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49]</ref> and computer vision [CV; <ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b62">63]</ref>.</p><p>Most of these approaches rely on pre-training an encoder to create expressive representation of input data. If a sequential decoder is needed for downstream tasks (i.e., for generative tasks), it is often trained with task-specific supervised data. The most common approaches for automatic speech recognition (ASR) follow this encoder-decoder paradigm <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b60">61]</ref>, regardless if they use sequence transducers <ref type="bibr" target="#b19">[20]</ref> or sequence-to-sequence <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b54">55]</ref> architectures. Because all existing self-supervised learning approaches for speech focus on pre-training an encoder model only, when adapted to an encoder-decoder architecture, the decoder has to be either randomly initialized or borrowed from a pre-trained NLP decoder <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b59">60]</ref>.</p><p>In this paper, we propose Wav2Seq, the first self-supervised approach to jointly pre-train the encoder and decoder. We automatically induce pseudo subwords that form a compact discrete representation of spoken language. We treat these as audio transcripts in a pseudo ASR task, and use them as the targets for Seq2Seq learning (see <ref type="figure">Fig. 1</ref>). When fine-tuned on a downstream task (e.g., ASR or speech translation), the input and output embedding layers are replaced in order to adapt to natural language.</p><p>We conduct extensive experiments on ASR, spoken named entity recognition (SNER), and speech-totext translation (ST) tasks. Focusing on settings with limited labeled audio data (i.e., 10h or less), our ASR results show that with a pre-trained encoder only, CTC models outperform encoder-decoder models significantly in few-example scenarios; however, Wav2Seq boosts the performance of encoder-  <ref type="figure">Figure 1</ref>: Pseudo ASR task. The Wav2Seq model is pre-trained to transcribe an audio input into a sequence of pseudo language tokens. The embedding layers are replaced during fine-tuning on real ASR or speech translation tasks. "&lt;sos&gt;" is the start-of-sequence token and "&lt;eos&gt;" is the end-of-sequence token. See <ref type="figure">Fig. 2</ref> for how the pseudo language tokens are generated.</p><p>decoder models and closes this gap. When we apply Wav2Seq as a low-cost second-stage pre-training method, models based on existing pre-trained encoders (e.g., HuBERT) achieve even better results. On the SLUE-VoxPopuli SNER benchmark, Wav2Seq initialized with HuBERT achieves the best end-to-end results. For ST tasks, we conduct experiments on four from-English language pairs and 16 to-English pairs with both low-and high-resource setups. Wav2Seq consistently outperforms models initialized with HuBERT or XLS-R pre-trained models and achieves similar BLEU scores as models trained on additional machine translation annotated text data. Pre-trained models and code are available at https://github.com/asappresearch/wav2seq.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Speech-to-Text Models Automatic speech recognition (ASR) and speech-to-text translation (ST) are two of the most commonly studied speech-to-text tasks. The former produces a monotonic mapping while the latter may involve re-ordering (e.g. translating English to German). When re-ordering exists, it is advantageous to use sequence-to-sequence (Seq2Seq) models <ref type="bibr" target="#b54">[55]</ref> with encoder-decoder attention <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9]</ref>, such as doen in popular open-source speech translation toolkits <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b56">57]</ref>.</p><p>When the mapping is monotonic, such as with ASR, various approaches are applicable. Connectionist temporal classification [CTC; 19] is one of the simplest and efficient approaches. It only requires an encoder that generates a sequence of feature vectors where each feature vector represents the input within a time window. A linear classifier can be applied to classify each feature vector into a character and the repeated and blank characters will be removed. When decoding with an LM, beam-search can also be used. CTC is used in various systems <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b53">54]</ref> and is becoming the default approach to fine-tune a self-supervised pre-trained model such as wav2vec 2.0 <ref type="bibr" target="#b4">[5]</ref>. The sequence transducer <ref type="bibr" target="#b19">[20]</ref> is another architecture dedicated to monotonic mapping in ASR. Similar to Seq2Seq, it has an encoder for audio inputs and an auto-regressive decoder for text inputs; however, the decoder does not have access to the full encoded speech features. The decoder starts with using the speech feature of the first frame from the encoder and decides either to emit a text token or shift to use the speech feature of the next frame. State-of-the-art supervised speech models are based on the transducer framework <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>Text-to-Text Encoder-Decoder Pre-training In NLP, it has been observed that pre-trained Seq2Seq models outperforms pre-trained encoders on text generation tasks <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b48">49]</ref> despite being less competitive in discriminative tasks. Lewis et al. <ref type="bibr" target="#b32">[33]</ref> introduced BART which is a Seq2Seq Transformer pre-trained on a text denoising task. Xu et al. <ref type="bibr" target="#b63">[64]</ref> extend BART to a multilingual setup and introduce mBART. Concurrently, T5 <ref type="bibr" target="#b48">[49]</ref> is pre-trained on a large collection of NLP tasks, showing effective task transfer behavior <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b66">67]</ref>.</p><p>Speech Self-supervised Learning CPC <ref type="bibr" target="#b55">[56]</ref> and wav2vec <ref type="bibr" target="#b50">[51]</ref> are two early approaches for selfsupervised speech representation learning based on contrastive loss. Wav2vec 2.0 <ref type="bibr" target="#b4">[5]</ref> is the first self-supervised model to outperform purely supervised approaches on ASR. Instead of directly reconstructing masked features, it is uses a contrastive loss -distinguishing the quantized version of the correct features from several negative samples. Lai et al. <ref type="bibr" target="#b30">[31]</ref> show that many of the weights in wav2vec 2.0 are redundant and can be pruned. Wu et al. <ref type="bibr" target="#b61">[62]</ref> provides a deeper understanding of the efficiency of various components of wav2vec 2.0 and propose SEW-D, a more compact and efficient variant. Hsu et al. <ref type="bibr" target="#b26">[27]</ref> found that adding a loss to intermediate layer improves the quality of wav2vec 2.0 pre-training. Rather than contrastive learning, Hsu et al. <ref type="bibr" target="#b25">[26]</ref> propose HuBERT, a self-supervised approach based on masked language modeling. Unlike wav2vec 2.0 using online quantization, HuBERT extracts speech features and clusters them offline. The model is trained to predict the cluster indices of the features at the masked positions. WavLM <ref type="bibr" target="#b9">[10]</ref> extends HuBERT by using a larger dataset with non-audiobook audios and adding noise to the audio during pre-training. W2v-BERT <ref type="bibr" target="#b12">[13]</ref> further combines these two approaches (contrastive learning and masked language modeling) and sets a new state of the art in ASR. Concurrently to our paper, Baevski et al. <ref type="bibr" target="#b5">[6]</ref> introduce data2vec which is trained to match the outputs of an exponentially moving averaged teacher (similar to BYOL <ref type="bibr" target="#b20">[21]</ref> and DINO <ref type="bibr" target="#b7">[8]</ref> in CV) and works for NLP and CV data as well. Ao et al. <ref type="bibr" target="#b1">[2]</ref> introduce SpeechT5, a multimodal encoder-decoder model, which is pre-trained on both text and speech data. Unlike our approach, their decoder is trained to reconstruct the log Mel-filterbank features of the speech audio.</p><p>3 Self-supervision using Pseudo Subwords</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pseudo ASR</head><p>Seq2seq ASR models use an encoder to extract speech features and a decoder to generate text tokens conditioned on the speech features. Given an audio input X = (x 1 , x 2 , ..., x m ) ? X , the ASR model f : X ? Y is trained to transcribe it into text tokens Y = (y 1 , y 2 , .., y n ) ? Y, where X is the space of audio and Y is the space of text sentences and m and n are input and output lengths. Seq2seq models are commonly trained by minimizing the negative log likelihood using training data D train of audio-transcription pairs:</p><formula xml:id="formula_0">NLL = ? (X,Y )?Dtrain n t=1</formula><p>log P (y t |X, y 1 , ..., y t?1 ) .</p><p>During pre-training, in contrast, transcriptions are not available, making this objective inapplicable.</p><p>Our key insight is that we can use unsupervised techniques to induce a pseudo language given raw audio only, and annotate raw audio inputs with it automatically. We then apply the likelihood sequence-to-sequence objective, and train Wav2Seq, a pre-trained encoder-decoder architecture. <ref type="figure">Fig. 1</ref> illustrates the pseudo ASR task.</p><p>While our focus is the negative log-likelihood objective, the pseudo language formulation is not restricted to this objective, and is broadly compatible with other training objectives. For example, we also experiment with pre-training Wav2Seqs with a sequence transducer architecture using a transducer loss <ref type="bibr" target="#b19">[20]</ref> (Subsubsec. 5.1.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Inducing Pseudo Languages</head><p>We construct sequences of discrete tokens from speech. Because Transformer decoders require quadratic computation with respect to sequence length, we need to balance the length of these sequences (i.e., wanting them to be short) with the quality of the pre-trained model. <ref type="figure">Fig. 2</ref> illustrates how our pseudo subwords are generated. We extract a sequence of hidden feature vectors from an audio file using a pre-trained HuBERT <ref type="bibr" target="#b25">[26]</ref> model. We apply average pooling to reduce the sequence length and then k-means clustering to discretize these hidden feature vectors. This results in a sequence of cluster indices, which have also been referred to as hidden units in prior literature <ref type="bibr" target="#b25">[26]</ref>.</p><p>We treat these cluster indices as characters because they have been found to correlate with the phonemes of speech audio. We observe many consecutive repetitions of cluster indices, and propose to deduplicate them (i.e., remove the duplicates). For example, "k k t t t" will be mapped to "k t". We treat the deduplicated cluster indices as the characters of pseudo subwords and refer to them as pseudo characters.  <ref type="figure">Figure 2</ref>: Mapping an audio input into pseudo subwords. Here we use the first 0.6 seconds of a real audio from LibriSpeech <ref type="bibr" target="#b40">[41]</ref> training set as an example. We use models with 25 clusters and 1000 BPE tokens in this example. Pseudo subword is a compact representation (in terms of the sequence length) of the audio input.</p><formula xml:id="formula_2">k k t t t t t s o h d v v v x k t v s h x o d ktvs xod h</formula><p>We apply a subword tokenization algorithm such as byte-pair encoding [BPE; 17] to further shorten the target sequence length. This process merges common co-occurring characters into newly created tokens, and is widely used in NLP tasks such as language modeling and machine translation to balance between the benefits and costs of character and word representations (i.e., vocabulary size, unit semantics, handling of unseen words, etc). For example, a word "negotiation'' can be represented by "ne" "go" "ti" and "ation" with a subword tokenizer, which allows the model to share the embedding of the common suffix "ation" across different words. Similarly, common pseudo characters sequences are merged to pseudo subwords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>LibriSpeech We use LibriSpeech <ref type="bibr" target="#b40">[41]</ref> data for pre-training and focus on low-resource ASR using 10h and 100h subsets of labels. LibriSpeech contains 960 hours of audiobook recordings for training, two 5-hour development sets (devclean and dev-other) and two 5-hour test sets (test-clean and test-other). For self-supervised pre-training, we randomly sample 1% of the 960h training set as the validation set and use the rest of the raw data (i.e., without labels) as the training set. For ASR experiments, we use the LibriSpeech train-clean 100h split and the 10h splits provided by Kahn et al. <ref type="bibr" target="#b29">[30]</ref> as the low-resource labelled data. We use dev-other for hyper-parameter tuning, ablations, and early stopping.</p><p>LibriLight Similar to the training of wav2vec 2.0 large and HuBERT-large, we use LibriLight <ref type="bibr" target="#b29">[30]</ref> to pre-train large models. LibriLight contains 60K hours of unlabelled audiobook recordings.</p><p>CoVoST-2 We use CoVoST-2 speech translation data set <ref type="bibr" target="#b57">[58]</ref> for ST experiments. There are 21 X-to-English and 15 English-to-X language pairs. For X-to-English pairs, we use the 12 low-resource pairs with less than 10h audios and 4 high-resource pairs with more than 100h audios. All English-to-X pairs have 430h of English audio. We therefore use the four language pairs used in prior work <ref type="bibr" target="#b59">[60]</ref> and subsample a 10h subset for each pair to simulate a low-resource setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SLUE-VoxPopuli</head><p>We use SLUE-VoxPopuli dataset <ref type="bibr" target="#b52">[53]</ref> for spoken NER experiments. SLUE-VoxPopuli contains several subsets: fine-tune, dev, and test sets with 14.5h, 5h, and 5h of transcribed audios and their corresponding named entity labels. The audio is from recordings of European Parliament events and the transcriptions are provided by Wang et al. <ref type="bibr" target="#b58">[59]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pre-training</head><p>We use the official baselines for most prior work <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b25">26]</ref>, which are implemented in fairseq <ref type="bibr" target="#b39">[40]</ref>. Our Wav2Seq is also implemented as a plugin of fairseq. We use the mini-batch k-means algorithm <ref type="bibr" target="#b51">[52]</ref> with k-mean++ initialization <ref type="bibr" target="#b2">[3]</ref> implementation in scikit-learn <ref type="bibr" target="#b42">[43]</ref>. Following HuBERT's best hyper-parameters, the number of clusters C is set to 500 by default unless specified separately. We use the byte-pair encoding (BPE) <ref type="bibr" target="#b16">[17]</ref> implementation from Huggingface's tokenizers library 1 as the subtokenization method where the vocabulary size of subword tokens V is set to 30K (or 10K for the tiny models).</p><p>Following Baevski et al. <ref type="bibr" target="#b4">[5]</ref>, we pre-trained models with a batch size of at most 87.5 seconds per GPU on 8 GPUs and an update frequency of 4 to simulate 32-GPU training as Baevski et al. <ref type="bibr" target="#b4">[5]</ref>. We pre-train the models for 100K updates (or 25K updates in second-stage pre-training) unless specified otherwise. We use the same hyper-parameters as prior work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b61">62]</ref>. The peak learning rate is set to 2 ? 10 ?4 with 32K linear warm-up steps and linearly decaying to 0 at step 400K. All models are pre-trained with eight NVIDIA V100 GPUs with half-precision. We tie the weights of input and output embeddings of the decoder during pre-training and fine-tuning <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b45">46]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Fine-tuning</head><p>We use BPE with vocabulary size 1,000 to tokenize the text in ASR, spoken NER, or ST tasks. Similar to prior work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">26]</ref>, we use learning rate 5 ? 10 ?5 for fine-tuning and use tri-stage learning rate scheduler with 2,000 steps for linear warm-up and the last 50% of the updates for exponential decay. <ref type="bibr" target="#b1">2</ref> The number of fine-tuning steps depends on the datasets. When fine-tuned on LibriSpeech 10h (or 100h) data, we use a batch size with at most 50 seconds per GPU of audio for 20K (or 80K) and updates on eight GPUs with half-precision. For spoken NER and low-resource ST, we use the same hyper-parameters as the LibriSpeech 10h setup. For high resource ST, we increase the number of updates to 320K because it has more data. We use beam size 10 for beam search decoding of Seq2Seq models and no length penalty for ASR and SNER tasks. For ST, we tune length penalty ? {0.5, 1.0, 1.5} on the development sets because some languages may be more (or less) compact. For CTC baselines, we follow the hyperparamters provided by Hsu et al. <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Automatic Speech Recognition (ASR)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Small Model Experiment</head><p>We conduct initial small-scale experiments using tiny models with embedding size 256, four attention heads, and feed-forward embedding size 1,024 in each Transformer block. Each model has 6 or 12 Transformer blocks. We use a compact wave feature extractor (WFE-C-c128l0) <ref type="bibr" target="#b61">[62]</ref> to speed up the model. This extractor has been shown to perform similar to the wave feature extractor used in wav2vec 2.0 and HuBERT, but faster. All models are pre-trained on LibriSpeech with a semi-supervised setup using 960h unlabelled recordings for pre-training and 10h labelled data for fine-tuning. To be comparable, we use the 9th layer of the official second iteration HuBERT-base model to extract both hidden units and pseudo subwords for pre-training HuBERT baselines and our Wav2Seq models. All HuBERT models in this subsubsection are third-iteration of HuBERT models. <ref type="table" target="#tab_2">Table 1</ref> shows word error rate (WER) of HuBERT and Wav2Seq with different fine-tuning setups. In the first two rows, as is already known, HuBERT performs strongly when fine-tuning with CTC objective which does not require a Transformer decoder. However, it is challenging to train a HuBERT encoder with a randomly initialized decoder with only 10h labelled data (rows 4 and 5). In contrast, our Wav2Seq can easily adapt to an ASR task with only limited supervision (row 6). In row 7, we show that it is more important to have a deeper encoder (8 layers) and a shallower decoder (4 layers), which allows closing the gap between CTC fine-tuning and Seq2Seq models. In row 8, we use Wav2Seq as a second stage pre-training -the encoder of Wav2Seq is initialized with a pre-trained HuBERT model. As we can see, with only a relatively small amount of additional pre-training cost, Wav2Seq is able to significantly improve HuBERT's Seq2Seq performance. This demonstrates that HuBERT and Wav2Seq pre-training are complimentary.  Transducers require O(mnV ) space complexity (m, n, and V are input sequence length, output sequence length, and output vocabulary size), we set C = 25 and V = 1000 to fit the model into a single GPU memory, which may hurt its performance compared to the Seq2Seq counterparts. We study the impact of C and V in Subsec. A.1. We also reduce the number of decoder (a.k.a., label encoder in a transducer) layers because of memory issues. The pre-training learning rate is set to 5 ? 10 ?4 for Transducers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Speech-to-text Transformer Models</head><p>We show that Wav2Seq generalizes to other model architectures using the S2T Transformer model implemented by Wang et al. <ref type="bibr" target="#b56">[57]</ref>, which is a transformer-based Seq2Seq ASR model <ref type="bibr" target="#b38">[39]</ref> that takes log-mel filterbank features as inputs and is widely used as a baseline under the fully supervised setup. In this experiment, we closely follow the default hyperparameters in the codebase, <ref type="bibr" target="#b2">3</ref> except that we reduce the number of training iterations (300K originally) because the official example uses the complete 960h labelled data, while we use a semi-supervised setup with only 10h or 100h labelled data. <ref type="table" target="#tab_3">Table 2</ref> shows the WERs of an S2T Transformer-small trained from scratch and a Wav2Seq pre-trained counterpart. With merely 10h labelled data, it is impossible to train the S2T Transformer from scratch (row 1). In contrast, with Wav2Seq pre-training, the model can learn meaningful patterns from limited labels (row 2). With 100h labelled data, a model Wav2Seq pre-trained with 960h unlabelled data can significantly improve the WER of the model (32.8% to 26.8% on LibriSpeech test-other) while requiring merely one third of the fine-tuning time on the 100h ASR data (rows 3 and 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Standard Model Size Model Experiments</head><p>We use the official HuBERT-base and HuBERT-large pre-trained on LibriSpeech and LibriLight as the initialization of Wav2Seq encoders following our observation that Wav2Seq works best as second stage pre-training. We further pre-train the models on the same corpus with relatively few    <ref type="bibr" target="#b52">[53]</ref>. Wav2Seq achieves the best performance among all end-to-end methods without access to a language model. A pipeline model using an NLP pre-trained model (DeBERTa-large) remains the best among all the approaches. The numbers in the first four rows are provided by Shon et al. <ref type="bibr" target="#b52">[53]</ref>. The last row is the state-of-the-art using external data and knowledge distillation from an NLP model, the test score is not available in their currently published paper <ref type="bibr" target="#b41">[42]</ref>.</p><p>updates (25K or 100K iterations compared to 400K interactions for HuBERT models). For the decoder, we use six Transformer blocks with the same width and number of heads as the encoder. Our Wav2Seq (from HuBERT-base) and Wav2Seq (from HuBERT-large) models take 14 and 49 hours to be fine-tuned for 25K updates on eight NVIDIA V100 GPUs, a relatively small compute budget (less than 10%) compared to training HuBERT models (usually 400K updates). <ref type="table" target="#tab_5">Table 3</ref> shows WER for standard sized models. Even with a well trained HuBERT-base, fine-tuning with the Seq2Seq architecture using a randomly initialized decoder is inferior to the simple CTC objective (rows 2 and 3). Using Wav2Seq closes the gap or even makes Seq2Seq models outperform CTC counterparts (row 4). However, we observe that pre-training longer does not help for Wav2Seq (from HuBERT-base) (row 5). The reason might be that the encoder has already been pre-trained. This shows that the second-stage pre-training can be stopped early without hurting performance. We also compare models with 100h labelled data to confirm our observation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Spoken Named Entity Recognition (SNER)</head><p>Spoken named entity recognition has recentely received significant attention <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b64">65]</ref>. It is often addressed in one of two ways: end-to-end (E2E) or pipeline (i.e., a combination of an ASR model and an NLP NER model). Shon et al. <ref type="bibr" target="#b52">[53]</ref> show that the pipeline approach is the state-of-the art, while more recent work <ref type="bibr" target="#b41">[42]</ref> shows that with additional external data the order can be reverted.</p><p>End-to-end models are usually trained with a CTC objective <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b64">65]</ref>. In our experiment, we explore the potential of applying Seq2Seq to SNER. Following Shon et al. <ref type="bibr" target="#b52">[53]</ref>, we add special tokens around named entities in the transcription and train the model in an ASR manner, which results in a model that detects named entities as it transcribes the audio inputs.   <ref type="table">Table 5</ref>: Test BLEU scores on CoVoST 2 low-resource X-to-En language pairs. When limited labels are provided, using Wav2Seq as a second stage pre-training consistently improves the performance of HuBERT-large and XLS-R (0.3B). Wav2Seq (from XLS-R (0.3B)) is on par with the XLS-R (0.3B) + mBART-ML50N1 which uses a large mBART decoder pre-trained on a 50-language text corpus and fine-tuned on 50 languages to English machine translation corpus with 4? parameters. The scores in the second and third groups are from Babu et al. <ref type="bibr" target="#b3">[4]</ref>. VP-100K is a wav2vec 2.0 large pre-trained on VoxPopuli 100K hour multilingual data <ref type="bibr" target="#b58">[59]</ref>, XLSR-53 <ref type="bibr" target="#b14">[15]</ref> is a wav2vec 2.0 pre-trained on 53-language audios, and XMEF-En and XMEF-X are efficient fine-tuning method proposed by Li et al. <ref type="bibr" target="#b34">[35]</ref>.  <ref type="table">Table 6</ref>: Test BLEU scores on CoVoST-2 X-to-En high-resource language pairs. We compare with the same baselines as <ref type="table">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Speech-to-text Translation (ST)</head><p>Encoder-decoder (i.e., Seq2Seq) models are considered particularly suitable for speech-to-text translation tasks, where the input and output sequences are not monotonically aligned.</p><p>Our experiments extend beyond English. Besides the English Wav2Seq (from HuBERT-large), we also pre-train a Wav2Seq (from XLS-R (0.3B)) which uses a multi-lingual XLS-R encoder <ref type="bibr" target="#b3">[4]</ref> pre-trained on 500K hours of audio in 128 languages. We use LibriLight for second stage pre-training and pre-train the model for only 25K updates. 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>De Ca Ar Tr Avg.</p><p>HuBERT-large 5.0 7.9 1.6 1.5 4.0 Wav2Seq (from HuBERT-large) 9.7 13.0 3.1 2.9 7.2  <ref type="table">Table 8</ref>: Test BLEU scores on CoVoST-2 En-to-X language pairs. The complete 430h labelled audio data is used. Our Wav2Seq (from HuBERT-large) outperforms XLS-R (1B) <ref type="bibr" target="#b3">[4]</ref> without using any multi-lingual data or using an mBART <ref type="bibr" target="#b36">[37]</ref> decoder pretrained on text data. It also reaches a similar performance by self-training <ref type="bibr" target="#b59">[60]</ref> which is computationally expensive since it has to label 60K hours of audios and fine-tuning on them in a second iteration. We also compare with VP-100K <ref type="bibr" target="#b58">[59]</ref>, XLSR-53 <ref type="bibr" target="#b14">[15]</ref>, and XMEF-JT <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X-to-English Low-resource Experiments</head><p>We experiment with 12 X-to-English language pairs where less than 10 hours of labelled data is available. We fine-tune both baselines and our models in a multi-task fine-tuning learning setup. <ref type="table">Table 5</ref> shows test BLEU scores on each language pair. Wav2Seq improves HuBERT and XLS-R (0.3B) models on most language pairs and achieves better average performance. Notably, our Wav2Seq (from XLS-R (0.3B) can match the performance of XLS-R (0.3B) with an mBART-ML50N1 decoder which has 4? the number of parameters in the decoder, which is pre-trained on 50 language unlabelled texts and then fine-tuned on a 50-languageto-English machine translation corpus. Using Wav2Seq as a second-stage pre-training method makes the decoder size flexible and requires no additional text data. <ref type="table">Table 6</ref> shows the test BLEU scores of the four highresource X-to-English language pairs. Our Wav2Seq (from XLS-R (0.3B)) outperforms the counterpart using an mBART-ML50N1 decoder (row 1 vs. 3). Since the input audio is always in English, we do not observe performance gain with a multi-lingual pre-trained encoder (row 1 vs. 2). XMEF models <ref type="bibr" target="#b34">[35]</ref> and XLS-R (2B) <ref type="bibr" target="#b3">[4]</ref> achieve better BLEU scores with a mBART-ML50N1 decoder and more parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X-to-English High-resource Experiments</head><p>English-to-X Low-resource Experiments Since English is a high resource language, all Englishto-X language pairs in CoVoST-2 have 430 hours of labelled audios. We subsample a 10 hour subset for de-en, ca-en, ar-en, tr-en language pairs. We choose these four language pairs because they are studied more often in prior works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b59">60]</ref>. <ref type="table" target="#tab_9">Table 7</ref> shows the test BLEU scores of the models fine-tuned on each 10h training set. Wav2Seq (from HuBERT-large) outperforms HuBERT-large baseline.</p><p>English-to-X High-resource Experiments <ref type="table">Table 8</ref> shows the test BLEU scores on four English-to-X pairs with the full 430h labelled data used for fine-tuning. Our Wav2Seq (from HuBERT-large) model outperforms most of the models with the same size encoder and match the performance of the models with 2? or 3? encoder size (W2V2 (0.72B) and XLS-R (1B)). It also matches the performance of a wav2vec 2.0 large using 60K hours of audios for self-training. <ref type="table" target="#tab_11">Table 9</ref> shows the performance gain of Wav2Seq with different amounts of labelled data. A pre-trained decoder has higher impact on low resource setup, but the gain diminishes with more supervision.   <ref type="table" target="#tab_2">Table 10</ref>: WER (%) on LibriSpeech dev-other set for pseudo ASR tasks with different target sequences. The length compression rate is the ratio between the decoder sequence length and the encoder sequence length (100% means no compression) computed on dev-other. We also show the compression rate of English tokens. The compression rate of pseudo subwords lies between English subwords and English tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Study</head><p>We conduct ablation study using the small scale model in Subsubsec. 5.1.1 to understand the gain of each component in the generation of pseudo subwords. <ref type="table" target="#tab_2">Table 10</ref> shows the results of pre-training with different type of target sequences. Deduplication plays a vital role and using BPE tokenization to convert characters into subwords further reduces WER. We hypothesize that the duration of these characters is less relevant to the semantic of speech and it is more important to capture the transition of different characters. We provide further ablation studies in Subsec. A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>We present Wav2Seq, a self-supervised learning framework for pre-training speech processing models. Unlike existing methods, Wav2Seq pre-trains both encoder and decoder parameters, thereby allowing the two main components of common encoder-decoder architectures to benefit from pre-training. Critically, Wav2Seq requires raw audio data only. Instead of using aligned text, we create a pseudo ASR task in which models transcribe audio inputs into pseudo subword tokens. We show that Wav2Seq closes the performance gap between encoder-decoder models and CTC models under low-resource conditions in ASR. On the SLUE-VoxPopuli spoken NER task, Wav2Seq achieves the best performance among all E2E models. On speech-to-text translation tasks, Wav2Seq consistently improves the performance of HuBERT and XLS-R and rivals mBART decoder initialization, which requires additional language data, while being more flexible and having fewer parameters.</p><p>We demonstrate the potential of encoder-decoder models, which are applicable to more diverse tasks than CTC models. Understanding the impact of additional data (e.g., via knowledge distillation or self-training) or combining with external models (e.g., LMs trained on large text corpora) is an important direction for future studies. Another interesting direction is to apply pseudo subwords to generative spoken language models <ref type="bibr" target="#b31">[32]</ref>. Last but not least, it is possible to add CTC loss to the encoder during fine-tuning and do joint CTC/attention decoding <ref type="bibr" target="#b24">[25]</ref> to further improve the performance of Wav2Seq.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>We study ASR models that do not use language models (LMs). As seen in prior work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">26]</ref>, an external language model can boost the performance of CTC models. Due to the architecture difference between CTC and Seq2Seq, different LMs have to be used with CTC and Seq2Seq models which further complicates the experimental setup. The order of the performance may be swapped when different LMs are applied. Moreover, because CTC models are faster during inference, CTC models remain a valid choice for ASR if the performance difference is small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional Experiments</head><p>A.1 More Ablation Study <ref type="table" target="#tab_2">Table 11</ref> shows the ablation study on the number of clusters and vocabulary size. As we can see, number of clusters has a large impact on the compression rate of the target sequence length. Number of clusters has a larger impact on the length compression rate and the WER after fine-tuned on LibriSpeech. A moderate compression is preferable; too much or too less can hurt the performance. Finally, <ref type="table" target="#tab_2">Table 12</ref> shows the performance of using the optional average pooling. As we can see, having kernel size 2 is beneficial. The main goal for applying average pooling is to reduce prepossessing and pre-training cost: we store fewer HuBERT features and reduce the time spent on k-means and BPE tokenization for free.  <ref type="table" target="#tab_2">Table 12</ref>: Ablation study on average pooling kernel size K. We use C = 500, V = 30000 in this experiment.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2 9 SoTA:</head><label>29</label><figDesc>Use an mBART decoder (pretrained on multilingual machine translation data): XLS-R (0.3B) + mBART-ML50N1 315 459 4Use a larger encoder + an mBART decoder (pretrained on multilingual machine translation data): XLS-R (2B) + mBART-ML50N1 2162 459 16.7 17.1 11.1 1.6 31.7 29.6 19.5 19.6 0.5 3.5 16.5 14.0 15.1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Transducer Models We conduct similar experiments on a Transducer<ref type="bibr" target="#b19">[20]</ref> model architecture and show that Wav2Seq pre-training is not restricted to Seq2Seq models (rows 11 and 12). Because Small model ASR experiment with unsupervised pretraining on LibriSpeech 960h and fine-tuning with LibriSpeech 10h ASR labels. All models have embedding size 256, FFN size 1,024, and four attention heads in their Transformer layers. We show that Wav2Seq works with both Seq2Seq and Transducer architectures. Moreover, Wav2Seq is complementary to HuBERT pre-trainingwhen initializing the encoder with HuBERT and having a second stage pre-training with Wav2Seq, we observe the best results.</figDesc><table><row><cell cols="3">ASR Type # Pre-training Method</cell><cell></cell><cell cols="3">Pre-train Iterations Enc Dec WER (%) Layers dev-other</cell></row><row><cell>CTC</cell><cell>1 HuBERT 2 HuBERT</cell><cell></cell><cell></cell><cell>100K 100K</cell><cell cols="2">6 12 0 0</cell><cell>51.7 36.5</cell></row><row><cell></cell><cell cols="2">3 No pre-training</cell><cell></cell><cell>0</cell><cell>6</cell><cell>6</cell><cell>?100.0</cell></row><row><cell></cell><cell>4 HuBERT</cell><cell></cell><cell></cell><cell>100K</cell><cell>6</cell><cell>6</cell><cell>?100.0</cell></row><row><cell>Seq2Seq</cell><cell cols="2">5 HuBERT 6 Wav2Seq</cell><cell></cell><cell>100K 100K</cell><cell cols="2">12 6 6 6</cell><cell>85.8 38.1</cell></row><row><cell></cell><cell cols="2">7 Wav2Seq</cell><cell></cell><cell>100K</cell><cell>8</cell><cell>4</cell><cell>36.6</cell></row><row><cell></cell><cell cols="5">8 Wav2Seq (from HuBERT) 100K + 25K 6</cell><cell>6</cell><cell>34.7</cell></row><row><cell></cell><cell cols="2">9 No pre-training</cell><cell></cell><cell>0</cell><cell>6</cell><cell>3</cell><cell>93.5</cell></row><row><cell>Transducer</cell><cell cols="2">10 HuBERT 11 Wav2Seq</cell><cell></cell><cell>100K 100K</cell><cell>6 6</cell><cell>3 3</cell><cell>93.6 44.2</cell></row><row><cell></cell><cell cols="5">12 Wav2Seq (from HuBERT) 100K + 25K 6</cell><cell>3</cell><cell>40.8</cell></row><row><cell cols="2">Labels Model</cell><cell cols="5">PT Iter. ASR Iter. dev-c dev-o test-c test-o</cell></row><row><cell cols="2">10h S2T</cell><cell>0</cell><cell>15K</cell><cell cols="3">?100 ?100 ?100 ?100</cell></row><row><cell cols="3">10h Wav2Seq-S2T 300K</cell><cell>5K</cell><cell cols="3">45.0 54.5 48.0 55.1</cell></row><row><cell cols="2">100h S2T</cell><cell>0</cell><cell>150K</cell><cell cols="3">16.4 31.6 16.9 32.8</cell></row><row><cell cols="3">100h Wav2Seq-S2T 300K</cell><cell>50K</cell><cell cols="3">12.6 25.8 12.8 26.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>LibriSpeech WER (%) using fairseq S2T Transformer-small model [57]. 10h or 100h of labelled transcripts are provided. With 10h labelled data, the model cannot learn effectively without Wav2Seq pre-training. With 100h labelled data, pre-training the S2T model using Wav2Seq on 960h unlabelled audio results in the same model architecture achieving better performance with only 1/3 of the training time.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Librispeech dev-other WER with different amount of labelled data. Wav2Seq as a second stage pre-training method significantly improves the Seq2Seq model performance and allows it to match or even outperform CTC fine-tuning. No language models are used.</figDesc><table><row><cell cols="2">Model Type PT method</cell><cell>dev test</cell></row><row><cell>Pipeline</cell><cell>W2V2-large + DeBERTa-large</cell><cell>63.3 57.8</cell></row><row><cell>Pipeline</cell><cell>W2V2-large + LM + DeBERTa-large</cell><cell>74.9 69.6</cell></row><row><cell>E2E CTC</cell><cell>W2V2-large</cell><cell>55.6 50.9</cell></row><row><cell>E2E CTC</cell><cell>W2V2-large + LM</cell><cell>70.2 64.8</cell></row><row><cell cols="2">E2E Seq2Seq HuBERT-large</cell><cell>64.0 58.5</cell></row><row><cell cols="2">E2E Seq2Seq Wav2Seq (from HuBERT-large)</cell><cell>71.7 65.4</cell></row><row><cell cols="2">SoTA using external data:</cell><cell></cell></row><row><cell>E2E CTC</cell><cell cols="2">W2V2-base + Distill-NLP (500h ASR data) 82.2 N/A</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Development and test F1 scores (%) on SLUE-VoxPopuli NER</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 shows</head><label>4</label><figDesc>F1 scores on SLUE-VoxPopuli dev and test sets. While pipeline models remains the best without external data, Seq2Seq models significantly outperform the CTC baseline 4 without a language model (LM). Wav2Seq (from HuBERT-large) further improves performance (from 58.5% to 65.4% test F1), and obtains the best F1 score among all E2E models without external data, outperforming the best CTC model with an LM. Using external data<ref type="bibr" target="#b41">[42]</ref> remains the state-of-the-art. We leave the study of leveraging external data for Seq2Seqs model for future work.</figDesc><table><row><cell></cell><cell>Fr</cell><cell>De</cell><cell>Es</cell><cell>Ca Avg.</cell></row><row><cell>Wav2Seq (from HuBERT-large)</cell><cell cols="4">33.2 26.9 34.0 29.7 31.0</cell></row><row><cell>Wav2Seq (from XLS-R (0.3B))</cell><cell cols="4">33.0 28.0 33.8 29.9 31.1</cell></row><row><cell cols="5">Use an mBART decoder (pretrained on multilingual machine translation data):</cell></row><row><cell>XLS-R (0.3B) + mBART-ML50N1</cell><cell cols="4">32.9 26.7 34.1 28.7 30.6</cell></row><row><cell>XLSR-53 + mBART-ML50N1</cell><cell cols="4">32.3 26.9 33.3 28.6 30.3</cell></row><row><cell>VP-100K + mBART-ML50N1</cell><cell cols="4">30.4 23.4 31.1 25.7 27.7</cell></row><row><cell>XMEF-En + mBART-ML50N1</cell><cell cols="4">35.0 28.2 35.2 31.1 32.4</cell></row><row><cell>XMEF-X + mBART-ML50N1</cell><cell cols="4">36.1 30.6 38.1 31.8 34.2</cell></row><row><cell cols="2">SoTA: Use a large encoder and an mBART decoder:</cell><cell></cell><cell></cell></row><row><cell>XLS-R (2B) + mBART-ML50N1</cell><cell cols="4">37.6 33.6 39.2 33.8 36.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Test BLEU scores on CoVoST-2 En-to-X language pairs under a low-resource fine-tuning setup where only 10h labelled audio is provided.</figDesc><table><row><cell>De Ca</cell><cell>Ar</cell><cell>Tr Avg.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Test BLUE scores on CoVoST-2 En-to-De with different amounts of labelled data. The gain from self-supervised pre-trained decoder decreases as the amount of the labelled data increases.</figDesc><table><row><cell>Method</cell><cell>Target Tokens</cell><cell cols="2">Length Compression WER (%)</cell></row><row><cell>Wav2Seq</cell><cell>Psuedo Subwords</cell><cell>17.4 %</cell><cell>38.1</cell></row><row><cell cols="2">-BPE tokenization Pseudo Characters</cell><cell>39.2 %</cell><cell>44.1</cell></row><row><cell>-Deduplication</cell><cell>Hidden Units</cell><cell>100.0 %</cell><cell>96.5</cell></row><row><cell></cell><cell>English Words</cell><cell>5.2 %</cell><cell></cell></row><row><cell></cell><cell>English Subwords</cell><cell>8.4 %</cell><cell></cell></row><row><cell></cell><cell>English Characters</cell><cell>27.6 %</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>Ablation study on number of clusters C and BPE vocabulary size V . We fix average pooling size K = 2. We use small-scale model and report WER on LibriSpeech dev-other.</figDesc><table><row><cell cols="3">Ave. Pool size (K) Length Compression WER (%)</cell></row><row><cell>1</cell><cell>18.5 %</cell><cell>41.1</cell></row><row><cell>2</cell><cell>14.8 %</cell><cell>40.3</cell></row><row><cell>4</cell><cell>10.7 %</cell><cell>41.3</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/huggingface/tokenizers<ref type="bibr" target="#b1">2</ref> We tune the learning rate ? {2 ? 10 ?4 , 10 ?4 , 5 ? 10 ?5 , 2 ? 10 ?5 } on the small models on ASR and fix them.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/pytorch/fairseq/blob/main/examples/speech_to_text/docs/ librispeech_example.md</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Shon et al.<ref type="bibr" target="#b52">[53]</ref> show that W2V2-base is slightly better (50.2% vs. 49.8% test F1) than HuBERT-base and only report the scores of W2V2-large, so we choose it as the CTC baseline.<ref type="bibr" target="#b4">5</ref> 25K updates is about 0.3 epoch of LibriLight, so the model only sees about 20K hour audio during second-stage pre-training.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep speech 2: End-to-end speech recognition in english and mandarin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishita</forename><surname>Sundaram Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingliang</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Unified-modal encoder-decoder pre-training for spoken language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyi</forename><surname>Ao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihua</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.07205</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">K-means++: The advantages of careful seeding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergei</forename><surname>Vassilvitskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SODA</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Xls-r: Self-supervised cross-lingual speech representation learning at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andros</forename><surname>Tjandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kushal</forename><surname>Lakhotia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiantong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kritika</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yatharth</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Saraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pino</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.09296</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework for self-supervised learning of speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11477</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Data2vec: A general framework for self-supervised learning in speech, vision and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiantong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14294</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Quoc Le, and Oriol Vinyals. Listen, attend and spell: A neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Large-scale self-supervised pre-training for full stack speech processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoyuki</forename><surname>Kanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Yoshioka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiong</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.13900</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. PMLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Attentionbased models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">W2v-bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-An</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.06209</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Wav2letter: an end-to-end convnet-based speech recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03193</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Unsupervised cross-lingual representation learning for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdel</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.13979</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A new algorithm for data compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Gage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">C Users Journal</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">End-to-end named entity and semantic concept extraction from speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahar</forename><surname>Ghannay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Caubri?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannick</forename><surname>Est?ve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathalie</forename><surname>Camelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edwin</forename><surname>Simonnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Morin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SLT</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">ICML</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1211.3711</idno>
		<title level="m">Sequence transduction with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Conformer: Convolution-augmented transformer for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anmol</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.08100</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anmol</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.03191</idno>
		<title level="m">Contextnet: Improving convolutional neural networks for automatic speech recognition with global context</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Joint ctc/attention decoding for end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="518" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">How much can a bad teacher benefit asr pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Bolte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hubert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Self-Supervised Learning for Speech and Audio Processing Workshop @ NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Robust wav2vec 2.0: Analyzing domain shift in self-supervised pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuroop</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Likhomanenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiantong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineel</forename><surname>Pratap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01027</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Espnet-st: All-in-one speech translation toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirofumi</forename><surname>Inaguma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun</forename><surname>Kiyono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigeki</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelson</forename><forename type="middle">Enrique</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalta</forename><surname>Soplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10234</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Tying word vectors and word classifiers: A loss framework for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khashayar</forename><surname>Hakan Inan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01462</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Librilight: A benchmark for asr with limited or no supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rivi?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kharitonov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Mazar?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Karadayi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fuegen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Likhomanenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/libri-light" />
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Parp: Prune, adjust and re-prune for self-supervised speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-I Jeff</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Lun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Sung</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaizhi</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Khurana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glass</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05933</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Adelrahman Mohamed, et al. Generative spoken language modeling from raw audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kushal</forename><surname>Lakhotia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Kharitonov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Bolte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tu-Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jade</forename><surname>Copet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.01192</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<meeting><address><addrLine>Bart</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Efficient self-supervised vision transformers for representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09785</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Multilingual speech translation with efficient finetuning of pretrained models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chau</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Pino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12829</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multilingual denoising pre-training for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="726" to="742" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajie</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Gowayyed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASRU</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Okhonko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.11660</idno>
		<title level="m">Transformers with convolutional context for asr</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Librispeech: An asr corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassil</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">On the use of external data for spoken named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankita</forename><surname>Pasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suwon</forename><surname>Shon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyu J</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.07648</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Wav2letter++: A fast open-source speech recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineel</forename><surname>Pratap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Awni</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiantong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaliy</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6460" to="6464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Using the output embedding to improve language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Prophetnet: Predicting future n-gram for sequence-to-sequence pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhen</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeyun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayiheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiusheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruofei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04063</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Multitask prompted training enables zero-shot task generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lintang</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Alyafeai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Chaffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Stiegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raja</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.08207</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">wav2vec: Unsupervised pretraining for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Web-scale k-means clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sculley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suwon</forename><surname>Shon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankita</forename><surname>Pasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Brusco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyu J</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.10367</idno>
		<title level="m">Slue: New benchmark tasks for spoken language understanding evaluation on natural speech</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Neural speech recognizer: Acoustic-to-word lstm model for large vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hagen</forename><surname>Soltau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hank</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hasim</forename><surname>Sak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09975</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A?ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Fairseq s2t: Fast speech-to-text modeling with fairseq</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xutai</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Okhonko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Pino</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.05171</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Covost 2 and massively multilingual speech-to-text translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Pino</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.10310</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Voxpopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rivi?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Talnikar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Haziza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Dupoux</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00390</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Large-scale selfand semi-supervised learning for speech translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.06678</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigeki</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiro</forename><surname>Nishitoba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuya</forename><surname>Unno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelson</forename><forename type="middle">Enrique</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalta</forename><surname>Soplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jahn</forename><surname>Heymann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Wiesner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanxin</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00015</idno>
		<title level="m">End-to-end speech processing toolkit</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Performanceefficiency trade-offs in unsupervised pre-training for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwangyoun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Artzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.06870</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Self-training and pre-training are complementary for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiantong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Likhomanenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paden</forename><surname>Tomasello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11430</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">End-to-end named entity recognition from English speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hemant</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sreyan</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajiv Ratn</forename><surname>Shah</surname></persName>
		</author>
		<editor>Interspeech</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Han</forename><surname>Shu-Wen Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Sung</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-I Jeff</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kushal</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lakhotia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><forename type="middle">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuankai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan-Ting</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.01051</idno>
		<title level="m">Speech processing universal performance benchmark</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Pegasus: Pre-training with extracted gap-sentences for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PMLR, 2020. Number of clusters (C) BPE vocab size (V) Length Compression WER (%)</title>
		<imprint>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
	<note>International Conference on Machine Learning</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
