<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Skeleton Transformer Networks: 3D Human Pose and Skinned Mesh from Single RGB Image</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Yoshiyasu</surname></persName>
							<email>yusuke-yoshiyasu@aist.go.jp</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryusuke</forename><surname>Sagawa</surname></persName>
							<email>ryusuke.sagawa@aist.go.jp</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ko</forename><surname>Ayusawa</surname></persName>
							<email>k.ayusawa@aist.go.jp</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiko</forename><surname>Murai</surname></persName>
							<email>a.murai@aist.go.jp</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">National Institute of Advanced Industrial Science and Technology (AIST)</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">CNRS-AIST JRL (Joint Robotics Laboratory)</orgName>
								<address>
									<postCode>UMI3218/RL</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Skeleton Transformer Networks: 3D Human Pose and Skinned Mesh from Single RGB Image</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Convolutional neural networks</term>
					<term>3D human pose</term>
					<term>skeleton</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present Skeleton Transformer Networks (SkeletonNet), an end-to-end framework that can predict not only 3D joint positions but also 3D angular pose (bone rotations) of a human skeleton from a single color image. This in turn allows us to generate skinned mesh animations. Here, we propose a two-step regression approach. The first step regresses bone rotations in order to obtain an initial solution by considering skeleton structure. The second step performs refinement based on heatmap regressor using a 3D pose representation called cross heatmap which stacks heatmaps of xy and zy coordinates. By training the network using the proposed 3D human pose dataset that is comprised of images annotated with 3D skeletal angular poses, we showed that SkeletonNet can predict a full 3D human pose (joint positions and bone rotations) from a single image in-the-wild.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Estimating 3D human pose from a single image is an important yet very challenging problem in computer vision, where applications range from surveillance to robotics. Recent work has shown that convolutional neural networks (Con-vNets) can detect 2D joint positions accurately. The key to achieving accurate predictions is to represent 2D joint locations as heatmaps and iteratively refines them gradually by incorporating context information <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b2">3]</ref>-early approaches <ref type="bibr" target="#b24">[25]</ref> on the other hand directly perform regression of the 2D joint coordinates using ConvNets, which is a difficult problem that needs to model a highly-nonlinear mapping from an input image to real values. Recent techniques, such as Open-Pose <ref type="bibr" target="#b4">[5]</ref>, can robustly detect 2D joints of multiple people in an image.</p><p>In contrast to its 2D counterpart, the progress of 3D human pose detection has been relatively slow. The main challenges regarding to 3D human pose estimation is as follows: 3D pose representation To predict 3D joint locations using ConvNets, 3D pose representation used is critical, which affects prediction accuracy. Previous approaches have shown that regression of a 3D pose using heatmaps (e.g., volumetric <ref type="bibr" target="#b17">[18]</ref> and 2D heatmaps + depth <ref type="bibr" target="#b27">[28]</ref>) leads to accurate 3D joint predictions.</p><p>On the other hand, regression of joint angles using ConvNets <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b9">10]</ref> has not been successful so far in contributing to accurate 3D joint localizations, because they are difficult to learn with ConvNets due to their high non-linearity. From the application point of view, such as computer animation and biomechanics, it is desirable to predict not only 3D joint locations but also angular pose of skeleton, e.g., joint angles or segment rotations. Data scarcity Compare to 2D human pose dataset, 3D human pose dataset is smaller in size. This is because obtaining a 3D human pose dataset where the images paired with 3D joint annotations requires time and effort. In particular, annotations of 3D angular skeletal poses are difficult to obtain. One common way to achieve this is to use a motion capture (MoCap) system and RGB video cameras at the same time. However, such dataset are usually limited to a small variety of subjects-for example, Human 3.6M dataset <ref type="bibr" target="#b8">[9]</ref>, which is the most common dataset for 3D human pose, is limited to around 10 subjects. Consequently, it is difficult to learn sufficient visual features from 3D pose dataset solely to localize 2D/3D joints accurately.</p><p>Skeletal structure has been incorporated into 3D human pose estimation as a form of constraints or subspace. In biomechanics and robotics, forward and inverse kinematics have been well-studied and are used to generate human pose from MoCap by controlling joint angles of a skeleton. Previous approaches in the computer vision field estimated 3D human pose from 2D key points using a statistical model and enforcing constraints such as segment length <ref type="bibr" target="#b20">[21]</ref>, joint limit <ref type="bibr" target="#b0">[1]</ref> and symmetry. In computer graphics, human skeletal pose is often represented using linear or affine transformation matrices, as can be seen for example in linear blend skinning for character animation.</p><p>In this paper, we propose skeleton transformer networks (SkeletonNet) for 3D human pose detection which respects skeletal structure while attaining 3D joint prediction accuracy. SkeletonNet combines and benefits from the two paradigms, skeleton and heatmap representations. SkeletonNet first regresses bone rotations to an input image in order to have an initial solution which is not precisely accurate but considers skeletal structure. Starting from this initial solution, the second step refines it using ConvNet heamap regressor. This strategy allows us to recover reasonably accurate predictions of full 3D human poses (joint positions and bone rotations) from a single in-the-wild image. To contribute to solving the data scarcity problem, we also construct a dataset where 3D angular skeletal poses are annotated on in-the-wild images, based on a human-validation approach. By fusing the proposed dataset and Mocap dataset captured under a controlled environment (Human3.6M), SkeletonNet can predict a full 3D pose (joint positions and bone rotation) from a single image in-the-wild. In addition, experimental results showed that SkeletonNet outperforms previous approaches based on joint angles <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b28">29]</ref> in terms of MPJPE joint position accuracy.</p><p>The contributions of this work is summarized as follows:</p><p>-We propose an end-to-end ConvNet framework for predicting a full 3D human pose (joint positions and bone rotations).</p><p>-We propose a bone rotation regressor which predicts 3D human pose using 3 ? 3 transformation matrices. To make arbitrarily linear transformations into rotations, we propose a Gram Schmidt orthogonalization layer. This combination is the key to learning angular pose accurately with using Con-vNets. -We propose a 3D human pose representation called cross heatmap for accurate 3D joint detection. This representation combines two heat maps, one for representing 2D joints in image space (xy space) and the other for zy space. The benefit of this representation is that it is more efficient than the volumetric heatmaps <ref type="bibr" target="#b17">[18]</ref>, while accurately predicting 3D joint positions when trained using Mocap-video dataset (Human 3.6M) and in-the wild dataset (MPII with 3D pose annotations). -We built a 3D human pose dataset in-the-wild which includes annotations of 3D bone rotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Estimating 2D joint positions using ConvNets Recent work has shown that the detection of 2D joint positions can be done very accurately using convolutional neural networks (ConvNets). Toshev et al. <ref type="bibr" target="#b24">[25]</ref> first proposed a method based on ConvNets for detecting human pose i.e., 2D key points representing joint locations from a single image. Tompson et al. instead represented joint locations in images using 2D heat maps so that it can avoid complicated a nonlinear mapping that goes from an image to xy pixel coordinates. The recent techniques, such as the stacked hourglass network <ref type="bibr" target="#b15">[16]</ref> and its variants <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b5">6]</ref>, accurately predict 2D joint positions by iteratively refining 2D heatmaps.</p><p>Predicting 3D joint positions The early approaches predicts 3D joint positions from key points <ref type="bibr" target="#b19">[20]</ref>. These approaches assume that the almost perfect 2D key points are already extracted from an image. Li et al. <ref type="bibr" target="#b12">[13]</ref> first used ConvNets to directly regress 3D human joints with an image. There are two main reasons for the improvements on accuracy of 3D human pose detection. First, the recent approach combines multiple data sources to increase the 3D pose dataset <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b27">28]</ref>. Second, the recent techniques make use of more natural 3D joint representation. For example, Pavlakos et al. <ref type="bibr" target="#b17">[18]</ref> uses a volumetric heatmap representation, which can avoid regressing the real values in a highly nonlinear manner. Other methods first predicts 2D joints with heatmaps and then regress 3D joint positions or depths from them. Tome et al. <ref type="bibr" target="#b23">[24]</ref> have tried to iteratively update 3D joints represented as a weighted combination of PCA basis that is constructed from 3D MoCap dataset. Exploiting skeletal structure and predicting angular pose In biomechanics, robotics and computer animation fields, inverse kinematics has been well-studied and used to generate human pose from MoCap by controlling joint angles. Previous approaches <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b3">4]</ref> estimated 3D human pose from 2D key points by combining a statistical model and constraints such as joint limit <ref type="bibr" target="#b0">[1]</ref>, segment length <ref type="bibr" target="#b19">[20]</ref> and symmetry. Some methods perform regression of joint angles or axis angles <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b9">10]</ref> to estimate angular skeletal pose using ConvNets but the high nonlinearity prevents them from accurate prediction of joint locations.</p><p>Weakly supervision and predictions from in-the-wild images Recent works tackle the data scarcity problem by using both 3D human pose dataset captured in the experimental room and 2D human pose dataset captured in a wide range of environment <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23]</ref>. Sun et al. <ref type="bibr" target="#b22">[23]</ref> used compositional loss function that is defined by integrating 2D positions and depths by properly normalizing the regression target values. Zhou et al. <ref type="bibr" target="#b27">[28]</ref> took a weak supervised approach and used bone length constraint when 3D information (depths) is not available. The use of this approach not only enables 3D joint predictions from in-the wild images but also performance boost in joint prediction accuracy. We go further by building a 3D human pose dataset in-the-wild which includes annotations of 3D bone rotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Skeleton Transformer Networks</head><p>Skeleton Transformer Networks (SkeletonNet) is based on deep ConvNets, which predicts not only 3D joint positions but also rotations of body segments. Our approach is based on an end-to-end two-step regression approach. The network architecture of SkeletonNet is depicted in <ref type="figure" target="#fig_0">Fig. 1 (a)</ref>. The first step is called bone rotation regressor that performs regression of angular skeletal pose using 3 ? 3 rotation matrices to provide an initial solution. The resulting pose is not precisely accurate but it respects skeletal structure, without having to produce large errors such as left and right confusions. The second step, cross heatmap regressor, starts from this initial solution and refine the joint predictions using heatmap regression. We benefit from the two different paradigms, i.e., skeleton and ConvNets, to achieve accuracy and preservation of structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Bone rotation regressor</head><p>Bone rotation regressor predicts bone rotations of a human skeleton. We achieve this by solving two simpler problems separately: predicting 1) a global rotation and 2) transformations of bone segments relative to the root. The idea behind this strategy is that the global orientation of the body has some discrete patterns e.g., sit, stand and lie, which can be effectively solved as a classification problem.</p><p>On the other hand, bone rotations have more continuous distributions within some range, which can be effectively modeled as a regression problem.</p><p>To predict a global rotation, we convert the rotation estimation problem into a classification problem. Specifically, we first cluster the training dataset into 200 clusters based on its global rotations with k-means clustering. The samples within the same cluster is put in the same class. ConvNets is trained to output rotation class probabilities p g using Softmax. We use a classification loss (cross entropy) for supervision:</p><formula xml:id="formula_0">L RotG = L cls (p g ,p g )</formula><p>where L cls is the log loss andp g is the one-hot class label of global rotation. From the output probabilities p g , we obtain a 3?3 global transformation matrix by blending cluster centers C g , R g = C g p g . We tried linear blending of axis angles but found that blending matrices works better.</p><p>Since bone rotations relative to the root have more continuous distributions than global rotation, regression is more suitable in this case than classification.</p><formula xml:id="formula_1">Bone rotations R b = [R b 1 . . . R b n ],</formula><p>where n is the number of bones, are thus predicted directly using a 3?3 rotation matrix (9 parameters). The loss for bone rotations is defined using the mean squared error (MSE) loss as:</p><formula xml:id="formula_2">L RotB = n i ||vec(R b i ) ? vec(R b i )|| 2 2</formula><p>where vec(?) makes a matrix to a vector andR b is the ground truth bone rotations. Note that regression of rotation matrices demands more memory spaces because they need more parameters than other rotation representation, such as Euler angles, quaternions and axis angles. However, for 3D human pose detection, we have under 20 joints to predict, which means that the additional costs are almost negligible. The down side of Euler angles and quaternions are their nonlinearities and ambiguities (sign flips for quaternions and periodical angle jumps for Euler angles), which is difficult to use as the regression targets-we could not train a network properly using Euler angles as supervisions as reported in <ref type="bibr" target="#b28">[29]</ref>.</p><p>Gram Schmidt orthogonalization layer The problem of the above strategy is that it does not guarantee to produce orthonormal matrices. This means the resulting skeleton is deformed in an undesirable way with scales and shears.</p><p>To solve this issue, we propose the Gram Schmidt (GS) orthogonalization layer which performs GS to make transformations into rotations. GS requires elemental functions only, such as dot product, subtraction and division, which is differentiable and can be relatively easily incorporated into ConvNets. We input global transformation R g and bone transformations R b into the GS layer to make transformations to rotations, obtaining R g and R b . Once transformations are orthonormalized, we multiply a global rotation R g and bone rotations R b in order to obtain the absolute bone rotations. Finally, 3D joint positions are computed by applying these absolute rotations to the original bone vectors in the rest pose and performing linear integration to add up bone vectors from the root ( <ref type="figure" target="#fig_0">Fig. 1 (b)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Cross heatmap regressor</head><p>We propose cross heatmap regressor which is used for refining the 3D joints obtained using bone rotation regressor. In the current design, cross heatmap regressor stacks xy and zy heatmaps ( <ref type="figure" target="#fig_0">Fig. 1 (c)</ref>) because they sufficiently cover xyz coordinates and the variance of human joint locations in zy coordinates are usually larger than that of zx space.</p><p>To integrate bone rotation regressor and cross heatmap regresor, we project 3D joint positions obtained in Sec 3.1 into the image plane (xy plane). Here, we did not estimate a camera pose and scale explicitly. Instead we scale the xy coordinates to 90% of the width of the first upsampling layer, which is 16 pixels. From the projected 2D joints, 2D Gaussian maps <ref type="bibr" target="#b17">[18]</ref> are obtained and, after convolutions, they are summed up with the feature maps from bone rotation regressor to serve as approximate positions of 2D joints for cross heatmap regressor. Note that all of these process are differentiable, which can be optimized using back propagation.</p><p>Once the feature maps are up-sampled three times to the size of 64 ? 64, a single stack of hourglass module <ref type="bibr" target="#b15">[16]</ref> is used to compute cross heatmaps. The cross heatmap representation concatenates two heatmaps, one for representing 2D joints in image space (xy space) h xy and the other for the zy space, h zy . For training we use the MSE loss as:</p><formula xml:id="formula_3">L hm = m i j,k ||h xy (j,k) ?h xy (j,k) || 2 2 + m i j,k ||h zy (j,k) ?h zy (j,k) || 2 2</formula><p>whereh xy andh zy are the ground truth heatmaps for xy and zy spaces. In addition, m is the number of joints. The benefit of this representation is that it is more compact and efficient than the volumetric heatmaps, while maintaining accuracy. To extract xyz coordinates from cross heatmaps in an end-to-end manner, we use spatial argmax layers similar to those proposed in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b11">12]</ref>. Finally, we compute rotations dR that align bone vectors obtained using bone rotation regressor with that of the final predicted positions. They are multiplied with the predicted absolute bone rotations R init to make them consistent with the final joint positions x. This way, we can exploit accurate heatmaps to refine rotations, avoiding a difficult regression problem of nonlinear angle parameters. Now that the loss for the final positions x and rotations R are defined as:</p><formula xml:id="formula_4">L pos = m i ||x i ?x i || 2 2 , L Rot = n i ||vec(R i ) ? vec(R i )|| 2 2</formula><p>where andx i andR i are the ground truth labels of positions and rotations.</p><p>With the final positions x and rotations R, linear blend skinning can be done to produce a 3D mesh. Note that we perform skinning outside the network but this process can also be done within the network in an end-to-end manner, as this is a linear matrix multiplication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Loss function</head><p>For supervision, a standard cross entropy loss and MSE loss is used for comparing the predictions and ground truth labels of the global rotation class probability, bone rotations and cross heatmaps. In total, we minimize the loss function of the form: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">In-the-wild 3D Human Pose Dataset</head><p>To build 3D human pose dataset that have variations in clothes and background, we annotate 3D positions and bone rotations on MPII human pose dataset as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. To this end, we followed the human verification approach <ref type="bibr" target="#b16">[17]</ref>. Here a 3D human pose is obtained from 2D key points using a previous technique, which will then be sorted by human annotators whether each result is acceptable or not. This is in spirit similar to the concurrent work of <ref type="bibr" target="#b10">[11]</ref>. The aim for constructing our dataset is not to provide accurate 3D pose annotations but to obtain reasonable ones on in-the-wild images, including joint positions and rotations, such that they can be used for training our bone rotation regressor. In fact, the annotators are instructed to judge whether the pose is 'acceptable' or 'bad', e.g., if the global rotation of the resulting pose looks deviating from the true pose more than 30 deg, the pose is 'bad'. As a consequence, our dataset have more 3D pose annotations than <ref type="bibr" target="#b10">[11]</ref>, possibly at the cost of accuracy.</p><p>To obtain 3D joint positions from 2D key points, we use the projected matching pursuit (PMP) approach <ref type="bibr" target="#b19">[20]</ref>. This method calculates a camera pose, scale and 3D joint positions as a combination of PCA basis that is constructed from Mocap database. From the resulting 3D joint positions, rotations of bones are obtained based on a method which is conceptually similar non-rigid surface deformation techniques <ref type="bibr" target="#b21">[22]</ref>. Specifically, the skeleton in the rest shape is fitted to the PMP result by balancing the rigidity of bones, the smoothness between bone rotations and the position constraints to attract the skeleton to them. The initial rotations are obtained from local coordinate frames, which are defined in a similar manner using <ref type="bibr" target="#b0">[1]</ref>. Reconstructing one model from 2D key points takes approx. 1min.</p><p>We also designed a simple annotation tool <ref type="figure" target="#fig_1">Fig. 2 (b)</ref> to simplify the process of 3D pose annotations. With this tool, human annotators are just required to decide a 3D pose is acceptable or not. In addition, we obtained skin meshes from the 3D pose using linear blend skinning and showed rendered images. By visualizing a skin mesh, it makes the annotators' decisions significantly easier and quicker. From among the images in MPII dataset, we extracted those with all 16 joints are inside the image region, which was approximately 20000 poses. After annotations, we were able to collect 10291 images with 3D pose annotations. Note that we remove 'bad' poses and do not use them in the training. It took about 2-3 hours for an annotator to process 1000 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset and evaluation protocols</head><p>MPII <ref type="bibr" target="#b1">[2]</ref> This dataset contains in-the-wild images for 2D human pose estimation, which includes 25k training images and 3k validation images. Those images are annotated with 2D joint locations and bounding boxes. In Section 4, we constructed a 3D pose dataset on top of MPII dataset by annotating 3D joints to images. This dataset is used in training.</p><p>Human3.6M Human 3.6M dataset <ref type="bibr" target="#b8">[9]</ref> is used in training and testing. Human 3.6M dataset is a large scale dataset for 3D human pose detection. This dataset contains 3.6 million images of 15 everyday activities, such as walking, sitting and making a phone call, which is performed by 7 professional actors. 3D positions of joint locations captured by motion capture (Mocap) systems are also available in the dataset. In addition, 2D projections of those 3D joint locations into images are available. The images are taken from four different views. As with previous researches, we down-sampled the video from 50fps to 10fps in order to reduce redundancy in video frames. We followed the same evaluation protocol used in previous approaches <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b27">28]</ref> for evaluation, where we use 5 subjects (S1, S5, S6, S7, S8) for training and the rest 2 subjects (S9, S11) for testing. We used the 3D model of an actor provided in Human3.6 to generate mesh animations but this could be replaced with any 3D models.</p><p>The error metric used is called mean per joint position error (MPJPE) in mm. In the evaluation protocol, the position of the root joints is aligned with the ground truth but the global orientation is kept as is. Following <ref type="bibr" target="#b27">[28]</ref> the output joint positions from ConvNets is scaled so that the sum of all 3D bone lengths is equal to that of a canonical average skeleton. This is done by:</p><formula xml:id="formula_5">P j = (P pred j ? P pred 0 ) ? l ave /l pred (1)</formula><p>where P pred j is the predicted position, P 0 is the root position, l pred is the sum of skeleton length of the predicted skeleton and l ave is the average of sum of skeleton length for all the training subjects in Human 3.6M dataset.</p><p>We also evaluated the method with the error measure called the reconstruction error, where, before calculating the error, the result is aligned to the ground truth with a similarity transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baselines</head><p>Four baseline methods are implemented to conduct ablation studies. We trained the first three networks using Human3.6M dataset only and the fourth one with both Human3.6M and MPII dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rotation regress (Rot reg) only This only uses bone rotation regressor. The position is computed from linear integration of bone rotations. Heatmap (HM) only This on the other hand only uses cross heatmap regressor.</head><p>Rot reg + HM This method is our proposed method which combines bone rotation regressor and cross heatmap regressor. All This is our proposed method trained using Human 3.6M and MPII dataset with 3D annotations obtained using the method presented in Sec 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Implementation and training detail</head><p>Our method is implemented using MatConvNet toolbox <ref type="bibr" target="#b25">[26]</ref>. For the bone rotation regressor, we use ResNet50 <ref type="bibr" target="#b7">[8]</ref> as the base network, which is pre-trained on the ImageNet dataset. A single up-sampling layer and a single hourglass module is followed by the base network to predict heatmaps. We also use skip connections to connect skeleton regression layers and up-sampling layers. We used a skeleton with 16 joints and 15 segments each of them have 9 rotational parameters. The definition of joints is same as that of MPII dataset. Training a whole model takes about 1 day using three NVIDIA Quadro P6000 graphics cards with 24 GB memory. The batch size is 30 for each GPU. As for augmentation, we used left/right flip only-no scaling and rotation augmentation is used. We trained a model with SGD for 70 epochs, starting from the learning late of 0.001 and decreasing it to 0.00001. During test time, a single forward pass of our network is approx. 0.12 sec, which means the performance of our method is approx. 8-9 fps.</p><p>We set the parameters in the loss function as ? = ? = ? = 0.1 and ? = 0.001. When training with both Human 3.6M and MPII datasets, we randomly selected the images from both dataset such that half of the batch is filled with Human 3.6M and the other half by MPII dataset, following <ref type="bibr" target="#b27">[28]</ref>. Since the 3D annotations of MPII are not accurate, we use them for supervising bone rotation regressor only. Thus, when training images are from MPII, we do not back propagate gradients from the losses that include the final 3D pose, h zy , x and R, (i.e., L pos , L Rot and the right term of L hm ) to the network. On the other hand, when the training sample is from Human3.6M dataset, which has accurate 3D annotations, we minimize all the losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results</head><p>Qualitative results In <ref type="figure" target="#fig_3">Figs. 3 and 6</ref>, we show the result of our 3D pose prediction method. Our method is able to predict 3D joint positions and bone orientations reasonably accurately even on in-the wild-images. Because our method can predict bone rotations of human body skeletons, we can produce mesh animations from 3D joint positions and rotations using linear blend skinning. Note that the rotations of hands and feet are not supervised.</p><p>Comparisons to other state-of-the-art We have compared our techniques against other state-of-the-art in <ref type="table" target="#tab_0">Table 1</ref>. Our technique is comparable with other state-of-the-art in terms of MPJPE. Volumetric heatamaps <ref type="bibr" target="#b17">[18]</ref> can achieve MPJPE approx. 71 mm. However, it got worse results when including MPII (MPJPE 78 mm) with their decoupled structure, whereas we are around MPJPE 70 mm. Also, compared with <ref type="bibr" target="#b17">[18]</ref> with two stacks of hourglass networks, cross heatmap is more compact, which requires 1/32 of memory spaces to store.</p><p>Compared with the previous techniques that predict angular poses <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b9">10]</ref>, SkeletonNet is more accurate. In fact, MPJPE of our result is 69.9 mm, whereas that of Kanazawa et al. <ref type="bibr" target="#b9">[10]</ref> is 87.97 mm. In <ref type="table" target="#tab_1">Table 2</ref>, we also compared the reconstruction error with previous approaches. Our technique outperforms previous techniques that iteratively optimizes joint angles <ref type="bibr" target="#b3">[4]</ref> and perform regression of joint angles <ref type="bibr" target="#b18">[19]</ref>. The benefit of SkeletonNet is, in addition to estimating 3D joint positions relatively accurately, we can predict 3D bone rotations, which is useful in animating a human body mesh or possibly predicting dynamics such as joint torques. Comparisons between baselines In <ref type="table" target="#tab_2">Table 3</ref> and <ref type="figure" target="#fig_4">Fig. 4</ref>, we show comparisons between the baselines. As can be seen from <ref type="figure" target="#fig_4">Fig. 4</ref> a, the result of bone rotation regressor preserves skeletal structure, but the joint positions are not accurate  enough. With only heatmaps, however, skeletal structure is sometimes destructed e.g., by the left and right flips. By combining our bone rotation and cross heatmap regressor, a more accurate result is produced, while preserving skeletal structure. Note that in previous work this kind of confusions are remedied by incorporating recurrence <ref type="bibr" target="#b2">[3]</ref> or using many stacks of a hourglass module <ref type="bibr" target="#b15">[16]</ref>. By training with both Human 3.6M and MPII, we get the best result <ref type="table" target="#tab_2">(Table 3</ref>). In addition, we found that annotating 3D rotations is important for reconstructing human poses from in-the-wild images <ref type="figure" target="#fig_4">(Fig. 4 b)</ref>. Thus, the key to our improvements in MPJPE is the use of cross heatmap and the use of MPII dataset in training. Even when MPII dataset is not provided for training, SkeletonNet can predict reasonably accurately 3D human pose by exploiting the combination of skeletal structure and heatmaps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparisons between rotation representation</head><p>We have also compared the results of bone rotation regressor by changing its rotation representation. Specifically, we tested the network that 1) regressess axis angles but indirectly supervised with rotation matrices (AA), 2) regressess axis angles but supervised with relative joint rotation matrices and converts them back to the absolute space using forward kinematics, which is equivalent as SMPL <ref type="bibr" target="#b3">[4]</ref> (FKAA), 3) regresses absolute rotations (AbsRotReg), 4) regresses rotations without the GS layer (w/o GS), 5) regresses a global rotation (GlobalReg), 6) classifies a global rotation (GlobalClass) and 7) is same as GlobalClass but aligns rotations with heatmaps (All), respectively. The networks are trained with Human 3.6M, except for All that was trained with both MPII and Human 3.6M. To compare the rotation prediction accuracy, we compute relative rotations between the ground truth and predicted bone rotations, convert them to axis angles and take the norms in degrees, which reflects all three DoFs of rotations.</p><p>In <ref type="table" target="#tab_3">Table 4</ref>, Global Rot. Err. indicates the error of global rotations. Bone Rot. Err. indicates the average error of bone rotations relative to the root. As shown in <ref type="table" target="#tab_3">Table 4</ref>, the proposed method based on the GS layer, which classifies a global rotation, is the best in terms of MPJPE accuracy. AbsRotReg is also high in accuracy but it produces bone rotations with its determinant of -1, which collapse skeletal structure. The method based on axis angle tends to produce large errors probably because of their high non-linearity, requiring an iterative process <ref type="bibr" target="#b9">[10]</ref> or a more informative geometric loss, e.g., the one using differences between silhouettes <ref type="bibr" target="#b18">[19]</ref>. In summary, our method can benefit from the use of 3?3 rotation matrices, which can probably be modeled as simpler functions than other angle representations, which is more friendly to ConvNets to learn with. As reported in <ref type="bibr" target="#b28">[29]</ref>, we could not train a network properly using Euler angles as supervisions, where training and validation losses remained high. In contrast, SkeletonNet can model subtle pose appearances due to e.g., medial and lateral rotations around segment axes by providing supervisions on both rotations and positions. With joint position supervisions only and no rotational supervisions, it is possible to get reasonable results in joint position predictions <ref type="bibr" target="#b28">[29]</ref> but is difficult to obtain good results for bone orientations. Failure cases and limitations In <ref type="figure" target="#fig_5">Fig. 5</ref>, we show failure cases. Our technique fails when there are large self-occlusions and occlusions by objects or other humans. In addition, our network are currently designed for the single-person de- tection and thus fails when multiple humans exist in the image. Since we scale a skeleton, we are not be able to model absolute bone lengths. Cross heatmap regressor possesses the ability to alter relative bone lengths but our method have generalization issues when the body type is extremely different from the original skeleton, e.g., prediction of small children's poses. Also our network does not take into account hand and foot orientations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have presented SkeletonNet, a novel end-to-end 3D human pose detection technique from a single image. The first step regresses bone segment rotations to obtain an initial solutions without large errors by considering skeleton structure. The second step performs refinement based on heatmap regressor that is based on the representation called cross heatmap which stacks heatmaps of xy and zy coordinates. This combination allows us to predict bone orientations and joint positions accurately, which may provide useful information to applications like animation and biomechanics. We also presented a 3D human pose dataset constructed by adding 3D rotational annotations to publicly-available 2D human pose dataset.</p><p>In future work, we would like to address monocular detections of other human body properties, such as body shape, body weight, contact forces and joint forces/torques. We are also interested in generative adversarial networks (GAN) to improve pose prediction results using an unsupervised manner based on the image dataset that does not have 3D annotations. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Network architecture of SkeletonNet. (a) SkeletonNet is a two-step regressor. (b) The first part performs regression of bone rotations. (c) The second part detects heatmaps of xy and zy spaces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>3D pose annotation system. (a) Given 2D key points as inputs, we compute 3D joints using the projected matching pursuit (PMP) technique. Next, skeleton rotations are obtained using a skeleton transformation optimization method. 3D mesh is obtained from rotations and positions. (b) With the annotation tool, the annotator is just needed to judge whether the 3D pose is acceptable or not by comparing the real image and the rendered image created from the mesh.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>L</head><label></label><figDesc>total = L RotG + ?L RotB + ?L Rot + ?L pos + ?L hm where L RotG , L RotB , L Rot , L pos and L hm are a cross entropy loss for global rotation and MSE loss for bone rotations R b , final rotations R, final positions x and cross heatmaps (h xy and h zy ), respectively. In addition, ?, ?, ? and ? are the respective weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Some results on in-the-wild images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Comparisons of baselines. a) Bone rotation regressor preserves skeletal structure but the joint positions are not accurate enough. With only heatmaps, skeletal structure is sometimes destructed e.g., with the left and right flips. By combining both, it produces more accurate result while preserving structure. b) The rotation annotation is important for reconstructing a pose from in the wild images. c) With the proposed Gram Schmidt (GS) orthogonalization layer, undesirable deformations such as shears and scalings are removed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Failure cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>More results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparisons to other state-of-the-art. MPJPE [mm] is used for error metric.</figDesc><table><row><cell></cell><cell cols="8">Directions Discussion Eating Greeting Phoning Photo Posing Purchases</cell></row><row><cell>Zhou et al. [29]</cell><cell>91.8</cell><cell>102.4</cell><cell>97.0</cell><cell>98.8</cell><cell>113.4</cell><cell>90.0</cell><cell>93.8</cell><cell>132.2</cell></row><row><cell>Tome et al. [24]</cell><cell>64.98</cell><cell>73.47</cell><cell>76.82</cell><cell>86.43</cell><cell>86.28</cell><cell>110.67</cell><cell>68.93</cell><cell>74.79</cell></row><row><cell>Mehta et al. [15]</cell><cell>59.69</cell><cell>69.74</cell><cell>60.55</cell><cell>68.77</cell><cell>76.36</cell><cell>85.42</cell><cell>59.05</cell><cell>75.04</cell></row><row><cell cols="2">Pavlakos et al. [18] 67.38</cell><cell>71.95</cell><cell>66.70</cell><cell>69.07</cell><cell>71.95</cell><cell>76.97</cell><cell>65.03</cell><cell>68.30</cell></row><row><cell>Ours (All)</cell><cell>63.33</cell><cell>71.59</cell><cell>61.39</cell><cell>70.40</cell><cell>69.90</cell><cell>83.17</cell><cell>62.98</cell><cell>68.77</cell></row><row><cell></cell><cell cols="8">Sitting SittingDown Smoking Waiting WalkDog Walking WalkPair Average</cell></row><row><cell>Zhou et al. [29]</cell><cell>159.0</cell><cell>106.9</cell><cell>125.2</cell><cell>94.4</cell><cell>79.0</cell><cell>126.0</cell><cell>99.0</cell><cell>107.3</cell></row><row><cell>Tome et al. [24]</cell><cell>110.19</cell><cell>172.91</cell><cell>84.95</cell><cell>85.78</cell><cell>86.26</cell><cell>71.36</cell><cell>73.14</cell><cell>88.39</cell></row><row><cell>Mehta et al. [15]</cell><cell>96.19</cell><cell>122.92</cell><cell>70.82</cell><cell>68.45</cell><cell>54.41</cell><cell>82.03</cell><cell>59.79</cell><cell>74.14</cell></row><row><cell cols="2">Pavlakos et al. [18] 83.66</cell><cell>96.51</cell><cell>71.74</cell><cell>65.83</cell><cell>74.89</cell><cell>59.11</cell><cell>63.24</cell><cell>71.90</cell></row><row><cell>Ours (All)</cell><cell>76.81</cell><cell>98.90</cell><cell>68.24</cell><cell>67.45</cell><cell>73.74</cell><cell>57.72</cell><cell>57.13</cell><cell>69.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison of reconstruction errors on Human 3.6M dataset.</figDesc><table><row><cell cols="5">Zhou et al. [30] Bogo et al. [4] Lassener et al. [11] Pavlakos et al. [19] Ours</cell></row><row><cell>106.7</cell><cell>82.3</cell><cell>80.7</cell><cell>75.9</cell><cell>61.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparisons between baselines. MPJPE [mm] is used for error metric.</figDesc><table><row><cell>Rot reg only</cell><cell>Heatmap only</cell><cell>Rot reg + HM</cell><cell>All</cell></row><row><cell>112.43</cell><cell>128.55</cell><cell>87.05</cell><cell>69.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparison of rotation representation.</figDesc><table><row><cell></cell><cell>AA</cell><cell cols="4">FKAA AbsRotReg w/o GS GlobalReg</cell><cell>GlobalClass</cell><cell>All</cell></row><row><cell>MPJPE</cell><cell cols="2">175.06 197.44</cell><cell>114.70</cell><cell>124.06</cell><cell>119.18</cell><cell>112.42</cell><cell>69.95</cell></row><row><cell cols="3">Global Rot. Err. 30.46 35.81</cell><cell>18.83</cell><cell>21.28</cell><cell>21.64</cell><cell>20.81</cell><cell>12.93</cell></row><row><cell cols="3">Bone Rot. Err. 37.34 44.77</cell><cell>-</cell><cell>28.72</cell><cell>29.08</cell><cell>29.46</cell><cell>21.94</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pose-conditioned joint angle limits for 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1446" to="1455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bernt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Recurrent human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.02914</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Keep it smpl: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="561" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Adversarial posenet: A structureaware convolutional network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<idno>abs/1705.00389</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Multi-context attention for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1702.07432</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Regognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unite the people: Closing the loop between 3d and 2d human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">End-to-end training of deep visuomotor policies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno>abs/1504.00702</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3d human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="332" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Human pose regression by combining indirect part detection and contextual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<idno>abs/1710.02322</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Monocular 3d human pose estimation using transfer learning and improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09813</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">We don&apos;t need no bounding-boxes: Training object class detectors using only human verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<idno>abs/1602.08405</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<idno>abs/1611.07828</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to estimate 3D human pose and shape from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<title level="m">Reconstructing 3d human pose from 2d image landmarks. ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="573" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Single image 3d human pose estimation from noisy observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramisa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Aleny?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Torras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2673" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">As-rigid-as-possible surface modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sorkine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alexa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Eurographics Symposium on Geometry Processing, SGP &apos;07</title>
		<meeting>the Fifth Eurographics Symposium on Geometry Processing, SGP &apos;07<address><addrLine>Aire-la-Ville, Switzerland, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Eurographics Association</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="109" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00159</idno>
		<title level="m">Compositional human pose regression</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Lifting from the deep</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.00295</idno>
	</analytic>
	<monogr>
		<title level="m">Convolutional 3d pose estimation from a single image</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Matconvnet -convolutional neural networks for matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the ACM Int. Conf. on Multimedia</title>
		<meeting>eeding of the ACM Int. Conf. on Multimedia</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning feature pyramids for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1708.01101</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Weakly-supervised transfer for 3d human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02447</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep kinematic pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="186" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sparseness meets deepness: 3d human pose estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4966" to="4975" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
