<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchically Decomposed Graph Convolutional Networks for Skeleton-Based Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungho</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhyeok</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dogyoon</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangyoon</forename><surname>Lee</surname></persName>
							<email>syleee@yonsei.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchically Decomposed Graph Convolutional Networks for Skeleton-Based Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph convolutional networks (GCNs) are the most commonly used method for skeleton-based action recognition and have achieved remarkable performance. Generating adjacency matrices with semantically meaningful edges is particularly important for this task, but extracting such edges is challenging problem. To solve this, we propose a hierarchically decomposed graph convolutional network (HD-GCN) architecture with a novel hierarchically decomposed graph (HD-Graph). The proposed HD-GCN effectively decomposes every joint node into several sets to extract major adjacent and distant edges, and uses them to construct an HD-Graph containing those edges in the same semantic spaces of a human skeleton. In addition, we introduce an attention-guided hierarchy aggregation (A-HA) module to highlight the dominant hierarchical edge sets of the HD-Graph. Furthermore, we apply a new two-stream-three-graph ensemble method, which uses only joint and bone stream without any motion stream. The proposed model is evaluated and achieves state-of-the-art performance on three large, popular datasets: NTU-RGB+D 60, NTU-RGB+D 120, and Northwestern-UCLA. Finally, we demonstrate the effectiveness of our model with various comparative experiments. Recent approaches <ref type="bibr" target="#b21">(Shi et al. 2019b;</ref><ref type="bibr" target="#b15">Liu et al. 2020;</ref><ref type="bibr" target="#b2">Cheng et al. 2020a;</ref><ref type="bibr" target="#b0">Chen et al. 2021a</ref>) have adopted graph</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Human action recognition (HAR) is a task that categorizes action classes by receiving video data as input. HAR is used in many applications, such as human-computer interaction and virtual reality. Recently, several RGB-based and skeleton-based HAR methods have been proposed with the development of deep learning technology. However, RGBbased methods <ref type="bibr" target="#b25">Veeriah, Zhuang, and Qi 2015)</ref> cannot robustly recognize human actions because they are strongly influenced by environmental noises such as background color, brightness of light, and clothing. Therefore, methods using skeleton modality <ref type="bibr" target="#b32">(Yan, Xiong, and Lin 2018;</ref><ref type="bibr" target="#b21">Shi et al. 2019b;</ref><ref type="bibr" target="#b33">Zhang et al. 2020;</ref><ref type="bibr" target="#b23">Si et al. 2019;</ref><ref type="bibr">Cheng et al. 2020b,a;</ref><ref type="bibr" target="#b15">Liu et al. 2020;</ref><ref type="bibr" target="#b0">Chen et al. 2021a</ref>) have attracted attention because they are not affected by these noises. These methods recognize action by receiving 2D or 3D coordinates of major human joints as time-series inputs. <ref type="figure">Figure 1</ref>: The framework of HD-GCN. The input skeleton is applied with various edge sets through a hierarchically decomposed graph (HD-Graph). The red lines are the edges included in the corresponding hierarchy edge set. The network highlights the major edge sets through the attention map.</p><p>convolutional networks (GCNs) to apply human-skeleton graphs to convolutional layers. Those GCN-based methods use a handcrafted graph proposed by <ref type="bibr" target="#b32">(Yan, Xiong, and Lin 2018)</ref>, which identifies physical connections in the human skeleton. They extract the spatial features representing the relationships between physically connected (PC) edges among human skeleton, and they outperform other methods by using them to construct the major relationships between joint nodes in the human skeleton.</p><p>However, existing GCN-based methods <ref type="bibr" target="#b32">(Yan, Xiong, and Lin 2018;</ref><ref type="bibr" target="#b21">Shi et al. 2019b</ref><ref type="bibr" target="#b22">Shi et al. , 2020</ref><ref type="bibr" target="#b2">Cheng et al. 2020a;</ref><ref type="bibr" target="#b0">Chen et al. 2021a</ref>) have the following limitations. (1) With the widely used handcrafted graph proposed by <ref type="bibr" target="#b32">(Yan, Xiong, and Lin 2018)</ref>, the relationships between distant joint nodes are not identified, instead using only the relationships of PC edges in the human skeleton. However, for humans to recognize actions, not only are the relationships between adjacent joints important but also relationships between distant joints. (2) Some recent methods <ref type="bibr" target="#b32">(Yan, Xiong, and Lin 2018;</ref><ref type="bibr" target="#b21">Shi et al. 2019b;</ref><ref type="bibr" target="#b2">Cheng et al. 2020a;</ref><ref type="bibr" target="#b15">Liu et al. 2020)</ref> do not recognize which edges are significant for each skeleton sample because they simply aggregate the edge features, ignoring the contribution of each edge. Although several methods <ref type="bibr" target="#b21">(Shi et al. 2019b;</ref><ref type="bibr" target="#b0">Chen et al. 2021a</ref>) consider this limitation, their networks still do not adequately highlight those edges because they are generally biased by the physical connectivity of (Yan, Xiong, and Lin 2018)'s graph.</p><p>Motivated by these limitations, we propose a hierarchically decomposed graph convolutional network (HD-GCN) with a hierarchically decomposed graph (HD-Graph) and attention-guided hierarchy aggregation (A-HA) module. In addition, we present a six-way ensemble method, a twostream-three-graph ensemble, to use our HD-Graph effectively.</p><p>The HD-GCN incorporates GCNs with our HD-Graph, which identifies the relationships between distant joint nodes in the same semantic spaces (e.g., right and left hands, right and left feet). The same semantic spaces are formed by moving out step by step from the Center of Mass (CoM) node of the graph. For example, if belly is a CoM node, the first semantic space includes the belly node, the next space includes the chest and hip nodes, and the subsequent space includes the left and right shoulder and the left and right hip nodes. Nodes in the same semantic space are defined as hierarchy node set. We note that when humans recognize an action, they detect the relationships between distant joint nodes within the same hierarchy node set. Thus, the proposed HD-Graph contains both meaningful adjacent and distant joint nodes by connecting all the nodes in neighboring hierarchy node sets and identifies the connectivity between those nodes. In addition, we apply a spatial edge convolution (S-EdgeConv) layer to reflect the node relationships that the HD-Graph cannot capture. To form the S-EdgeConv layer, we borrow the structure of EdgeConv , which is widely used in 3D point clouds.</p><p>To consider the contribution of each edge set, the process of selecting the dominant hierarchical information should depend on the action data sample to give proper attention to the most dominant edge sets. For example, to recognize the clapping action, a hierarchy edge set that includes both hands must be emphasized. For this, we propose an attention-guided hierarchy aggregation (A-HA) module, which consists of two modules: representative spatial average pooling (RSAP) and hierarchical edge convolution (H-EdgeConv). A scaling bias problem occurs if we use the spatial average pooling module without any node extraction process because each node has a different number of adjacent nodes. To prevent this, we apply RSAP, which includes a representative node extraction process that triggers features after the pooling layer to represent each node. To effectively manage hierarchical features obtained by RSAP, we apply a hierarchical edge convolution (H-EdgeConv) layer whose base structure is EdgeConv ). H-EdgeConv treats each hierarchical feature as a graph node and extracts the attention map through node similarity via Euclidean distance. With RSAP and H-EdgeConv, our model successfully determines which hierarchy edge sets and spatial joints should be emphasized among the input features.</p><p>The existing ensemble method uses four-stream data composed of the joint, bone, joint motion, and bone motion streams, which are the original skeletal coordinates, spatial differential between joint coordinates, and temporal differential of joint, and temporal differential of the bone, respec-tively. Most existing ensemble methods <ref type="bibr" target="#b22">(Shi et al. 2020;</ref><ref type="bibr" target="#b0">Chen et al. 2021a</ref>) use motion data, but the network using the data does not perform particularly well. To address this problem, we present a new ensemble method, a two-streamthree-graph ensemble. We apply this six-way ensemble by setting three HD-Graphs with joint and bone stream data. Each graph has different CoM nodes to extract features of different semantic spaces.</p><p>We conduct extensive experiments on three benchmark action recognition datasets: NTU-RGB+D 60 <ref type="bibr" target="#b19">(Shahroudy et al. 2016)</ref>, NTU-RGB+D 120 , and Northwestern-UCLA <ref type="bibr" target="#b27">(Wang et al. 2014)</ref>.</p><p>Our main contributions are summarized as follows: -We propose a hierarchically decomposed graph (HD-Graph) to thoroughly identify the significant distant edges in the same hierarchy subsets. -We propose an attention-guided hierarchy aggregation (A-HA) module to highlight the key hierarchy edge sets with representative spatial average pooling (RSAP) and hierarchical edge convolution (H-EdgeConv). -We use a new two-stream-three-graph ensemble method for skeleton-based action recognition with HD-Graphs that have different center of mass (CoM) nodes. -Our HD-GCN outperforms state-of-the-art methods on three benchmarks.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Action Recognition with GCNs</head><p>In skeleton-based action recognition, human skeletal data are represented by a graph with joint nodes. Most recent approaches use GCN-based methods <ref type="bibr" target="#b32">(Yan, Xiong, and Lin 2018;</ref><ref type="bibr" target="#b21">Shi et al. 2019b;</ref><ref type="bibr" target="#b3">Cheng et al. 2020b;</ref><ref type="bibr" target="#b15">Liu et al. 2020;</ref><ref type="bibr" target="#b0">Chen et al. 2021a</ref>) with (Yan, Xiong, and Lin 2018)'s graph structure, which contains only PC edges. The spatio-temporal graph for human skeleton is represented by G(V, E), where V and E denote the joint and edge groups, respectively. GCN-based methods perform remarkably better than methods using handcrafted features <ref type="bibr" target="#b4">(Duvenaud et al. 2015;</ref><ref type="bibr" target="#b5">Hamilton, Ying, and Leskovec 2017;</ref><ref type="bibr" target="#b9">Kipf et al. 2018;</ref><ref type="bibr" target="#b17">Monti et al. 2017;</ref><ref type="bibr" target="#b18">Niepert, Ahmed, and Kutzkov 2016)</ref>. 3D time-series skeletal data are represented by X ? R 3?T ?V , where V and T are the number of joint nodes and the temporal window size, respectively. GCN's operation with input feature map X ? R C?T ?V is as follows:</p><formula xml:id="formula_0">F out = s?S A s X? s ,<label>(1)</label></formula><p>where S = {s id , s cf , s cp } denotes graph subsets, and s id , s cf , and s cp indicate identity, centrifugal, and centripetal joint subsets, respectively. ? s denotes the pointwise convolution operation. The adjacency matrix A is initialized as</p><formula xml:id="formula_1">? ? 1 2 A? ? 1 2 ? R N S ?V ?V ,</formula><p>where ? is a diagonal matrix for normalization and N S = 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Attention Modules for Action Recognition</head><p>The attention mechanism is an essential element for constructing a deep neural network. Using recent attention mod- Edges between all nodes in the same semantic space are obtained by connecting all the nodes in adjacent hierarchy edge sets.</p><p>ules <ref type="bibr" target="#b7">(Hu, Shen, and Sun 2018;</ref><ref type="bibr" target="#b31">Woo et al. 2018</ref>), networks emphasize important information along a specific dimension. For example, <ref type="bibr" target="#b7">(Hu, Shen, and Sun 2018)</ref> applies channel-wise attention, and <ref type="bibr" target="#b31">(Woo et al. 2018</ref>) applies both channel-wise and spatial-wise attentions. These techniques are divided into two categories for GCNs: (1) attention-based graph construction <ref type="bibr" target="#b21">(Shi et al. 2019b;</ref><ref type="bibr" target="#b0">Chen et al. 2021a</ref>) which is a method of forming topologies using a non-local block  or customized correlation matrices, and (2) spatial-wise, temporal-wise, channelwise attention, which are commonly used attentions in <ref type="bibr" target="#b22">(Shi et al. 2020;</ref><ref type="bibr" target="#b24">Song et al. 2021)</ref>, and several other networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In Sec. 3.1, we detail the HD-Graph convolution to solve the problems of the conventional human-skeleton graph <ref type="bibr" target="#b32">(Yan, Xiong, and Lin 2018)</ref>, which includes only PC edges. We also explain the A-HA module in Sec. 3.2 to highlight dominant hierarchical features. In Sec. 3.3, we replace the widely used four-stream ensemble method <ref type="bibr" target="#b22">(Shi et al. 2020;</ref><ref type="bibr" target="#b0">Chen et al. 2021a;</ref><ref type="bibr" target="#b2">Cheng et al. 2020a</ref>) with a two-stream-threegraph ensemble without motion data streams. Finally, we introduce the HD-GCN, which uses these proposed methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Hierarchically Decomposed Graph</head><p>Most recent methods have adopted the handcrafted graph proposed by <ref type="bibr" target="#b32">(Yan, Xiong, and Lin 2018)</ref>, but the HD-Graph is derived through a newly presented method. Decomposition into a Rooted Tree. The first step is to decompose the graph with PC edges and construct a rooted tree. To decompose a given skeleton into the tree, we need to determine a CoM node, which allows nodes in the same hierarchy edge set to exist in the same semantic space. For example, nodes in the same semantic space, such as elbow and knee joints, or hands and feet, must exist in a hierarchy node set. After choosing the CoM node, the graph is converted into a rooted tree, which includes the hierarchical information of the graph, and defines the adjacency matrix A ? R N L ?V ?V with N L hierarchy layers for N H hierarchy edge sets:</p><formula xml:id="formula_2">A = E(H 1 ? H 2 ) || ? ? ? || E(H N H ?1 ? H N H ) , (2)</formula><p>where H k denotes the k-th hierarchy node set and E(H k ? H k+1 ) denotes a set of edges from H k to H k+1 . N L and N H are the number of hierarchy layers and hierarchy edge sets, respectively, and N L = N H ? 1. Note that is the concatenation operation. However, A includes only the directed centrifugal edges of <ref type="bibr" target="#b32">(Yan, Xiong, and Lin 2018)</ref>. For consistency with existing methods, all the reverse-directed edges from the leaf nodes of the rooted tree in <ref type="figure" target="#fig_0">Figure 2</ref> to the CoM node must be reflected in the adjacency matrices to cover the centripetal edges. In addition, to get the features of the nodes themselves, the identity edges for each hierarchy node set must be considered. Thus, the adjacency matrices A ? R N L ?N S ?V ?V are defined as follows:</p><formula xml:id="formula_3">A = E 1 || E 2 || ? ? ? || E N L ,<label>(3)</label></formula><formula xml:id="formula_4">E k = E(H k + H k+1 ||H k ? H k+1 ||H k+1 ? H k ), (4)</formula><p>where E k denotes the concatenation of the three edge subsets of S = {s id , s cp , s cf } and s id , s cp , s cf indicate the identity, centripetal, and centrifugal edge subsets, respectively. Through this construction policy, we create an skeletal graph with bidirectional and identity edges.</p><p>Fully Connected Inter-Hierarchy Edges. Decomposed graph A has a different number of edge sets from the conventional graph, but the edges are all the same. To identify the relationships between major distant joint nodes, especially those in the same semantic space, we connect all nodes between neighboring hierarchy node sets. In addition, since (Yan, Xiong, and Lin 2018)'s graph contains the connectivity of only PC edges, not distant relationships, the receptive field is very small with this sparse graph. Applying our fully connected (FC) edges to the rooted tree, the graph becomes denser and makes the receptive field larger than before with more meaningful distant connectivity as shown in <ref type="figure" target="#fig_0">Figure 2</ref> (b). Then, the adjacency matrices are normalized with degree matrices for training stability and we leave all elements of the matrices as learnable parameters for training adaptability.</p><p>HD-Graph Convolution. Our HD-Graph convolution includes four parallel branch operations: three graph convolution through HD-Graph and an additional EdgeConv  operation. To reduce the computational complexity, a linear transformation is applied to all four operations. For three of these operations, our method performs a subset-wise GCN operation in the same way as <ref type="bibr" target="#b32">(Yan, Xiong, and Lin 2018;</ref><ref type="bibr" target="#b21">Shi et al. 2019b)</ref> for each hierarchy edge set with three edge subsets. However, rather than summing the output values for each subset as in Eq. (1), we concatenate these output values to the channel dimension:</p><formula xml:id="formula_5">F (k) = s?S A (k) s ?(X)? (k) s ,<label>(5)</label></formula><p>where X and F denote the input and output feature maps of the HD-Graph convolution and function ? denotes a linear transformation with parameter W ? R C ?C . Note that is a concatenation operation. Although our HD-Graph defines more meaningful node relationships than conventional graph, it may still not be able to extract sample-wise key relationships that reflect the similarities between all nodes in the feature space. To improve this limitation, we adopt EdgeConv  as the remaining operation, which is used for extracting graphical features through local neighborhood graphs in the feature space. With spatial EdgeConv (S-EdgeConv), our network extracts sample-wise node connectivity, which the HD-Graph does not capture. For our method, S-EdgeConv initially takes the average pooling as the temporal dimension for computational efficiency. Local graphs with local edges are then formed via k-nearest neighbor (k-NN) based on the Euclidean distance, and the local edges as well as identity edges based on the graphs are aggregated via trainable parameters W edge ? R C ?(2?C ) . For the deep neural network, physically close edges are reflected to the initial shallow layers, but as they become deeper, the relationship between semantically similar edges in the feature space are identified and learned. Our whole GCN process is shown in <ref type="figure" target="#fig_2">Figure 3</ref> and computed as follows:</p><formula xml:id="formula_6">F = N L k=1 s?S A (k) s X ? (k) s z (k) V T t=1 X t ,<label>(6)</label></formula><p>where X = ?(X) and z V denote the linearly transformed input and S-EdgeConv operation, respectively. All four branch outputs are concatenated to the channel dimension, with all four computed in the same way for N L hierarchy edge sets. Due to the inherent characteristics of skeletal data, the number of joint nodes included in each dataset is different, and, consequently, the number of hierarchy sets is different. Therefore, we adopt an addition policy for N L hierarchy-wise outputs and a concatenation policy for N S subset-wise outputs. In this way, the dimensionality is maintained, and the common hierarchy-wise aggregation policy is followed for every skeletal dataset by adding all the outputs for different numbers of hierarchical sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Attention-Guided Hierarchy Aggregation</head><p>The HD-Graph convolution uses an aggregation policy of adding all the hierarchy-wise outputs. However, because each data sample has relationship between specific major edges, we propose an attention-guided hierarchy aggregation (A-HA) module, which applies a weighted-sum policy to the hierarchy dimension with proper attention to the hierarchy-wise outputs. The framework of the HD-Graph convolution with the A-HA module is shown in <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>Representative Spatial Average Pooling. Our A-HA module is applied to feature map F ? R C?N L ?T ?V after the HD-Graph convolution. The first step is to extract the temporal frame with the highest score on F. RSAP, ?, is then applied, which is preceded by the extraction of representative nodes in each hierarchy layer. If spatial average pooling is applied without this extraction process, scaling bias occurs because the number of edges connected to each node is different. Therefore, representative node extraction is essential to obtain an appropriate score for attention without any bias. After the extraction, spatial average pooling is applied to hierarchy-wise outputs. Our pooling function ? is as follows:</p><formula xml:id="formula_7">?(F (k) ) = 1 N k + N k+1 v?H k,k+1 max t ? F (k) t (v) ,<label>(7)</label></formula><p>where N k denotes the number of vertices in the k-th hierarchy set, and ? denotes linear transformation for computational efficiency.</p><p>Hierarchical Edge Convolution. After the RSAP layer, N L hierarchy-wise features in attention feature map M have not yet shared their information with each other. We treat all N L features as nodes on a graph to learn and reflect similarities in the hierarchical feature space. To apply this process, representative features of these nodes are fed into EdgeConv , and the similarities of those nodes are learned based on the Euclidean distance. We also include the self-loop shown in the bottom section of <ref type="figure">Figure</ref> 3 so that the node's own features can be reflected. Our attention map M operates as follows:</p><formula xml:id="formula_8">M = ? z L k?L ? F (k) ,<label>(8)</label></formula><p>where z L denotes H-EdgeConv and ? denotes the sigmoid function.</p><p>The attention map M obtained is multiplied by the HD-Graph convolution output feature map F, and the output feature map F out is obtained through the hierarchy-dimension weighted sum as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. Similar to S-EdgeConv in Sec. 3.1, H-EdgeConv reflects physically close hierarchy edge sets in the earlier layers. However, semantically similar sets are reflected in the deeper layers, emphasizing different hierarchical sets for each sample. <ref type="bibr" target="#b21">(Shi et al. 2019b</ref>) and <ref type="bibr" target="#b22">(Shi et al. 2020</ref>) introduce bone and motion streams of skeletal data for ensemble methods. Recent studies <ref type="bibr">(Cheng et al. 2020b,a;</ref><ref type="bibr" target="#b0">Chen et al. 2021a)</ref> have applied a four-stream ensemble method using streams for joints, bones, joint motion, and bone motion. However, because motion data performance is not as good as coordinate data, we adopt an ensemble method with the joint and bone streams without any motion data, using three different graphs with different CoM nodes. Based on the NTU-RGB+D <ref type="bibr" target="#b19">(Shahroudy et al. 2016</ref>) dataset, the existing graph <ref type="bibr" target="#b32">(Yan, Xiong, and Lin 2018)</ref> uses the belly node as the CoM node. However, because it is necessary to distinguish <ref type="figure">Figure 4</ref>: The architecture of HD-GCN. HD-GCN receives the skeleton sequence as input and obtains the class label through nine GCN blocks, and an FC layer, and the softmax function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Two-Stream-Three-Graph Ensemble</head><p>whether a major movement in each data sample is in the upper or lower body, we train the network by dividing the CoM node into the belly, hip, and chest nodes. For example, the lower body should be primarily considered in the case of the "Kicking" class, and the upper body should be considered in the "Take off bag" class, so we train networks with different CoM nodes and then ensemble them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Network Architecture</head><p>As shown in <ref type="figure">Figure 4</ref>, we adopt <ref type="bibr" target="#b21">(Shi et al. 2019b</ref>) as our baseline network architecture with a total of nine stacked GCN blocks. The numbers of output channels for the blocks are <ref type="bibr">64, 64, 64, 128, 128, 128, 256, 256, and 256</ref>. Each block contains a residual connection <ref type="bibr" target="#b6">(He et al. 2016)</ref> and is divided into a spatial module, in which the GCN operation proceeds, and a temporal module, which includes the temporal convolutions. Our method use the temporal module of <ref type="bibr" target="#b0">(Chen et al. 2021a)</ref>, whose baseline module is <ref type="bibr" target="#b15">(Liu et al. 2020;</ref><ref type="bibr" target="#b24">Szegedy et al. 2015)</ref>. This module consists of four branch operations. Two are atrous temporal convolutions with kernel size five and dilation one and two, respectively. The remaining branch operations are pointwise convolution and max pooling with kernel size three. Our spatial module consists of an HD-Graph convolution operation and an A-HA module, as introduced in Sec. 3.1 and Sec. 3.2. After passing through all GCN layers with attention to the hierarchy-wise features, the network compresses the feature map through the global average pooling layer and classifies the action sample through the softmax function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Experimental Settings</head><p>NTU-RGB+D 60. NTU-RGB+D 60 <ref type="bibr" target="#b19">(Shahroudy et al. 2016</ref>) is a large dataset widely used in skeleton-based action recognition. It contains 56,880 skeleton action samples, performed by 40 different participants and classified into 60 classes. Each sample is captured through three Microsoft Kinect v2 cameras with horizontal angles of 45 ? , 0 ? , ?45 ? , and includes one or two subjects. The authors of this dataset recommend two benchmarks. (1) Cross-Subject (X-Sub): 20 of the 40 subjects' actions are used for training, and the  remaining 20 are for validation.</p><p>(2) Cross-View (X-View): Two of the three camera-views are used for training, and the other one is used for validation.</p><p>NTU-RGB+D 120. NTU-RGB+D 120 ) is a dataset in which 57,367 new action samples are added to the NTU-RGB+D 60 dataset. It contains a total of 114,480 skeleton action samples over 120 classes, performed by 106 different subjects. Each sample is captured in three cameraviews. The dataset includes 32 settings, all taken in different places and backgrounds. The authors of this dataset recommend two benchmarks: (1) Cross-Subject (X-Sub): 53 of the 106 subjects' actions are used for training, and the remaining 53 are used for validation.</p><p>(2) Cross-Setup (X-Set): Of the 32 setups, data with even setup IDs are used for training, and the remaining data with odd IDs are used for validation.</p><p>Northwestern-UCLA. The Northwestern-UCLA skeleton dataset <ref type="bibr" target="#b27">(Wang et al. 2014</ref>) contains 1494 video clips over 10 classes. Each action is captured through three Kinect cameras with different camera views and is performed by 10 subjects. We adopt the same protocol as NW-UCLA: Two of the three camera-views are used for training, and the other one is used for validation.</p><p>Experimental Settings. In our experiments, we adopt <ref type="bibr" target="#b21">(Shi et al. 2019b</ref>) as the backbone. The SGD optimizer is employed with a Nesterov momentum of 0.9 and a weight decay of 0.0004. The number of learning epochs is set to 90, with a warm-up strategy <ref type="bibr" target="#b6">(He et al. 2016)</ref> applied to the first five epochs for more stable learning. We set the learning rate to decay with cosine annealing <ref type="bibr" target="#b16">(Loshchilov and Hutter 2016)</ref>, with a maximum learning rate of 0.1 and a minimum learning rate of 0.0001. For the NTU-RGB+D datasets, we set the batch size to 64 and use the data preprocessing method from . For the Northwestern-UCLA dataset, we set the batch size to 16 and use the data preprocessing method from <ref type="bibr" target="#b3">(Cheng et al. 2020b</ref>). All our experiments are conducted on a single RTX 3090 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with State-of-the-Arts Methods</head><p>Most recent state-of-the-art networks <ref type="bibr" target="#b22">(Shi et al. 2020;</ref><ref type="bibr">Cheng et al. 2020b,a;</ref><ref type="bibr" target="#b0">Chen et al. 2021a</ref>) adopt a four-way ensemble method, but we adopt the six-way ensemble method described in Sec. 3.3. We compare ours with state-of-the-art networks on three datasets: NTU-RGB+D 60 <ref type="bibr" target="#b19">(Shahroudy et al. 2016</ref>), NTU-RGB+D 120 , and Northwestern-UCLA <ref type="bibr" target="#b27">(Wang et al. 2014)</ref>. Comparisons for each dataset are shown in <ref type="table" target="#tab_1">Table 1</ref>. The recognition performance of our HD-GCN has exceeded the state-of-the-arts on every dataset, without any motion streams, as shown in <ref type="table" target="#tab_1">Table 1</ref>. With our proposed ensemble method, HD-GCN outperforms the state-of-the-art by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>In this section, we demonstrate the effectiveness of the proposed HD-GCN. Performance is specified as the crosssubject and cross-setup classification accuracy on the NTU-RGB+D 120    Hierarchically Decomposed Graph. To proceed with the ablation study for HD-Graph, we set the (Yan, Xiong, and Lin 2018) graph as the conventional graph. Here, we use the temporal convolution module of <ref type="bibr" target="#b15">(Liu et al. 2020)</ref>, as    <ref type="table" target="#tab_3">Table 2</ref>.</p><p>We set the edges of the HD-Graph in different ways to show a gradual performance increase according to the type of graph. There are four main versions of HD-Graph, the first of which is graph A containing only the PC edges. Unlike the conventional graph with one edge set including three fixed subsets, HD-Graph has a flexible number of edge sets, each divided by hierarchy layers with three subsets. Graph B is an extension of A, with the additional operation S-EdgeConv. Graph C contains FC edges for N H hierarchy node sets, and graph D is similar to C but includes S-EdgeConv. The HD-Graph with only PC edges performs better than the conventional graph by a large margin, even though they share the same edges. This proves that it is meaningful to divide the joint nodes by hierarchy edge sets. In addition, the HD-Graph with FC edges and S-EdgeConv performs better for every datasets.</p><p>Attention-Guided Hierarchy Aggregation. To prove the effectiveness of the A-HA module, we use a method to change or remove specific parts of our attention module, with the results shown in <ref type="table" target="#tab_5">Table 3</ref>. Spatial average pooling (SAP) simply averages along the spatial axis without the representative node extraction process, which performs worse than our RSAP. The poorer performance is due to two factors: (1) scaling bias occurs because the number of nodes in each hierarchy node set is different, and (2) attention through SAP does not represent the corresponding hierarchy node set because it brings the average of the feature vectors of all nodes, not a specific node set. Furthermore, it performs better with H-EdgeConv, which recognizes each hierarchy edge set as a graph node. This proves that because the major edge sets are different for each data sample, it is important to find and highlight edge sets with high similarity based on the Euclidean distance through H-EdgeConv.</p><p>The results of the attention score M of our A-HA module are shown in <ref type="figure" target="#fig_3">Figure 5</ref>. These results show that our module scores edge sets 4 and 5 higher for the "running" class, which includes knees and feet, elbows and hands. For the "Kicking" class, A-HA gives the highest score to edge set 3, which includes shoulders and hips, followed by edge set 4 and 5. It is reasonable for human visual recognition that the dynamically moving edge set 4, 5 are more important than the stationary and barely moving edge set 3 when running rather than when kicking something.</p><p>Two-Stream-Three-Graph Ensemble. We use the ensemble method to which three graphs with different CoM nodes are applied, excluding motion streams. <ref type="table" target="#tab_1">Table 1</ref> shows that HD-GCN without motion data performs better than the state-of-the-art four-stream methods with motion data and performs almost the same as those methods even when our network uses the two-stream data with a single graph ensemble method. In addition, when the two-stream ensemble with two different graphs is applied to HD-GCN, it outperforms the state-of-the-art methods by a large margin, and when combined with three graphs, it performs best by the largest margin. This proves that the features extracted with different CoM nodes show different learning patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this work, we propose a novel hierarchically decomposed graph convolutional network (HD-GCN) for skeleton-based action recognition. We also propose a new framework (HD-Graph) that replaces the existing framework, decomposes all the joint nodes by hierarchy edge sets and considers the connectivity between major distant nodes, which is difficult to identify naturally. We also present an effective attention module (A-HA) composed of representative spatial average pooling (RSAP) layer and hierarchical edge convolution (H-EdgeConv), which applies hierarchy-wise attention for the HD-Graph. In addition, our HD-GCN learns graph-wise features with different patterns through a two-stream-threegraph ensemble method. We derive an effective feature extractor by combining these three methods and empirically verify its effectiveness. Our approach outperforms current state-of-the-art methods on three benchmark datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>(a) Structure of HD-Graph with physically connected (PC) edges. The human skeleton graph is decomposed into a rooted tree, where PC edges are included in hierarchy sets. (b) Structure of HD-Graph with fully connected (FC) edges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 2 shows the framework of the HD-Graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>HD-Graph convolution operation block with an A-HA module. The left side shows an operation for one hierarchy edge set, and the right side shows an operation block that concatenates the results for N L edge sets and applies A-HA. The lower part of the figure is EdgeConv, where the EdgeConv subscript indicates the feature space to extract graphical features. The ? and ? operations denote matrix and element-wise multiplication.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Hierarchy-wise attention scores by A-HA for "Running" and "Kicking" class. Each score indicates the value of the attention map M of the A-HA module, and is scaled between 0 and 1 by the sigmoid function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparisons of the top-1 accuracy (%) against state-of-the-art methods on the NTU-RGB+D 60, 120, and Northwestern-UCLA datasets. Joint v denotes the joint dataset with v as the CoM node, and nodes b, h, and c denote the belly, hip, and chest nodes, respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>joint stream data.</figDesc><table><row><cell>Graph type</cell><cell cols="2">Edges S-EdgeConv X-Sub (%)</cell><cell>X-Set (%)</cell></row><row><cell>Conventional</cell><cell>PC</cell><cell>83.5</cell><cell>85.4</cell></row><row><cell>HD-Graph</cell><cell></cell><cell></cell><cell></cell></row><row><cell>A</cell><cell>PC</cell><cell cols="2">84.3 (? 0.8) 86.1 (? 0.7)</cell></row><row><cell>B</cell><cell>PC</cell><cell cols="2">84.6 (? 1.1) 86.3 (? 0.9)</cell></row><row><cell>C</cell><cell>FC</cell><cell cols="2">84.9 (? 1.4) 86.5 (? 1.1)</cell></row><row><cell>D</cell><cell>FC</cell><cell cols="2">85.1 (? 1.6) 86.7 (? 1.3)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison of the conventional graph and four types of HD-Graph.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Comparison of various types of attention modules. SAP denotes the spatial average pooling and H-EdgeConv denotes hierarchical edge convolution which treats each hierarchy-wise feature as a graph node. mentioned in Sec. 3.4, to compare the performance of networks fairly with various graphs. The experimental results are shown in</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Channel-wise topology refinement graph convolution for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13359" to="13368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-Scale Spatial Temporal Graph Convolutional Network for Skeleton-Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1113" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Decoupling GCN with DropGraph Module for Skeleton-Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with shift graph convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Towards To-aT Spatio-Temporal Focus for Skeleton-Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.02314</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural relational inference for interacting systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2688" to="2697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ddgcn: A dynamic directed graph convolutional network for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Korban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="761" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ntu rgb+ d 120: A large-scale benchmark for 3d human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2684" to="2701" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Skeleton-based action recognition using spatiotemporal LSTM network with trust gates. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="3007" to="3021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Global context-aware attention lstm networks for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1647" to="1656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Enhanced skeleton visualization for view invariant human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="346" to="362" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Disentangling and unifying graph convolutions for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<title level="m">Sgdr: Stochastic gradient descent with warm restarts</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5115" to="5124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ntu rgb+ d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1010" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Skeletonbased action recognition with directed graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7912" to="7921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Two-stream adaptive graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12026" to="12035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Skeletonbased action recognition with multi-stream adaptive graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="9532" to="9545" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An attention enhanced graph convolutional lstm network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1227" to="1236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ef-ficientGCN: Constructing Stronger and Faster Baselines for Skeleton-based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.15125</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
	<note>Going deeper with convolutions</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Differential recurrent neural networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Veeriah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4041" to="4049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning actionlet ensemble for 3D human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="914" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cross-view action modeling, learning and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2649" to="2656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Nonlocal neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Transactions On Graphics (tog)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision</title>
		<meeting>the European conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semantics-guided neural networks for efficient skeleton-based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1112" to="1121" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
