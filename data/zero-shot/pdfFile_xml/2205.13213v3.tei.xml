<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast Vision Transformers with HiLo Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizheng</forename><surname>Pan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Data Science &amp; AI</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Data Science &amp; AI</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Zhuang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Data Science &amp; AI</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fast Vision Transformers with HiLo Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Vision Transformers (ViTs) have triggered the most recent and significant breakthroughs in computer vision. Their efficient designs are mostly guided by the indirect metric of computational complexity, i.e., FLOPs, which however has a clear gap with the direct metric such as throughput. Thus, we propose to use the direct speed evaluation on the target platform as the design principle for efficient ViTs. Particularly, we introduce LITv2, a simple and effective ViT which performs favourably against the existing state-of-the-art methods across a spectrum of different model sizes with faster speed. At the core of LITv2 is a novel self-attention mechanism, which we dub HiLo. HiLo is inspired by the insight that high frequencies in an image capture local fine details and low frequencies focus on global structures, whereas a multi-head self-attention layer neglects the characteristic of different frequencies. Therefore, we propose to disentangle the high/low frequency patterns in an attention layer by separating the heads into two groups, where one group encodes high frequencies via self-attention within each local window, and another group performs the attention to model the global relationship between the average-pooled low-frequency keys from each window and each query position in the input feature map. Benefiting from the efficient design for both groups, we show that HiLo is superior to the existing attention mechanisms by comprehensively benchmarking FLOPs, speed and memory consumption on GPUs and CPUs. For example, HiLo is 1.4? faster than spatial reduction attention and 1.6? faster than local window attention on CPUs. Powered by HiLo, LITv2 serves as a strong backbone for mainstream vision tasks including image classification, dense detection and segmentation. Code is available at https://github.com/ziplab/LITv2.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Real-world applications usually require a model to have an optimal speed and accuracy trade-off under limited computational budget, such as UAV and autonomous driving. This motivates substantial works toward efficient vision Transformer (ViT) design, such as PVT <ref type="bibr" target="#b50">[51]</ref>, Swin <ref type="bibr" target="#b31">[32]</ref> and Focal Transformer <ref type="bibr" target="#b59">[60]</ref>, among others. To measure the computational complexity, a widely adopted metric in recent ViT design is the number of float-point operations, i.e., FLOPs. However, FLOPs is an indirect metric, which can not directly reflect the real speed on the target platform. For example, Focal-Tiny is much slower than Swin-Ti on GPUs although their FLOPs are comparable.</p><p>In general, the discrepancy between the indirect metric (FLOPs) and the direct metric (speed) in recent ViTs can be attributed to two main reasons. First, although self-attention is efficient on low-resolution feature maps, the quadratic complexity in both memory and time makes it much slower on high-resolution images due to intensive memory access cost <ref type="bibr" target="#b33">[34]</ref>, where fetching data from off-chip DRAM can be speed-consuming. Second, some efficient attention mechanisms in ViTs have low theoretical complexity guarantee but are actually slow on GPUs due to particular operations that are not hardware-friendly or cannot be parallelized, such as the multi-scale window partition <ref type="bibr" target="#b59">[60]</ref>, recursion <ref type="bibr" target="#b43">[44]</ref> and dilated window <ref type="bibr" target="#b19">[20]</ref>.</p><p>With these observations, in this paper we propose to evaluate ViT by the direct metric, i.e., throughput, not only FLOPs. Based on this principle, we introduce LITv2, a novel efficient and accurate vision Transformer that outperforms most state-of-the-art (SoTA) ViTs on standard benchmarks while being practically faster on GPUs. LITv2 is bulit upon LITv1 <ref type="bibr" target="#b35">[36]</ref>, a simple ViT baseline which removes all multi-head self-attention layers (MSAs) in the early stages while applying standard MSAs in the later stages. Benefit from this design, LITv1 is faster than many existing works on ImageNet classification due to no computational cost from the early MSAs while the later MSAs only need to process downsampled low-resolution feature maps. However, the standard MSA still suffers from huge computational cost on high-resolution images, especially for dense prediction tasks.</p><p>To address this problem, we propose a novel efficient attention mechanism, termed HiLo. HiLo is motivated by the fact that natural images contain rich frequencies where high/low frequencies play different roles in encoding image patterns, i.e., local fine details and global structures, respectively. A typical MSA layer enforces the same global attention across all image patches without considering the characteristics of different underlying frequencies. This motivates us to propose to separate an MSA layer into two paths where one path encodes high-frequency interactions via local self-attention with relatively high-resolution feature maps while the other path encodes low-frequency interactions via global attention with down-sampled feature maps, which leads to a great efficiency improvement.</p><p>Specifically, HiLo employs two efficient attentions to disentangle High/Low frequencies in feature maps. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, in the upper path, we allocate a few heads to the high frequency attention (Hi-Fi) to capture fine-grained high frequencies by local window self-attention (e.g., 2 ? 2 windows), which is much more efficient than standard MSAs. The lower path, implementing the low-frequency attention (Lo-Fi), first applies average pooling to each window to obtain low-frequency signals. Then, we allocate the remaining heads for Lo-Fi to model the relationship between each query position in the input feature map and the average-pooled low-frequency keys from each window. Benefit from the reduced length of keys and values, Lo-Fi also achieves significant complexity reduction. Finally, we concatenate the refined high/low-frequency features and forward the resulting output into subsequent layers. Since both Hi-Fi and Lo-Fi are not equipped with time-consuming operations such as dilated windows and recursion, the overall framework of HiLo is fast on both CPUs and GPUs. We show by comprehensive benchmarks that HiLo achieves advantage over the existing attention mechanisms in terms of performance, FLOPs, throughput and memory consumption.</p><p>Besides, we find the fixed relative positional encoding in LITv1 dramatically slows down its speed on dense prediction tasks due to the interpolation for different image resolutions. For better efficiency, we propose to adopt one 3 ? 3 depthwise convolutional layer with zero-padding in each FFN to incorporate the implicitly learned position information from zero-padding <ref type="bibr" target="#b26">[27]</ref>. Moreover, the 3 ? 3 convolutional filters simultaneously help to enlarge the receptive field of the early multi-layer perceptron (MLP) blocks in LITv1. Finally, we conduct extensive experiments on ImageNet, COCO and ADE20K to evaluate the performance of LITv2. Comprehensive comparisons with SoTA models show that our architecture achieves competitive performance with faster throughput, making ViTs more feasible to run low-latency applications for real-world scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Vision Transformers. Vision Transformers are neural networks that adopt self-attention mechanisms into computer vision tasks. In <ref type="bibr" target="#b17">[18]</ref>, Dosovitskiy et al. propose a ViT for image classification, which inherits the similar architecture from a standard Transformer <ref type="bibr" target="#b47">[48]</ref> in natural language processing (NLP) tasks. Since then, subsequent works have been proposed to improve ViT by incorporating more convolutional layers <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b60">61]</ref>, introducing pyramid feature maps <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b31">32]</ref>, enhancing the locality <ref type="bibr" target="#b61">[62]</ref>, as well as automatically searching a well-performed architecture <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b2">3]</ref> with neural architecture search (NAS). Some others also seek for token pruning to accelerate the inference speed of ViTs <ref type="bibr" target="#b36">[37]</ref> or applying ViT into low-level vision tasks <ref type="bibr" target="#b46">[47]</ref>. Compared to existing works, this paper focuses on a general ViT-based backbone for computer vision (CV) tasks and aims to achieve better efficiency on GPUs while maintaining competitive performance.</p><p>Efficient attention mechanisms. Efficient attention mechanisms aim to reduce the quadratic complexity of standard MSAs. Existing efforts in NLP can be roughly categories into low-rank decomposition <ref type="bibr" target="#b49">[50]</ref>, kernelization <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b38">39]</ref>, memory <ref type="bibr" target="#b39">[40]</ref> and sparsity mechanism <ref type="bibr" target="#b9">[10]</ref>. However, simply adopting these method usually performs suboptimally in CV tasks <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b62">63]</ref>. In CV, representative efficient self-attention mechanisms includes spatial reduction attention (SRA) <ref type="bibr" target="#b50">[51]</ref>, local window attention <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b25">26]</ref> and Twins attention <ref type="bibr" target="#b11">[12]</ref>. However, they only focus on either local or global attention at the same layer. To address this problem, TNT <ref type="bibr" target="#b20">[21]</ref> introduced additional global tokens and MixFormer <ref type="bibr" target="#b5">[6]</ref> mixed local window attention with depthwise convolutional layers. Some other attention mechanisms consider both simultaneously, such as Focal <ref type="bibr" target="#b59">[60]</ref> and QuadTree <ref type="bibr" target="#b43">[44]</ref>. However, due to the inefficient operations which are not hardware-friendly and cannot be reflected in FLOPs (e.g., multi-scale window partition, recursion), they are slow on GPUs even compared to standard MSA. To this end, the proposed HiLo attention simultaneously captures rich local-global information at the same MSA layer and is faster and more memory-efficient compared to the existing works.</p><p>Frequency domain analysis in vision. The frequency domain analysis in CV has been well studied in the literature. According to <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16]</ref>, the low frequencies in an image usually capture global structures and color information while the high frequencies contain fine details of objects (e.g., sharp edges). Based on this insight, a plethora of solutions have been proposed for image superresolution <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b18">19]</ref>, generalization <ref type="bibr" target="#b24">[25]</ref>, image re-scaling <ref type="bibr" target="#b55">[56]</ref> and neural network compression <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b6">7]</ref>. Furthermore, Octave convolution <ref type="bibr" target="#b8">[9]</ref> targeted convolutional layers and proposed to locally applies convolution on high/low-resolution feature maps, separately. Different from it, the proposed HiLo is a novel attention mechanism that captures both local and global relationships with self-attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head><p>Multi-head self-attention. Transformers are built upon multi-head self-attention, which enables to capture long-range relationships for tokens at different positions. Specifically, let X ? R N ?D be the input sequence into a standard MSA layer, where N is the length of the input sequence and D refers to the number of hidden dimensions. Each self-attention head calculates the query Q, key K and value V matrices with a linear transformation from X,</p><formula xml:id="formula_0">Q = XW q , K = XW k , V = XW v ,<label>(1)</label></formula><p>where W q , W k , W v ? R D?D h are learnable parameters and D h is the number of hidden dimensions for a head. Next, the output of a self-attention head is a weighted sum over N value vectors,</p><formula xml:id="formula_1">SA h (X) = Softmax( QK ? D h )V.<label>(2)</label></formula><p>For an MSA layer with N h heads, the final output is computed by a linear projection of the concatenated outputs from each self-attention head, which can be formulated by</p><formula xml:id="formula_2">MSA(X) = concat h?[N h ] [SA h (X)]W o ,<label>(3)</label></formula><p>where Transformer blocks. A standard vision Transformer as described in <ref type="bibr" target="#b17">[18]</ref> consists of a patch embedding layer, several blocks and a prediction head. Let l be the index of a block. Then each block contains an MSA layer and a position-wise feed-forward network (FFN), which can expressed as</p><formula xml:id="formula_3">X l?1 = X l?1 + MSA(LN(X l?1 )),<label>(4)</label></formula><formula xml:id="formula_4">X l = X l?1 + FFN(LN(X l?1 )),<label>(5)</label></formula><p>where LN denotes the LayerNorm <ref type="bibr" target="#b1">[2]</ref> and an FFN consists of two FC layers with GELU <ref type="bibr" target="#b23">[24]</ref> nonlinearity in between. Recent works on ViT have proposed to divide the blocks into several stages (typically 4 stages) to generate pyramid feature maps for dense prediction tasks. Furthermore, to reduce the computational cost on high-resolution feature maps in the early stages, the MSA in Eq. (4) has been replaced with efficient alternatives, such as SRA <ref type="bibr" target="#b50">[51]</ref> and W-MSA <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bottlenecks of LITv1.</head><p>Recent studies have shown that the MSA layers in the early stages in a model still focus on local patterns <ref type="bibr" target="#b13">[14]</ref>. With the same observation, LITv1 <ref type="bibr" target="#b35">[36]</ref> removes all early MSAs (i.e., exclude Eq. (4) in each block) while applying standard MSAs at the later stages. This design principle has achieved better efficiency with competitive performance on ImageNet compared to PVT <ref type="bibr" target="#b50">[51]</ref> and Swin <ref type="bibr" target="#b31">[32]</ref>. However, LITv1 still has two main bottlenecks in speed: 1) Given a high-resolution image, the standard MSAs in the later stages still result in huge computational cost.</p><p>2) The fixed relative positional encoding <ref type="bibr" target="#b31">[32]</ref> dramatically slows down the speed when dealing with different image resolutions. This is due to interpolating the fixed-size positional encoding for each different image resolution. In the next section, we describe a novel attention mechanism with zero padding positional encoding to comprehensively accelerate LITv1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">HiLo Attention</head><p>We propose to separately process high/low frequencies in a feature map at an attention layer. We name the new attention mechanism as HiLo, which is depicted in <ref type="figure" target="#fig_0">Figure 1</ref>. Essentially, the low-frequency attention branch (Lo-Fi) is to capture the global dependencies of the input (image/features), which does not need a high-resolution feature map but requires global attention. On the other hand, the high-frequency attention branch (Hi-Fi) is to capture the fine detailed local dependency, which requires a high-resolution feature map but can be done via local attention. In the next, we describe the two attentions in detail.</p><p>High-frequency attention. Intuitively, as high frequencies encode local details of objects, it can be redundant and computationally expensive to apply global attention on a feature map. Therefore, we propose to design Hi-Fi to capture fine-grained high frequencies with local window self-attention (e.g., 2 ? 2 windows), which saves significant computational complexity. Furthermore, we employ the simple non-overlapping window partition in Hi-Fi, which is more hardware-friendly compared to the time-consuming operations such as window shifting <ref type="bibr" target="#b31">[32]</ref> or multi-scale window partition <ref type="bibr" target="#b59">[60]</ref>.</p><p>Low-frequency attention. Recent studies have shown that the global attention in MSA helps to capture low frequencies <ref type="bibr" target="#b37">[38]</ref>. However, directly applying MSA to high-resolution feature maps requires huge computational cost. As averaging is a low-pass filter <ref type="bibr" target="#b48">[49]</ref>, Lo-Fi firstly applies average pooling to each window to get low-frequency signals in the input X. Next, the average-pooled feature maps are projected into keys K ? R N/s 2 ?D h and values V ? R N/s 2 ?D h , where s is the window size. The queries Q in Lo-Fi still comes from the original feature map X. We then apply the standard attention to capture the rich low-frequency information in feature maps. Note that due to the spatial reduction of K and V, Lo-Fi simultaneously reduces the complexity for both Eq. (1) and Eq. (2).</p><p>Head splitting. A naive solution for head assignment is to allocate both Hi-Fi and Lo-Fi the same number of heads as the standard MSA layer. However, doubling heads results in more computational cost. In order to achieve better efficiency, HiLo separates the same number of heads in an MSA into two groups with a split ratio ?, where (1 ? ?)N h heads will be employed for Hi-Fi and the other ?N h heads are used for Lo-Fi. By doing so, as each attention has a lower complexity than a standard MSA, the entire framework of HiLo guarantees a low complexity and ensures high throughput on GPUs. Moreover, another benefit of head splitting is that the learnable parameter W o can be decomposed into two smaller matrices, which helps to reduce model parameters. Finally, the output of HiLo is a  concatenation of the outputs from each attention</p><formula xml:id="formula_5">HiLo(X) = [Hi-Fi(X); Lo-Fi(X)],<label>(6)</label></formula><p>where [?] denotes the concatenation operation.</p><p>Complexity Analysis. Without loss of generality, we assume Hi-Fi and Lo-Fi have an equal number of heads (i.e., ? = 0.5) and the feature map has equal width and height. Then, Hi-Fi and Lo-Fi have a computational cost of <ref type="bibr">7 4</ref> </p><formula xml:id="formula_6">N D 2 + s 2 N D and ( 3 4 + 1 s 2 )N D 2 + 1 s 2 N 2 D, respectively.</formula><p>Derivation for this result can be found in the supplementary material. As shown in <ref type="figure" target="#fig_3">Figure 2</ref>-(a) and (b), under a small input image resolution and a small value of s (e.g., s = 2), both Hi-Fi and Lo-Fi are comparably efficient. However, with a much higher resolution, Lo-Fi will result in a huge computational cost as it still has a quadratic complexity in terms of N in Eq. (2), i.e., 1 s 2 N 2 D. In this case, slightly increasing s (e.g., s = 4) helps Lo-Fi achieve better efficiency while preserving the accuracy. Combining the two attentions together, a larger window size also helps the overall framework of HiLo to reduce more FLOPs on high-resolution images, as shown in <ref type="figure" target="#fig_3">Figure 2</ref>-(c). Thus, we suggest a practical guideline for adopting HiLo into existing frameworks: increasing the window size in order to get better efficiency on high-resolution images. We further show in Section 5.2 that this principle helps LITv2 achieve a better speed and accuracy trade-off on downstream tasks, e.g., dense object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Positional Encoding</head><p>Positional encoding is essential to self-attention due to its permutation-invariant property. In LITv1, the later MSAs adopt the same relative positional encoding (RPE) scheme as Swin <ref type="bibr" target="#b31">[32]</ref>. This approach has significantly improves Swin by 0.7% in Top-1 accuracy on ImageNet compared to using absolute positional encoding <ref type="bibr" target="#b31">[32]</ref>. However, on dense prediction tasks, the fixed RPE has to be interpolated for different image resolutions, which dramatically slows down the training/inference speed of LITv1. As a recent study <ref type="bibr" target="#b26">[27]</ref> has shown that position information can be implicitly learned from zero-padding in CNNs, we propose to adopt one layer of 3 ? 3 depthwise convolutional layer with zero-padding in each FFN to replace the time-consuming RPE. Notably, due to the elimination of early MSAs, the early blocks in LITv1 only have FFNs left, which results in a tiny receptive field of 1 ? 1. To this end, we show in Section 5.4 that the 3 ? 3 convolutional filters adopted in each FFN also improve LITv2 by simultaneously enlarging the receptive field in the early stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model Architecture</head><p>LITv2 has three variants: LITv2-S, LITv2-M and LITv2-B, corresponding to the small, medium and base settings in LITv1, respectively. For a fair comparison, we keep the network width and depth as the same as LITv1. The overall modifications are simply in two steps: 1) Adding one layer of depthwise convolution with zero-padding in each FFN and removing all relative positional encodings in all MSAs. 2) Replacing all attention layers with the proposed HiLo attention. Detailed architecture configurations can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment</head><p>In this section we conduct experiments to validate the effectiveness of the proposed LITv2. Following common practice <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b59">60]</ref>, we experiment LITv2 on three tasks, including image classification on ImageNet-1K <ref type="bibr" target="#b42">[43]</ref>, object detection and instance segmentation on COCO <ref type="bibr" target="#b30">[31]</ref> and semantic segmentation on ADE20K <ref type="bibr" target="#b64">[65]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Image Classification on ImageNet-1K</head><p>We conduct image classification experiments on ImageNet-1K <ref type="bibr" target="#b42">[43]</ref>, a large-scale image dataset which contains ?1.2M training images and 50K validation images from 1K categories. We measure the model performance by Top-1 accuracy. Furthermore, we report the FLOPs, throughput, as well as training/test memory consumption on GPUs. We compare with two CNN-based models <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b32">33]</ref> and several representative SoTA ViTs <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b11">12]</ref>. Note that this paper does not consider mobile-level architectures <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b34">35]</ref>. Instead, we focus on models with the similar model size. Besides, we are also not directly comparable with NAS-based methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref> as LITv2 is manually designed.</p><p>Implementation details. All models are trained for 300 epochs from scratch on 8 V100 GPUs. At training time, we set the total batch size as 1,024. The input images are resized and randomly cropped into 224 ? 224. The initial learning rate is set to 1 ? 10 ?3 and the weight decay is set to 5 ? 10 ?2 . We use AdamW optimizer with a cosine decay learning rate scheduler. All training strategies including the data augmentation are same as in LITv1. For HiLo, the window size s is set to 2. The split ratio ? is set to 0.9, which is chosen from a simple grid search on ImageNet-1K. The depthwise convolutional layers in FFNs are set with a kernel size of 3 ? 3, stride of 1 and zero padding size of 1. Results. In <ref type="table" target="#tab_0">Table 1</ref>, we report the experiment results on ImageNet-1K. First, compared to LITv1 baselines, LITv2 achieves consistent improvement on Top-1 accuracy while using less FLOPs. Moreover, benefit from HiLo, LITv2 achieves faster throughput and significant training time memory reduction (e.g., 13%, 27%, 36% inference speedup for the small, medium and base settings, respectively) compared to LITv1. Second, compared to CNNs, LITv2 models outperform all counterparts of ResNet and ConvNext in terms of FLOPs, throughput and memory consumption while achieving comparable performance. Last, compared to SoTA ViTs, LITv2 surpasses many models in terms of throughput and memory consumption with competitive performance. For example, under the similar amount of FLOPs, LITv2-S achieves faster inference speed than PVT-S and Twins-PCPVT-S with better performance. Although Focal-Tiny achieves better Top-1 accuracy than LITv2-S, it runs much slower (i.e., 384 vs. 1,471 images/s) and requires a large amount of memory to train. Besides, when finetuning on a higher resolution, LITv2-B outperforms both DeiT-B and Swin-B with a faster throughput and lower complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Object Detection and Instance Segmentation on COCO</head><p>In this section, we conduct experiments on COCO 2017, a common benchmark for object detection and instance segmentation which contains ?118K images for the training set and ?5K images for the validation set. Following common practice <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b50">51]</ref>, we experiment with two detection frameworks: RetinaNet <ref type="bibr" target="#b29">[30]</ref> and Mask R-CNN <ref type="bibr" target="#b21">[22]</ref>. We measure model performance by Average Precision (AP).</p><p>Implementation details. All backbones are initialized with pretrained weights on ImageNet-1K. We train each model on 8 GPUs with 1? schedule (12 epochs) and a total batch size of 16. For a fair comparison, we adopt the same training strategy and hyperparameter settings as in LITv1 <ref type="bibr" target="#b35">[36]</ref>. Note that we pretrain LITv2 with a local window size of 2 and ? = 0.9 on ImageNet-1K. Under the same ?, a larger window size helps to achieve lower complexity and thus improves the speed at high resolution, as explained in Section 4.1. In this case, we also train models with a slightly larger window size of s = 4 for better efficiency, which we denote with "*". By default, FLOPs is evaluated based on the input resolution of 1280 ? 800. FPS is measured on one RTX 3090 GPU based on the mmdetection <ref type="bibr" target="#b3">[4]</ref> framework.</p><p>Results. In <ref type="table" target="#tab_1">Table 2</ref>, we report the experimental results on COCO. In general, LITv2 outperforms LITv1 by a large margin in almost all metrics. Besides, our LITv2 significantly surpasses ResNet in terms of AP, though it runs slightly slower in some cases. More importantly, our LITv2 beats all the compared SoTA ViTs, achieving the best AP with compelling fast inference speed. Furthermore, by adopting a larger window size (i.e., s = 4), LITv2 achieves better efficiency with a slightly performance drop.  <ref type="figure">Figure 3</ref>: Comparison with other attention mechanisms based on LITv2-S. We report the FLOPs, throughput, and training/test time memory consumption. Evaluations are based on a batch size of 64 on one RTX 3090 GPU. The black cross symbol means "out-of-memory". In this section, we evaluate LITv2 on the semantic segmentation task. We conduct experiments on ADE20K <ref type="bibr" target="#b64">[65]</ref>, a widely adopted dataset for semantic segmentation which has ?20K training images, ?2K validation images and ?3K test images. Following prior works, we adopt the framework of Semantic FPN <ref type="bibr" target="#b28">[29]</ref> and measure the model performance by mIoU. We train each model on 8 GPUs with a total batch size of 16 with 80K iterations. All backbones are initialized with pretrained weights on ImageNet-1K. The stochastic depth for the small, medium and base models of LITv2 are 0.2, 0.2 and 0.3, respectively. All other training strategies are the same as in LITv1 <ref type="bibr" target="#b35">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Semantic Segmentation on ADE20K</head><p>Results. In <ref type="table" target="#tab_3">Table 3</ref>, we compare LITv2 with ResNet and representative ViTs on ADE20K. In general, LITv2 achieves fast speed while outperforming many SoTA models. For example, our LITv2-S, LITv2-M and LITv2-B surpass Swin-Ti, Swin-S and Swin-B by 2.8%, 0.5% and 1.2% in mIoU with higher FPS, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Study</head><p>In this section, we provide ablation studies for LITv2, including the comparison with other efficient attention variants, the effect of ? in HiLo, as well as the effect of architecture modifications. By default, the throughput and memory consumption are measured on one RTX 3090 GPU with a batch size of 64 under the resolution of 224 ? 224.</p><p>Comparing HiLo with other attention mechanisms. Based on LITv2-S, we compare the performance of HiLo with other efficient attention mechanisms on ImageNet-1K, including spatial reduction attention (SRA) in PVT <ref type="bibr" target="#b50">[51]</ref>, shifted-window based attention (W-MSA) in Swin <ref type="bibr" target="#b31">[32]</ref> and alternated local and global attention (T-MSA) in Twins <ref type="bibr" target="#b11">[12]</ref>. In our implementation, we directly replace HiLo FLOPs (G) <ref type="figure">Figure 4</ref>: Effect of ? based on LITv2-S.  with each compared method. The results are reported in <ref type="table" target="#tab_2">Table 4</ref>. In general, HiLo reduces more FLOPs while achieving better performance and faster speed than the compared methods. Furthermore, in <ref type="figure">Figure 3</ref>, we provide comprehensive benchmarks for more attention mechanisms based on different image resolutions, including Focal <ref type="bibr" target="#b59">[60]</ref>, QuadTree <ref type="bibr" target="#b43">[44]</ref> and Performer <ref type="bibr" target="#b10">[11]</ref>. Suffering from weak parallelizability, they are even slower than that of using standard MSAs on GPUs. Compared to them, HiLo achieves competitive results in terms of the FLOPs, throughput and memory consumption. Moreover, we conduct experiments based on ADE20K and Semantic FPN and show that HiLo achieves more performance gain than other attention mechanisms on the downstream dense prediction task.</p><p>Effect of ?. As shown in <ref type="figure">Figure 4</ref>, since the complexity of Lo-Fi is lower than Hi-Fi under the resolution of 224 ? 224 and the window size of 2, a larger ? helps to reduce more FLOPs as we allocate more heads to Lo-Fi. Moreover, we found HiLo performs badly with ? = 0, in which case only the Hi-Fi is left and HiLo only focuses on high frequencies. We speculate that low frequencies play an important role in self-attention. For other values of ?, we find the performance difference is around 0.2%, where ? = 0.9 achieves the best performance. However, it is worth noting that although the pure Lo-Fi branch (? = 1.0) can achieve competitive results on ImageNet-1K, high-frequency signals play an important role in capturing fine object details, which is particularly important for dense prediction tasks such as semantic segmentation. For example, with ? = 0.9, LITv2-S based Semantic FPN achieves more performance gain (+0.6%) than that of using ? = 1.0 (43.7%).</p><p>Effect of architecture modifications. Based on LITv2-S, we explore the effect of architecture modifications. As shown in <ref type="table" target="#tab_5">Table 5</ref>, benefit from the enlarged receptive field in the early stages, the adoption of depthwise convolutions improves the performance on both ImageNet and COCO. Next, by removing the relative positional encoding, we significantly improve FPS on dense prediction tasks with a slightly performance drop on both datasets. Also note that since depthwise convolutions have encoded positional information by zero paddings <ref type="bibr" target="#b26">[27]</ref>, the elimination of RPE does not result in a significant performance drop compared to prior works <ref type="bibr" target="#b31">[32]</ref>. Finally, benefit from HiLo, we achieve more gains in model efficiency on both ImageNet and COCO.</p><p>Spectrum analysis of HiLo. In <ref type="figure" target="#fig_4">Figure 5</ref>, we visualize the magnitude of frequency component <ref type="bibr" target="#b41">[42]</ref> by applying Fast Fourier Transform (FFT) to the output feature maps from Hi-Fi and Lo-Fi attentions, respectively. The visualisation indicates that Hi-Fi captures more high frequencies and Lo-Fi mainly focuses on low frequencies. This strongly aligns with our aim of disentangling high and low frequencies in feature maps at a single attention layer.  Speed and performance comparisons with more ViTs on different GPUs. We compare the inference speed with more models and on more types of GPUs. <ref type="table" target="#tab_6">Table 6</ref> reports the results. It shows that LITv2-S still achieves consistent faster throughput (images/s) than many ViTs on NVIDIA A100, Tesla V100, RTX 6000, and RTX 3090. It is also worth noting that under similar performance (82.0%), LITv2-S is 2.1? faster than PVTv2-B2 [52], 1.7? faster than XCiT-S12 <ref type="bibr" target="#b0">[1]</ref> and ConvNext-Ti <ref type="bibr" target="#b32">[33]</ref>, and 3.5? faster than Focal-Tiny <ref type="bibr" target="#b59">[60]</ref> on V100, which is another common GPU version for speed test in previous works <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b63">64]</ref>.</p><p>Throughput comparisons with more attention mechanisms on CPUs and GPUs. In <ref type="figure" target="#fig_5">Figure 6</ref>, we show that HiLo is consistently faster than many attention mechanisms <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b17">18]</ref> on both CPUs and GPUs. In particular, under CPU testing, HiLo is 1.4? faster than SRA [51], 1.6? faster than local window attention <ref type="bibr" target="#b31">[32]</ref> and 17.4? faster than VAN <ref type="bibr" target="#b19">[20]</ref>. Detailed benchmark configurations can be found in Section A.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this paper, we have introduced LITv2, a novel efficient vision Transformer backbone with fast speed on GPUs and outperforms most SoTA models on ImageNet and downstream tasks. We have also presented HiLo attention, the core of LITv2 which helps to achieve better efficiency especially on high-resolution images. With competitive performance, HiLo achieves great advantage over the existing attention mechanisms across FLOPs, throughput and memory consumption. Future work may include incorporating convolutional stem <ref type="bibr" target="#b56">[57]</ref> and overlapping patch embedding <ref type="bibr" target="#b51">[52]</ref> for better performance, or extending HiLo on more tasks such as speech recognition and video processing.</p><p>Limitations and societal impact. HiLo adopts a head splitting ratio to assign different numbers of heads into Hi-Fi and Lo-Fi. In our experiments, this ratio is determined by a grid search on ImageNet (i.e., ? = 0.9). However, different tasks may have different importance on high and low frequencies. Thus, the optimal value of ? is task-specific and needs to be set manually. Besides, our work potentially brings some negative societal impacts, such as the huge energy consumption and carbon emissions from large-scale training on GPU clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>We organize our supplementary material as follows.</p><p>? In Section A.1, we describe the architecture specifications of LITv2.</p><p>? In Section A.2, we provide the derivation for the computational cost of HiLo attention.</p><p>? In Section A.3, we study the effect of window size based on CIFAR-100.</p><p>? In Section A.4, we provide additional study on the Lo-Fi branch where we directly compute the queries from pooled feature maps.</p><p>? In Section A.5, we describe more details of the throughput benchmark for more attention mechanisms on CPUs and GPUs.</p><p>? In Section A.6, we provide more visualisation examples for spectrum analysis of HiLo attention. <ref type="table">Table 7</ref>: Architecture specifications of LITv2. P denotes the patch size in the patch embedding layer and C is the channel dimension. H is the number of self-attention heads. ? and s are the split ratio and window size in HiLo, respectively. E is the expansion ratio in the FFN layer. "DTM" refers to the deformable token merging module in LITv1. We use "ConvFFN Block" to differentiate our modified FFNs in the early stages from the previous MLP Blocks in LITv1 <ref type="bibr" target="#b35">[36]</ref>.</p><formula xml:id="formula_7">Stage Output Size Layer Name LITv2-S LITv2-M LITv2-B Stage 1 H 4 ? W 4</formula><p>Patch Embedding P 1 = 4 C 1 = 96 </p><formula xml:id="formula_8">P 1 = 4 C 1 = 96 P 1 = 4 C 1 = 128 ConvFFN Block E 1 = 4 ? 2 E 1 = 4 ? 2 E 1 = 4 ? 2 Stage 2 H 8 ? W 8 DTM P 2 = 2 C 2 = 192 P 2 = 2 C 2 = 192 P 2 = 2 C 2 = 256 ConvFFN Block E 2 = 4 ? 2 E 2 = 4 ? 2 E 2 = 4 ? 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Architecture Specifications of LITv2</head><p>The overall framework of LITv2 is depicted in <ref type="figure">Figure 7</ref>. We also provide detailed architecture specifications of LITv2 in <ref type="table">Table 7</ref>. In general, we set the same network depth and width as LITv1. It is worth noting that recent works <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b53">54]</ref> usually adopt standard MSAs at the last stage, including LITv1. Following common practice, we set ? = 1.0 and s = 1 at the last stage to make HiLo behave as a standard MSA. LITv2 also excludes MSAs in the first two stages due to the tiny receptive field of attention heads, as visualized in <ref type="figure">Figure 3</ref> of LITv1 <ref type="bibr" target="#b35">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Computational Cost of HiLo Attention</head><p>Let N and D be the number of tokens and the number of hidden dimensions in an HiLo attention layer. We denote s as the window size. For simplicity, we assume Hi-Fi and Lo-Fi have an equal number of heads and the feature map has equal width and height. Then, the computational cost of each attention comes from three parts: 1) The projections of Q, K, V matrices.</p><p>2) The attention  <ref type="figure">Figure 7</ref>: Framework of LITv2. C i and L i refer to the number of hidden dimensions and the number of blocks at the i-th stage. "ConvFFN" denotes our modified FFN layer where we adopt one layer of depthwise convolution in the FFN. </p><formula xml:id="formula_9">N ? D ? D 2 ? 3 = 3 2 N D 2 ,<label>(7)</label></formula><formula xml:id="formula_10">s 2 ? s 2 ? D 2 ? N s 2 ? 2 = s 2 N D,<label>(8)</label></formula><formula xml:id="formula_11">N ? D 2 ? D 2 = 1 4 N D 2 ,<label>(9)</label></formula><p>respectively. Overall, this gives rise to a total computational cost of 7 4 N D 2 + s 2 N D for Hi-Fi. Next, the computational cost for each part in Lo-Fi is</p><formula xml:id="formula_12">N ? D ? D 2 + N s 2 ? D ? D 2 ? 2 = ( 1 2 + 1 s 2 )N D 2 ,<label>(10)</label></formula><formula xml:id="formula_13">N ? N s 2 ? D 2 ? 2 = N 2 s 2 D,<label>(11)</label></formula><formula xml:id="formula_14">N ? D 2 ? D 2 = 1 4 N D 2 ,<label>(12)</label></formula><p>respectively. Thus, the total computational cost of Lo-Fi is</p><formula xml:id="formula_15">( 3 4 + 1 s 2 )N D 2 + 1 s 2 N 2 D.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Effect of Window Size</head><p>Based on LITv2-S, we study the effect of window size in HiLo by experimenting on CIFAR-100. As shown in <ref type="table" target="#tab_9">Table 8</ref>, the window size does not affect the model parameters since the parameters of HiLo do not depend on it. Moreover, as both Hi-Fi and Lo-Fi are comparably efficient under the small  resolution of image classification (i.e., 224?224), all settings have the comparable FLOPs, speed and memory footprint, where the difference is mainly due to the extra cost from padding on feature maps for window partition <ref type="bibr" target="#b31">[32]</ref>. Overall, we find the window size of 2 performs the best, which therefore serves as our default setting in LITv2 for image classification. Also note that as discussed in the main manuscript, a slightly larger window size (e.g., 4) can help LITv2 achieve better efficiency on larger resolutions with a slightly performance drop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Additional Study on the Lo-Fi Branch</head><p>In the proposed HiLo attention, the Lo-Fi branch computes queries from the original input feature maps. An alternative approach is to directly compute the queries from the average-pooled feature maps. However, in self-attention, the number of queries determines the spatial size of the output feature maps. When computing the queries from pooled feature maps, the spatial size of the output feature maps is inconsistent with that of the original input. One solution is to use interpolation (e.g. bilinear) and concatenate the interpolated feature maps with the outputs from Hi-Fi. However, as shown in <ref type="table" target="#tab_10">Table 9</ref>, this approach (denoted as "pooled queries") brings inferior performance and much slower throughput than our proposed design. Note that although computing queries from pooled feature maps can slightly achieve a lower theoretical model complexity, frequently applying interpolation on GPUs results in a high memory access cost (MAC). Therefore, it instead slows down the inference speed on GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Details of Throughput Benchmark for Different Attention Mechanisms</head><p>To evaluate the inference speed of HiLo on CPUs and GPUs, we benchmark the throughput based on a single attention layer and the standard settings of training ViT-B <ref type="bibr" target="#b17">[18]</ref> on ImageNet. Specifically, under the input resolution of 224?224, attention layers in ViT-B need to handle 14?14 (1/16 scale) feature maps, where each attention layer has 12 heads and each head has 64 dimensions. For a fair comparison, we adopt the aforementioned configurations for all compared methods by default. Besides, since different methods have distinct hyperparameters, we adopt their default settings for dealing with 1/16 scale feature maps. For example, HiLo adopts a window size of 2 and alpha of 0.9 when processing 1/16 scale feature maps. In <ref type="table" target="#tab_0">Table 10</ref>, we report more benchmark results. Overall, we show that under a similar amount of parameters, a single layer of HiLo uses less FLOPs than compared methods, meanwhile it is faster on both CPUs and GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 More Visualisations on Spectrum Analysis</head><p>In <ref type="figure">Figure 8</ref> and <ref type="figure">Figure 9</ref>, we provide frequency magnitude visualisations for Hi-Fi and Lo-Fi attention outputs, respectively. Clearly, the results indicate that Hi-Fi captures more high frequencies in LITv2 while Lo-Fi mainly focuses on low frequencies. We also provide the PyTorch-style code in Algorithm 1 to explain our visualisation. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Framework of HiLo attention. N h refers to the total number of self-attention heads at this layer. ? denotes the split ratio for high/low frequency heads. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>W o ? R (N h ?D h )?D is a learnable parameter. In practice, D is usually equal to N h ? D h . Overall, a standard MSA layer have the computational cost of 4N D 2 + 2N 2 D, where 2N 2 D comes from Eq. (2), 3N D 2 and N D 2 comes from Eq. (1) and Eq. (3), respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>FLOPs comparison for Hi-Fi and Lo-Fi under different image resolutions and equal number of heads (Figures a and b). A larger window size helps HiLo achieve better efficiency on high-resolution images (Figure c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Frequency magnitude (14 ? 14) from 8 output channels of Hi-Fi and Lo-Fi in LITv2-B. The magnitude is averaged over 100 samples. The lighter the color, the larger the magnitude. A pixel that is closer to the centre means a lower frequency. Visualization code can be found in Section A.6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Throughput comparisons with more attention mechanisms on CPUs and GPUs based on a single attention layer and 14?14 feature maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Algorithm 1 Figure 8 : 6 Figure 9 :</head><label>1869</label><figDesc>PyTorch-style Code for Visualising Frequency Magnitude. import matplotlib.pyplot as plt import torch def visualize_freq(x): ''' x : The output feature maps from either Hi-Fi or Lo-Fi attention. Tensor shape: (batch_size, hidden_dim, height, width) ''' fft_output = torch.fft.fft2(x.float()) freq_img = torch.log(torch.abs(torch.fft.fftshift(fft_output))) num_plots = 8 # average over samples freq_img_mean = freq_img.mean(dim=0).cpu() fig, axis = plt.subplots(1, num_plots, figsize=(num_plots * 4, 4)) for i in range(num_plots): axis[i].imshow(freq_img_mean[i, ...].numpy()) axis[i].axes.xaxis.set_visible(False) axis[i].axes.yaxis.set_visible(False) plt.axis('off') plt.tight_layout() plt.show() Frequency magnitude (14 ? 14) from 8 output channels of Hi-Fi in LITv2-S. The magnitude is averaged over 100 samples. Frequency magnitude (14 ? 14) from 8 output channels of Lo-Fi in LITv2-S. The magnitude is averaged over 100 samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Image classification results on ImageNet-1K. By default, the FLOPs, throughput and memory consumption are measured based on the resolution 224 ? 224. We report the throughput and training/test time memory consumption with a batch size of 64. Throughput is tested on one NVIDIA RTX 3090 GPU and averaged over 30 runs. ResNet results are from "ResNet Stikes Back"<ref type="bibr" target="#b52">[53]</ref>. "? 384" means a model is finetuned at the resolution 384 ? 384. "OOM" means "out-of-memory".</figDesc><table><row><cell>Model</cell><cell>Param (M)</cell><cell>FLOPs (G)</cell><cell>Throughput (imgs/s)</cell><cell>Train Mem (GB)</cell><cell>Test Mem (GB)</cell><cell>Top-1 (%)</cell></row><row><cell>ResNet-50 [53]</cell><cell>26</cell><cell>4.1</cell><cell>1,279</cell><cell>7.9</cell><cell>2.8</cell><cell>80.4</cell></row><row><cell>ConvNext-Ti [33]</cell><cell>28</cell><cell>4.5</cell><cell>1,079</cell><cell>8.3</cell><cell>1.7</cell><cell>82.1</cell></row><row><cell>PVT-S [51]</cell><cell>25</cell><cell>3.8</cell><cell>1,007</cell><cell>6.8</cell><cell>1.3</cell><cell>79.8</cell></row><row><cell>Swin-Ti [32]</cell><cell>28</cell><cell>4.5</cell><cell>961</cell><cell>6.1</cell><cell>1.5</cell><cell>81.3</cell></row><row><cell>CvT-13 [54]</cell><cell>20</cell><cell>4.5</cell><cell>947</cell><cell>6.1</cell><cell>1.5</cell><cell>81.6</cell></row><row><cell>Focal-Tiny [60]</cell><cell>29</cell><cell>4.9</cell><cell>384</cell><cell>12.2</cell><cell>3.3</cell><cell>82.2</cell></row><row><cell>Twins-PCPVT-S [12]</cell><cell>24</cell><cell>3.8</cell><cell>998</cell><cell>6.8</cell><cell>1.2</cell><cell>81.2</cell></row><row><cell>LITv1-S [36]</cell><cell>27</cell><cell>4.1</cell><cell>1,298</cell><cell>5.8</cell><cell>1.2</cell><cell>81.5</cell></row><row><cell>LITv2-S</cell><cell>28</cell><cell>3.7</cell><cell>1,471</cell><cell>5.1</cell><cell>1.2</cell><cell>82.0</cell></row><row><cell>ResNet-101 [53]</cell><cell>45</cell><cell>7.9</cell><cell>722</cell><cell>10.5</cell><cell>3.0</cell><cell>81.5</cell></row><row><cell>ConvNext-S [33]</cell><cell>50</cell><cell>8.7</cell><cell>639</cell><cell>12.3</cell><cell>1.8</cell><cell>83.1</cell></row><row><cell>PVT-M [51]</cell><cell>44</cell><cell>6.7</cell><cell>680</cell><cell>9.3</cell><cell>1.5</cell><cell>81.2</cell></row><row><cell>Twins-SVT-B [12]</cell><cell>56</cell><cell>8.3</cell><cell>621</cell><cell>9.8</cell><cell>1.9</cell><cell>83.2</cell></row><row><cell>Swin-S [32]</cell><cell>50</cell><cell>8.7</cell><cell>582</cell><cell>9.7</cell><cell>1.7</cell><cell>83.0</cell></row><row><cell>LITv1-M [36]</cell><cell>48</cell><cell>8.6</cell><cell>638</cell><cell>12.0</cell><cell>1.4</cell><cell>83.0</cell></row><row><cell>LITv2-M</cell><cell>49</cell><cell>7.5</cell><cell>812</cell><cell>8.8</cell><cell>1.4</cell><cell>83.3</cell></row><row><cell>ResNet-152 [53]</cell><cell>60</cell><cell>11.6</cell><cell>512</cell><cell>13.4</cell><cell>2.9</cell><cell>82.0</cell></row><row><cell>ConvNext-B [33]</cell><cell>89</cell><cell>15.4</cell><cell>469</cell><cell>16.9</cell><cell>2.9</cell><cell>83.8</cell></row><row><cell>Twins-SVT-L [12]</cell><cell>99</cell><cell>14.8</cell><cell>440</cell><cell>13.7</cell><cell>3.1</cell><cell>83.7</cell></row><row><cell>Swin-B [32]</cell><cell>88</cell><cell>15.4</cell><cell>386</cell><cell>13.4</cell><cell>2.4</cell><cell>83.3</cell></row><row><cell>LITv1-B [36]</cell><cell>86</cell><cell>15.0</cell><cell>444</cell><cell>16.4</cell><cell>2.1</cell><cell>83.4</cell></row><row><cell>LITv2-B</cell><cell>87</cell><cell>13.2</cell><cell>602</cell><cell>12.2</cell><cell>2.1</cell><cell>83.6</cell></row><row><cell>DeiT-B? 384 [45]</cell><cell>86</cell><cell>55.4</cell><cell>159</cell><cell>39.9</cell><cell>2.5</cell><cell>83.1</cell></row><row><cell>Swin-B? 384 [32]</cell><cell>88</cell><cell>47.1</cell><cell>142</cell><cell>OOM</cell><cell>6.1</cell><cell>84.5</cell></row><row><cell>LITv2-B? 384</cell><cell>87</cell><cell>39.7</cell><cell>198</cell><cell>35.8</cell><cell>4.6</cell><cell>84.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Object detection and instance segmentation performance on the COCO val2017 split using the RetinaNet<ref type="bibr" target="#b29">[30]</ref> and Mask R-CNN<ref type="bibr" target="#b21">[22]</ref> framework. AP b and AP m denote the bounding box AP and mask AP, respectively. "*" indicates the model adopts a local window size of 4 in HiLo. FPS AP b Params FLOPs (G) FPS AP b AP m</figDesc><table><row><cell cols="3">RetinaNet Params FLOPs (G) ResNet-50 [23] Backbone 38M 239</cell><cell>18.5 36.3</cell><cell>44M</cell><cell cols="2">Mask R-CNN 260 27.1 38.0 34.4</cell></row><row><cell>PVT-S [51]</cell><cell>34M</cell><cell>273</cell><cell>13.0 40.4</cell><cell>44M</cell><cell>292</cell><cell>16.2 40.4 37.8</cell></row><row><cell>Swin-T [32]</cell><cell>38M</cell><cell>251</cell><cell>17.0 41.5</cell><cell>48M</cell><cell>270</cell><cell>21.1 42.2 39.1</cell></row><row><cell>Twins-SVT-S [12]</cell><cell>34M</cell><cell>225</cell><cell>15.5 43.0</cell><cell>44M</cell><cell>244</cell><cell>20.4 43.4 40.3</cell></row><row><cell>LITv1-S [36]</cell><cell>39M</cell><cell>305</cell><cell>3.3 41.6</cell><cell>48M</cell><cell>324</cell><cell>3.2 42.9 39.6</cell></row><row><cell>LITv2-S</cell><cell>38M</cell><cell>242</cell><cell>18.7 44.0</cell><cell>47M</cell><cell>261</cell><cell>18.7 44.9 40.8</cell></row><row><cell>LITv2-S*</cell><cell>38M</cell><cell>230</cell><cell>20.4 43.7</cell><cell>47M</cell><cell>249</cell><cell>21.9 44.7 40.7</cell></row><row><cell>ResNet-101 [23]</cell><cell>57M</cell><cell>315</cell><cell>15.2 38.5</cell><cell>63M</cell><cell>336</cell><cell>20.9 40.4 36.4</cell></row><row><cell>PVT-M [51]</cell><cell>54M</cell><cell>348</cell><cell>10.5 41.9</cell><cell>64M</cell><cell>367</cell><cell>10.8 42.0 39.0</cell></row><row><cell>Swin-S [32]</cell><cell>60M</cell><cell>343</cell><cell>13.3 44.5</cell><cell>69M</cell><cell>362</cell><cell>15.8 44.8 40.9</cell></row><row><cell>Twins-SVT-B [12]</cell><cell>67M</cell><cell>358</cell><cell>10.8 45.3</cell><cell>76M</cell><cell>377</cell><cell>12.7 45.2 41.5</cell></row><row><cell>LITv2-M</cell><cell>59M</cell><cell>348</cell><cell>12.2 46.0</cell><cell>68M</cell><cell>367</cell><cell>12.6 46.8 42.3</cell></row><row><cell>LITv2-M*</cell><cell>59M</cell><cell>312</cell><cell>14.8 45.8</cell><cell>68M</cell><cell>315</cell><cell>16.0 46.5 42.0</cell></row><row><cell>ResNeXt101-64x4d [58]</cell><cell>96M</cell><cell>473</cell><cell>10.3 41.0</cell><cell>102M</cell><cell>493</cell><cell>12.4 42.8 38.4</cell></row><row><cell>PVT-L [51]</cell><cell>71M</cell><cell>439</cell><cell>9.5 42.6</cell><cell>81M</cell><cell>457</cell><cell>8.3 42.9 39.5</cell></row><row><cell>Swin-B [32]</cell><cell>98M</cell><cell>488</cell><cell>11.0 44.7</cell><cell>107M</cell><cell>507</cell><cell>11.3 45.5 41.3</cell></row><row><cell>Twins-SVT-L [12]</cell><cell>111M</cell><cell>504</cell><cell>9.9 45.7</cell><cell>120M</cell><cell>524</cell><cell>10.1 45.9 41.6</cell></row><row><cell>LITv2-B</cell><cell>97M</cell><cell>481</cell><cell>9.5 46.7</cell><cell>106M</cell><cell>500</cell><cell>9.3 47.3 42.6</cell></row><row><cell>LITv2-B*</cell><cell>97M</cell><cell>430</cell><cell>11.8 46.3</cell><cell>106M</cell><cell>449</cell><cell>11.5 46.8 42.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Performance comparisons with other efficient attention mechanisms in ViTs based on LITv2-S. We report the Top-1 accuracy on ImageNet-1K and mIoU on ADE20K.</figDesc><table><row><cell cols="2">Method</cell><cell></cell><cell cols="2">Params</cell><cell cols="2">FLOPs</cell><cell cols="5">ImageNet-1K Throughput Train Mem</cell><cell cols="2">Test Mem</cell><cell>Top-1</cell><cell></cell><cell cols="2">Params</cell><cell cols="2">ADE20K FLOPs</cell><cell>mIoU</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(M)</cell><cell></cell><cell>(G)</cell><cell></cell><cell cols="2">(images/s)</cell><cell></cell><cell>(GB)</cell><cell></cell><cell></cell><cell>(GB)</cell><cell>(%)</cell><cell></cell><cell></cell><cell>(M)</cell><cell>(G)</cell><cell></cell><cell>(%)</cell></row><row><cell cols="2">MSA</cell><cell></cell><cell>28</cell><cell></cell><cell>4.1</cell><cell></cell><cell cols="2">1,293</cell><cell></cell><cell>6.5</cell><cell></cell><cell></cell><cell>1.2</cell><cell>82.3</cell><cell></cell><cell></cell><cell>32</cell><cell>46.5</cell><cell></cell><cell>43.7</cell></row><row><cell cols="2">SRA [51]</cell><cell></cell><cell>32</cell><cell></cell><cell>4.0</cell><cell></cell><cell cols="2">1,425</cell><cell></cell><cell>5.1</cell><cell></cell><cell></cell><cell>1.3</cell><cell>81.7</cell><cell></cell><cell></cell><cell>35</cell><cell>42.4</cell><cell></cell><cell>42.8</cell></row><row><cell cols="3">W-MSA [32]</cell><cell>28</cell><cell></cell><cell>4.0</cell><cell></cell><cell cols="2">1,394</cell><cell></cell><cell>5.3</cell><cell></cell><cell></cell><cell>1.2</cell><cell>81.9</cell><cell></cell><cell></cell><cell>32</cell><cell>42.7</cell><cell></cell><cell>41.9</cell></row><row><cell cols="3">T-MSA [12]</cell><cell>30</cell><cell></cell><cell>4.0</cell><cell></cell><cell cols="2">1,462</cell><cell></cell><cell>5.0</cell><cell></cell><cell></cell><cell>1.3</cell><cell>81.8</cell><cell></cell><cell></cell><cell>33</cell><cell>42.5</cell><cell></cell><cell>44.0</cell></row><row><cell cols="2">HiLo</cell><cell></cell><cell>28</cell><cell></cell><cell>3.7</cell><cell></cell><cell cols="2">1,471</cell><cell></cell><cell>5.1</cell><cell></cell><cell></cell><cell>1.2</cell><cell>82.0</cell><cell></cell><cell></cell><cell>31</cell><cell>42.6</cell><cell></cell><cell>44.3</cell></row><row><cell>FLOPs (G)</cell><cell>5 10</cell><cell>250</cell><cell>300</cell><cell>350</cell><cell>Throughput (imgs/s)</cell><cell>500 1000 1500</cell><cell>250</cell><cell>300</cell><cell>350</cell><cell>Train Memory (GB)</cell><cell>5 10 15</cell><cell>250</cell><cell>300</cell><cell>350</cell><cell>Test Memory (GB)</cell><cell>2 3</cell><cell>250</cell><cell>300</cell><cell>350</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Resolution</cell><cell></cell><cell></cell><cell cols="3">Resolution</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Resolution</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Resolution</cell></row><row><cell></cell><cell></cell><cell></cell><cell>MSA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SRA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">T-MSA</cell><cell></cell><cell></cell><cell></cell><cell cols="2">QuadTree</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Performer</cell><cell></cell><cell></cell><cell></cell><cell cols="2">W-MSA</cell><cell></cell><cell></cell><cell></cell><cell>Focal</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">HiLo (Ours)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Semantic segmentation performance of different backbones on the ADE20K validation set. FLOPs is evaluated based on the image resolution of 512 ? 512.</figDesc><table><row><cell>Backbone</cell><cell>Params (M)</cell><cell>FLOPs (G)</cell><cell>FPS</cell><cell>mIoU (%)</cell></row><row><cell>ResNet-50 [23]</cell><cell>29</cell><cell>45</cell><cell cols="2">45.4 36.7</cell></row><row><cell>PVT-S [51]</cell><cell>28</cell><cell>40</cell><cell cols="2">38.7 39.8</cell></row><row><cell>Swin-Ti [32]</cell><cell>32</cell><cell>46</cell><cell cols="2">39.6 41.5</cell></row><row><cell>Twins-SVT-S [12]</cell><cell>28</cell><cell>37</cell><cell cols="2">34.5 43.2</cell></row><row><cell>LITv1-S [36]</cell><cell>32</cell><cell>46</cell><cell cols="2">18.1 41.7</cell></row><row><cell>LITv2-S</cell><cell>31</cell><cell>41</cell><cell cols="2">42.6 44.3</cell></row><row><cell>ResNet-101 [23]</cell><cell>48</cell><cell>66</cell><cell cols="2">36.7 38.8</cell></row><row><cell>PVT-M [51]</cell><cell>48</cell><cell>55</cell><cell cols="2">29.7 41.6</cell></row><row><cell>Swin-S [32]</cell><cell>53</cell><cell>70</cell><cell cols="2">24.4 45.2</cell></row><row><cell>Twins-SVT-B [12]</cell><cell>60</cell><cell>67</cell><cell cols="2">28.0 45.3</cell></row><row><cell>LITv2-M</cell><cell>52</cell><cell>63</cell><cell cols="2">28.5 45.7</cell></row><row><cell>PVT-L [51]</cell><cell>65</cell><cell>71</cell><cell cols="2">20.5 42.1</cell></row><row><cell>Swin-B [32]</cell><cell>107</cell><cell>107</cell><cell cols="2">25.5 46.0</cell></row><row><cell>Twins-SVT-L [12]</cell><cell>104</cell><cell>102</cell><cell cols="2">25.9 46.7</cell></row><row><cell>LITv2-B</cell><cell>90</cell><cell>93</cell><cell cols="2">27.5 47.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Effect of architecture modifications based on LITv1-S. "ConvFNN" means we add one layer of 3 ? 3 depthwise convolutional layer into each FFN. "RPE" refers to relative positional encoding<ref type="bibr" target="#b31">[32]</ref>.</figDesc><table><row><cell>Name</cell><cell cols="2">ImageNet-1K FLOPs (G) Mem (GB) Top-1 (%)</cell><cell>COCO (RetinaNet) FLOPs (G) FPS AP</cell></row><row><cell>LITv1-S [36]</cell><cell>4.1</cell><cell cols="2">5.8 81.5 305 3.3 41.6</cell></row><row><cell>+ ConvFFN</cell><cell>4.1</cell><cell cols="2">6.5 82.5 306 3.1 45.1</cell></row><row><cell cols="2">+ Remove RPE 4.1</cell><cell cols="2">6.5 82.3 306 13.3 44.7</cell></row><row><cell>+ HiLo</cell><cell>3.7</cell><cell cols="2">5.1 82.0 224 18.7 44.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Speed and performance comparisons between LITv2-S and other recent ViTs on different GPUs. All throughput results are averaged over 30 runs with a total batch size of 64 and image resolution of 224 ? 224 on one GPU card. We also report the Top-1 accuracy on ImageNet-1K.</figDesc><table><row><cell>Model</cell><cell cols="7">Params (M) FLOPs (G) A100 V100 RTX 6000 RTX 3090 Top-1 (%)</cell></row><row><cell>ResNet-50 [53]</cell><cell>26</cell><cell>4.1</cell><cell cols="2">1,424 1,123</cell><cell>877</cell><cell>1,279</cell><cell>80.4</cell></row><row><cell>PVT-S [51]</cell><cell>25</cell><cell>3.8</cell><cell>1,460</cell><cell>798</cell><cell>548</cell><cell>1,007</cell><cell>79.8</cell></row><row><cell>Twins-PCPVT-S [12]</cell><cell>24</cell><cell>3.8</cell><cell>1,455</cell><cell>792</cell><cell>529</cell><cell>998</cell><cell>81.2</cell></row><row><cell>Swin-Ti [32]</cell><cell>28</cell><cell>4.5</cell><cell cols="2">1,564 1,039</cell><cell>710</cell><cell>961</cell><cell>81.3</cell></row><row><cell>TNT-S [21]</cell><cell>24</cell><cell>5.2</cell><cell>802</cell><cell>431</cell><cell>298</cell><cell>534</cell><cell>81.3</cell></row><row><cell>CvT-13 [54]</cell><cell>20</cell><cell>4.5</cell><cell>1,595</cell><cell>716</cell><cell>379</cell><cell>947</cell><cell>81.6</cell></row><row><cell>CoAtNet-0 [15]</cell><cell>25</cell><cell>4.2</cell><cell>1,538</cell><cell>962</cell><cell>643</cell><cell>1,151</cell><cell>81.6</cell></row><row><cell>CaiT-XS24 [46]</cell><cell>27</cell><cell>5.4</cell><cell>991</cell><cell>484</cell><cell>299</cell><cell>623</cell><cell>81.8</cell></row><row><cell>PVTv2-B2 [52]</cell><cell>25</cell><cell>4.0</cell><cell>1,175</cell><cell>670</cell><cell>451</cell><cell>854</cell><cell>82.0</cell></row><row><cell>XCiT-S12 [1]</cell><cell>26</cell><cell>4.8</cell><cell>1,727</cell><cell>761</cell><cell>504</cell><cell>1,068</cell><cell>82.0</cell></row><row><cell>ConvNext-Ti [33]</cell><cell>28</cell><cell>4.5</cell><cell>1,654</cell><cell>762</cell><cell>571</cell><cell>1,079</cell><cell>82.1</cell></row><row><cell>Focal-Tiny [60]</cell><cell>29</cell><cell>4.9</cell><cell>471</cell><cell>372</cell><cell>261</cell><cell>384</cell><cell>82.2</cell></row><row><cell>LITv2-S</cell><cell>28</cell><cell>3.7</cell><cell cols="2">1,874 1,304</cell><cell>928</cell><cell>1,471</cell><cell>82.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Effect of window size based on LITv2-S. We report the Top-1 accuracy on CIFAR-100.</figDesc><table><row><cell>Window Size</cell><cell>Params (M)</cell><cell>FLOPs (G)</cell><cell>Throughput (imgs/s)</cell><cell>Train Memory (GB)</cell><cell>Test Memory (GB)</cell><cell>Top-1 (%)</cell></row><row><cell>2</cell><cell>27</cell><cell>3.7</cell><cell>1,476</cell><cell>5.1</cell><cell>1.2</cell><cell>85.1</cell></row><row><cell>3</cell><cell>27</cell><cell>3.7</cell><cell>1,437</cell><cell>5.1</cell><cell>1.2</cell><cell>84.6</cell></row><row><cell>4</cell><cell>27</cell><cell>3.8</cell><cell>1,417</cell><cell>5.1</cell><cell>1.2</cell><cell>84.4</cell></row><row><cell>5</cell><cell>27</cell><cell>3.7</cell><cell>1,434</cell><cell>5.1</cell><cell>1.2</cell><cell>84.6</cell></row><row><cell>6</cell><cell>27</cell><cell>3.9</cell><cell>1,413</cell><cell>5.2</cell><cell>1.2</cell><cell>84.8</cell></row><row><cell>7</cell><cell>27</cell><cell>3.6</cell><cell>1,442</cell><cell>4.9</cell><cell>1.2</cell><cell>84.8</cell></row><row><cell cols="7">computation and weighted-sum of values. 3) The final linear projection of the weighted-sum values.</cell></row><row><cell cols="4">For Hi-Fi, the computational cost for each part is</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Effect of directly computing queries from the pooled feature maps. We report the Top-1 accuracy on ImageNet-1K.</figDesc><table><row><cell>Model</cell><cell cols="4">Params (M) FLOPs (G) Throughput (imgs/s) Top-1 (%)</cell></row><row><cell>LITv2-S</cell><cell>28</cell><cell>3.7</cell><cell>1,471</cell><cell>82.0</cell></row><row><cell>LITv2-S w/ pooled queries</cell><cell>28</cell><cell>3.5</cell><cell>1,084</cell><cell>81.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Throughput benchmark for different attention mechanisms based on a single attention layer. We report the throughput on both CPU (Intel? Core? i9-10900X CPU @ 3.70GHz) and GPU (NVIDIA GeForce RTX 3090).</figDesc><table><row><cell>Name</cell><cell cols="4">Params (M) FLOPs (M) CPU (imgs/s) GPU (imgs/s)</cell></row><row><cell>MSA [18]</cell><cell>2.36</cell><cell>521.4</cell><cell>505</cell><cell>4,403</cell></row><row><cell>Cross Window [17]</cell><cell>2.37</cell><cell>493.3</cell><cell>325</cell><cell>4,334</cell></row><row><cell>DAT [55]</cell><cell>2.38</cell><cell>528.7</cell><cell>223</cell><cell>3,074</cell></row><row><cell>Performer [11]</cell><cell>2.36</cell><cell>617.2</cell><cell>181</cell><cell>3,180</cell></row><row><cell>Linformer [50]</cell><cell>2.46</cell><cell>616.6</cell><cell>518</cell><cell>4,578</cell></row><row><cell>SRA [51]</cell><cell>4.72</cell><cell>419.6</cell><cell>710</cell><cell>4,810</cell></row><row><cell>Local Window [32]</cell><cell>2.36</cell><cell>477.2</cell><cell>631</cell><cell>4,537</cell></row><row><cell>Shifted Window [32]</cell><cell>2.36</cell><cell>477.2</cell><cell>374</cell><cell>4,351</cell></row><row><cell>Focal [60]</cell><cell>2.44</cell><cell>526.9</cell><cell>146</cell><cell>2,842</cell></row><row><cell>XCA [1]</cell><cell>2.36</cell><cell>481.7</cell><cell>583</cell><cell>4,659</cell></row><row><cell>QuadTree [44]</cell><cell>5.33</cell><cell>613.3</cell><cell>72</cell><cell>3,978</cell></row><row><cell>VAN [20]</cell><cell>1.83</cell><cell>358.0</cell><cell>59</cell><cell>4,213</cell></row><row><cell>HorNet [41]</cell><cell>2.23</cell><cell>436.5</cell><cell>132</cell><cell>3,996</cell></row><row><cell>HiLo</cell><cell>2.20</cell><cell>298.3</cell><cell>1,029</cell><cell>5,104</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Xcit: Cross-covariance image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Layer normalization. ArXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Glit: Neural architecture search for global and local image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">MMDetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Autoformer: Searching transformers for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12250" to="12260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mixformer: Mixing features across windows and dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="5239" to="5249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Compressing convolutional neural networks in the frequency domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1475" to="1484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mobile-former: Bridging mobilenet and transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="5260" to="5269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Drop an octave: Reducing spatial redundancy in convolutional neural networks with octave convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3434" to="3443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rethinking attention with performers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sarl?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Q</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Twins: Revisiting the design of spatial attention in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9355" to="9366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The fast fourier transform and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Cooley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A W</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Welch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Education</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="34" />
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On the relationship between self-attention and convolutional layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cordonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Coatnet: Marrying convolution and attention for all data sizes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3965" to="3977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An adaptive gaussian filter for noise reduction and edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cahill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference Record Nuclear Science Symposium and Medical Imaging Conference</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1615" to="1619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cswin transformer: A general vision transformer backbone with cross-shaped windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="12114" to="12124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale. ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Frequency separation for real-world super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritsche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-10" />
			<publisher>ICCVW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.09741</idno>
		<title level="m">Visual attention network</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Transformer in transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15908" to="15919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rda: Robust domain adaptation via fourier adversarial attacking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8988" to="8999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Shuffle transformer: Rethinking spatial shuffle for vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03650</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">How much position information do convolutional neural networks encode</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D B</forename><surname>Bruce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Transformers are rnns: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5156" to="5165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6399" to="6408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2999" to="3007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<editor>D. J. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9992" to="10002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A convnet for the 2020s</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="11966" to="11976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mobilevit: Light-weight, general-purpose, and mobile-friendly vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Less is more: Pay less attention in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="2035" to="2043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scalable visual transformers with hierarchical pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="377" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">How do vision transformers work</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Random feature attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Compressive transformers for long-range sequence modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Hornet: Efficient high-order spatial interactions with recursive gated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-L</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Global filter networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="980" to="993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="211" to="252" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Quadtree attention for vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Going deeper with image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="32" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Maxim: Multi-axis mlp for image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Talebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="5759" to="5770" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Low-pass filters for signal averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Voigtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Winefordner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Review of Scientific Instruments</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="957" to="966" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="548" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Pvtv2: Improved baselines with pyramid vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Resnet strikes back: An improved training procedure in timm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<idno>abs/2110.00476</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Cvt: Introducing convolutions to vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="22" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Vision transformer with deformable attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="4794" to="4803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Invertible image rescaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="126" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Early convolutions help transformers see better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mintun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="30392" to="30400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning in the frequency domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Focal self-attention for local-global interactions in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="30008" to="30022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Incorporating convolution designs into visual transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="559" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">E</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="538" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Multi-scale vision longformer: A new vision transformer for high-resolution image encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2978" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Vsa: Learning varied-size window attention in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<title level="m">Semantic understanding of scenes through the ADE20K dataset. IJCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="302" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Guided frequency separation network for real-world superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1722" to="1731" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
