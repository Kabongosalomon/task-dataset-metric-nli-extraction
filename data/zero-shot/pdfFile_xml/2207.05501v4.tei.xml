<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Next-ViT: Next Generation Vision Transformer for Efficient Deployment in Realistic Industrial Scenarios</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Li</surname></persName>
							<email>lijiashi@bytedance.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Xin Pan ByteDance Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xin Pan ByteDance Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xin Pan ByteDance Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huixia</forename><surname>Li</surname></persName>
							<email>lihuixia@bytedance.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Xin Pan ByteDance Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wang</surname></persName>
							<email>wangxing.613@bytedance.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Xin Pan ByteDance Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuefeng</forename><surname>Xiao</surname></persName>
							<email>xiaoxuefeng.ailab@bytedance.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Xin Pan ByteDance Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
							<email>ruiwang.rw@bytedance.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Xin Pan ByteDance Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zheng</surname></persName>
							<email>zhengmin.666@bytedance.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Xin Pan ByteDance Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Next-ViT: Next Generation Vision Transformer for Efficient Deployment in Realistic Industrial Scenarios</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>* Equal contribution. ? Corresponding author.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Due to the complex attention mechanisms and model design, most existing vision Transformers (ViTs) can not perform as efficiently as convolutional neural networks (CNNs) in realistic industrial deployment scenarios, e.g. TensorRT and CoreML. This poses a distinct challenge: Can a visual neural network be designed to infer as fast as CNNs and perform as powerful as ViTs? In these work, we propose a next generation vision Transformer for efficient deployment in realistic industrial scenarios, namely Next-ViT, which dominates both CNNs and ViTs from the perspective of latency/accuracy trade-off. The Next Convolution Block (NCB) and Next Transformer Block (NTB) are respectively developed to capture local and global information with deployment-friendly mechanisms. Then, Next Hybrid Strategy (NHS) is designed to stack NCB and NTB in an efficient hybrid paradigm, which boosts performance in various downstream tasks. Extensive experiments show that Next-ViT significantly outperforms existing CNNs, ViTs and CNN-Transformer hybrid architectures with respect to the latency/accuracy trade-off across various vision tasks. On TensorRT, Next-ViT surpasses ResNet by 5.5 mAP (from 40.4 to 45.9) on COCO detection and 7.7% mIoU (from 38.8% to 46.5%) on ADE20K segmentation under similar latency. Meanwhile, it achieves comparable performance with CSWin, while the inference speed is accelerated by 3.6?. On CoreML, Next-ViT surpasses EfficientFormer by 4.6 mAP (from 42.6 to 47.2) on COCO detection and 3.5% mIoU (from 45.1% to 48.6%) on ADE20K segmentation under similar latency. Our code and models are made public at:https://github.com/bytedance/Next-ViT</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently, vision Transformers (ViTs) have received increasing attention in industry and academia, and demonstrated much success in various computer vision tasks, such as image classification, object detection, semantic segmentation and etc. However, CNNs still dominate vision tasks from a real-world deployment perspective, because ViTs are usually much slower than classical CNNs, e.g. ResNets. There are some factors that limit the inference speed of the Transformer model, including quadratic complexity with respect to token length of the Multi-Head Self Attention (MHSA) mechanism, non-foldable LayerNorm and GELU layers, the complex model design causes frequent memory access and copying, etc.</p><p>Many works have struggled to free ViTs from high latency dilemma. For example, Swin Transformer <ref type="bibr" target="#b20">[21]</ref> and PVT <ref type="bibr" target="#b35">[36]</ref> try to design more efficient spatial attention mechanisms to alleviate the quadratic-increasing computation complexity of MHSA. The others <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25]</ref> consider combining efficient convolution blocks and powerful Transformer blocks to design CNN-Transformer hybrid architecture to obtain a better trade-off between accuracy and latency. Coincidentally, almost all existing hybrid architectures <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25]</ref> adopt convolution blocks in the shallow stages and just stack Transformer block in the last few stages. However, we observe that such a hybrid strategy is effortless to lead to performance saturation on downstream tasks (e.g. segmentation and detection). Furthermore, we found that both convolution blocks and Transformer blocks in existing works can not possess characteristics of efficiency and performance at the same time. Although the accuracy-latency trade-off has been improved when compared with Vision Transformer, the overall performance of the existing hybrid architecture is still far away from satisfactory.</p><p>To address the above issues, this work develops three important components to design efficient vision Transformer networks. Firstly, we introduce the Next Convolution Block (NCB), which is skilled at capturing short-term dependency information in visual data with a novel deployment-friendly Multi-Head Convolutional Attention (MHCA). Secondly, we build the Next Transformer Block (NTB), NTB is not only an expert in capturing long-term dependency information but also works as a lightweight and high-and-lowfrequency signal mixer to enhance modeling capability. Finally, we design Next Hybrid Strategy (NHS) to stack NCB and NTB in a novel hybrid paradigm in each stage, which greatly reduce the proportion of the Transformer block and retaining the high precision of the vision Transformer network in various downstream tasks.</p><p>Based on the above-proposed approaches, we propose next generation vision Transformer for realistic industrial deployment scenarios (abbreviated as Next-ViT). In this paper, to present a fair comparison, we provide a view that treats the latency on the specific hardware as direct efficiency feedback. TensorRT and CoreML represent generic and easy-to-deploy solutions for server-side and mobileside devices, respectively, that help provide convincing hardware-oriented performance guidance. With this direct and accurate guidance, we redraw the accuracy and latency trade-off diagram of several existing competitive models in <ref type="figure" target="#fig_0">Figure 1</ref>. As depicted in <ref type="figure" target="#fig_0">Figure 1</ref>(a)(d), Next-ViT achieves best latency/accuracy trade-off on ImageNet-1K classification task. More importantly, Next-ViT shows a more significant latency/accuracy trade-off superiority on downstream tasks. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>(b)(c), on TensorRT, Next-ViT outperforms ResNet by 5.5 mAP (from 40.4 to 45.9) on COCO detection and 7.7% mIoU (from 38.8% to 46.5%) on ADE20K segmentation under similar latency. Next-ViT achieves comparable performance with CSWin, while the inference speed is increased by 3.6?. As depicted in <ref type="figure" target="#fig_0">Figure  1</ref>    <ref type="figure">Figure 2</ref>. The left column is the overall hierarchical architecture of Next-ViT. The medium column are the Next Convolution Block (NCB) and the Next Transformer Block (NTB). The right column are the detailed visualization of multi-head convolutional attention (MHCA), efficient multi-head self-attention (E-MHSA) and the optimized MLP modules.</p><formula xml:id="formula_0">Conv 1 ? 1 !"# !"# !"# !"# * (1 ? ) !"# * (1 ? ) !"# * !"# * NTB Grouped Conv 3 ? 3 BN Conv 1 ? 1 ReLU MHCA Q V K Avg-pool Multi-Head Self-Attention ? $ ? BN E-MHSA Conv 1 ? 1</formula><p>to build advanced CNN-Transformer hybrid architecture.</p><p>? We design an innovative CNN-Transformer hybrid strategy from a new insight that boosts performance with high efficiency.</p><p>? We present Next-ViT, a family of powerful vision Transformer architecture. Extensive experiments demonstrate the advantage of Next-ViT. It achieves SOTA latency/accuracy trade-off on image classification, object detection and semantic segmentation on TensorRT and CoreML.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Convolutional Networks. Over the past decade, Convolutional Neural Networks (CNNs) have dominated vision architectures in a variety of computer vision tasks, including image classification, object detection, and semantic segmentation. ResNet <ref type="bibr" target="#b9">[10]</ref> uses residual connec-tions to eliminate network degradation, ensuring that the network builds deeper and can capture high-level abstractions. DenseNet <ref type="bibr" target="#b12">[13]</ref> alternately enhances feature reuse and concatenates feature maps through dense connections. MobileNets <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">30]</ref> introduce depthwise convolution and point-wise convolution to build models with small memory and low latency. ShuffleNet <ref type="bibr" target="#b45">[46]</ref> adopts group pointwise convolution and channel shuffle to reduce the computational cost further. ShuffleNetv2 <ref type="bibr" target="#b23">[24]</ref> propose that network architecture design should consider the direct metric such as speed, instead of the indirect metric like FLOPs. ConvNeXt <ref type="bibr" target="#b21">[22]</ref> reviews the design of the vision Transformers and proposes a pure CNN model that can compete favorably with SOTA hierarchical vision Transformers across multiple computer vision benchmarks, while retaining the simplicity and efficiency of standard CNNs.</p><p>Vision Transformers. Transformer is first proposed in the field of natural language processing (NLP). ViT <ref type="bibr" target="#b6">[7]</ref> splits the image into patches and treats these patches as words to perform self-attention, which shows that Trans- <ref type="bibr" target="#b21">[22]</ref> MHSA MLP (c) Transformer <ref type="bibr" target="#b6">[7]</ref> Pooling MLP (d) PoolFormer <ref type="bibr" target="#b42">[43]</ref> MHRA MLP DPE (e) UniFormer <ref type="bibr" target="#b16">[17]</ref> Group Conv 3?3</p><formula xml:id="formula_1">Conv 1?1 Conv 3?3 Conv 1?1 (a) BottleNeck [10] DW 7?7 Conv 1?1 Conv 1?1 (b) ConvNeXt</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MLP</head><p>Conv 1?1 former also achieves impressive performance on various vision tasks. DeiT <ref type="bibr" target="#b32">[33]</ref> introduces a teacher-student strategy specific to Transformers. T2T-ViT <ref type="bibr" target="#b43">[44]</ref> introduces a novel tokens-to-token (T2T) process to progressively tokenize images to tokens and structurally aggregate tokens. Swin Transformer <ref type="bibr" target="#b20">[21]</ref> proposes a general-purpose Transformer backbone, which constructs hierarchical feature maps and has linear computational complexity to image size. PiT <ref type="bibr" target="#b10">[11]</ref> incorporates a pooling layer into ViT, and shows that these advantages can be well harmonized to ViT through extensive experiments. Today, researchers pay more attention to efficiency, including efficient self-attention, training strategy, pyramid design, and etc.</p><p>Hybrid Models. Recent works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b47">48]</ref> have shown that combining convolution and Transformer as a hybrid architecture helps absorb the strengths of both architectures. BoTNet <ref type="bibr" target="#b30">[31]</ref> replaces the spatial convolutions with global self-attention in the final three bottleneck blocks of ResNet. CvT <ref type="bibr" target="#b36">[37]</ref> introduces the depthwise and pointwise convolution in front of self-attention. CMT <ref type="bibr" target="#b7">[8]</ref> proposes a new Transformer based hybrid network by taking advantage of Transformers to capture long-range dependencies and CNN to model local features. In Mobile-ViT <ref type="bibr" target="#b24">[25]</ref>, introduces a light-weight and general-purpose vision Transformer for mobile devices. Mobile-Former <ref type="bibr" target="#b1">[2]</ref> combines with the proposed lightweight cross attention to model the bridge, which is not only computationally efficient, but also has more representation power. Efficient-Former <ref type="bibr" target="#b18">[19]</ref> complies with a dimension consistent design that smoothly leverages hardware-friendly 4D MetaBlocks and powerful 3D MHSA blocks. In this paper, we design a family of Next-ViT models that adapt more to the realistic industrial scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>In this section, we first demonstrate the overview of the proposed Next-ViT. Then, we discuss some core designs within Next-ViT, including the Next Convolution Block (NCB), Next Transformer Block (NTB) and the Next Hybrid Strategy (NHS). Moreover, we provide the architecture specifications with different model sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>We present the Next-ViT as illustrated in <ref type="figure">Figure 2</ref>. By convention, Next-ViT follows the hierarchical pyramid architecture equipped with a patch embedding layer and a series of convolution or Transformer blocks in each stage. The spatial resolution will be progressively reduced by 32? while the channel dimension will be expanded across different stages. In this chapter, we first dive deeper into designing the core blocks for information interaction and respectively develop powerful NCB and NTB to model shortterm and long-term dependencies in visual data. The fusion of local and global information is also performed in NTB which further boosts modeling capability. Finally, we systematically study the manners of integrating convolution and Transformer blocks. To overcome the inherent defects of existing methods, we introduce Next Hybrid Strategy which stacks innovative NCB and NTB to build our advanced CNN-Transformer hybrid architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Next Convolution Block (NCB)</head><p>To present the superiority of the proposed NCB, we first revisit some classical structural designs of convolution and Transformer blocks as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. BottleNeck block proposed by ResNet <ref type="bibr" target="#b9">[10]</ref> has dominance in visual neural networks for a long time by its inherent inductive biases and deployment-friendly characteristics in most hardware platforms. Unfortunately, the effectiveness of the Bot-tleNeck block is inadequate compared to the Transformer block. ConvNeXt block <ref type="bibr" target="#b21">[22]</ref> modernizes the BottleNeck block by imitating designs of Transformer block. While ConvNeXt block partly improves network performance, its inference speed on TensorRT/CoreML is severely limited by inefficient components, such as 7 ? 7 depthwise convolution, LayerNorm, and GELU. Transformer block has achieved excellent results in the various visual task and its intrinsic superiority is jointly endowed by the paradigm of MetaFormer <ref type="bibr" target="#b42">[43]</ref> and the attention-based token mixer module <ref type="bibr" target="#b20">[21]</ref>  <ref type="bibr" target="#b5">[6]</ref>. However, the inference speed of Transformer block is much slower than BottleNeck block due to its complex attention mechanisms, which is unbearable in most realistic industrial scenarios.</p><p>To overcome the defeats of the above blocks, we introduce a Next Convolution Block (NCB), which maintains the deployment advantage of BottleNeck block while obtaining prominent performance as Transformer block. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>(f), NCB follows the general architecture of MetaFormer <ref type="bibr" target="#b42">[43]</ref>, which is verified to be essential to the Transformer block. In the meantime, an efficient attention-based token mixer is equally important. We design a novel Multi-Head Convolutional Attention (MHCA) as an efficient token mixer with deployment-friendly convolution operation. Finally, we build NCB with MHCA and MLP layer in the paradigm of MetaFormer <ref type="bibr" target="#b42">[43]</ref>. Our proposed NCB can be formulated as follows:</p><formula xml:id="formula_2">z l = MHCA(z l?1 ) + z l?1 z l = MLP(z l ) +z l<label>(1)</label></formula><p>where z l?1 denotes the input from the l ? 1 block,z l and z l are the outputs of MHCA and the l NCB. We will introduce MHCA in detail in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Multi-Head Convolutional Attention (MHCA)</head><p>To free the existing attention-based token mixer from the high latency dilemma, we design a novel attention mechanism with efficient convolution operation, i.e. Convolutional Attention (CA), for fast inference speed. In the meantime, inspired by the effective multi-head design in MHSA <ref type="bibr" target="#b33">[34]</ref>, we build our convolutional attention with multi-head paradigm which jointly attend to information from different representation subspaces at a different position for effective local representation learning. The definition of proposed Multi-Head Convolutional Attention (MHCA) can be summarized as follows:</p><formula xml:id="formula_3">MHCA(z) = Concat(CA 1 (z 1 ), CA 2 (z 2 ), ..., CA h (z h ))W P<label>(2)</label></formula><p>Here, MHCA captures information from h parallel representation subspaces. z = [z 1 , z 2 , ..., z h ] indicates to divide the input feature z into multi-head form in channel dimension. To promote the information interaction across the multiple heads, we also equip MHCA with a projection layer (W P ). CA is single-head convolutional attention which can be defined as:  where T m and T n are adjacent tokens in input feature z. O is an inner product operation with trainable parameter W and input tokens T {m,n} . CA is capable of learning affinity between different tokens in the local receptive field through iteratively optimizing trainable parameter W . Concretely, the implementation of MHCA is carried out with a group convolution (multi-head convolution) and a point-wise convolution, as shown in <ref type="figure" target="#fig_2">Figure 3</ref>(f). We uniformly set head dim to 32 in all MHCA for fast inference speed with various date-type on TensorRT. Besides, we adopt efficient Batch-Norm (BN) and ReLU activation function in NCB rather than LayerNorm (LN) and GELU in traditional Transformer blocks, which further accelerates inference speed. Experimental results in the ablation study show the superiority of NCB compared with existing blocks, e.g BottleNeck block, ConvNext block, LSA block and etc.</p><formula xml:id="formula_4">CA(z) = O(W, (T m , T n )) where T {m,n} ? z<label>(3)</label></formula><formula xml:id="formula_5">CB ? 1 TB ?0 ? 1 CNN CB ? 1 CB ? 2 CB ? 3 CB ? 4 CB ? 1 CB ? 2 CB ? 3 TB ? 4 CB ? 1 CB ? 2 TB ? 3 TB ? 4 CB ? 1 TB ?0 CB ? 2 TB ?1 CB ? 3 TB ?1 CB ? 4 TB ?1 CB ? 2 TB ?1 ? 2 CB ? 3 TB ?1 ? 3 CB ? 4 TB ?1 ? 4 (a) (b) (c) (d) (e)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Next Transformer Block (NTB)</head><p>Although the local representation has been effectively learned via NCB, the capture of global information is urgent to be addressed. Transformer block has a strong ability to capture low-frequency signals which provide global information (e.g global shapes and structures). Nevertheless, relevant studies <ref type="bibr" target="#b26">[27]</ref> have observed that Transformer blocks may deteriorate high-frequency information, such as local textures information, to a certain extent. Signals in different frequency segments are indispensable in the human visual system [1] <ref type="bibr" target="#b14">[15]</ref> and will be fused in some specific way to extract more essential and distinct features.</p><p>Motivated by these observations, we develop the Next Transformer Block (NTB) to capture multi-frequency signals in the lightweight mechanism. Furthermore, NTB works as an effective multi-frequency signals mixer to further enhance overall modeling capability. As shown in <ref type="figure">Figure 2</ref>, NTB firstly captures low-frequency signals with an Efficient Multi-Head Self Attention(E-MHSA) which can be depicted as:</p><formula xml:id="formula_6">E-MHSA(z) = Concat(SA 1 (z 1 ), SA 2 (z 2 ), ..., SA h (z h ))W P<label>(4)</label></formula><p>where z = [z 1 , z 2 , ..., z h ] denotes to divide the input feature z into multi-head form in channel dimension. SA is a spatial reduction self-attention operator which is inspired by Linear SRA <ref type="bibr" target="#b34">[35]</ref> and performing as:</p><formula xml:id="formula_7">SA(X) = Attention(X ? W Q , P s (X ? W K ), P s (X ? W V ))<label>(5)</label></formula><p>where Attention represents a standard attention calculating as Attention(Q, K, V ) = softmax( QK T d k )V , in which d k denotes the scaling factor. W Q , W K , W V are linear layers for context encoding. P s is an avg-pool operation with stride s for downsampling the spatial dimension before the attention operation to reduce computational cost. Specifically, We observe the time consumption of the E-MHSA module is also greatly affected by its number of channels. NTB thus performs a channel dimension reduction before the E-MHSA module with point-wise convolutions to further accelerate inference. A shrinking ratio r is introduced for channel reduction. We also utilize Batch Normalization in the E-MHSA module for extremely efficient deployment.</p><p>Furthermore, NTB is equipped with an MHCA module that cooperates with the E-MHSA module to capture multi-frequency signals. After that, output features from E-MHSA and MHCA are concatenated to mix high-lowfrequency information. Finally, an MLP layer is borrowed at the end to extract more essential and distinct features. Briefly, the implementation of NTB can be formulated as follows:? l = Proj(z l?1 )</p><formula xml:id="formula_8">z l = E-MHSA(? l ) +? l z l = Proj(z l ) z l = MHCA(z l ) +z l z l = Concat(z l ,? l ) z l = MLP(z l ) + z l<label>(6)</label></formula><p>wherez l ,? l and z l denote the output of E-MHSA, MHCA and NTB, respectively. Proj denotes the point-wise convolution layer for channel projection. Also, NTB uniformly adopts BN and ReLU as the efficient norm and activation layers instead of LN and GELU. Compared with traditional Transformer block, NTB is capable of capturing and mixing multi-frequency information in the lightweight mechanism which greatly boosts model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Next Hybrid Strategy (NHS)</head><p>Some recent works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b37">38]</ref> have paid great efforts to combine CNN and Transformer for efficient deployment. As shown in <ref type="figure" target="#fig_3">Figure 4</ref>(b)(c), almost all of them monotonously adopt convolution blocks in the shallow stages and just stack Transformer blocks in the last one or two stages which presents effective results in the classification task. Unfortunately, we observe that these traditional hybrid strategies are effortless to reach performance saturation on downstream tasks (e.g. segmentation and detection). The reason is, the classification task just uses outputs from the last stage for prediction while downstream tasks (e.g. segmentation and detection) usually rely on features from each stage to gain better results. The traditional hybrid strategies, however, just stack Transformer blocks in the last few stages. The shallow stages thus fail to capture global information, e.g. global shapes and structures of object, which is vital to segmentation and detection tasks.</p><p>To overcome the defeats of existing hybrid strategies, we propose a Next Hybrid Strategy (NHS) from a new insight, which creatively stacks convolution block (NCB) and Transformer block (NTB) with (N + 1) * L hybrid paradigm. NHS significantly promotes model performance in downstream tasks under controlling the proportion of Transformer block for efficient deployment. Firstly, in order to endow the shallow stages with the capability of capturing global information, we present a novel hybrid strategy in (NCB ? N + NTB ? 1) pattern, which sequen-tially stack N NCB and one NTB in each stage as shown in <ref type="figure" target="#fig_3">Figure 4(d)</ref>. Specifically, the Transformer block(NTB) is placed at the end of each stage, which enables the model to learn global representation in the shallow layers. We conduct a series of experiments to verify the superiority of the proposed hybrid strategy. The performances of difference hybrid strategies are shown in <ref type="table" target="#tab_3">Table 1</ref>. C denotes uniformly stacking convolution block(NCB) in one stage and T denotes consistently building one stage with Transformer block(NTB). Specially, H N indicates stacking NCB and NTB with (NCB?N +NTB?1) pattern in the corresponding stage. All models in <ref type="table" target="#tab_3">Table 1</ref> are equipped with four stages. For example, C C C C represents consistently using convolution block in all of the four stages. For fair comparison, we build all the model under similar TensorRT latency. More implementation details are presented in Section 4. As shown in <ref type="table" target="#tab_3">Table 1</ref>, the proposed hybrid strategy significantly promotes model performance compared with existing methods in the downstream tasks. C H N H N H N achieve the best overall performance. For example, C H N H N H N surpasses C C C T 0.8 mAP in detection and 0.8% mIoU in segmentation. Besides, The results of H N H N H N H N shows that placing Transformer block in the first stage will deteriorate the latency-accuracy trade-off of model.</p><p>We further verify the general effectiveness of C H N H N H N on large model by increasing the number of blocks in the third stage as ResNet <ref type="bibr" target="#b9">[10]</ref>. Experimental results of the first three rows in <ref type="table">Table 2</ref> show that the performance of large model is hard to promote and gradually reaches saturation. Such a phenomenon indicates that expanding model size by enlarging the N of (NCB ? N + NTB ? 1) pattern, i.e. simply adding more convolution block is not the best choice. It also implies that the value of N in (NCB ? N + NTB ? 1) pattern may seriously affect the model performance. We thus begin to explore the impact of the value of N on the model performance through extensive experiments. As shown in <ref type="table">Table 2</ref> (middle), we build models with different configurations of N on the third stage. To build model with similar latency for fair comparison, we stack L groups of (NCB ? N + NTB ? 1) pattern when the value of N is small. Surprisingly, we found that stack NCB and NTB in (NCB ? N + NTB ? 1) ? L pattern achieve better model performance compared to (NCB ? N + NTB ? 1) pattern. It denotes that repeatedly combining low-frequency signal extractors and high-frequency signal extractors in a proper manner((NCB ? N + NTB ? 1)) leads to higher quality representation learning. As shown in <ref type="table">Table 2</ref>, model with N = 4 in the third stage achieving the best trade-off between performance and latency. We further build the larger model by enlarging L of (NCB ? 4 + NTB ? 1) ? L pattern in the third stage. As shown in <ref type="table">Table 2</ref> (bottom), the performance of Base (L = 4) and Large (L = 6) model are significantly promote compared to small model, which verifies the general effectiveness of proposed (NCB ? N + NTB ? 1) ? L pattern. We use N = 4 as the basic configurations in the rest of the paper. We stack NCB and NTB with the above Next Hybrid Strategy to build Next-ViT, which can be formally defined as:</p><formula xml:id="formula_9">Next-ViT(X) = i {[? (? (X) ? N i )] ? L i }<label>(7)</label></formula><p>where i ? (1, 2, 3, 4) denotes the stage index. ? means NCB. ? means identity layer when i = 1, otherwise, NTB. Finally, indicates the operation of stacking the stages sequentially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Next-ViT Architectures</head><p>To provide a fair comparison with existing SOTA networks, we present three typical variants, namely, Next-ViT-S/B/L. The architecture specifications are listed in <ref type="table" target="#tab_4">Table 3</ref>, in which C represents output channel and S denotes stride of each stage. Additionally, the channel shrink ratio r in NTB is uniformly set as 0.75 and the spatial reduction ratio s in E-MHSA is <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b0">1]</ref> in different stages. The expansion ratios of MLP layer are set as 3 for NCB and 2 for NTB, respectively. The head dim in E-MHSA and MHCA is set as 32. For normalization layer and activation functions, both NCB and NTB use BatchNorm and ReLU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">ImageNet-1K Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Implementation</head><p>We carry out the image classification experiment on the ImageNet-1K <ref type="bibr" target="#b28">[29]</ref>, which contains about 1.28M training images and 50K validation images from 1K categories. For a fair comparison, we follow the training settings of the recent vision Transformer <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b41">42]</ref> with minor changes. Concretely, all of the Next-ViT variants are trained for 300 epochs on 8 V100 GPUs with a total batch size of 2048. The resolution of the input image is resized to 224 ? 224. We adopt the AdamW <ref type="bibr" target="#b22">[23]</ref> as the optimizer with weight decay 0.1. The learning rate is gradually decayed based on the cosine strategy with the initialization of 2e-3 and the use of a linear warm-up strategy with 20 epochs for all Next-ViT variants. Besides, we have also employed the increasing stochastic depth augmentation <ref type="bibr" target="#b13">[14]</ref> with the maximum drop-path rate of 0.1, 0.2, 0.2 for Next-ViT-S/B/L. Models with ? are trained on large-scale dataset follow SSLD <ref type="bibr" target="#b3">[4]</ref>. For 384 ? 384 input size, we fine-tune the models for 30 epochs with the weight decay of 1e-8, learning rate of 1e-5, batch size of 1024. With the input size corresponding to the respective method, latency in <ref type="table" target="#tab_5">Table 4</ref> is uniformly measured based on the TensorRT-8.0.3 framework with a T4 GPU (batch size=8) and CoreML framework on an iPhone12 Pro Max with iOS 16.0 (batch size=1). Note that both the iPhone 12 and iPhone 12 Pro Max are equipped with the same A14 processor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Comparison with State-of-the-art Models</head><p>As shown in  <ref type="bibr" target="#b18">[19]</ref>, Next-ViT-L predict with 20% fewer runtime on CoreML and 25% fewer runtime on TensorRT while the performance is improved from 83.3% to 83.6%. Next-ViT-L also obtains a 15% inference latency gain and achieves a better performance than TRT-ViT-D. These results demonstrate that the proposed Next-ViT design is an effective and promising paradigm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">ADE20K Semantic Segmentation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Implementation</head><p>To further verify the capacity of our Next-ViT, we conduct the semantic segmentation experiment on ADE20K <ref type="bibr" target="#b46">[47]</ref>, which contains about 20K training images and 2K validation images from 150 categories. To make fair comparisons, we also follow the training conventions of the previous vision Transformers <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b35">36]</ref> on the Semantic FPN <ref type="bibr" target="#b15">[16]</ref> and UperNet <ref type="bibr" target="#b38">[39]</ref> frameworks. Most of models are pre-trained on the ImageNet-1k and models with ? are pre-trained on  <ref type="table" target="#tab_7">Table 5</ref> and <ref type="table" target="#tab_8">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Comparison with State-of-the-art Models</head><p>In <ref type="table" target="#tab_7">Table 5</ref>, we make a comparison with CNNs, ViTs, and recent hybrid methods as well. Next-ViT-S surpasses ResNet101 <ref type="bibr" target="#b9">[10]</ref> and ResNeXt101-32x4d <ref type="bibr" target="#b39">[40]</ref> by 7.7% and 6.8% mIoU, respectively. Next-ViT-B beats CSwin-T by 0.4% mIoU and the inference speed is accelerated by </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Object Detection and Instance Segmentation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Implementation</head><p>Next, we evaluate Next-ViT on the objection detection and instance segmentation task <ref type="bibr" target="#b19">[20]</ref> based on the Mask R-CNN <ref type="bibr" target="#b8">[9]</ref> frameworks with COCO2017 <ref type="bibr" target="#b19">[20]</ref>. Specifically, all of our models are pre-trained on ImageNet-1K and then finetuned following the settings of the previous works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b35">36]</ref>. As for the 12 epochs (1?) experiment, we use the AdamW optimizer with the weight decay of 0.05. There are 500 iterations for a warm-up during the training, and the learning rate will decline by 10? at epochs 8 and 11. Based on the 36 epochs (3?) experiment with multiscale (MS) training, models are trained with the resized images such that the shorter side ranges from 480 to 800 and the longer side is at most 1333. The learning rate will decline by 10? at epochs 27 and 33. The other settings are the same as 1?. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Comparison with State-of-the-art Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study and Visualization</head><p>To understand our Next-ViT better, we ablate each critical design by evaluating its performance on ImageNet-1K classification and downstream tasks. We also visualize the Fourier spectrum and heat map of output features to show the intrinsic superiority of Next-ViT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Impact of Next Convolution Block</head><p>To verify the effectiveness of the proposed NCB, we replace NCB in Next-ViT with famous blocks, such as Bottleneck in ResNet <ref type="bibr" target="#b9">[10]</ref>, ConvNeXt <ref type="bibr" target="#b21">[22]</ref> block, LSA block in Twins <ref type="bibr" target="#b2">[3]</ref>, and etc. For a fair comparison, we consistently use NTB and NHS to build different models under similar latency on TensorRT.</p><p>As shown in <ref type="table">Table 7</ref>, NCB achieves the best latency/accuracy trade-off on all of the three tasks, which verifies the advantage of the proposed NCB. For example, NCB outperforms the recent ConvNeXt block [10] by 2.9% in classification, 4.5 AP b in detection and 2.8% mIoU in segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Impact of Different Shrink Ratios in NTB</head><p>Furthermore, we explore the effect of shrink ratio r of Next Transformer Block on the overall performance of Next-ViT. As stated in <ref type="table">Table 8</ref>, decreasing the shrinking ratio r, i.e. the number of channels in E-MHSA module, will reduce the model latency. Furthermore, model with r = 0.75 and r = 0.5 achieve better performance over model with pure Transformer (r = 1). This denotes that fusing multi-frequency signals in a proper manner will enhance the model ability of representation learning. Specially, model with r = 0.75 achieve the best latency/accuracy trade-off. It outperforms baseline model (r = 1.0) with 0.4%, 0.5 AP b and 1.0% mIoU on classification, detection and segmentation while is more lightweight. The above results indicate the effectiveness of the proposed NTB block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Impact of Normalization and Activation</head><p>We further study the impact of different normalization layers and activation functions in Next-ViT. As shown in Table 9, both the LN and GELU bring negligible performance improvement but with significantly higher inference latency on TensorRT. On the other hand, BN and ReLU achieve the best latency/accuracy trade-off on overall tasks. Therefore, we uniformly use BN and ReLU in Next-ViT for efficient deployment in realistic industrial scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.4">Visualization</head><p>To verify the superiority of our Next-ViT, we visualize the Fourier spectrum and heat maps of the output features from ResNet, Swin Transformer and Next-ViT in <ref type="figure">Figure 5</ref> (a).</p><p>The spectrum distribution of ResNet denotes that convolution blocks tend to capture high-frequency signals while difficult to focus on low-frequency information. On the other hand, ViT experts in capturing low-frequency signals but ignore high-frequency signals. Finally, Next-ViT is capable of simultaneously capturing high-quality and multifrequency signals, which shows the effectiveness of NTB.</p><p>Furthermore, As shown in <ref type="figure">Figure 5</ref> (b), we can see that Next-ViT can capture richer texture information and more accurate global information (e.g. edge shape) compared with ResNet and Swin, which shows the stronger modeling capability of Next-ViT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we present a family of Next-ViT that stacks efficient Next Convolution Block and Next Transformer Block in a novel strategy to build powerful CNN-Transformer hybrid architecture for efficient deployment on both mobile device and server GPU. Experimental results demonstrate that Next-ViT achieves a state-of-the-art latency/accuracy trade-off across diverse visual tasks, such as image classification, object detection and semantic segmentation. We believe that our work builds a stable bridge between academic research and industrial deployment in terms of visual neural network design. We hope that our work will provide new insights and promote more research in neural network architecture design for realistic industrial deployment.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>(a) ImageNet-1K classification on TensorRT (b) COCO detection on TensorRT (c) ADE20K segmentation on TensorRT (d) ImageNet-1K classification on CoreML (e) COCO detection on CoreML (f) ADE20K segmentation on CoreML Comparison among Next-ViT and efficient Networks, in terms of accuracy-latency trade-off.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Comparison of different Transformer-based and convolution-based blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Comparison of traditional hybrid strategies and NHS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>e)(f), on CoreML, Next-ViT surpasses EfficientFormer by 4.6 mAP (from 42.6 to 47.2) on COCO detection and 3.5% mIoU (from 45.1% to 48.6%) on ADE20K segmentation under similar CoreML latency.</figDesc><table><row><cell>NCB</cell></row><row><cell>MHCA</cell></row><row><cell>MLP</cell></row><row><cell>Conv 1 ? 1</cell></row><row><cell>E-MHSA</cell></row><row><cell>MHCA</cell></row><row><cell>Concat</cell></row><row><cell>MLP</cell></row></table><note>Our main contributions are summarized as follows: ? We develop powerful convolution block and Trans- former block, i.e. NCB and NTB, with deployment- friendly mechanisms. Next-ViT stacks NCB and NTB</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Comparison of different hybrid strategies. Cls denotes ImageNet-1K classification task. Det denotes detection task on COCO dataset with Mask-RCNN 1?. Seg denotes segmentation task with Semantic FPN 80k on ADE20K dataset. TensorRT latency is uniformly measured with the input size of 8 ? 3 ? 224 ? 224.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Latency(ms)</cell><cell>Cls</cell><cell></cell><cell>Det</cell><cell>Seg</cell></row><row><cell></cell><cell></cell><cell>Model</cell><cell></cell><cell cols="2">TensorRT</cell><cell cols="5">Acc(%) AP b mIoU(%)</cell></row><row><cell></cell><cell cols="2">ResNet101</cell><cell></cell><cell></cell><cell>7.8</cell><cell cols="2">80.8</cell><cell>40.4</cell><cell>38.8</cell></row><row><cell></cell><cell cols="2">C C C C</cell><cell></cell><cell></cell><cell>7.8</cell><cell cols="2">82.1</cell><cell>44.1</cell><cell>43.2</cell></row><row><cell></cell><cell cols="2">C C C T</cell><cell></cell><cell></cell><cell>7.8</cell><cell cols="2">82.4</cell><cell>44.7</cell><cell>45.2</cell></row><row><cell></cell><cell cols="2">C C T T</cell><cell></cell><cell></cell><cell>7.8</cell><cell cols="2">81.5</cell><cell>44.0</cell><cell>44.8</cell></row><row><cell></cell><cell cols="2">C T T T</cell><cell></cell><cell></cell><cell>8.0</cell><cell cols="2">80.6</cell><cell>42.6</cell><cell>44.1</cell></row><row><cell></cell><cell cols="2">C C C H N</cell><cell></cell><cell></cell><cell>7.7</cell><cell cols="2">82.1</cell><cell>44.6</cell><cell>44.8</cell></row><row><cell></cell><cell cols="2">C C H N H N</cell><cell></cell><cell></cell><cell>7.7</cell><cell cols="2">82.3</cell><cell>45.2</cell><cell>45.8</cell></row><row><cell></cell><cell cols="2">C H N H N H N</cell><cell></cell><cell></cell><cell>7.7</cell><cell cols="2">82.3</cell><cell>45.5</cell><cell>46.0</cell></row><row><cell></cell><cell cols="3">H N H N H N H N</cell><cell></cell><cell>7.8</cell><cell cols="2">81.7</cell><cell>44.5</cell><cell>45.3</cell></row><row><cell cols="11">Table 2. Comparison of different patterns in NHS and exploration</cell></row><row><cell cols="11">of different hyper-parameters configurations. S1,S2,S3 and S4 de-</cell></row><row><cell cols="10">note Stage1, Stage2, Stage3 and Stage4 respectively.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Configs</cell><cell cols="3">Latency(ms)</cell><cell>Cls</cell><cell>Det</cell><cell>Seg</cell></row><row><cell cols="2">Hybrid Strategy</cell><cell>Model</cell><cell cols="2">S1 S2</cell><cell>S3</cell><cell>S4</cell><cell cols="2">TensorRT</cell><cell cols="2">Acc(%) AP b mIoU(%)</cell></row><row><cell cols="2">NCB ? N NTB ? 1</cell><cell>Small Base Large</cell><cell>3 3 3</cell><cell>4 4 4</cell><cell>(11+1) (22+1) (33+1)</cell><cell>3 3 3</cell><cell></cell><cell>7.7 9.7 11.7</cell><cell>82.3 83.0 83.2</cell><cell>45.5 45.8 46.2</cell><cell>46.0 46.7 47.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell>3</cell><cell cols="3">4 (1+1)?3 3</cell><cell></cell><cell>7.4</cell><cell>82.1</cell><cell>44.9</cell><cell>45.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell>3</cell><cell cols="3">4 (2+1)?3 3</cell><cell></cell><cell>8.0</cell><cell>82.5</cell><cell>45.6</cell><cell>45.8</cell></row><row><cell>NCB ? N NTB ? 1</cell><cell>? L</cell><cell>Small</cell><cell>3 3 3</cell><cell cols="3">4 (3+1)?3 3 4 (4+1)?2 3 4 (5+1)?2 3</cell><cell></cell><cell>8.5 7.7 8.1</cell><cell>82.6 82.5 82.5</cell><cell>46.1 45.9 45.9</cell><cell>46.7 46.5 46.6</cell></row><row><cell></cell><cell></cell><cell>Base</cell><cell>3</cell><cell cols="3">4 (4+1)?4 3</cell><cell></cell><cell>10.5</cell><cell>83.2</cell><cell>47.2</cell><cell>48.6</cell></row><row><cell></cell><cell></cell><cell>Large</cell><cell>3</cell><cell cols="3">4 (4+1)?6 3</cell><cell></cell><cell>13.0</cell><cell>83.6</cell><cell>48.0</cell><cell>49.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Detailed configurations of Next-ViT variants. Conv 3 ? 3, C = 32, S = 1 Conv 3 ? 3, C = 64, S = 1 Conv 3 ? 3, C = 64, S = 2</figDesc><table><row><cell cols="4">Stages Output size</cell><cell>Layers</cell><cell>Next-ViT-S</cell><cell></cell><cell>Next-ViT-B</cell><cell>Next-ViT-L</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Conv 3 ? 3, C = 64, S = 2</cell><cell></cell><cell></cell></row><row><cell>Stem</cell><cell>H 4</cell><cell>?</cell><cell>W 4</cell><cell>Convolution Layers</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Patch Embedding</cell><cell></cell><cell></cell><cell>Conv 1 ? 1, C = 96</cell><cell></cell><cell></cell></row><row><cell>Stage 1</cell><cell>H 4</cell><cell>?</cell><cell>W 4</cell><cell>Next-ViT Block</cell><cell></cell><cell></cell><cell>NCB ? 3, 96 ? 1</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Avg pool, S = 2</cell><cell></cell><cell></cell></row><row><cell>Stage 2</cell><cell>H 8</cell><cell>?</cell><cell>W 8</cell><cell>Patch Embedding Next-ViT Block</cell><cell></cell><cell></cell><cell>Conv 1 ? 1, C = 192 NCB ? 3, 192 NTB ? 1, 256 ? 1</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Avg pool, S = 2</cell><cell></cell><cell></cell></row><row><cell>Stage 3</cell><cell>H 16</cell><cell>?</cell><cell>W 16</cell><cell>Patch Embedding Next-ViT Block</cell><cell>NCB ? 4, 384 NTB ? 1, 512</cell><cell>? 2</cell><cell>Conv 1 ? 1, C = 384 NCB ? 4, 384 NTB ? 1, 512 ? 4</cell><cell>NCB ? 4, 384 NTB ? 1, 512</cell><cell>? 6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Avg pool, S = 2</cell><cell></cell><cell></cell></row><row><cell>Stage 4</cell><cell>H 32</cell><cell>?</cell><cell>W 32</cell><cell>Patch Embedding Next-ViT Block</cell><cell></cell><cell></cell><cell>Conv 1 ? 1, C = 768 NCB ? 2, 768 NTB ? 1, 1024 ? 1</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Comparison of different state-of-the-art methods on ImageNet-1K classification. HardSwish is not well supported by CoreML, * denotes we replace it with GELU for fair comparison. ? denotes we use large-scale dataset follow SSLD<ref type="bibr" target="#b3">[4]</ref>.</figDesc><table><row><cell>Method</cell><cell cols="3">Image Param FLOPs</cell><cell cols="2">Latency(ms)</cell><cell>Top-1</cell></row><row><cell></cell><cell>Size</cell><cell>(M)</cell><cell>(G)</cell><cell cols="2">TensorRT CoreML</cell><cell>(%)</cell></row><row><cell>ResNet101 [10]</cell><cell>224</cell><cell>44.6</cell><cell>7.9</cell><cell>7.8</cell><cell>4.0</cell><cell>80.8</cell></row><row><cell>ResNeXt101-32x4d [40]</cell><cell>224</cell><cell>44.2</cell><cell>8.0</cell><cell>8.0</cell><cell>4.0</cell><cell>78.8</cell></row><row><cell>RegNetY-8G [28]</cell><cell>224</cell><cell>44.2</cell><cell>8.0</cell><cell>11.4</cell><cell>4.1</cell><cell>81.7</cell></row><row><cell>ResNeSt50 [45]</cell><cell>224</cell><cell>27.5</cell><cell>5.4</cell><cell>102.7</cell><cell>36.6</cell><cell>81.1</cell></row><row><cell>EfficientNet-B3 [32]</cell><cell>300</cell><cell>12.0</cell><cell>1.8</cell><cell>12.5</cell><cell>5.8</cell><cell>81.5</cell></row><row><cell>MobileViTv2-1.0 [26]</cell><cell>256</cell><cell>4.9</cell><cell>4.9</cell><cell>-</cell><cell>2.9</cell><cell>78.1</cell></row><row><cell>MobileViTv2-2.0 [26]</cell><cell>256</cell><cell>18.5</cell><cell>7.5</cell><cell>-</cell><cell>6.7</cell><cell>81.2</cell></row><row><cell>ConvNeXt-T [22]</cell><cell>224</cell><cell>29.0</cell><cell>4.5</cell><cell>19.0</cell><cell>83.8</cell><cell>82.1</cell></row><row><cell>DeiT-T [33]</cell><cell>224</cell><cell>5.9</cell><cell>1.2</cell><cell>6.7</cell><cell>4.5</cell><cell>72.2</cell></row><row><cell>DeiT-S [33]</cell><cell>224</cell><cell>22.0</cell><cell>4.6</cell><cell>11.4</cell><cell>9.0</cell><cell>79.8</cell></row><row><cell>Swin-T [21]</cell><cell>224</cell><cell>29.0</cell><cell>4.5</cell><cell>-</cell><cell>-</cell><cell>81.3</cell></row><row><cell>PVTv2-B2 [35]</cell><cell>224</cell><cell>25.4</cell><cell>4.0</cell><cell>34.5</cell><cell>96.7</cell><cell>82.0</cell></row><row><cell>Twins-SVT-S [3]</cell><cell>224</cell><cell>24.0</cell><cell>2.9</cell><cell>17.3</cell><cell>-</cell><cell>81.7</cell></row><row><cell>PoolFormer-S24 [43]</cell><cell>224</cell><cell>21.1</cell><cell>3.4</cell><cell>14.4</cell><cell>6.2</cell><cell>80.3</cell></row><row><cell>PoolFormer-S36 [43]</cell><cell>224</cell><cell>31.2</cell><cell>5.0</cell><cell>21.8</cell><cell>6.7</cell><cell>81.4</cell></row><row><cell>CMT-T  *  [8]</cell><cell>160</cell><cell>9.5</cell><cell>0.6</cell><cell>11.8</cell><cell>4.6</cell><cell>79.1</cell></row><row><cell>CMT-XS  *  [8]</cell><cell>192</cell><cell>15.2</cell><cell>1.5</cell><cell>21.9</cell><cell>8.3</cell><cell>81.8</cell></row><row><cell>CoaT Tiny [41]</cell><cell>224</cell><cell>5.5</cell><cell>4.4</cell><cell>52.9</cell><cell>55.4</cell><cell>78.3</cell></row><row><cell>CvT-13 [37]</cell><cell>224</cell><cell>20.1</cell><cell>4.5</cell><cell>18.1</cell><cell>62.6</cell><cell>81.6</cell></row><row><cell>Next-ViT-S</cell><cell>224</cell><cell>31.7</cell><cell>5.8</cell><cell>7.7</cell><cell>3.5</cell><cell>82.5</cell></row><row><cell>Next-ViT-S</cell><cell>384</cell><cell>31.7</cell><cell>17.3</cell><cell>21.6</cell><cell>8.9</cell><cell>83.6</cell></row><row><cell>Next-ViT-S  ?</cell><cell>224</cell><cell>31.7</cell><cell>5.8</cell><cell>7.7</cell><cell>3.5</cell><cell>84.8</cell></row><row><cell>Next-ViT-S  ?</cell><cell>384</cell><cell>31.7</cell><cell>17.3</cell><cell>21.6</cell><cell>8.9</cell><cell>85.8</cell></row><row><cell>ResNet152 [10]</cell><cell>224</cell><cell>60.2</cell><cell>4.0</cell><cell>11.3</cell><cell>5.0</cell><cell>81.7</cell></row><row><cell>ResNeXt101-64x4d [40]</cell><cell>224</cell><cell>83.5</cell><cell>15.6</cell><cell>13.6</cell><cell>6.8</cell><cell>79.6</cell></row><row><cell>ResNeSt101 [45]</cell><cell>224</cell><cell>48.0</cell><cell>10.2</cell><cell>149.8</cell><cell>45.4</cell><cell>83.0</cell></row><row><cell>ConvNeXt-S [22]</cell><cell>224</cell><cell>50.0</cell><cell>8.7</cell><cell>28.1</cell><cell>159.5</cell><cell>83.1</cell></row><row><cell>Swin-S [21]</cell><cell>224</cell><cell>50.0</cell><cell>8.7</cell><cell>-</cell><cell>-</cell><cell>83.0</cell></row><row><cell>PVTv2-B3 [35]</cell><cell>224</cell><cell>45.2</cell><cell>6.9</cell><cell>55.8</cell><cell>107.7</cell><cell>83.2</cell></row><row><cell>Twins-SVT-B [3]</cell><cell>224</cell><cell>56.0</cell><cell>8.6</cell><cell>32.0</cell><cell>-</cell><cell>83.2</cell></row><row><cell>PoolFormer-M36 [43]</cell><cell>224</cell><cell>56.1</cell><cell>8.8</cell><cell>28.2</cell><cell>-</cell><cell>82.1</cell></row><row><cell>CSWin-T [6]</cell><cell>224</cell><cell>23.0</cell><cell>4.3</cell><cell>29.5</cell><cell>-</cell><cell>82.7</cell></row><row><cell>CoaT Mini [41]</cell><cell>224</cell><cell>10.0</cell><cell>6.8</cell><cell>68.0</cell><cell>60.8</cell><cell>81.0</cell></row><row><cell>CvT-21 [37]</cell><cell>224</cell><cell>32.0</cell><cell>7.1</cell><cell>28.0</cell><cell>91.4</cell><cell>82.1</cell></row><row><cell>UniFormer-S [17]</cell><cell>224</cell><cell>22.1</cell><cell>3.6</cell><cell>14.4</cell><cell>4.6</cell><cell>82.9</cell></row><row><cell>TRT-ViT-C [38]</cell><cell>224</cell><cell>67.3</cell><cell>5.9</cell><cell>9.2</cell><cell>5.0</cell><cell>82.7</cell></row><row><cell>Next-ViT-B</cell><cell>224</cell><cell>44.8</cell><cell>8.3</cell><cell>10.5</cell><cell>4.5</cell><cell>83.2</cell></row><row><cell>Next-ViT-B</cell><cell>384</cell><cell>44.8</cell><cell>24.6</cell><cell>29.6</cell><cell>12.4</cell><cell>84.3</cell></row><row><cell>Next-ViT-B  ?</cell><cell>224</cell><cell>44.8</cell><cell>8.3</cell><cell>10.5</cell><cell>4.5</cell><cell>85.1</cell></row><row><cell>Next-ViT-B  ?</cell><cell>384</cell><cell>44.8</cell><cell>24.6</cell><cell>29.6</cell><cell>12.4</cell><cell>86.1</cell></row><row><cell>RegNetY-16G [28]</cell><cell>224</cell><cell>84.0</cell><cell>16.0</cell><cell>18.0</cell><cell>7.4</cell><cell>82.9</cell></row><row><cell>EfficientNet-B5 [32]</cell><cell>456</cell><cell>30.0</cell><cell>9.9</cell><cell>64.4</cell><cell>23.2</cell><cell>83.7</cell></row><row><cell>ConvNeXt-B [22]</cell><cell>224</cell><cell>88.0</cell><cell>15.4</cell><cell>37.3</cell><cell>247.6</cell><cell>83.9</cell></row><row><cell>DeiT-B [33]</cell><cell>224</cell><cell>87.0</cell><cell>17.5</cell><cell>31.0</cell><cell>18.2</cell><cell>81.8</cell></row><row><cell>Swin-B [21]</cell><cell>224</cell><cell>88.0</cell><cell>15.4</cell><cell>-</cell><cell>-</cell><cell>83.3</cell></row><row><cell>PVTv2-B4 [35]</cell><cell>224</cell><cell>62.6</cell><cell>10.1</cell><cell>70.8</cell><cell>139.8</cell><cell>83.6</cell></row><row><cell>Twins-SVT-L [3]</cell><cell>224</cell><cell>99.2</cell><cell>15.1</cell><cell>44.1</cell><cell>-</cell><cell>83.7</cell></row><row><cell>PoolFormer-M48 [43]</cell><cell>224</cell><cell>73.2</cell><cell>11.6</cell><cell>38.2</cell><cell>-</cell><cell>82.5</cell></row><row><cell>CSWin-S [6]</cell><cell>224</cell><cell>35.0</cell><cell>6.9</cell><cell>45.0</cell><cell>-</cell><cell>83.6</cell></row><row><cell>CMT-S  *  [8]</cell><cell>224</cell><cell>25.1</cell><cell>4.0</cell><cell>52.0</cell><cell>14.6</cell><cell>83.5</cell></row><row><cell>CoaT Small [41]</cell><cell>224</cell><cell>22.0</cell><cell>12.6</cell><cell>82.7</cell><cell>122.4</cell><cell>82.1</cell></row><row><cell>UniFormer-B [17]</cell><cell>224</cell><cell>50.2</cell><cell>8.3</cell><cell>31.0</cell><cell>9.0</cell><cell>83.9</cell></row><row><cell>TRT-ViT-D [38]</cell><cell>224</cell><cell>103.0</cell><cell>9.7</cell><cell>15.1</cell><cell>8.3</cell><cell>83.4</cell></row><row><cell>EfficientFormer-L7 [19]</cell><cell>224</cell><cell>82.0</cell><cell>7.9</cell><cell>17.4</cell><cell>6.9</cell><cell>83.3</cell></row><row><cell>Next-ViT-L</cell><cell>224</cell><cell>57.8</cell><cell>10.8</cell><cell>13.0</cell><cell>5.5</cell><cell>83.6</cell></row><row><cell>Next-ViT-L</cell><cell>384</cell><cell>57.8</cell><cell>32.0</cell><cell>36.0</cell><cell>15.2</cell><cell>84.7</cell></row><row><cell>Next-ViT-L  ?</cell><cell>224</cell><cell>57.8</cell><cell>10.8</cell><cell>13.0</cell><cell>5.5</cell><cell>85.4</cell></row><row><cell>Next-ViT-L  ?</cell><cell>384</cell><cell>57.8</cell><cell>32.0</cell><cell>36.0</cell><cell>15.2</cell><cell>86.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>, compared to the latest state-of-</cell></row><row><cell>the-art methods (e.g. CNNs, ViTs and hybrid networks),</cell></row><row><cell>we achieve the best trade-off between accuracy and la-</cell></row><row><cell>tency. Specifically, compared with the famous CNNs such</cell></row><row><cell>as ResNet101 [10], Next-ViT-S improves the accuracy by</cell></row><row><cell>1.7% with a similar latency on TensorRT and faster speed</cell></row><row><cell>on CoreML(from 4.0ms to 3.5ms). Meanwhile, Next-ViT-</cell></row><row><cell>L achieves the similar accuracy as EfficientNet-B5 [32]</cell></row><row><cell>and ConvNeXt-B while 4.0? and 1.4? faster on TensorRT,</cell></row><row><cell>3.2? and 44? faster on CoreML. In terms of the advanced</cell></row><row><cell>ViTs, Next-ViT-S outperforms Twins-SVT-S [3] by 0.8%</cell></row><row><cell>with 1.3? faster inference speed on TensorRT. Next-ViT-B</cell></row><row><cell>surpasses CSwin-T [6] by 0.5% while the inference latency</cell></row><row><cell>is compressed by 64% on TensorRT. Finally, compared with</cell></row><row><cell>recent hybrid methods, Next-ViT-S beats CMT-XS by 0.7%</cell></row><row><cell>with 1.8? and 1.4? faster speed on TensorRT and CoreML.</cell></row><row><cell>Compared to EfficientFormer-L7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>Comparison of different backbones on ADE20K semantic segmentation task. FLOPs are measured with the input size of 512?2048. ? denotes training Semantic FPN-80K for 80k iteration with a total batch size of 32, which is 2? training data iteration compared to regular setting. ? indicates that the model is pre-trained on large-scale dataset.</figDesc><table><row><cell></cell><cell cols="2">Latency(ms)</cell><cell cols="2">Semantic FPN 80k</cell><cell></cell><cell></cell><cell>UperNet 160k</cell><cell></cell></row><row><cell>Backbone</cell><cell cols="8">TensorRT CoreML Param(M) FLOPs(G) mIoU(%) Param(M) FLOPs(G) mIoU/MS mIoU(%)</cell></row><row><cell>ResNet101 [10]</cell><cell>32.8</cell><cell>13.2</cell><cell>48.0</cell><cell>260</cell><cell>38.8</cell><cell>96</cell><cell>1029</cell><cell>-/44.9</cell></row><row><cell>ResNeXt101-32x4d [40]</cell><cell>37.0</cell><cell>15.3</cell><cell>47.1</cell><cell>-</cell><cell>39.7</cell><cell>-</cell><cell>-</cell><cell>-/-</cell></row><row><cell>ResNeSt50 [45]</cell><cell>522.3</cell><cell>372.3</cell><cell>30.4</cell><cell>204</cell><cell>39.7</cell><cell>68.4</cell><cell>973</cell><cell>42.1/-</cell></row><row><cell>ConvNeXt-T [22]</cell><cell>78.5</cell><cell>349.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>60.0</cell><cell>939</cell><cell>-/46.7</cell></row><row><cell>Swin-T [21]</cell><cell>-</cell><cell>-</cell><cell>31.9</cell><cell>182</cell><cell>41.5</cell><cell>59.9</cell><cell>945</cell><cell>44.5/45.8</cell></row><row><cell>PVTv2-B2 [35]</cell><cell>167.6</cell><cell>101.0</cell><cell>29.1</cell><cell>-</cell><cell>45.2</cell><cell>-</cell><cell>-</cell><cell>-/-</cell></row><row><cell>Twins-SVT-S [3]</cell><cell>127.2</cell><cell>-</cell><cell>28.3</cell><cell>144</cell><cell>43.2</cell><cell>54.4</cell><cell>901</cell><cell>46.2/47.1</cell></row><row><cell>PoolFormer-S12 [43]</cell><cell>30.3</cell><cell>18.2</cell><cell>15.7</cell><cell>-</cell><cell>37.2</cell><cell>-</cell><cell>-</cell><cell>-/-</cell></row><row><cell>PoolFormer-S24 [43]</cell><cell>59.1</cell><cell>23.0</cell><cell>23.2</cell><cell>-</cell><cell>40.3</cell><cell>-</cell><cell>-</cell><cell>-/-</cell></row><row><cell>TRT-ViT-C  *  [38]</cell><cell>40.6</cell><cell>20.7</cell><cell>70.6</cell><cell>213</cell><cell>46.2</cell><cell>105.0</cell><cell>978</cell><cell>47.6/48.9</cell></row><row><cell>EfficientFormer-L3 [19]</cell><cell>35.9</cell><cell>10.6</cell><cell>-</cell><cell>-</cell><cell>43.5</cell><cell>-</cell><cell>-</cell><cell>-/-</cell></row><row><cell>Next-ViT-S</cell><cell>38.2</cell><cell>18.1</cell><cell>36.3</cell><cell>208</cell><cell>46.5</cell><cell>66.3</cell><cell>968</cell><cell>48.1/49.0</cell></row><row><cell>ResNeXt101-64x4d [40]</cell><cell>65.7</cell><cell>25.6</cell><cell>86.4</cell><cell>-</cell><cell>40.2</cell><cell>-</cell><cell>-</cell><cell>-/-</cell></row><row><cell>ResNeSt101 [45]</cell><cell>798.9</cell><cell>443.6</cell><cell>51.2</cell><cell>305</cell><cell>42.4</cell><cell>89.2</cell><cell>1074</cell><cell>44.2/-</cell></row><row><cell>ConvNeXt-S [22]</cell><cell>131.5</cell><cell>658.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>82.0</cell><cell>1027</cell><cell>-/49.6</cell></row><row><cell>Swin-S [21]</cell><cell>-</cell><cell>-</cell><cell>53.2</cell><cell>274</cell><cell>45.2</cell><cell>81.3</cell><cell>1038</cell><cell>47.6/49.5</cell></row><row><cell>PVTv2-B3 [35]</cell><cell>230.9</cell><cell>114.5</cell><cell>49.0</cell><cell>-</cell><cell>47.3</cell><cell>-</cell><cell>-</cell><cell>-/-</cell></row><row><cell>Twins-SVT-B [3]</cell><cell>231.8</cell><cell>-</cell><cell>60.4</cell><cell>261</cell><cell>45.3</cell><cell>88.5</cell><cell>1020</cell><cell>47.7/48.9</cell></row><row><cell>PoolFormer-S36 [43]</cell><cell>87.7</cell><cell>27.7</cell><cell>34.6</cell><cell>-</cell><cell>42.0</cell><cell>-</cell><cell>-</cell><cell>-/-</cell></row><row><cell>PoolFormer-M36 [43]</cell><cell>127.8</cell><cell>32.0</cell><cell>59.8</cell><cell>-</cell><cell>42.4</cell><cell>-</cell><cell>-</cell><cell>-/-</cell></row><row><cell>CSWin-T [6]</cell><cell>182.3</cell><cell>-</cell><cell>26.1</cell><cell>202</cell><cell>48.2</cell><cell>59.9</cell><cell>959</cell><cell>49.3/50.4</cell></row><row><cell>UniFormer-S  *  [17]</cell><cell>90.7</cell><cell>33.5</cell><cell>25.0</cell><cell>247</cell><cell>46.6</cell><cell>52.0</cell><cell>1088</cell><cell>47.6/48.5</cell></row><row><cell>TRT-ViT-D  *  [38]</cell><cell>58.1</cell><cell>29.4</cell><cell>105.9</cell><cell>296</cell><cell>46.7</cell><cell>143.7</cell><cell>1065</cell><cell>48.8/49.8</cell></row><row><cell>EfficientFormer-L7 [19]</cell><cell>84.0</cell><cell>23.0</cell><cell>-</cell><cell>-</cell><cell>45.1</cell><cell>-</cell><cell>-</cell><cell>-/-</cell></row><row><cell>Next-ViT-B</cell><cell>51.6</cell><cell>24.4</cell><cell>49.3</cell><cell>260</cell><cell>48.6</cell><cell>79.3</cell><cell>1020</cell><cell>50.4/51.1</cell></row><row><cell>ConvNeXt-B [22]</cell><cell>181.3</cell><cell>1074.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>122.0</cell><cell>1170</cell><cell>-/49.9</cell></row><row><cell>Swin-B [21]</cell><cell>-</cell><cell>-</cell><cell>91.2</cell><cell>422</cell><cell>46.0</cell><cell>121.0</cell><cell>1188</cell><cell>48.1/49.7</cell></row><row><cell>PVTv2-B4 [35]</cell><cell>326.4</cell><cell>147.6</cell><cell>66.3</cell><cell>-</cell><cell>47.9</cell><cell>-</cell><cell>-</cell><cell>-/-</cell></row><row><cell>Twins-SVT-L [3]</cell><cell>409.7</cell><cell>-</cell><cell>103.7</cell><cell>404</cell><cell>46.7</cell><cell>133.0</cell><cell>1164</cell><cell>48.8/49.7</cell></row><row><cell>PoolFormer-M48 [43]</cell><cell>168.7</cell><cell>39.5</cell><cell>77.1</cell><cell>-</cell><cell>42.7</cell><cell>-</cell><cell>-</cell><cell>-/-</cell></row><row><cell>CSWin-S [6]</cell><cell>298.2</cell><cell>-</cell><cell>38.5</cell><cell>271</cell><cell>49.2</cell><cell>64.4</cell><cell>1027</cell><cell>50.4/51.5</cell></row><row><cell>UniFormer-B  *  [17]</cell><cell>195.1</cell><cell>70.6</cell><cell>54.0</cell><cell>471</cell><cell>48.0</cell><cell>80.0</cell><cell>1227</cell><cell>50.0/50.8</cell></row><row><cell>Next-ViT-L</cell><cell>65.3</cell><cell>30.1</cell><cell>62.4</cell><cell>331</cell><cell>49.1</cell><cell>92.4</cell><cell>1072</cell><cell>50.1/50.8</cell></row><row><cell>Swin-B  ? [21]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>121.0</cell><cell>1841</cell><cell>50.0/51.7</cell></row><row><cell>CSWin-B  ? [21]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>109.2</cell><cell>1941</cell><cell>51.8/52.6</cell></row><row><cell>Next-ViT-S  ?</cell><cell>38.2</cell><cell>18.1</cell><cell>36.3</cell><cell>208</cell><cell>48.8</cell><cell>66.3</cell><cell>968</cell><cell>49.8/50.8</cell></row><row><cell>Next-ViT-B  ?</cell><cell>51.6</cell><cell>24.4</cell><cell>49.3</cell><cell>260</cell><cell>50.2</cell><cell>79.3</cell><cell>1020</cell><cell>51.8/52.8</cell></row><row><cell>Next-ViT-L  ?</cell><cell>65.3</cell><cell>30.1</cell><cell>62.4</cell><cell>331</cell><cell>50.5</cell><cell>92.4</cell><cell>1072</cell><cell>51.5/52.0</cell></row><row><cell cols="4">large-scale dataset. All the models are pretrained with res-</cell><cell cols="5">tasks, due to some modules in Mask R-CNN and Upernet</cell></row><row><cell cols="4">olution 224?224 and then trained on ADE20K with the in-</cell><cell cols="5">are not easy to be deployed on TensorRT and CoreML, we</cell></row><row><cell cols="4">put size of 512?512. For the Semantic FPN framework,</cell><cell cols="5">only measure the latency of the backbone for a fair compar-</cell></row><row><cell cols="4">we adopt the AdamW optimizer with both the learning rate</cell><cell cols="5">ison, with the same test environments as classification. For</cell></row><row><cell cols="4">and weight decay being 0.0001. Then we train the whole</cell><cell cols="5">simplicity, the input size of 512?512 is uniformly used to</cell></row><row><cell cols="4">network for 40K iterations with a total batch size of 32</cell><cell cols="3">measure latency in</cell><cell></cell><cell></cell></row><row><cell cols="4">based on the stochastic depth of 0.2 for Next-ViT-S/B/L.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">For the training and testing on the UperNet framework, we</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">also train the models for 160K iterations with the stochastic</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">depth of 0.2. AdamW optimizer is used as well but with the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">learning rate 6 ? 10 ?5 , total batch size 16, and weight de-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">cay 0.01. Then we test the mIoU based on both single-scale</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">and multi-scale (MS) where the scale goes from 0.5 to 1.75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">with an interval of 0.25. For detection and segmentation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 .</head><label>6</label><figDesc>Comparison of different backbones on Mask R-CNN-based object detection and instance segmentation tasks. FLOPs are measured with the inpus size of 800 ? 1280. The superscript b and m denote the box detection and mask instance segmentation.</figDesc><table><row><cell>Backbone</cell><cell cols="2">Param FLOPs (M) (G)</cell><cell cols="4">Latency(ms) TensorRT CoreML AP b AP b 50</cell><cell cols="3">Mask R-CNN 1? AP b 75 AP m AP m 50</cell><cell>AP m 75</cell><cell cols="5">Mask R-CNN 3? + MS AP b AP b 50 AP b 75 AP m AP m 50</cell><cell>AP m 75</cell></row><row><cell>ResNet101 [10]</cell><cell>63.2</cell><cell>336</cell><cell>32.8</cell><cell>13.2</cell><cell cols="2">40.4 61.1</cell><cell>44.2</cell><cell>36.4</cell><cell>57.7</cell><cell cols="3">38.8 42.8 63.2</cell><cell>47.1</cell><cell>38.5</cell><cell>60.1</cell><cell>41.3</cell></row><row><cell>ResNeXt101-32x4d [40]</cell><cell>62.8</cell><cell>-</cell><cell>37.0</cell><cell>15.3</cell><cell cols="2">41.9 62.5</cell><cell>45.9</cell><cell>37.5</cell><cell>59.4</cell><cell>40.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ResNeSt50 [45]</cell><cell>47.4</cell><cell>400</cell><cell>522.3</cell><cell>372.3</cell><cell>42.6</cell><cell>-</cell><cell>-</cell><cell>38.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ConvNext-T [22]</cell><cell>-</cell><cell>262</cell><cell>78.5</cell><cell>349.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">46.2 67.9</cell><cell>50.8</cell><cell>41.7</cell><cell>65.0</cell><cell>44.9</cell></row><row><cell>Swin-T [21]</cell><cell>47.8</cell><cell>264</cell><cell>-</cell><cell>-</cell><cell cols="2">42.2 64.4</cell><cell>46.2</cell><cell>39.1</cell><cell>64.6</cell><cell cols="3">42.0 46.0 68.2</cell><cell>50.2</cell><cell>41.6</cell><cell>65.1</cell><cell>44.8</cell></row><row><cell>PVTv2-B2 [35]</cell><cell>45.0</cell><cell>-</cell><cell>167.6</cell><cell>101.0</cell><cell cols="2">45.3 67.1</cell><cell>49.6</cell><cell>41.2</cell><cell>64.2</cell><cell>44.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Twins-SVT-S [3]</cell><cell>44.0</cell><cell>228</cell><cell>127.2</cell><cell>-</cell><cell cols="2">43.4 66.0</cell><cell>47.3</cell><cell>40.3</cell><cell>63.2</cell><cell cols="3">43.4 46.8 69.2</cell><cell>51.2</cell><cell>42.6</cell><cell>66.3</cell><cell>45.8</cell></row><row><cell>PoolFormer-S12 [43]</cell><cell>31.6</cell><cell>-</cell><cell>30.3</cell><cell>18.2</cell><cell cols="2">37.3 59.0</cell><cell>40.1</cell><cell>34.6</cell><cell>55.8</cell><cell>36.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PoolFormer-S24 [43]</cell><cell>41.0</cell><cell>-</cell><cell>59.1</cell><cell>22.9</cell><cell cols="2">40.1 62.2</cell><cell>43.4</cell><cell>37.0</cell><cell>59.1</cell><cell>39.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TRT-ViT-C [38]</cell><cell>86.3</cell><cell>294</cell><cell>40.6</cell><cell>20.8</cell><cell cols="2">44.7 66.9</cell><cell>48.8</cell><cell>40.8</cell><cell>63.9</cell><cell cols="4">44.0 47.3 68.8. 51.9</cell><cell>42.7</cell><cell>65.9</cell><cell>46.0</cell></row><row><cell>EfficientFormer-L3 [19]</cell><cell>-</cell><cell>-</cell><cell>35.9</cell><cell>10.6</cell><cell cols="2">41.4 63.9</cell><cell>44.7</cell><cell>38.1</cell><cell>61.0</cell><cell>40.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Next-ViT-S</cell><cell>51.8</cell><cell>290</cell><cell>38.2</cell><cell>18.1</cell><cell cols="2">45.9 68.3</cell><cell>50.7</cell><cell>41.8</cell><cell>65.1</cell><cell cols="3">45.1 48.0 69.7</cell><cell>52.8</cell><cell>43.2</cell><cell>66.8</cell><cell>46.7</cell></row><row><cell cols="2">ResNeXt101-64x4d [40] 101.9</cell><cell>-</cell><cell>65.7</cell><cell>25.6</cell><cell cols="2">42.8 63.8</cell><cell>47.3</cell><cell>38.4</cell><cell>60.6</cell><cell>41.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ResNeSt101 [45]</cell><cell>68.1</cell><cell>499</cell><cell>798.9</cell><cell>443.6</cell><cell>45.2</cell><cell>-</cell><cell>-</cell><cell>40.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Swin-S [21]</cell><cell>69.1</cell><cell>354</cell><cell>-</cell><cell>-</cell><cell cols="2">44.8 66.6</cell><cell>48.9</cell><cell>40.9</cell><cell>63.4</cell><cell cols="3">44.2 48.5 70.2</cell><cell>53.5</cell><cell>43.3</cell><cell>67.3</cell><cell>46.6</cell></row><row><cell>PVTv2-B3 [35]</cell><cell>64.9</cell><cell>-</cell><cell>230.9</cell><cell>114.5</cell><cell cols="2">47.0 68.1</cell><cell>51.7</cell><cell>42.5</cell><cell>65.7</cell><cell>45.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Twins-SVT-B [3]</cell><cell>76.3</cell><cell>340</cell><cell>231.8</cell><cell>-</cell><cell cols="2">45.2 67.6</cell><cell>49.3</cell><cell>41.5</cell><cell>64.5</cell><cell cols="3">44.8 48.0 69.5</cell><cell>52.7</cell><cell>43.0</cell><cell>66.8</cell><cell>46.6</cell></row><row><cell>CSWin-T [6]</cell><cell>42.0</cell><cell>279</cell><cell>182.3</cell><cell>-</cell><cell cols="2">46.7 68.6</cell><cell>51.3</cell><cell>42.2</cell><cell>65.6</cell><cell cols="3">45.4 49.0 70.7</cell><cell>53.7</cell><cell>43.6</cell><cell>67.9</cell><cell>46.6</cell></row><row><cell>PoolFormer-S36 [43]</cell><cell>50.5</cell><cell>-</cell><cell>87.7</cell><cell>27.7</cell><cell cols="2">41.0 63.1</cell><cell>44.8</cell><cell>37.7</cell><cell>60.1</cell><cell>40.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CMT-S [6]</cell><cell>30.2</cell><cell>-</cell><cell>200.5</cell><cell>73.4</cell><cell cols="2">44.6 66.8</cell><cell>48.9</cell><cell>40.7</cell><cell>63.9</cell><cell>43.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CoaT Mini [41]</cell><cell>41.6</cell><cell>-</cell><cell>509.6</cell><cell>476.9</cell><cell>45.1</cell><cell>-</cell><cell>-</cell><cell>40.6</cell><cell>-</cell><cell>-</cell><cell>46.5</cell><cell>-</cell><cell>-</cell><cell>41.8</cell><cell>-</cell><cell>-</cell></row><row><cell>UniFormer-S h14 [17]</cell><cell>41</cell><cell>269</cell><cell>164.0</cell><cell>-</cell><cell cols="2">45.6 68.1</cell><cell>49.7</cell><cell>41.6</cell><cell>64.8</cell><cell cols="3">45.0 48.2 70.4</cell><cell>52.5</cell><cell>43.4</cell><cell>67.1</cell><cell>47.0</cell></row><row><cell>TRT-ViT-D [38]</cell><cell>121.5</cell><cell>375</cell><cell>58.1</cell><cell>29.5</cell><cell cols="2">45.3 67.9</cell><cell>49.6</cell><cell>41.6</cell><cell>64.7</cell><cell cols="3">44.8 48.1 69.3</cell><cell>52.7</cell><cell>43.4</cell><cell>66.7</cell><cell>46.8</cell></row><row><cell>EfficientFormer-L7 [19]</cell><cell>-</cell><cell>-</cell><cell>84.0</cell><cell>23.0</cell><cell cols="2">42.6 65.1</cell><cell>46.1</cell><cell>39.0</cell><cell>62.2</cell><cell>41.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Next-ViT-B</cell><cell>64.9</cell><cell>340</cell><cell>51.6</cell><cell>24.4</cell><cell cols="2">47.2 69.6</cell><cell>51.6</cell><cell>42.8</cell><cell>66.5</cell><cell cols="3">45.9 49.5 71.1</cell><cell>54.2</cell><cell>44.4</cell><cell>68.3</cell><cell>48.0</cell></row><row><cell>Swin-B [21]</cell><cell>107</cell><cell>496</cell><cell>-</cell><cell>-</cell><cell>46.9</cell><cell>-</cell><cell>-</cell><cell>42.3</cell><cell>-</cell><cell>-</cell><cell cols="2">48.5 69.8</cell><cell>53.2</cell><cell>43.4</cell><cell>66.8</cell><cell>46.9</cell></row><row><cell>PVTv2-B4 [35]</cell><cell>82.2</cell><cell>-</cell><cell>326.4</cell><cell>147.6</cell><cell cols="2">47.5 68.7</cell><cell>52.0</cell><cell>42.7</cell><cell>66.1</cell><cell>46.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Twins-SVT-L [3]</cell><cell>111</cell><cell>474</cell><cell>409.7</cell><cell>-</cell><cell>45.7</cell><cell>-</cell><cell>-</cell><cell>41.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CSWin-S [6]</cell><cell>54.0</cell><cell>342</cell><cell>298.2</cell><cell>-</cell><cell cols="2">47.9 70.1</cell><cell>52.6</cell><cell>43.2</cell><cell>67.1</cell><cell cols="3">46.2 50.0 71.3</cell><cell>54.7</cell><cell>44.5</cell><cell>68.4</cell><cell>47.7</cell></row><row><cell>CoaT Small [41]</cell><cell>54.0</cell><cell>-</cell><cell>601.7</cell><cell>612.9</cell><cell>46.5</cell><cell>-</cell><cell>-</cell><cell>41.8</cell><cell>-</cell><cell>-</cell><cell>49.0</cell><cell>-</cell><cell>-</cell><cell>43.7</cell><cell>-</cell><cell>-</cell></row><row><cell>UniFormer-B h14 [17]</cell><cell>69</cell><cell>399</cell><cell>367.7</cell><cell>-</cell><cell cols="2">47.4 69.7</cell><cell>52.1</cell><cell>43.1</cell><cell>66.0</cell><cell cols="3">46.5 50.3 72.7</cell><cell>55.3</cell><cell>44.8</cell><cell>69.0</cell><cell>48.3</cell></row><row><cell>Next-ViT-L</cell><cell>77.9</cell><cell>391</cell><cell>65.3</cell><cell>30.1</cell><cell cols="2">48.0 69.8</cell><cell>52.6</cell><cell>43.2</cell><cell>67.0</cell><cell cols="3">46.8 50.2 71.6</cell><cell>54.9</cell><cell>44.8</cell><cell>68.7</cell><cell>48.2</cell></row><row><cell cols="6">2.5? on TensorRT. Compared with the Uniformer-S/B [17],</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Next-ViT-B/L achieves 2.0% and 1.1% mIoU performance</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">gain while 0.4?/1.3? faster on CoreML and 0.8?/1.6?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">faster on TensorRT. Next-ViT-B surpasses EfficientFormer-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">L7 [19] by 3.5% mIoU with similar CoreML runtime and</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">38% fewer latency on TensorRT. In terms of the Uper-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Net [39] framework, Next-ViT-S surpasses recent SOTA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">CNN model ConvNeXt [22] 2.3% MS mIoU while 1.0?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">and 18.0? faster on TensorRT and CoreML respectively.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Compared to the CSWin-S [6], Next-ViT-L achieves 3.6?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">faster speed on TensorRT with similar performance. Exten-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">sive experiments reveal that our Next-ViT achieves excel-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">lent potential on segmentation tasks.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>shows the evaluation results with the Mask R-CNN</cell></row><row><cell>framework. Based on the 1? schedule, Next-ViT-S sur-</cell></row><row><cell>passes ResNet101 [10] and ResNeSt50 [45] by 5.5 AP b and</cell></row><row><cell>3.3 AP b . Next-ViT-L beats PVTv2-B4 [35] by 0.5 AP b and</cell></row><row><cell>predict with 4.0? and 3.9? faster runtime on TensorRT</cell></row><row><cell>and CoreML. Compared to the EfficientFormer-L7 [19],</cell></row><row><cell>Next-ViT-B improves AP b from 42.6 to 47.2 with similar</cell></row><row><cell>CoreML latency and 39% fewer TensorRT runtime. Next-</cell></row><row><cell>ViT-B outperforms TRT-ViT-D [38] by 1.9 AP</cell></row></table><note>b but is still faster on both TensorRT and CoreML. Based on the 3? schedule, Next-ViT shows the same superiority as the 1?.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 .Table 9 .</head><label>79</label><figDesc>Comparison of different convolution blocks.Table 8. Comparison of results of different ratios. Different normalizations and activations comparison.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Latency(ms)</cell><cell>Cls</cell><cell>Det</cell><cell>Seg</cell></row><row><cell cols="2">Block type</cell><cell></cell><cell cols="2">TensorRT</cell><cell cols="2">Acc(%) AP b mIoU(%)</cell></row><row><cell cols="3">BottleNeck Block [10]</cell><cell cols="2">7.8</cell><cell>81.9</cell><cell>44.5</cell><cell>45.5</cell></row><row><cell cols="3">ConvNeXt Block [22]</cell><cell cols="2">7.8</cell><cell>79.6</cell><cell>41.4</cell><cell>43.7</cell></row><row><cell cols="2">LSA Block [3]</cell><cell></cell><cell cols="2">8.4</cell><cell>78.2</cell><cell>38.7</cell><cell>40.1</cell></row><row><cell cols="3">PoolFormer Block [43]</cell><cell cols="2">7.6</cell><cell>80.9</cell><cell>42.3</cell><cell>44.0</cell></row><row><cell cols="3">Local MHRA Block [17]</cell><cell cols="2">7.8</cell><cell>80.5</cell><cell>42.4</cell><cell>43.4</cell></row><row><cell cols="2">NCB (ours)</cell><cell></cell><cell cols="2">7.7</cell><cell>82.5</cell><cell>45.9</cell><cell>46.5</cell></row><row><cell></cell><cell cols="3">Latency(ms)</cell><cell>Cls</cell><cell>Det</cell><cell>Seg</cell></row><row><cell>r</cell><cell cols="3">TensorRT</cell><cell cols="3">Acc(%) AP b mIoU(%)</cell></row><row><cell>0.0</cell><cell></cell><cell>6.6</cell><cell></cell><cell>80.9</cell><cell>43.1</cell><cell>42.4</cell></row><row><cell cols="2">0.25</cell><cell>6.9</cell><cell></cell><cell>81.3</cell><cell>45.0</cell><cell>45.2</cell></row><row><cell cols="2">0.50</cell><cell>7.3</cell><cell></cell><cell>82.2</cell><cell>45.4</cell><cell>46.0</cell></row><row><cell cols="2">0.75</cell><cell>7.7</cell><cell></cell><cell>82.5</cell><cell>45.9</cell><cell>46.5</cell></row><row><cell>1.0</cell><cell></cell><cell>8.2</cell><cell></cell><cell>82.1</cell><cell>45.4</cell><cell>45.5</cell></row><row><cell></cell><cell></cell><cell cols="3">Latency(ms)</cell><cell>Cls</cell><cell>Det</cell><cell>Seg</cell></row><row><cell cols="3">Norm Activation</cell><cell cols="2">TensorRT</cell><cell cols="2">Acc(%) AP b mIoU(%)</cell></row><row><cell>LN</cell><cell>GELU</cell><cell></cell><cell>9.3</cell><cell></cell><cell>82.7</cell><cell>46.0</cell><cell>46.7</cell></row><row><cell>LN</cell><cell>ReLU</cell><cell></cell><cell>9.1</cell><cell></cell><cell>82.7</cell><cell>46.0</cell><cell>46.6</cell></row><row><cell>BN</cell><cell>GELU</cell><cell></cell><cell>8.0</cell><cell></cell><cell>82.5</cell><cell>45.9</cell><cell>46.6</cell></row><row><cell>BN</cell><cell>ReLU</cell><cell></cell><cell>7.7</cell><cell></cell><cell>82.5</cell><cell>45.9</cell><cell>46.5</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Integrated model of visual processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Bullier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain research reviews</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="96" to="107" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.05895,2021.4</idno>
		<title level="m">Mobileformer: Bridging mobilenet and transformer</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Twins: Revisiting the design of spatial attention in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13840</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Beyond self-supervision: A simple yet effective network distillation alternative to improve backbones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zewu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiwen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.05959</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Coatnet: Marrying convolution and attention for all data sizes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Cswin transformer: A general vision transformer backbone with cross-shaped windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00652</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cmt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.06263</idno>
		<title level="m">Convolutional neural networks meet vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rethinking spatial dimensions of vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong Joon</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11936" to="11945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The neural bases of spatial frequency processing during scene perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louise</forename><surname>Kauffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Ramano?l</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carole</forename><surname>Peyrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in integrative neuroscience</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">37</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6399" to="6408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Uniformer: Unifying convolution and self-attention for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunchang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanglu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.09450</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuefeng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiping</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.15380</idno>
		<title level="m">Sepvit: Separable vision transformer</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Efficientformer: Vision transformers at mobilenet speed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Evangelidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.01191</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted win</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">dows. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.03545</idno>
	</analytic>
	<monogr>
		<title level="m">Trevor Darrell, and Saining Xie. A convnet for the 2020s</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Mobilevit: lightweight, general-purpose, and mobile-friendly vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.02178</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Separable selfattention for mobile vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.02680</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">How do vision transformers work?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namuk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songkuk</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.06709,2022.5</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10428" to="10436" />
		</imprint>
	</monogr>
	<note>Kaiming He, and Piotr Doll?r</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bottleneck transformers for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16519" to="16529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13797</idno>
		<title level="m">Pvtv2: Improved baselines with pyramid vision transformer</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12122</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cvt: Introducing convolutions to vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="22" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuefeng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.09579</idno>
		<title level="m">Trtvit: Tensorrt-oriented vision transformer</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Coscale conv-attentional image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Scalablevit: Rethinking the context-oriented generalization of vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuefeng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.10790</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Metaformer is actually what you need for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>2022. 4, 5, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi-Hang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="558" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Resnest: Split-attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manmatha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="633" to="641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Understanding the robustness in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaowei</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Animashree</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose M</forename><surname>Alvarez</surname></persName>
		</author>
		<idno>PMLR, 2022. 4</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="27378" to="27394" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
