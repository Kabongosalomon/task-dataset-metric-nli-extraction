<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000. LSDNet: Trainable Modification of LSD Algorithm for Real-Time Line Segment Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Teplyakov</surname></persName>
							<email>teplyakov@iitp.ru.</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Information Transmission Problems</orgName>
								<orgName type="institution">Russian Academy of Sciences</orgName>
								<address>
									<postCode>119991</postCode>
									<settlement>Moscow</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Erlygin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Information Transmission Problems</orgName>
								<orgName type="institution">Russian Academy of Sciences</orgName>
								<address>
									<postCode>119991</postCode>
									<settlement>Moscow</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Moscow Institute of Physics and Technology</orgName>
								<address>
									<postCode>117303</postCode>
									<settlement>Moscow</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Shvets</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Information Transmission Problems</orgName>
								<orgName type="institution">Russian Academy of Sciences</orgName>
								<address>
									<postCode>119991</postCode>
									<settlement>Moscow</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Teplyakov</surname></persName>
						</author>
						<title level="a" type="main">Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000. LSDNet: Trainable Modification of LSD Algorithm for Real-Time Line Segment Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/ACCESS.2017.DOI</idno>
					<note>Corresponding author: This work was supported by the Russian Science Foundation (Project No. 20-61-47089)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As of today, the best accuracy in line segment detection (LSD) is achieved by algorithms based on convolutional neural networks -CNNs. Unfortunately, these methods utilize deep, heavy networks and are slower than traditional model-based detectors. In this paper we build an accurate yet fast CNNbased detector, LSDNet, by incorporating a lightweight CNN into a classical LSD detector. Specifically, we replace the first step of the original LSD algorithm -construction of line segments heatmap and tangent field from raw image gradients -with a lightweight CNN, which is able to calculate more complex and rich features. The second part of the LSD algorithm is used with only minor modifications. Compared with several modern line segment detectors on standard Wireframe dataset, the proposed LSDNet provides the highest speed (among CNN-based detectors) of 214 FPS with a competitive accuracy of 78 F H . Although the best-reported accuracy is 83 F H at 33 FPS, we speculate that the observed accuracy gap is caused by errors in annotations and the actual gap is significantly lower. We point out systematic inconsistencies in the annotations of popular line detection benchmarks -Wireframe and York Urban, carefully reannotate a subset of images and show that (i) existing detectors have improved quality on updated annotations without retraining, suggesting that new annotations correlate better with the notion of correct line segment detection; (ii) the gap between accuracies of our detector and others diminishes to negligible 0.2 F H , with our method being the fastest.</p><p>INDEX TERMS Convolutional neural networks, edge detection, line segment detection, U-net, LSD</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>A UTOMATIC general-purpose line segment detection is a long-standing computer vision problem of high practical importance. Line segment detectors are exploited to construct an intermediate representation of image contents in visual recognition systems over a wide range of applications, such as autonomous vehicle localization <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b3">[3]</ref>, infrastructure maintenance with an UAV <ref type="bibr" target="#b4">[4]</ref>, <ref type="bibr" target="#b5">[5]</ref>, document recognition <ref type="bibr" target="#b6">[6]</ref>, <ref type="bibr" target="#b7">[7]</ref>.</p><p>Traditionally, the problem of line segment detection was approached with so-called model-based algorithms <ref type="bibr" target="#b8">[8]</ref>- <ref type="bibr" target="#b11">[11]</ref>. These algorithms operate by searching an image for elements that satisfy an explicit definition of a salient line segment, for example, "line segment is a strip-like set of image pixels with similar gradients" <ref type="bibr" target="#b8">[8]</ref>, <ref type="bibr" target="#b9">[9]</ref> or "an image region is a line segment if its contour map triggers a peak in Hough space" <ref type="bibr" target="#b10">[10]</ref>, <ref type="bibr" target="#b11">[11]</ref>. These algorithms typically have the benefits of being fast and having interpretable parameters. However, they may miss the segments that are salient for human, but for some reason don't match the exact explicit implemented definition. They are also prone to over-segmentation (splitting a single segment into parts) and sometimes demand nontrivial problem-specific postprocessing <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b13">[13]</ref>.</p><p>The troubling problem of formulating an explicit criterion that matches the human expectation of what exactly constitutes a "salient line segment" can be avoided by manually annotating images and training a CNN, which then learns an implicit algorithm from data samples. This is an approach that yields the best accuracy in line segment detection task today <ref type="bibr" target="#b14">[14]</ref>- <ref type="bibr" target="#b20">[20]</ref>. Skipping ahead, let us note that such annotation is not a simple task either -existing datasets on line segment detection have numerous and sometimes extreme internal inconsistencies -probably caused by the inherent ambiguity of the task, lack of clear labeling instructions and the tediousness of the task, leading to missed segments.</p><p>From a technical perspective, there also is a challenge in designing a CNN-based line detector. The typical solution is that CNN constructs an intermediate representation -encoding -which is then converted into a set of answers by some hand-crafted algorithm. Object detection networks have solved this problem by using so-called anchors <ref type="bibr" target="#b21">[21]</ref> and, later, more simple anchorless detectors such as FCOS <ref type="bibr" target="#b22">[22]</ref>. But line segment detectors need alternative encodings, suitable not for bounding boxes, but for line segments. The encodings should effectively deal with the fact that the line segments to be detected in a typical image intersect with each other a lot, while encodings for bounding boxes are effective only when "overlapping mostly happens between objects with considerably different sizes" <ref type="bibr" target="#b22">[22]</ref>, making them hard to exploit for line segment detection. While object detectors encondings -after many iterations of refinementhave become fast and elegant, we believe that intermediate representations of most line detector used today are still either imprecise, slow or unintuitive.</p><p>So whether it is the complexity of the interpreter or the sheer weight of the CNN backbone, CNN-based detectors that outperform the traditional ones in accuracy are also computationally harder <ref type="bibr" target="#b20">[20]</ref>. Their complexity limits the scope of application of such algorithms in cases where speed, energy consumption, or hardware price are critical.</p><p>In this work we propose a fast yet accurate CNN-based line segment detection algorithm, LSDNet, built on the basis of a widely used model-based detector, LSD <ref type="bibr" target="#b8">[8]</ref>. LSDNet overview is presented in <ref type="figure" target="#fig_0">Figure 1</ref>. The first step of LSD is the calculation of image gradient's orientation and magnitude. We view this step as an estimation of an intermediate representation, composed of line segments' heatmap, estimated as gradient's magnitude, and tangent field, estimated as gradient's orientation. We substitute this step with a lightweight CNN to generate the heatmap and tangent field from a more diverse and complex set of features than a simple gradient; the second step, conversion the intermediate representation into a set of line segments, is taken from the LSD almost asis. This substitution boosts the LSD accuracy and simplifies the postprocessing due to more accurate heatmap and tangent field. The heatmap and tangent field generation is a relatively simple task -the answer can be correctly inferred from the local context -which allows to use a lightweight CNN.</p><p>We compare LSDNet and a selection of existing detectors: LSD <ref type="bibr" target="#b8">[8]</ref>, L-CNN <ref type="bibr" target="#b18">[18]</ref>, HAWP <ref type="bibr" target="#b15">[15]</ref>, TP-LSD <ref type="bibr" target="#b17">[17]</ref>, M-LSD and M-LSD-tiny <ref type="bibr" target="#b20">[20]</ref> and show that LSDNet provides competitive accuracy of 78 F H score on Wireframe dataset while being the fastest: 214 FPS for 288?288 input on conventional hardware. LSDNet ourperforms the second fastest approach, M-LSD-tiny <ref type="bibr" target="#b20">[20]</ref>, both in accuracy and speed.</p><p>We discuss considerable inconsistencies (section IV) in ground truth labeling of current benchmarks for line segment detection accuracy -datasets Wireframe <ref type="bibr" target="#b23">[23]</ref> and YorkUrban <ref type="bibr" target="#b24">[24]</ref>. The labeling in these datasets is inconsistent not only between images: within the very same image many segments, similar in appearance, are often marked up differently -some as a positives, others as negatives. We speculate that these datasets in their current state are flawed for assessing the accuracy of general purpose line segment detectors.</p><p>So, in addition to measuring the accuracy of our detector on these datasets, we select and reannotate a part of Wireframe dataset -consisting mainly of simple images with less ambiguity -and show that the reannotated subset correlates better with conventional notion of correct line segment detection. Specifically, it narrows the accuracy gap between LSD and CNN-based approaches, and makes the gap between the proposed LSDNet and L-CNN <ref type="bibr" target="#b18">[18]</ref>, one the most accurate CNN-based approaches, negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section we cover the model-based LSD algorithm <ref type="bibr" target="#b8">[8]</ref> and the existing CNN-based approaches to the problem of line segment detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. LSD DETECTOR</head><p>LSD is one of the most popular general-purpose line segment detectors and serves as a common baseline for CNN-based algorithms both in accuracy and speed <ref type="bibr" target="#b15">[15]</ref>- <ref type="bibr" target="#b17">[17]</ref>.</p><p>The first step of LSD detector is gradient calculation; then the algorithm builds so-called line support regions (LSRs) <ref type="bibr" target="#b25">[25]</ref>. LSRs are image segments (not to be confused with line segments), spanning actual line segments in an image. They are built by iteratively grouping neighbouring pixels with high gradients' magnitudes and similar orientations. After the formation of initial LSRs, they undergo several steps of filtering and refinement. These include, among others, splitting LSRs of "hockey stick" shape -effectively decoupling distinct but merged regions; removal of LSRs with a high deviation of gradients' orientations -possibly false-positives. Typical resulting LSRs are long, straight and a few pixels thick. Finally, each such region is individually encoded as a pair of points -effectively, a line segment.</p><p>LSD detector is fast, reaching 185 FPS for 320 ? 320 images on a conventional CPU, and provides an accuracy of 63.3 F H on the Wireframe dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. CNN-BASED APPROACHES</head><p>The CNN-based approaches are typically composed of two modules: the CNN itself predicts an intermediate representation, then a postprocessing module reconstructs line segments from this representation.</p><p>We consider the design of the intermediate representation to be the key growth point of CNN-based line segment detectors since the desired detector's output -a unknownsize set of line segments with potentially high overlap -is hard to represent as a "CNN-friendly" fixed-shape tensor <ref type="bibr" target="#b26">[26]</ref>. The survey below covers most popular intermediate representations used in existing CNN-based approaches.</p><p>The first CNN-based detectors represented line segments in an image as a set of endpoints and their connectivity graph <ref type="bibr" target="#b18">[18]</ref>, <ref type="bibr" target="#b23">[23]</ref>. The endpoints were detected as local maxima of a CNN-produced heatmap. The connectivity of the endpoints was deduced either with the help of edge map heuristics <ref type="bibr" target="#b23">[23]</ref> or with a trainable classifier <ref type="bibr" target="#b18">[18]</ref>. To generate the classifier's input, a fixed number of uniform spaced points was sampled from the feature map between the endpoints. This operation was called LoI pooling <ref type="bibr" target="#b18">[18]</ref>.</p><p>In <ref type="bibr" target="#b14">[14]</ref> a representation called attraction field was proposed (distance field <ref type="bibr" target="#b19">[19]</ref> is a similar concept). Line segments were represented as a 2D vector field of translations to a nearest line segment. The postprocessing step for such a representation required nontrivial line segments extraction from heatmap-like prediction. Interestingly, the postprocessing of even a perfect attraction field generated from dataset annotations did not provide absolute detection accuracy <ref type="bibr" target="#b14">[14]</ref> -in other words, such representation is inherently ambiguous.</p><p>The covered representations of endpoints and attraction field were combined and modified in <ref type="bibr" target="#b15">[15]</ref>. The proposed CNN predicted both the endpoints and the attraction field, which was enriched to encode the translations to the both segment's endpoints as 4D vector field. Then the endpoints and the attraction field were used to refine each other. The refined line segments proposals underwent final verification with the help of LoI pooling and a trainable classifier. The detector provides state-of-the-art quality of 83 F H on Wireframe <ref type="bibr" target="#b23">[23]</ref> dataset to date, but with low speed of 33 FPS on GPU.</p><p>The recently proposed "tri-points" representation <ref type="bibr" target="#b17">[17]</ref> is focused on speeding up the detector. A line segment was represented by its center point and two vectors to its endpoints. It allowed to significantly boost the speed up to 50 FPS, driven by a much faster postprocessing requiring trivial conversion from "tri-point" to a line segment and nonmaximum supression. The CNN itself remained comparably slow. In <ref type="bibr" target="#b20">[20]</ref> some further enhancements were proposed. A lightweight CNN was designed and the training procedure was improved by augmentations and more sophisticated loss function. It resulted in the fastest CNN-based detector to date with 200 FPS overall and 241 FPS for standalone CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED APPROACH</head><p>A perfect line segments representation should make it possible to design a CNN and a postprocessing module both being fast and accurate. We argue that in the search for such a representation there is no need to develop a brand new one from scratch; instead, the representation used implicitly by LSD detector -line segments heatmap and tangent field -already possesses all the desired properties. Indeed, the heatmap and tangent field could be inferred from local image context and does not require the reasoning of complex abstract features, which allows to use a lightweight CNN. On the other hand, as proven by LSD, the representation could be efficiently postprocessed to actual line segments. In the next sections we cover each step of the algorithm in detail. <ref type="bibr">VOLUME 4, 2016</ref> A. CNN Line Segment Representation. We represent a set of line segments in an image as a 2-channel feature map of the same height and width as the image. The first channel denoted by M contains a line segment mask. The second channel denoted as F contains tangent vector field of line segments.</p><p>Since the values of F are unit vectors, we encode F as one channel feature map.</p><p>Let l = (x 1 , y 1 , x 2 , y 2 ) be a line segment, ? l -segment's level line angle in range [0, ?), p = (x, y) -an image pixel,</p><formula xml:id="formula_0">L p = l 1 , l 2 , ... -set of line segments crossing p. Then if L p = ?, then M (p) = 0 and F (p) is arbitrary; otherwise M (p) = 1, F (p) = ? l1 .</formula><p>Note that in case of overlapping segments (||L p || &gt; 1) F (p) is defined by one arbitrary segment of overlap. This ambiguity probably can be ignored since only about 1% of line segments' pixels lie on overlaps -as measured on Wireframe dataset <ref type="bibr" target="#b23">[23]</ref>.</p><p>Loss Function. The loss function L = L mask + ?L f ield used to fit the network is composed of two independent weighted terms, one responsible for mask M , other -for the tangent vector field F .</p><p>While the prediction of mask M is a straightforward segmentation problem with L mask being a conventional cross enthropy loss, to correctly estimate error in prediction of the tangent field F we should account for the following property of line segments' level line angles -the distance between angles 0 0 , 10 0 and 0 0 , 170 0 should be equal.</p><p>This problem can be approached by computing several distances between the original angles and the angles, shifted by ??, and picking the minimum distance <ref type="bibr" target="#b27">[27]</ref>. The calculation can be done simpler: let ? 1 , ? 2 -angles between which the distance is to be computed, z 1 = e i?1 , z 2 = e i?2 ? C -the representation of the angles as complex numbers with unit length and phases ? 1 , ? 2 , then</p><formula xml:id="formula_1">?(? 1 , ? 2 ) = ||z 2 1 ? z 2 2 || 2<label>(1)</label></formula><p>This distance function has a geometrical interpretation, illustrated in <ref type="figure" target="#fig_1">Fig. 2a</ref>. It equals to 2 -norm of vector difference between unit vectors with phases 2? 1 , 2? 2 . Turning to the aforementioned example, the doubled phase makes vectors, corresponding to angles 10 0 and 170 0 , equally close to the horizontal, corresponding to angle 0 0 . Given the angle distance function ?, the predicted field F p , the reference mask M t and reference field F t , the tangent vector field loss L f ield is defined as</p><formula xml:id="formula_2">L f ield = 1 p M t (p) p:Mt(p)=1 ? 2 (F t (p), F p (p)),<label>(2)</label></formula><p>where M t is the reference line segment mask -essentially, loss is the average tangent angle discrepancy over the pixels that correspond to the ground truth line segments. CNN Architecture. To predict the proposed feature map, we use a CNN of U-Net <ref type="bibr" target="#b28">[28]</ref> family. The architecture we use differs from the original one in the following simplifications. We exploit padded convolutions providing the same input and output spatial sizes of convolutional layers, which allows not to crop the feature maps feeded to skip connections. Instead of transposed convolutions we use bilinear upsampling. We reduce the depth of encoder-decoder branches up to 3 maxpooling and 3 upsampling layers, correspondingly, and use fewer filters in convolutional blocks -16, 32, 64, 128 filters per block (the number of blocks is greater than the number if maxpooling layers by one). The resulting CNN has ? 0.5M trainable parameters and can run at 48 FPS on CPU and at 695 FPS on GPU (refer to section V for benchmarking details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. LINE SEGMENTS RECONSTRUCTION</head><p>Let us consider how the predicted segments mask M and tangent field F are converted into the desired output -a set of line segment ((x 1 , y 1 ), (x 2 , y 2 )), .... This process has three steps: firstly, the predicted features are coarsely segmented into lines and background (section "Foreground segmentation"). Secondly, the lines are finely segmented into several line support regions (LSRs) (section "Region grouping"). Finally, a line segment and its confidence is extracted from each LSR (section "Line segments extraction"). Foreground segmentation. The first step of line segments reconstruction is to segment foreground (lines) from background (not lines).</p><p>We use a coarse-to-fine binarization approach by multiplying the masks of global thresholding M (p) &gt; ? and local thresholding</p><formula xml:id="formula_3">M (p) &gt; d?K(p) W p (d)M (d) ? ?<label>(3)</label></formula><p>where ? -threshold, K p -a window centered at pixel p, W p (d) -Gaussian averaging weight. Global thresholding with a small threshold gives a coarse extraction of line segments mask, but often incorrectly joins close -but separate -line segments. On the contrary, local thresholding provides much finer local distinction of line segments, but can produce clumps of false positive detections in low intensity areas <ref type="figure" target="#fig_2">(Fig. 3)</ref>. The combination of these binarizations by simple multiplication of the resulting masks allows to filter out false positives of both types and achieve better accuracy. Region grouping. The goal of this step (being a modification of a similar step of LSD algorithm <ref type="bibr" target="#b8">[8]</ref>) is to split the foreground, binarized at the previous step, into narrow striplike LSRs, one per true line segment.</p><p>Informally, we want neighbouring pixels p 1 , p 2 to be assigned to one LSR, if the values of M (p 1 ), F (p 1 ) and M (p 2 ), F (p 2 ) are similar. The algorithm grows LSRs iteratively, starting from pixels with highest M value and adding new pixels to the existing LSR which are geometrically close to the pixels and have similar features.</p><p>Let us formally introduce the similarity measure used to decide whether pixel g is fit to be joined into a LSR. Let R = p 1 , p 2 , ..., p n be the set of pixels of this LSR, I R = 1/n ? p?R M (p) be the mean line segments' mask over it, and ? R = ? p?R e 2iF (p) -the average tangent field (here ?z = atan2(Im(z)/ Re(z)) is the phase of a complex number). Then the similarity function is given by</p><formula xml:id="formula_4">d M,F (g, R) = ? 2 (F (g), ? R ) + ?(M (g) ? I R ) 2 (4)</formula><p>The first term defines similarity of tangent field orientation (refer to Eq. (1) for details), the second -the similarity of line mask, ? -weighting coefficient. Given the distance function, LSRs are built with an iterative growing algorithm, presented in Algorithm 1.</p><p>Line segments extraction. Each LSR R = p 1 , p 2 , ..., p n , should be converted into a line segment satisfying the following criteria.</p><p>? The segment goes through LSR's center of mass p ?</p><formula xml:id="formula_5">p ? = 1 p?R M (p) p?R M (p)p.<label>(5)</label></formula><p>? The segment is collinear with minor eigenvector a of region's inertia tensor I defines as follows</p><formula xml:id="formula_6">I = p?R M (p)I * (p ? p ? )<label>(6)</label></formula><formula xml:id="formula_7">I * (x, y) = y 2 ?xy ?xy x 2<label>(7)</label></formula><p>? The segment spans the furthest LSR's points, projected onto axis a. The segment's confidence is mean value of M over the region R. </p><formula xml:id="formula_8">P = (p i ), such that B(p) = 1, M (p i ) ? M (p i+1 ); P used ? ?; for p ? P do if p ? P used then continue; end R ? p; do region_updated ? F alse; for g ? N 8 (R) \ P used do if d M,F (g, R) &lt; ? then Add R ? g; Add P used ? g; region_updated ? T rue; end end while region_updated; Add R ? R; end for R ? R do if |R| &lt; n then remove R from R; end end</formula><p>Line segment extraction is visualized in <ref type="figure" target="#fig_1">Figure 2b</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DATASET</head><p>In this section we analyze the issues of the existing line segment detection datasets and propose a dataset Wireframe-tiny++, a subset of Wireframe dataset <ref type="bibr" target="#b23">[23]</ref> with refined annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. THE EXISTING DATASETS</head><p>To the best of our knowledge, there are two widely-used public line segment detection datasets: Wireframe <ref type="bibr" target="#b23">[23]</ref> and YorkUrban <ref type="bibr" target="#b24">[24]</ref>. The former is composed of 5.000 train and 462 test images, the latter is composed of 120 test images. The datasets contain both indoor and outdoor colour images of various man-made environments. Some samples from the datasets are presented in <ref type="figure" target="#fig_4">Figures 4 and 5</ref>. The datasets are annotated with a list of point pairs, representing line segments.</p><p>York dataset was annotated under so-called Manhattan world assumption <ref type="bibr" target="#b29">[29]</ref>, which means that the annotated line segments are those aligned with the basis of some Cartesian VOLUME 4, 2016 coordinate system (specifically, with axes parallel to image sides), while the others are ignored. Wireframe dataset did not follow Manhattan world assumption and was annotated with line segments, from which "meaningful geometric information of the scene can be extracted" <ref type="bibr" target="#b23">[23]</ref>, which also resulted in some salient line segments being not annotated.</p><p>So, the ground truth labeling in these datasets is explicitly limited to some category of line segments -which means other categories of line segments are viewed as negatives. CNNs that are trained on these datasets (and/or with high accuracy on them) will have to systematically classify these categories of line segments as negatives and therefore -by design! -can't be viewed as general-purpose line detectors.</p><p>While such annotations can be useful to train and test some specific niche line segment detectors (e.g. for indoor robot navigation <ref type="bibr" target="#b30">[30]</ref>) we believe they are flawed as datasets for general purpose line segment detection. Specifically, we would like to highlight the following problems (also illustrated in <ref type="figure" target="#fig_4">Fig. 4b)</ref>.</p><p>One problem is the inconsistency of the annotations -it is easily noticeable on strip-like objects having two side line segments looking almost exactly alike -however one of them is annotated while the other is not.</p><p>Some categories of salient line segments are systematically not annotated -e.g. shadows and reflections. We believe the fact that these segments are not "real" physical objects should not be considered in the context of general purpose line segment detection and these segments should be annotated as well.</p><p>Finally, some line segments lying on the same straight line are falsely merged (vertical segments on the bed canopy's frame in <ref type="figure" target="#fig_4">Fig. 4b)</ref>. It happens when a long line segment is intercepted by another object. Although for some applications it could be desirable to avoid such a splitting and there are approaches to achieve that <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b13">[13]</ref>, we believe, that for general-purpose detector splitting is the desired detector's behaviour.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. WIREFRAME-TINY++</head><p>To approach the covered issues with the existing datasets, we selected 20 random images from Wireframe test subset and reannotated them to make the annotations more accurate and consistent. We call the selected subset of images with the original markup Wireframe-tiny, and the resulting dataset with enhanced annotations -Wireframe-tiny++.</p><p>Comparing to the original annotations, we mainly added unannotated segments, 9 per image on average. Some segments are removed as undetectable. Some segments are divided into several smaller segments due to occlusion. The refined annotations are presented in <ref type="figure" target="#fig_4">Fig. 4c</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL SETTING</head><p>Datasets. The proposed algorithm is trained and evaluated with the following datasets. Wireframe dataset <ref type="bibr" target="#b23">[23]</ref> consisting of 5000 training and 462 test images is used both to train and evaluate LSDNet. Datasets YorkUrban <ref type="bibr" target="#b24">[24]</ref>, Wireframetiny and its reannotated version Wireframe-tiny++ (refer to the previous section for details), composed of 120 and 20 images correspondingly, are used solely for evaluation.</p><p>Accuracy. To evaluate LSDNet accuracy we use standard <ref type="bibr" target="#b15">[15]</ref>, <ref type="bibr" target="#b18">[18]</ref> quality score F H = 100 ? 2 ? p ? r/(p + r), where p, r stand for precision and recall. Multiplier 100 is added for readability, making F H fall in [0, 100] range. The score is evaluated pixel-wise by rasterizing both the predicted and the reference line segments. A pixel of a predicted line segment is considered true positive, if its distance to a pixel of a reference segment does not exceed 1% of image diagonal. For evaluation we use F H implementation provided with L-CNN <ref type="bibr" target="#b18">[18]</ref>.</p><p>Quality score F H was criticised <ref type="bibr" target="#b18">[18]</ref> for being not sensitive towards overlapping and splitted line segments. We consider such an insensibility is not critical: LSDNet can not produce overlapping segments by design, since line support regions (LSRs) can not overlap, and we did not observe a notable amount of splitted line segments for any CNN-based algorithm.</p><p>Speed. CNN is benchmarked on Quadro GV100 GPU for comparison with other detectors. Reconstruction algorithm is benchmarked on Core i5 9300hf CPU. CNN and the reconstruction algorithm are benchmarked independently. The speed of the latter depends on its input, we report the average speed over the dataset given the trained preprocessing network.</p><p>Baselines. We compare the proposed LSDNet with classical algorithm LSD <ref type="bibr" target="#b8">[8]</ref> and several state-of-the-art CNNbased detectors L-CNN <ref type="bibr" target="#b18">[18]</ref>, HAWP <ref type="bibr" target="#b15">[15]</ref>, TP-LSD <ref type="bibr" target="#b17">[17]</ref>, M-LSD and M-LSD-tiny <ref type="bibr" target="#b20">[20]</ref>.</p><p>The reported FPS for all CNN-based methods is cited as in <ref type="bibr" target="#b20">[20]</ref>, where benchmarking was performed on Tesla V100 GPU with practically the same characteristics as the GPU used in our experiments.</p><p>The reported F H is also cited as in <ref type="bibr" target="#b20">[20]</ref> for all methods except LSD and L-CNN <ref type="bibr" target="#b18">[18]</ref>, for which it was reproduced by our means. We tried to reproduce the stated quality measurements for other approaches with the help of their opensource implementations, but they appeared notably lower than the reported ones. Therefore on datasets Wireframe-tiny and Wireframe-tiny++ we compare LSDNet only to L-CNN and LSD.</p><p>Preprocessing. For LSDNet, all images are resized to 288 ? 288, which appeared to be the optimal input shape in terms of speed-accuracy tradeoff. Pixel intensities are simply converted from 8-bit unsigned integer to 32-bit floating-point with 1/255 scaling coefficient. During training, random horizontal and vertical flips and gamma correction are applied.</p><p>For baseline methods, the preprocessing from the corresponding paper is applied. For LSD, we use 320 ? 320 image shape.</p><p>Hyperparameters. LSDNet is initialized with He uniform initialization <ref type="bibr" target="#b31">[31]</ref> and trained by Adam optimizer <ref type="bibr" target="#b32">[32]</ref> with 10 ?4 weight decay and 8 images per batch for 180 epochs. The initial learning rate is 10 ?3 and is reduced by half if the value of loss function does not improve for 15 epochs. Implementation. CNN training and inference is implemented in TensorFlow <ref type="bibr" target="#b33">[33]</ref> and ONNX Runtime <ref type="bibr" target="#b34">[34]</ref>, correspondingly. Reconstruction algorithm is implemented in C++ with the help of OpenCV <ref type="bibr" target="#b35">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RESULTS AND ANALYSIS</head><p>In this section we quantitatively and qualitatively analyze LSDNet performance and compare it to a wide range of stateof-the-art line segment detectors. Please refer to Section V for evaluation and comparison details.      Compared to the fastest detector outperformimg LSDNet in accuracy on Wireframe dataset, M-LSD, the proposed approach is approximately two times faster with 214 FPS against 115 FPS. The fastest algorithm to outperform LSD-Net on both datasets is TP-LSD, which is approximately four times slower with 49 FPS.</p><p>Custom datasets. <ref type="table" target="#tab_3">Table 2</ref> summarizes the results on Wireframe dataset, its subset Wireframe-tiny and its reannotated version Wireframe-tiny++. Please refer to section IV-B for details.</p><p>On Wireframe-tiny++, all the detectors demonstrate higher accuracy than those on Wireframe-tiny. Since these datasets are composed of the same images and differ only in annotations, such a consistent accuracy growth indicates that Wireframe-tiny++ annotation is more suitable for the problem of general purpose line segment detection.</p><p>All the approaches, being arranged by F H , show the same relative order on all the datasets, but the absolute differences change significantly. The gap between LSDNet and L-CNN has shrinked from 3.7 F H on Wireframe to negligible 0.2 F H on Wireframe-tiny++. We believe it could be explained by different learning capacity of detectors' CNNs. Expressive L-CNN with 9.8M parameters managed to learn the subtle notion of a line segment implied by Wireframe train dataset annotation (discussed in Sec. IV-A); whereas lightweight LSDNet with only 0.5M parameters learned the general line segment detection with no capacity to learn the subtle details. It made L-CNN good for wireframe-like detection problems with the goal to detect line segments, from which "meaningful geometric information of the scene can be extracted". But it could possess confusing properties in terms of generalpurpose line segment detection, making LSDNet a better choice in such a case.</p><p>Qualitative results. Qualitative comparison of LSDNet to other line segment detectors is illustrated in <ref type="figure" target="#fig_6">Figure 6</ref>. In this section we refer to LSD and LSDNet as LSR-based and to HAWP and M-LSD-tiny as endpoint-based detectors, since the methods within these groups demonstrate similar behaviour.</p><p>Endpoint-based detectors demonstrate the selectivity of line segments, which could not be attributed to overall segments' saliency. This effect is mostly notable in the foreground in the right column in <ref type="figure" target="#fig_6">Figure 6</ref>. LSR-based LSD detects all the shadows and the carpet in front of the sofa, while it can't "see" floor tiles due to their low contrast. LSDNet detects all the shadows, the carpet and the floor tiles. Whereas endpoint-based detectors HAWP and M-LSD-tiny detect these objects poorly, but at the same time they detect way less salient segments in the background. We believe such a selectivity could be attributed to the combination of high expressive power of the underlying CNN and annotation inconsistencies of train dataset, discussed in Section IV-A. This effect could be undesirable in an application requiring that very class of segments, which is missed by endpointbased detectors.</p><p>Another interesting difference between the detectors' groups occurs due to the types of misdetections. In terms of a quality measure, LSR-based LSD and LSDNet can detect false positives, typically corresponding to line segmentlike patterns on highly structured image regions, and miss some annotated segments (false negatives), which are usually poorly visible. These errors could be, at least partially, attributed to the ill-posed nature of the task. The endpointsbased methods are also prone to miss poorly visible segments, and possess an advantage of not detecting false positives on structured image regions. However, an potential drawback of endpoints-based detectors is that they can produce hard false positives -line segments of high confidence score with no evidence of a true line segment in an image. We believe the reason for hard false positives is a classification error of line segment verification module. An example could be seen in the middle column in <ref type="figure" target="#fig_6">Figure 6</ref>, please note the salient diagonal segments in the bedhead (fourth row) and right part of the carpet (third row). This issue is to be approached prior to successful exploitation of an endpointbased detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>In this study we introduce a fast and accurate line segment detector LSDNet. The detector is composed of a lightweight encoder-decoder CNN, which predicts line segment heatmap and tangent field, and a postprocessing module -a modification of the famous LSD algorithm. When benchmarked on the traditional Wireframe dataset against several SOTA methods, LSDNet shows the highest FPS of 214 -though it achieves detection accuracy of 78 F H -lower than the best methods (82 and 83.1 F H ). However, we speculate that this gap in detection accuracy is primarily caused by the imperfections of the dataset rather than the network itself. We analyze the commonly used line segment detection datasets -Wireframe and York Urban -and point out numerous and significant inconsistencies in their annotation. By carefully reannotating a part of the Wireframe test dataset, we show that (i) all detectors demonstrate better quality on improved annotations (without any retraining), which indicates that the refined annotations correlate better with the notion of correct line segment detection, (ii) the gap between accuracies of our detector and others is reduced to almost non-existent -with our method being the fastest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. ACKNOWLEDGEMENTS</head><p>We would like to thank Marina Tepliakova for making the illustrations; Alexey Savchik and Veniamin Blinov for reviewing the early versions of the manuscript; Dmitry Nikolaev for his strong belief in the power of fusing model-based algorithms with light-weight neural networks, which inspired this work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIGURE 1 :</head><label>1</label><figDesc>.04642v1 [cs.CV] 10 Sep 2022 Overview of the proposed approach. A lightweight neural network predicts line segment mask and tangent vector field, which are then clustered and each cluster is represented as a line segment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIGURE 2 :</head><label>2</label><figDesc>(a) A visualization of the distance function between angles ? 1 , ? 2 . It accounts for the periodicity of angles. (b) Line segment extraction from a region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FIGURE 3 :</head><label>3</label><figDesc>Binarization of CNN prediction. Global binarization joins close regions, while local binarization is noisy for the regions of near-zero intensity. The combined binarization is free of both drawbacks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1 :</head><label>1</label><figDesc>Region grouping algorithm. d M,F (p, R) -distance function between pixel p and region R, N 8 (R) -8-neighbours of region R. input : Line heatmap M ? [0, 1] hw , tangent field F ? [0, ?) hw output: Set of line support regions R = {R} param: ? ? R -distance threshold param: n ? N -minimum region size B ? {0, 1} hw -foreground segmentation of M ;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>FIGURE 4 :</head><label>4</label><figDesc>(a) Original image (b) Wireframe annotation (c) Wireframe-tiny++ annotation An image from Wireframe dataset (a) and two versions of its annotation -original (b) and proposed (c). Best viewed in color and zoom-in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>FIGURE 5 :</head><label>5</label><figDesc>Speed (FPS) and accuracy (F H ) (accuracy) comparisons of different line segment detectors. Size of circles indicates the number of trainable parameters. LSD is marked as cross since it has no trainable parameters. The proposed LSDNet outperforms all previous detectors in speed with 214 FPS and outperforms the nearest fastest counterpart, M-LSDtiny, both in speed and F H quality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>FIGURE 6 :</head><label>6</label><figDesc>Qualitative evaluation. From left to right -images from Wireframe dataset of varying number of salient line segments. From top to bottom -original image, detection results from LSD [8], the model-based detector; HAWP [15], the most accurate CNN-based detector; M-LSD-tiny [20], the fastest CNN-based detector except for the proposed one; the proposed LSDNet. Best viewed in color and zoom-in. In comparison with the nearest fastest counterpart, M-LSD-tiny, LSDNet outperforms it both in quality and speed with the absolute increase in accuracy of +1.0 F H on Wireframe, +2.7 F H on York Urban and +14 FPS speed-up.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 :</head><label>1</label><figDesc>Public datasets.Table 1 and Figure 5 summarize the results on Wireframe and York Urban datasets. It shows that LSDNet achieves state-of-the-art inference speed of 695 FPS for standalone CNN and 214 FPS for overall detector alongside competitive accuracy with 77.5 F H and 64.6 F H on Wireframe and York Urban datasets, correspondingly.</figDesc><table><row><cell>Detection accuracy on datasets Wireframe and</cell></row><row><cell>York Urban. Bold and underlined values stand for top-1 and</cell></row><row><cell>top-2, correspondingly. Column "postproc" shows the speed</cell></row><row><cell>of postprocessing the CNN's prediction.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2 :</head><label>2</label><figDesc></figDesc><table /><note>Detection accuracy on datasets Wireframe, Wireframe-tiny, Wireframe-tiny++ datasets. Bold and under- lined values stand for top-1 and top-2, correspondingly.VOLUME 4, 2016</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">   VOLUME 4, 2016    Author et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">   VOLUME 4, 2016   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">   VOLUME 4, 2016    Author et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">   VOLUME 4, 2016    Author et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">   VOLUME 4, 2016   </note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Andrade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bueno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H Z</forename><surname>Neme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Margraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Omoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Farinelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Tusset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Okida</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A novel strategy for road lane detection and tracking based on a vehicle&apos;s forward monocular camera</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1497" to="1507" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Road lane detection by discriminating dashed and solid road lanes using a visible light camera sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">G</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Vokhidov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">1313</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Linear features observation model for autonomous vehicle localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Shipitko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kibalov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abramov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Power line detection using a circle based search with uav images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ceron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Prieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 International Conference on Unmanned Aircraft Systems (ICUAS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="632" to="639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic road network extraction from uav image in mountain area</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 5th International Congress on Image and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1024" to="1028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The method for homography estimation between two planes based on lines and points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shemiakina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhukovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nikolaev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tenth International Conference on Machine Vision (ICMV 2017)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">10696</biblScope>
			<biblScope unit="page">106961</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Segments graph-based approach for document capture in a smartphone video stream</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhukovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nikolaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Arlazarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Postnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Polevoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Skoryukina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chernov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shemiakina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mukovozov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Konovalenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="337" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lsd: A fast line segment detector with a false detection control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Von Gioi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jakubowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Randall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="722" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Edlines: Real-time line segment detection by edge drawing (ed)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Akinlar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 18th IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2837" to="2840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Connectivity-enforcing hough transform for the robust extraction of line segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Guerreiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Aguiar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4819" to="4829" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Accurate and robust line segment extraction using minimum entropy with hough transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-S</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="813" to="822" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Lsm: perceptually accurate line segment merging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">61620</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Plsd: A perceptually accurate line segment detection approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="42" to="595" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning attraction field representation for robust line segment detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1595" to="1603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Holistically-attracted wireframe parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2788" to="2797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep hough-transform line priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Pintea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.09493</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Tp-lsd: Tri-points based line segment detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.05505</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">End-to-end wireframe parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="962" to="971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Wireframe parsing with guidance of distance map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="141" to="177" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Towards real-time and light-weight line segment detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Go</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.00186</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Yolov4: Optimal speed and accuracy of object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><forename type="middle">M</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10934</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional onestage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to parse wireframes in images of man-made environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="626" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient edge-based methods for estimating manhattan frames in urban imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Elder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Estrada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="197" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Extracting straight lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Riseman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="425" to="455" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Line detection via a lightweight cnn with a hough layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Teplyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kaymakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shvets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nikolaev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.08884</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Lidar r-cnn: An efficient and universal 3d object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15297</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Manhattan world: Orientation and outlier detection by bayesian inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Coughlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1063" to="1088" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Automatic single-image 3d reconstructions of indoor manhattan world scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Delage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics Research</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="305" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for largescale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th symposium on operating systems design and implementation</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Optimize and accelerate machine learning inferencing and training</title>
		<ptr target="https://www.onnxruntime.ai/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Opencv library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<ptr target="https://opencv.org/" />
		<imprint>
			<date type="published" when="2022-04-22" />
		</imprint>
	</monogr>
	<note>Accessed on</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
