<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Entity Disambiguation by Reasoning over a Knowledge Base</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ayoola</surname></persName>
							<email>tayoola@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Amazon Alexa AI Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Fisher</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Amazon Alexa AI Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Pierleoni</surname></persName>
							<email>apierleo@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Amazon Alexa AI Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Entity Disambiguation by Reasoning over a Knowledge Base</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent work in entity disambiguation (ED) has typically neglected structured knowledge base (KB) facts, and instead relied on a limited subset of KB information, such as entity descriptions or types. This limits the range of contexts in which entities can be disambiguated. To allow the use of all KB facts, as well as descriptions and types, we introduce an ED model which links entities by reasoning over a symbolic knowledge base in a fully differentiable fashion. Our model surpasses stateof-the-art baselines on six well-established ED datasets by 1.3 F1 on average. By allowing access to all KB information, our model is less reliant on popularity-based entity priors, and improves performance on the challenging Shad-owLink dataset (which emphasises infrequent and ambiguous entities) by 12.7 F1.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Entity disambiguation (ED) is the task of linking mentions of entities in text documents to their corresponding entities in a knowledge base (KB). Recent ED models typically use a small subset of KB information (such as entity types or descriptions) to perform linking. These models have strong performance on standard ED datasets, which consist mostly of entities that appear frequently in the training data.</p><p>However, ED performance deteriorates for less common entities, to the extent that many recent models are outperformed by outdated feature engineering-based ED systems on datasets that focus on challenging or rare entities <ref type="bibr" target="#b29">(Provatorova et al., 2021)</ref>. This suggests models over-rely on prior probabilities, which are either implicitly learned or provided as features, rather than make effective use of the mention context. One reason for this is that the subset of KB information used by the models is not enough to discriminate between * Tom and Joseph contributed equally to this work. similar entities in all contexts, meaning the model has to fall back on predicting the most popular entity. Another explanation for the performance drop is that less common entities are prone to missing or inconsistent KB information (e.g. they may not have a description), which is problematic for models which rely on a single source of information. To illustrate, we find that 21% of the 25% least popular 1 entities in Wikidata have neither an English description nor any entity type 2 , leaving no mechanism for models which rely on these two sources of information alone to disambiguate them (other than their label). <ref type="bibr">3</ref> Over half of these entities have at least one KB fact (e.g. [Cafe Gratitude], [headquarters location], [San Francisco]); so by including KB facts the percentage of the least popular entities with no information aside from a label drops from 21% to 8%. In light of this, we introduce an ED model which has access to entity types and descriptions, and all KB facts. By using a larger variety of information, our model is more robust to missing KB information, and is able to disambiguate entities in a broader range of contexts without relying on entity priors. <ref type="figure" target="#fig_0">Figure 1</ref> shows an example sentence where there is insufficient information in the entity descriptions and types to disambiguate the mention, Clinton. Fine-grained KB information, such as facts about the birthplace or education of candidate entities, is required.</p><p>To incorporate KB facts, our model begins by reranking candidate entities using descriptions  and predicted entity types <ref type="bibr" target="#b30">(Raiman and Raiman, 2018)</ref>. We then predict, using the document context, the relations which exist between every pair of mentions in the document. For example, given the sentence in <ref type="figure" target="#fig_0">Figure 1</ref>, the model may predict that the [place of birth] relation exists between the mention Clinton and the mention Hope, Arkansas. 4 For this, we introduce a novel "coarseto-fine" document-level relation extraction (RE) module, which increases accuracy and reduces inference time relative to the standard RE approach. Given the relation predictions, we query the KB (Wikidata in our case) for facts which exist between any of the candidate entities for the mention Clinton and for the mention Hope, Arkansas. In this case we would find the Wikidata fact [Bill Clinton], [place of birth], [Hope], and would correspondingly boost the scores of both the [Bill Clinton] and [Hope] entities. We implement this mechanism with the KB stored in a one-hot encoded sparse tensor, which makes the architecture end-to-end differentiable.</p><p>Our model surpasses state-of-the-art (SOTA) baselines on well-established ED datasets by 1.3 F1 on average, and significantly improves performance on the challenging ShadowLink dataset by 12.7 F1. In addition, the model predictions are interpretable, in that the facts used by the model to make predictions are accessible.</p><p>Our contributions are summarised as follows:</p><p>1. We empirically show that using KB facts for ED increases performance above SOTA methods, which generally rely on a single source of KB information.</p><p>coarse-to-fine predictions to obtain competitive accuracy with high efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Recent work on ED has primarily focused on feature-based approaches, whereby a neural network is optimised so that the representation of the correct KB entity is most similar to the mention representation, and each mention is resolved independently. The way in which the KB entities are represented varies between work. Initial work (Ganea and Hofmann, 2017) learned entity embeddings directly from training examples, which performed well for entities seen during training, but could not resolve unseen entities. More recent work improved performance on common datasets by enabling linking to entities unseen during training by using a subset of KB information to represent entities, such as entity descriptions <ref type="bibr" target="#b19">(Logeswaran et al., 2019;</ref><ref type="bibr" target="#b36">Wu et al., 2020)</ref> or entity types <ref type="bibr" target="#b30">(Raiman and Raiman, 2018;</ref><ref type="bibr" target="#b26">Onoe and Durrett, 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">ED with KB context</head><p>Mulang <ref type="bibr">' et al. (2020)</ref> and <ref type="bibr" target="#b2">Cetoli et al. (2019)</ref> incorporate KB facts into ED models by lexicalising KB facts and appending them to the context sentence, then using a cross-encoder model to predict whether the facts are consistent with the sentence. Our model differs from this approach as we resolve entities in the document collectively rather than independently; enabling pairwise dependencies between entity predictions to be captured. Another potential limitation of the cross-encoder method is the high computational cost of encoding the long sequence length of every fact appended to the document context. By accessing KB facts from sparse tensors, we are able to avoid this bottleneck and scale to a larger volume of facts <ref type="bibr" target="#b4">(Cohen et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">ED with knowledge graph embeddings</head><p>Graph neural networks (GNN) have been used to represent KB facts to inform ED predictions <ref type="bibr" target="#b32">(Sevgili et al., 2019;</ref><ref type="bibr" target="#b20">Ma et al., 2021)</ref>. These approaches can potentially access the information in all KB facts, but are reliant on the quality of the graph embeddings, which may struggle to represent many basic semantics <ref type="bibr" target="#b13">(Jain et al., 2021)</ref> particularly for unpopular entities <ref type="bibr" target="#b22">(Mohamed et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Global ED</head><p>There has been a series of papers which aim to optimise the global coherence of entity choices across the document <ref type="bibr" target="#b3">Cheng and Roth, 2013;</ref><ref type="bibr" target="#b24">Moro et al., 2014;</ref><ref type="bibr" target="#b28">Pershina et al., 2015)</ref>. Our model differs from previous approaches in that the model predicts the relations which exist between mentions based on the document text and weights the coherence scores by these predictions, rather than considering coherence independently of document context. We also limit the model to pairwise coherence between mentions as opposed to global coherence for computational efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">ED with multiple modules</head><p>The most similar work to ours is <ref type="bibr" target="#b27">Orr et al. (2021)</ref>, which achieves strong results on tail entities by introducing an ED model which uses entity embeddings, relation embeddings, type embeddings, and a KB module to link entities. A key difference to our model is the way in which KB facts are used for disambiguation. In their work, KB facts are encoded independently of the document context in which the candidate entities co-occur, whereas our model is able to leverage the relevant KB facts for the document context.  <ref type="figure" target="#fig_1">Figure 2</ref> shows a high-level overview of our model. We use a transformer model to encode all mentions in the document in a single-pass. We use these mention embeddings both to generate initial candidate entity scores for each mention using the entity types and descriptions of KB entities and to predict relations between every pair of mentions in the document. We retrieve KB facts for every pair of mentions in the document, for each combination of candidate entities. We weight the retrieved KB facts by multiplying the initial candidate entity score for the subject entity, the predicted score for the relation, and the initial candidate entity score for the object entity. Then we generate KB scores by summing the weighted facts for each candidate entity. The final score used for ranking entities is a weighted sum of the initial score and KB score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Mention representation</head><p>We encode the tokens in the document X using a transformer-based model, giving contextual token embeddings H = {h 1 , h 2 , ..., h N }. <ref type="bibr">5</ref> We obtain mention embeddings m i for each mention m i by average pooling the contextualised token embeddings of the mention from the final transformer layer. This allows all mentions M in the document X to be encoded in a single forward pass.</p><p>3.4 Initial entity score ? a Initially, we score candidate entities using entity typing and description scores. We combine the two with a learned weighted sum ? a :</p><formula xml:id="formula_0">? a (c ik ) = w 1 ? t (c ik ) + w 2 ? d (c ik )<label>(1)</label></formula><p>where c ik is the mention-entity pair (m i , e k ), ? t is a scoring function based on candidate entity types, and ? d is a scoring function based on candidate entity descriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Entity typing score ? t</head><p>We construct a fixed set of types T = {(r, o) ? R ? E} by taking relation-object pairs (r, o) from the KB G; for example (instance of, song). We predict an independent unnormalised score for each type t ? T for every mention in the document by applying a linear layer FF 1 to the mention embedding m i . To compute entity scores ? t using the predicted types, we calculate the dot product between the predicted types and the candidate entity's types binary vector t k . 6 Additionally, we add a P (e|m) (PEM score) which expresses the probability of an entity given the mention text only, and is obtained from hyperlink count statistics as in previous work <ref type="bibr" target="#b30">(Raiman and Raiman, 2018)</ref>:</p><formula xml:id="formula_1">? t (c ik ) = (FF 1 (m i ) ? t k ) + P (e k |m i )<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Entity description score ? d</head><p>We use a bi-encoder architecture similar to  but altered to encode all mentions in a sequence in a single forward pass, as opposed to requiring one forward pass per mention. We represent KB entities as:</p><formula xml:id="formula_2">[CLS] label [SEP] description [SEP]</formula><p>where "label" and "description" are the tokens of the entity label and entity description in the KB. We refer to this as d k . To compute entity scores ? d using entity descriptions, we use a separate transformer model T R 1 to encode d k , taking the final layer embedding for the <ref type="bibr">[CLS]</ref>, and calculate the dot product between this embedding and the contextual mention embedding m i projected by linear layer FF 2 : <ref type="bibr">6</ref> We use 1 to indicate the presence of an entity type and 0 the absence of an entity type for our binary vector. We also follow this convention for the KB facts binary vector.</p><formula xml:id="formula_3">? d (c ik ) = FF 2 (m i ) ? TR 1 (d k )<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Relation extraction</head><p>Our relation extraction layer outputs a relation score vectorr ij ? R |R| for each pair of mentions m i and m j in the document, where R is the subset of relations chosen from the KB. To calculater ij we begin by passing m i and m j through a bilinear layer B with output dimension 1, to predict the likelihoodr coarse ij that a relation exists between mentions m i and m j .</p><formula xml:id="formula_4">r coarse ij = ?(B(m i , m j ))<label>(4)</label></formula><p>Note thatr coarse ij is a scalar, denoting the likelihood that any relation exists between mention m i and m j . We then take the top-k mention pairs with the highest values ofr coarse ij (in similar style to <ref type="bibr" target="#b17">Lee et al. (2018)</ref> who introduce a coarse-to-fine approach for coreference resolution), illustrated with K = 2 in <ref type="figure" target="#fig_3">Figure 3</ref>. These are the pairs of mentions which the model predicts have the highest likelihood of having a relation connecting them. For the surviving mention pairs, we pass each of the two mention embeddings individually through a linear layer, FF 3 , to reduce their dimension by a factor of two. This ensures that when we concatenate the two representations back together we get a representation of the mention pair m * ij of the same dimension as the contextual token embeddings H.</p><formula xml:id="formula_5">m * ij = concat(FF 3 (m i ), FF 3 (m j )) (5)</formula><p>We then pass the resulting embedding m * ij through a series of transformer layers TR 2 , where they can attend to the contextual embeddings of the original input tokens,</p><formula xml:id="formula_6">H = {h 1 , h 2 , ..., h N }.</formula><p>The mention-pair embeddings from the final transformer layer are passed through a linear layer FF 4 with output dimension |R| to give the score that each relation exists between this mention-pair,</p><formula xml:id="formula_7">r fine ij .r fine ij = FF 4 (TR 2 (m * ij , H)))<label>(6)</label></formula><p>Finally, to getr ij we multiply the coarse layer scorer coarse ij with the fine layer scorer fine ij , ensuring that gradients are propagated through the coarse layer during training, despite only the top-k mention pairs being passed to the fine layer.</p><formula xml:id="formula_8">r ij =r coarse ij * r fine ij (7)</formula><p>For all mention pairs outside the top-k pairs, we setr ij to a vector of 0s.</p><p>The relation extraction layer is trained end-toend using the signal from the entity disambiguation loss only, and is not pretrained with any taskspecific relation extraction data. To validate the effectiveness of the architecture, we include results with the RE module trained in isolation on the DO-CRED RE dataset in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">KB score ? b</head><p>We retrieve KB facts 7 for every mention-entity pair in the document and represent it as a 5-dimensional tensor r, where r ij,kn is a binary vector indicating the relations that exist in the KB between the two entities (e k and e n ) for mention-entity pair c ik and c jn . <ref type="bibr">8</ref> We weight KB facts r based on initial entity scores ? a and relation predictionsr, according their relevance to the document. To compute the KB score ? b for a mention-entity pair, we sum KB facts where the entity (from the mention-entity pair) is the subject entity to give score ? s and sum 7 Facts are efficiently retrieved by indexing into a sparse tensor. <ref type="bibr">8</ref> The dimensions of tensor r are: [n_mentions (M), n_mentions (M), n_candidates, n_candidates, n_relations (R)] the KB facts where the entity is the object entity to give score ? o :</p><formula xml:id="formula_9">?s(c ik ) =?a(c ik ) j?|M | j=1 n?|E| n=1 (r ij ? r ij,kn )?a(cjn) (8) ?o(c ik ) =?a(c ik ) j?|M | j=1 n?|E| n=1 (r ji ? r ji,nk )?a(cjn) (9)</formula><p>where? a is the initial entity scoring function ? a followed by the softmax function applied over the candidate entities for the given mention. We then combine the two scores with a weighted sum giving ? b :</p><formula xml:id="formula_10">? b (c ik ) = w 3 ? s (c ik ) + w 4 ? o (c ik )<label>(10)</label></formula><p>Note that for computational efficiency, this scoring mechanism considers the coherence of entity predictions between pairs of mentions only, in contrast to methods which consider global coherence .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Optimisation and inference</head><p>To obtain final entity scores ? f , we add the KB scores ? b to the initial entity scores ? a .</p><formula xml:id="formula_11">? f (c ik ) = ? a (c ik ) + ? b (c ik )<label>(11)</label></formula><p>We train our model on entity linked documents using cross-entropy loss. Our model is fully differentiable end-to-end, with the training signal propagating through all modules, including the relation extraction module. During ED inference, we take the candidate entity with the highest final entity score for each mention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Standard ED</head><p>We evaluate our model on the following wellestablished standard ED datasets: AIDA-CoNLL <ref type="bibr">), MSNBC (Cucerzan, 2007</ref>, AQUAINT <ref type="bibr" target="#b21">(Milne and Witten, 2008)</ref>, ACE2004 <ref type="bibr" target="#b31">(Ratinov et al., 2011)</ref>, CWEB <ref type="bibr" target="#b8">(Gabrilovich et al., 2013)</ref> and WIKI <ref type="bibr" target="#b10">(Guo and Barbosa, 2018)</ref>. We train our model on Wikipedia hyperlinks and report InKB micro-F1 (which only considers entities with a non-NIL entity label). To ensure fair comparisons to baselines, we use the same method to generate candidates as previous work <ref type="bibr" target="#b1">(Cao et al., 2021;</ref><ref type="bibr" target="#b15">Le and Titov, 2018)</ref>. Concretely, we use the top-30 entities based on entity priors (PEM) obtained by mixing hyperlink count statistics from Wikipedia hyperlinks, a large Web corpus, and YAGO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Long-tail and ambiguous ED</head><p>We use the ShadowLink ED dataset <ref type="bibr" target="#b29">(Provatorova et al., 2021)</ref> to evaluate our model on long-tail and ambiguous examples. 9 The dataset consists of 3 subsets. SHADOW where the correct entity is overshadowed by a more popular entity; TOP where the correct entity is the most popular entity; and TAIL where the correct entity a long-tail entity. <ref type="bibr">10</ref> All examples in SHADOW and TOP are ambiguous, whereas TAIL has some unambiguous examples, as it is a representative sample of long-tail entities. The original dataset consists of short text snippets from Web pages, which often only include one or two mentions of entities. This limits the ability of our model to use its document-level RE module, and reason over the relationships between entities. We therefore also evaluate on the full-text version of the SHADOW and TOP subsets, referred to as SHADOW-DOC and TOP-DOC in the results tables. <ref type="bibr">11</ref> The dataset consists of 1 annotated entity per document, so we run spaCy ("en_core_web_lg" model) <ref type="bibr">(Honnibal and Montani, 2017)</ref> to identify additional mentions to allow our model and baselines to utilise other mentions to disambiguate the annotated entity mention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model details</head><p>We use Wikidata (July 2021) as our KB, restricted to entities with a corresponding English Wikipedia page. This results in 6.2M entities. We use this data to generate lookups for entity types, entity descriptions, and KB facts. We select a fixed set of 1400 relation-object pairs, based on usefulness for disambiguation, to use as our entity types (Appendix A). For the KB facts, we represent the top 128 relations as separate classes and collapse the remaining relations into a single class we refer to as OTHER. Additionally, we add a special relation which exists between every entity and itself. We refer to this relation as the SAME AS relation, and the idea behind this is to enable the model to implicitly learn coreference resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Training details</head><p>We use Wikipedia hyperlinks (July 2021) with additional weak labels as our training dataset, which consists of approximately 100M labelled mentions. We limit candidate generation to top-30 entities based on entity priors obtained from Wikipedia hyperlink statistics. <ref type="bibr">12</ref> Our model operates at the document-level and is trained using multiple mentions simultaneously. We initialise the mention embedding Transformer model weights from the RoBERTa  model and train our model for 1M steps with a batch size of 64 and a maximum sequence length of 512 tokens. This requires approximately 4 days when using 8 V100 GPUs. For additional details, see Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Standard ED</head><p>The results in <ref type="table">Table 1</ref> show our model (KBED) achieves the highest average performance across the datasets by a margin of 1.3 F1, reducing errors by 11.5%. The ablation results indicate the majority of the improvements across the datasets are attributable to our novel KB module. We observe the largest improvement of 3.0 F1 on the WIKI dataset, which is likely due to the documents having high factual density, enabling our model to leverage more KB facts (see Section 6.1 for relation analysis). Despite our model only be trained on Wikipedia, we obtain competitive results on outof-domain datasets, such as MSNBC news articles, which implies the patterns learned from Wikipedia are applicable to other domains. In addition, the results demonstrate that our 3 modules (entity typing, entity descriptions, and KB facts) are complementary; when any module is used in isolation it reduces performance, demonstrating the benefits of a multifaceted approach to ED. Surprisingly, when our KB module is used in isolation it performs on par with the TagMe baseline, which suggests there is reasonable overlap between KB facts and the facts predicted from documents. Note that the AIDA results in <ref type="table">Table 1</ref> contain a mixture of models fine-tuned on this dataset (denoted with **) and trained on Wikipedia only (as in our case), so the numbers are not directly comparable. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Long-tail and ambiguous ED</head><p>Our model achieves an average F1 score of 70.1 on the original ShadowLink dataset <ref type="table" target="#tab_2">(Table 2)</ref> which substantially outperforms (+16.5 F1) embeddingsbased models (GENRE, REL) and moderately outperforms (+4.0 F1) the Bootleg model <ref type="bibr" target="#b27">(Orr et al., 2021)</ref> which is optimised for tail-performance and also uses entity types and KB facts. On the original dataset, the impact of our KB module is negligible because the limited document context reduces the chances of KB-related entities co-occurring; the strong performance is therefore largely due to the combination of entity types and descriptions. However, we see a notable average improvement of 12.7 F1 on the document-level version of the dataset, with the KB module having a considerable impact especially on the overshadowed entity subset where it contributes 6.7 F1. The performance margin between our model and Bootleg is greater when document-level context is used likely because Bootleg is designed for short contexts and has limited control over which KB facts to use for disambiguation, as all facts are weighted uniformly. We include a more extensive model ablation study in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Relation extraction module</head><p>To analyse the impact of the doc-level RE architecture introduced in Section 3.5 we present results in <ref type="table" target="#tab_2">Tables 1 and 2</ref> of performance with a standard bilinear RE layer <ref type="bibr" target="#b37">(Xu et al., 2021)</ref>. Our RE archi-tecture leads to an average increase of 0.9 F1 on the standard ED datasets, of 1.3 F1 on the standard ShadowLink splits, and of 1.5 F1 on the Shad-owLink doc-level splits. In addition, by avoiding the quadratic complexity bilinear layer, we achieve an increase in inference speed of approximately 2x, as measured on AIDA documents. We include doc-level RE results for our architecture on the DOCRED <ref type="bibr" target="#b40">(Yao et al., 2019)</ref> dataset in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Error Analysis</head><p>In   The results in <ref type="table" target="#tab_1">Table 3</ref> indicate that the largest source of error is the gold entity not being present in the top-30 candidates. This is particularly true for the ShadowLink SHADOW-DOC split, as this split contains a larger number of tail entities which are less likely to be mentioned on Wikipedia. For the AIDA dataset, there are also many cases which are in some sense ambiguous. 14 There are 8 cases in total where the model predicts a relation which it expects to be in the KB, but which is not in fact present. This is largely in the ShadowLink split, where tail entities are likely to be less well represented in Wikidata. The model is generally good at not depending on entity priors; despite every gold candidate in the Shadowlink SHADOW-DOC split being "overshadowed" by a more popular entity in the PEM table, there is only one example where the model fails to override this. Although the model often "over-predicts" relations between mentions, it rarely gets penalised for doing so, as in general the so the sum of the rows will not necessarily be 50. 14 These are often cases with national sports teams, such as "Little will miss Australia's fixture..." where "Australia" could refer to the country Australia or the Australian rugby team. extra facts it predicts are not in the KB, meaning the Incorrect RE pred. count is low.</p><p>To further explore the role of missing candidates, <ref type="table">Table 4</ref> shows the percentage of the gold entities present in the top-30 candidates we pass to the model, representing a hard upper-bound on the recall our model can achieve. The results vary from a high coverage of 99.5 for the MSNBC dataset, which largely contains head entities, to a lower coverage for the ShadowLink SHADOW (75.3) and TOP (83.6) splits. <ref type="table">Table 4</ref> also shows the coverage if we pass all PEM candidates to the model. For some datasets, such as WIKI, this increases the coverage significantly. However, for the ShadowLink SHADOW split, the coverage is still below 80%, indicating that better candidate generation strategies are an interesting avenue for future research.  <ref type="table">Table 4</ref>: Percentage of gold entities in top-n candidates by dataset. We set n=30 for this paper.</p><p>6 Analysis</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Relation predictions</head><p>To understand the relations which the model utilises to make predictions, <ref type="table" target="#tab_6">Table 5</ref> displays for the WIKI dataset the number of KB (Wikidata) facts which exist between gold annotated mentions in the documents (Gold), the number of facts between men-tions our model predicts with a score above 0.5 (Predicted) and the percentage of gold facts which our model also predicts (Recall). <ref type="bibr">15</ref> The SAME AS relation is used extensively by the model, demonstrating that using coreferences to other (potentially easier to disambiguate) mentions of the same entity in the document is a powerful addition for ED. We leave evaluation of the model on the coreference-specific task to future work. The OTHER relation is also commonly predicted, suggesting the long tail of relations in Wikidata still hold useful information. The other widely used relations are generally either geographical or sports related, which is expected given the large number of sports entities in Wikidata.</p><p>The recall numbers appear low, although this is expected behaviour in that the existence of a Gold fact does not necessarily imply that the text in the document infers this fact.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We presented a novel ED model, which achieves SOTA performance on well-established ED datasets by a margin of 1.3 F1 on average, and by 12.7 F1 on the challenging ShadowLink dataset. These results were achieved by introducing a method to incorporate large symbolic KB data into an ED model in a fully differentiable and scalable fashion. Our analysis shows that better candidategeneration strategies are an interesting avenue for future research, if results are to be pushed higher on ambiguous and tail entities. Dynamic expansion of the KB by incorporating facts identified by the ED model is also a potentially promising direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Entity Type Selection</head><p>Our entity types are formed from direct Wikidata relation-object pairs and relation-object pairs inferred from the Wikidata subclass hierarchy; for example, (instance of, geographical area) can be inferred from (instance of, city). We only consider types with the following relations: instance of, occupation, country, and sport. We select types by iteratively adding types that separate (assuming an oracle type classifier) the gold entity from negative candidates for the most examples in our Wikipedia training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Training Details</head><p>We use the Hugging Face implementation of RoBERTa <ref type="bibr" target="#b34">(Wolf et al., 2019)</ref>     . The result is in red when the performance drops by more than 0.7.</p><p>of our model that we measured have a positive impact on performance. Interestingly, the KB module (+2.2 F1) has a greater impact than the entity description (+0.48 F1) and entity typing (+0.9 F1) modules despite weaker performance when used on its own <ref type="table">(Table 1)</ref>. This implies there is less overlap between examples where KB module performs well, and the other modules perform well. We observe, the SAME AS relation improves performance by 0.72 F1, which demonstrates that using coreference improves ED. Finally, we find that when the KB module has greater control over how to weight KB facts (based on the context) it leads to better results, for example if we collapse all standard relations into a single relation our performance drops by 0.7 F1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Doc-level RE results on DOCRED</head><p>To verify the performance of our document-level RE architecture introduced in Section 3.5, we present results of models trained and evaluated on the DOCRED dataset <ref type="bibr" target="#b40">(Yao et al., 2019)</ref>. Our baseline implementation uses roberta-base as an encoder and a bilinear output layer. We show two variants in <ref type="table" target="#tab_11">Table 8</ref>, a bilinear layer with input dimension 128 and with input dimension 256, which give an F1 score of 57.8 and 58.4 respectively. This compares to a score of 59.5 for an equivalent baseline implemented in <ref type="bibr" target="#b37">(Xu et al., 2021)</ref>. The difference is explained by our baseline not giving the model access to the gold coreference information, which is allowed in the DOCRED task but which we exclude as it will not be available for our entity linking task.   Our coarse-to-fine approach, with 4 "fine" transformer layers, pushes the dev-level F1 up by 2.8 F1 to 61.2. This puts it slightly above the roberta-base version of the current state-of-the-art model, SSAN <ref type="bibr" target="#b37">(Xu et al., 2021)</ref>, which scores 60.9, and additionally has access to the gold coreference labels in the embedding layer of the model. This validates that our document-level RE architecture is capable of producing accurate relation predictions, which we see in the main results table <ref type="table">(Table 1)</ref> also translates into stronger ED performance.</p><p>By avoiding the bilinear layer, our implementation is also faster to train, achieving 106.2 seconds per epoch on the DOCRED dataset on a single Tesla V100 GPU, compared to 155.7 seconds for the baseline model with a 128-dimension bilinear layer, and 343.2 seconds for the more accurate baseline model with a 256 dimension bilinear layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Dataset details E.1 Dataset statistics</head><p>We present the topic, number of documents and number of mentions for each dataset used for evaluation <ref type="table" target="#tab_13">(Table 9</ref>). The datasets used cover a variety of sources including wikipedia text, news articles, web text and tweets. Note that the performance of the model outside these domains may be significantly different.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 ShadowLink Full Text versions</head><p>The authors of <ref type="bibr" target="#b29">Provatorova et al. (2021)</ref> kindly provided us with the full documents from which the shorter text snippets (usually one or two sentences) in the ShadowLink dataset were sourced. We were able to match 596 of the 904 examples in the SHADOW split to its corresponding document, and 530 out of the 904 examples in the TOP split. As some full articles were extremely long we limited the document-length to 10000 characters, centred around the single annotated entity. To validate that the subset of examples we were able to match to full documents were representative of the original dataset splits, we ran our model on the sentence-level versions of these subsets, achieving 47.7 on the SHADOW split (comparable to 47.6 in <ref type="table" target="#tab_2">Table 2</ref>) and 63.9 on the TOP split (comparable to 64.2 in <ref type="table" target="#tab_2">Table 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Additional relation analysis</head><p>To expand on the analysis in Section 6.1 we also include the number of gold and predicted relations in documents in the AIDA dataset <ref type="table">(Table  10</ref>). The first clear difference is that there is a far higher count of gold SAME AS facts in the AIDA dataset, which is potentially explained by pages on Wikipedia generally having hyperlinks for the first mention of an entity only.</p><p>It is also interesting to note that there are lower recall numbers for the AIDA dataset relative to WIKI <ref type="table" target="#tab_6">(Table 5)</ref>, indicating that the RE module may have "overfit" in some sense to the Wikipedia style of article, and may be less effective on AIDA style news articles.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Example of a sentence where fine-grained KB information is required for entity disambiguation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Our model architecture shown for a document with two mentions, England and 1966 World Cup. The model disambiguates all entity mentions in a single pass; making use of the KB facts connecting the candidates of each mention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Given a document X with mentions, M = {m 1 , m 2 , ...m |M | }, a KB with a set of facts G = {(s, r, o) ? E ? R ? E} which express relations r ? R between subject s ? E and object entities o ? E, and a description d k for each KB entity e k , the goal of ED is to assign each mention m ? M the correct corresponding KB entity e ? E.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>The model component for document-level relation extraction.r coarse ij denotes the predicted probability that any relation exists between mentions i and j. R denotes the number of relations we include in the model -set to 2 in the Figure for illustration purposes only.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>F1</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3</head><label>3</label><figDesc>we show the results from annotating 50 examples in which the model made an incorrect prediction for both the AIDA test split and the ShadowLink SHADOW-DOC split. Gold not in cands. refers to cases in which the gold entity was not in the top-30 candidates from the PEM table;Missing KB fact are cases where the model correctly predicted a relation connecting two mentions, but the corresponding fact was not in the KB; Dominant PEM is when the initial PEM score for one candidate was high (&gt; 0.8), and the model fails to override this score; Incorrect RE pred. are cases in which the model makes an incorrect RE prediction between two mentions, and where this wrong prediction leads to the wrong choice of entity; Ambiguous ann. refers to gold annotations that are either incorrect or ambiguous. 13 Method SHADOW TOP TAIL AVG SHADOW-DOC TOP-DOC DOC-AVG</figDesc><table><row><cell>AIDA (Yosef et al., 2011)</cell><cell>35</cell><cell>56</cell><cell>67</cell><cell>52.7</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TagMe 2 (Ferragina and Scaiella, 2012)</cell><cell>29</cell><cell>57</cell><cell>83</cell><cell>56.3</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GENRE  *  (Cao et al., 2021)</cell><cell>26</cell><cell>42</cell><cell>93</cell><cell>53.7</cell><cell>40.9</cell><cell>59.2</cell><cell>50.1</cell></row><row><cell>REL (van Hulst et al., 2020)</cell><cell>21</cell><cell>54</cell><cell>91</cell><cell>55.3</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Bootleg  *  (Orr et al., 2021)</cell><cell>44.5</cell><cell>60.0</cell><cell>93.7</cell><cell>66.1</cell><cell>46.9</cell><cell>62.7</cell><cell>54.8</cell></row><row><cell>KBED</cell><cell>47.6</cell><cell>64.2</cell><cell>98.5</cell><cell>70.1</cell><cell>60.8</cell><cell>74.2</cell><cell>67.5</cell></row><row><cell></cell><cell></cell><cell cols="2">Model Ablations</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>w/o KB</cell><cell>46.4</cell><cell>64.2</cell><cell>98.3</cell><cell>69.6</cell><cell>54.1</cell><cell>72.0</cell><cell>63.1</cell></row><row><cell>KB only</cell><cell>26.9</cell><cell>45.7</cell><cell>98.4</cell><cell>57.0</cell><cell>41.9</cell><cell>60.0</cell><cell>51.0</cell></row><row><cell>Entity descriptions only</cell><cell>42.1</cell><cell>54.7</cell><cell>97.8</cell><cell>64.9</cell><cell>52.6</cell><cell>65.0</cell><cell>58.8</cell></row><row><cell>Entity types only</cell><cell>39.6</cell><cell>55.6</cell><cell>98.5</cell><cell>64.6</cell><cell>47.3</cell><cell>62.1</cell><cell>54.7</cell></row><row><cell>Bilinear RE layer</cell><cell>47.1</cell><cell>61.5</cell><cell>97.7</cell><cell>68.8</cell><cell>59.2</cell><cell>72.7</cell><cell>66.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Entity disambiguation InKB micro F1 scores on ShadowLink test sets. SHADOW-DOC and TOP-DOC refers to the extended version of the dataset which includes the full-text of the document to use as additional context. The best value is bold. * We produced results using the code released by the authors.</figDesc><table><row><cell></cell><cell cols="2">AIDA SHADOW-DOC</cell></row><row><cell>Gold not in cands.</cell><cell>18</cell><cell>32</cell></row><row><cell>Missing KB fact</cell><cell>2</cell><cell>6</cell></row><row><cell>Dominant PEM</cell><cell>0</cell><cell>1</cell></row><row><cell>Incorrect RE pred.</cell><cell>1</cell><cell>2</cell></row><row><cell>Ambiguous ann.</cell><cell>24</cell><cell>4</cell></row><row><cell>Total</cell><cell>50</cell><cell>50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Counts per error category from 50 annotations on AIDA-CoNLL and ShadowLink-Shadow datasets.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: Analysis of relation predictions for WIKI</cell></row><row><cell>dataset with threshold 0.5. 320 documents with 6772</cell></row><row><cell>entity mentions.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Our model hyperparameters. this section, we measure the contribution of key aspects of our model. For each model ablation, we train our model from scratch on the AIDA-CoNLL training set and evaluate on the development set, keeping hyperparameters constant. Surprisingly, the performance of our model is strong in this limited data setting, which means that our model is not</figDesc><table><row><cell>In</cell></row></table><note>C Model Ablation Study</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>ED F1 score on AIDA-CoNLL development split for model ablations trained from scratch on AIDA-CoNLL training split using the standard CoNLL candidates</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Document-level relation extraction F1 scores on the DOCRED dev dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>Dataset statistics for entity disambiguation datasets.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We use the number of KB facts where the entity is the subject entity as a proxy for popularity, and only consider entities with an English Wikipedia page.2 See for example Q5017238.3  Conversely, 100% of the 25% most popular entities in Wikidata have either a description or type.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">. We introduce a scalable method of incorporating symbolic information into a neural network ED model. To our knowledge, this is the first time an end-to-end differentiable symbolic KB has been used for ED.3. We introduce a novel document-level relation extraction (RE) architecture which uses4  We use square brackets to denote relations and entities in the KB, and italics to represent mentions in the input text.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We use bold letters for vectors throughout our paper.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">A long-tail entity is an entity that is linked to less than 56 times from other Wikipedia pages.10  E.g. if the candidates and PEM scores for the mention England were ([England (country)], 0.92) and ([England football team], 0.08) then [England (country)] would be a TOP entity, and [England football team] would be a shadow entity. 11 Details in Appendix E.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">We add weak labels by labelling spans that match the title of the page with the entity for the page.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13">Note that some examples may contain more than one source of error (or contain an error not clearly in any category),</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15">Note that as the RE predictions are continuous, the quantity of facts our model predicts depends entirely on the choice of this threshold.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Vera Provatorva for providing us with the extended version of the ShadowLink dataset and Laurel Orr for assisting us with running the Bootleg baseline.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Inference Speed and Scalability</head><p>We measure the time taken to run inference on the AIDA-CoNLL test dataset and compare it to SOTA baselines. <ref type="table">Table 11</ref> shows the results alongside the average ED performance on the 6 standard ED datasets (used in <ref type="table">Table 1</ref>). Our model is an order of magnitude faster than the baselines with comparable ED performance.  The most computationally expensive part of our model (accounting for approximately 80% of the inference and training time) is computing the KB score due to the large number of pairwise interactions present in documents. The hyperparameter for coarse-to-fine relation extraction can be lowered to trade-off computation cost with ED performance by reducing the number of pairwise interactions. Alternatively, as computation of the initial entity score ? a is computationally cheap relative to the KB score ? b , candidate entities with low initial entity scores can be pruned to further increase training and/or inference speed. These approaches would also allow scaling of the initial number of candidate entities to more than the 30 used for inference in this paper, if the use case required it.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Petroni</surname></persName>
		</author>
		<idno>abs/2010.00904</idno>
	</analytic>
	<monogr>
		<title level="j">Autoregressive entity retrieval. ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Autoregressive entity retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A neural approach to entity linking on wikidata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Cetoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Bragaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">D</forename><surname>O&amp;apos;harney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Sloan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Akbari</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-15719-7_10</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="78" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Relational inference for wikification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1787" to="1796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scalable neural methods for reasoning with a symbolic knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Alex</forename><surname>Hofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Siegler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large-scale named entity disambiguation based on Wikipedia data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silviu Cucerzan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="708" to="716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Joint entity linking with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3308558.3313517</idno>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference, WWW &apos;19</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="438" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast and accurate annotation of short texts with wikipedia pages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Ferragina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ugo</forename><surname>Scaiella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Software</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="70" to="75" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Facc1: Freebase annotation of clueweb corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ringgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amarnag</forename><surname>Subramanya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep joint entity disambiguation with local neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugen</forename><surname>Octavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hofmann</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1277</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2619" to="2629" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Robust named entity disambiguation with random walks. Semantic Web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaochen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denilson</forename><surname>Barbosa</surname></persName>
		</author>
		<idno type="DOI">10.3233/sw-170273</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="21" />
			<pubPlace>Preprint(Preprint</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robust disambiguation of named entities in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">Amir</forename><surname>Yosef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilaria</forename><surname>Bordino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hagen</forename><surname>F?rstenau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Spaniol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilyana</forename><surname>Taneva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Edinburgh, Scotland, UK. Asso</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="782" to="792" />
		</imprint>
	</monogr>
	<note>ciation for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">2017. spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ines</forename><surname>Montani</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Do embeddings actually capture knowledge graph semantics?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitisha</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Christoph</forename><surname>Kalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolf-Tilo</forename><surname>Balke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Krestel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighteenth Extended Semantic Web Conference -Research Track</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving entity linking by modeling latent relations between mentions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1148</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1595" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Boosting entity linking performance by leveraging unlabeled documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1187</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1935" to="1945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Higher-order coreference resolution with coarse-tofine inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>abs/1804.05392</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>abs/1907.11692</idno>
	</analytic>
	<monogr>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Zero-shot entity linking by reading entity descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1335</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3449" to="3460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A knowledge graph entity disambiguation method based on entity-relationship embedding and graph structure embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangtao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duanyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonggang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaqiong</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haodong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuncai</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Intell. Neurosci</title>
		<imprint>
			<biblScope unit="page">2878189</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to link with wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Milne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;08</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Popularity agnostic evaluation of knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aisha</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shameem</forename><surname>Parambath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoi</forename><surname>Kaoudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashraf</forename><surname>Aboulnaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Conference on Uncertainty in</title>
		<meeting>the 36th Conference on Uncertainty in</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
				<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">of Proceedings of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="page" from="1059" to="1068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Entity linking meets word sense disambiguation: a unified approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Moro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Raganato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00179</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="231" to="244" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Evaluating the impact of knowledge graph context on entity disambiguation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaiah</forename><surname>Onando Mulang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;apos;</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuldeep</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitali</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Nadgeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<idno type="DOI">10.1145/3340531.3412159</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 29th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasumasa</forename><surname>Onoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020-02-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="8576" to="8583" />
		</imprint>
	</monogr>
	<note>The Thirty-Fourth AAAI Conference on Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bootleg: Chasing the tail with self-supervised named entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurel</forename><forename type="middle">J</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Megan</forename><surname>Leszczynski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neel</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simran</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
		<ptr target="ings.www.cidrdb.org" />
	</analytic>
	<monogr>
		<title level="m">11th Conference on Innovative Data Systems Research, CIDR 2021, Virtual Event</title>
		<imprint>
			<date type="published" when="2021-01-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Personalized page rank for named entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Pershina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Robustness evaluation of entity disambiguation using prior probes: the case of entity overshadowing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vera</forename><surname>Provatorova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samarth</forename><surname>Bhargav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svitlana</forename><surname>Vakulenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Online and Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10501" to="10510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deeptype: Multilingual entity linking by neural type system evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Raiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Raiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Local and global algorithms for disambiguation to Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1375" to="1384" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improving neural entity disambiguation with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?zge</forename><surname>Sevgili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-2044</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="315" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rel: An entity linker standing on the shoulders of giants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><forename type="middle">M</forename><surname>Van Hulst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faegheh</forename><surname>Hasibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koen</forename><surname>Dercksen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krisztian</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjen</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
		</author>
		<idno type="DOI">10.1145/3397271.3401416</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;20</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2197" to="2200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R&amp;apos;emi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Brew</surname></persName>
		</author>
		<idno>abs/1910.03771</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Zero-shot entity linking with dense entity retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Josifoski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>abs/1911.03814</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scalable zeroshot entity linking with dense entity retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Josifoski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.519</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6397" to="6407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Entity structure within and throughout: Modeling mention dependencies for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
		<idno>abs/2102.10249</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning dynamic context augmentation for global entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1026</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="271" to="281" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Collective entity disambiguation with structured gradient tree boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazi Shefaet</forename><surname>Rahman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1071</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="777" to="786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Docred: A large-scale document-level relation extraction dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1906.06127</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Aida: An online tool for accurate disambiguation of named entities in text and tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Amir Yosef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilaria</forename><surname>Bordino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Spaniol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
		<idno type="DOI">10.14778/3402755.3402793</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1450" to="1453" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
