<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">COTS: Collaborative Two-Stream Vision-Language Pre-Training Model for Cross-Modal Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Beijing Key Laboratory of Big Data Management and Analysis Methods</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyi</forename><surname>Fei</surname></persName>
							<email>feinanyi@ruc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Huo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhao</forename><surname>Gao</surname></persName>
							<email>gaoyizhao@ruc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
							<email>luzhiwu@ruc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Beijing Key Laboratory of Big Data Management and Analysis Methods</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
							<email>jrwen@ruc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Beijing Key Laboratory of Big Data Management and Analysis Methods</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">COTS: Collaborative Two-Stream Vision-Language Pre-Training Model for Cross-Modal Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large-scale single-stream pre-training has shown dramatic performance in image-text retrieval. Regrettably, it faces low inference efficiency due to heavy attention layers. Recently, two-stream methods like CLIP and ALIGN with high inference efficiency have also shown promising performance, however, they only consider instance-level alignment between the two streams (thus there is still room for improvement). To overcome these limitations, we propose a novel COllaborative Two-Stream vision-language pretraining model termed COTS for image-text retrieval by enhancing cross-modal interaction. In addition to instancelevel alignment via momentum contrastive learning, we leverage two extra levels of cross-modal interactions in our COTS: (1) Token-level interaction -a masked visionlanguage modeling (MVLM) learning objective is devised without using a cross-stream network module, where variational autoencoder is imposed on the visual encoder to generate visual tokens for each image. (2) Task-level interaction -a KL-alignment learning objective is devised between text-to-image and image-to-text retrieval tasks, where the probability distribution per task is computed with the negative queues in momentum contrastive learning. Under a fair comparison setting, our COTS achieves the highest performance among all two-stream methods and comparable performance (but with 10,800? faster in inference) w.r.t. the latest single-stream methods. Importantly, our COTS is also applicable to text-to-video retrieval, yielding new state-ofthe-art on the widely-used MSR-VTT dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The pretrain-then-finetune paradigm has achieved great success in the field of natural language processing (NLP), where models are first pre-trained with large-scale data (e.g., BERT <ref type="bibr" target="#b9">[10]</ref>, RoBERTa <ref type="bibr" target="#b29">[30]</ref>, and GPT3 <ref type="bibr" target="#b4">[5]</ref>) and then finetuned for each downstream task. Recently, this prac-* The corresponding author. tice has also shown its effectiveness in the vision-language (VL) domain <ref type="bibr">[9, 17-19, 28, 37, 52]</ref>, where the performance on various VL tasks (e.g., image-text retrieval, video-text retrieval, and visual question answering) has been significantly improved by vision-language pre-training (VLP). VLP models typically take huge image-text pairs as input and aim to learn joint image-text representations with single-and cross-modal pre-training objectives, such as masked token prediction and image-text matching.</p><p>Existing VLP models can be divided into two groups: single-stream models and two-stream ones. Single-stream VLP models (see <ref type="figure">Figure 1</ref>(a)) often utilize cross-modal fusion modules (e.g., Transformer <ref type="bibr" target="#b42">[43]</ref> layers) to model fine-grained interactions between image regions and text words. Although these models achieve promising performance, they have two limitations: <ref type="bibr" target="#b0">(1)</ref> During inference, all possible query-candidate pairs need to be fed into the fusion modules to calculate similarity scores, resulting in huge computational cost. <ref type="bibr" target="#b1">(2)</ref> To obtain meaningful image regions, single-stream models typically adopt object detectors, which are expensive in both computation and data annotation. For example, extracting object regions from a 800?1,333 image takes about 900ms for Faster R-CNN <ref type="bibr" target="#b38">[39]</ref>, while ViT-base <ref type="bibr" target="#b10">[11]</ref> only needs 15ms (i.e., 60? faster). In contrast, two-stream VLP models <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b43">44]</ref> apply separate image and text encoders and match image-text pairs on the final embedding level. Although two-stream models (see <ref type="figure">Figure 1</ref>(b)-(c)) are much more efficient than single-stream ones, they only achieve sub-optimal results due to the lack of closer image-text interactions. Therefore, a few works <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b45">46]</ref> (see <ref type="figure">Figure 1</ref>(b)) reconsider object detectors, and most recent ones (e.g., CLIP <ref type="bibr" target="#b36">[37]</ref>, ALIGN <ref type="bibr" target="#b17">[18]</ref>, and WenLan <ref type="bibr" target="#b16">[17]</ref>) resort to extra large pre-training data crawled from the Internet. However, they still fail to model fine-grained interactions between the two modalities.</p><p>To address the inefficiency of single-stream VLP models and the lack of closer vision-language interactions of two-stream ones, we propose a novel COllaborative Two-Stream vision-language pre-training model termed COTS  <ref type="figure">Figure 1</ref>. Four categories of vision-language pre-training (VLP) models. (a) Single-stream models (e.g., Oscar <ref type="bibr" target="#b27">[28]</ref> and VinVL <ref type="bibr" target="#b51">[52]</ref>). (b) Two-stream models with the object detector (e.g., LigntingDot <ref type="bibr" target="#b41">[42]</ref>). (c) Two-stream models with instance-level interaction (e.g., CLIP <ref type="bibr" target="#b36">[37]</ref> and ALIGN <ref type="bibr" target="#b17">[18]</ref>). (d) COTS: our two-stream model with multi-level interactions. The inference time and time complexity of each module are also reported, and more details can be found in Section 4.2.</p><p>for cross-modal retrieval, which retains the advantage of real-time inference speed and also enhances the interactions between the two modalities (see <ref type="figure">Figure 1</ref>(d)). Concretely, we consider three levels of cross-modal interactions in our COTS: (1) Instance-level interaction -an image-text matching learning objective at the final embedding level (typically adopted by two-stream VLP models) is devised via momentum contrastive learning <ref type="bibr" target="#b14">[15]</ref>, where we maintain two sample queues (one per modality) to have large size of negative samples. (2) Token-level interaction -a novel masked vision-language modeling (MVLM) learning objective is considered without using any cross-stream network module. To this end, we first tokenize both the image and the text for each input image-text pair, where variational autoencoder <ref type="bibr" target="#b20">[21]</ref> is imposed on the visual encoder (e.g., ViT <ref type="bibr" target="#b10">[11]</ref>) to generate visual tokens and BERT <ref type="bibr" target="#b9">[10]</ref> is adopted for the text encoder. We then perform masked visual token prediction based on the unmasked visual tokens and the feature of each image's paired text, and perform masked language token prediction similarly. (3) Tasklevel interaction -a novel KL-alignment learning objective is devised between text-to-image and image-to-text retrieval tasks by minimizing the Kullback-Leibler (KL) Divergence between probability distributions of the two retrieval tasks. For each image-text pair, the probability distribution of the text-to-image retrieval task is obtained with the similarities of the chosen text and its unpaired images in the negative image queue maintained in momentum contrastive learning, and we can obtain the other distribution similarly.</p><p>As the scale of pre-training data becomes large (e.g., tens of millions or even billions of image-text pairs crawled from the Internet), it is impossible to perform human-annotation and thus there inevitably exist noises in the large-scale data. Noisy data such as mis-matched image-text pairs and totally meaningless ones could bring negative effect for pre-training. In this paper, we thus propose an adaptive momentum filter (AMF) module for our COTS, which can make full use of the momentum mechanism in our contrastive learning-based training algorithm. Specifically, we first calculate the similarity scores of all image-text pairs from the dynamically maintained image and text queues to obtain an extra queue. Further, we model this queue of similarity scores as a normal distribution and filter out the noisy data with the distribution mean and variance on the fly.</p><p>Our contributions are summarized as follows: (1) We propose a novel COllaborative Two-Stream (COTS) VLP model to improve the performance of two-stream models and retain their efficiency advantage at the same time. We achieve this by leveraging two extra levels of crossmodal interactions in addition to the typical instance-level alignment: a masked vision-language modeling (MVLM) learning objective for token-level interaction, and a KLalignment learning objective for task-level interaction.</p><p>(2) To alleviate the negative effect caused by the noises in large-scale pre-training data, we propose an adaptive momentum filter (AMF) module. AMF makes full use of the momentum mechanism in our instance-level alignment and adaptively filters noisy image-text pairs during pre-training.</p><p>(3) Under a fair comparison setting, our COTS achieves the highest performance among all two-stream methods and performs comparably (but 10,800? faster in inference) with the latest single-stream ones. Importantly, our COTS is also applicable to text-to-video retrieval, yielding new state-ofthe-art on the widely-used MSR-VTT dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Vision-Language Pre-Training. Recently, VLP resorts to single-stream models or two-stream ones. Single-stream models <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b51">52]</ref> contain cross-modal fusion modules (e.g., Transformer <ref type="bibr" target="#b42">[43]</ref> layers) to model closer interactions between image regions and text words. Although single-stream models often achieve superior performance, they have several limitations in real-world scenarios: (1) When performing cross-modal retrieval during inference, all possible query-candidate pairs need to be fed into the fusion modules to calculate similarity scores, resulting in huge computational cost. (2) To obtain meaningful image regions, single-stream models often adopt object detectors, which are expensive in both computation and data annotation. In contrast, two-stream models project the two modalities into a joint embedding space and align them on the final embedding level. Early two-stream models <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b47">48]</ref> only achieve sub-optimal performance because they do not consider fine-grained cross-modal interactions. More recent works (e.g., CLIP <ref type="bibr" target="#b36">[37]</ref>, ALIGN <ref type="bibr" target="#b17">[18]</ref>, and WenLan <ref type="bibr" target="#b16">[17]</ref>) choose to improve their performance by leveraging extra large web data. However, they fail to model fine-grained interactions between the two modalities. Although the latest two-stream model LightingDot <ref type="bibr" target="#b41">[42]</ref> considers token-level interaction, it still relies on an object detector, thus suffering from heavy computation. In this work, our COTS integrates the advantages of single-stream and two-stream models by still utilizing the two-stream architecture but enhancing the modeling of cross-modal interactions.</p><p>Masked Vision Modeling. Many previous works on VLP <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28]</ref> adopt masked vision modeling based on object tags to achieve better performance. They typically deploy a bottom-up attention mechanism <ref type="bibr" target="#b1">[2]</ref> implemented by first extracting the object tags with Faster R-CNN <ref type="bibr" target="#b38">[39]</ref> and then predicting the masked tags with other unmasked tags and text tokens. Although higher performance can be achieved, they commonly face two issues: (1) A heavy detector is needed to extract object tags, which is computationally expensive. For example, a Faster R-CNN detector takes 900ms to extract fine-grained region information from an image, which is nearly 60? slower than our ViT-base backbone (15ms). (2) These VLP models are not end-toend trained, which may fail to cope with unknown objects. The latest work <ref type="bibr" target="#b18">[19]</ref> shows that simply predicting masked raw image pixels is hard to improve the performance. Different from these works, our COTS employs a variational autoencoder <ref type="bibr" target="#b20">[21]</ref> as an image tokenizer to tokenize a raw image into discrete image tokens for masked vision modeling, inspired by the vision Transformer BEIT <ref type="bibr" target="#b3">[4]</ref>. The tokenizer is pre-trained in an end-to-end unsupervised training style, avoiding inducing handcrafted tags or heavy object detectors. Importantly, compared with predicting raw pixels directly, our choice of predicting masked image tokens is more meaningful as each image token contains specific high-level visual information. Overall, by combining masked vision modeling with masked language modeling, we devise a novel masked vision-language modeling (MVLM) objective for closer token-level interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Framework Overview</head><p>The goal of our COTS model for VLP is to learn two separate encoders that can embed image and text samples into the same semantic space for effective cross-modal retrieval. As illustrated in <ref type="figure">Figure 2</ref>, images and texts are encoded by the vision Transformer and the language Transformer, respectively. We then devise three levels of cross-modal interactions as the pre-training objectives of our COTS. Concretely, the instance-level interaction aligns the global features of paired images and texts by momentum cross-modal contrastive learning, which is inspired by the single-modal MoCo <ref type="bibr" target="#b14">[15]</ref>. To model closer interactions than instancelevel alignment, we propose to devise a masked visionlanguage modeling (MVLM) loss to enhance token-level interaction. MVLM has two parts: cross-modal masked vision modeling (CMVM) and cross-modal masked language modeling (CMLM). For each image, CMVM aims to predict the label of the masked image patch token based on unmasked ones together with the global feature of its paired text. CMLM does similarly on the language side. Further, we consider task-level interaction in our COTS, which aims to align the probability distributions of text-to-image and image-to-text retrieval tasks. In addition, to cope with the noises in the large-scale pre-training data, we propose an adaptive momentum filter (AMF) module, which is seamlessly integrated into the pre-training process.</p><p>Our choice of adopting the two-stream architecture in COTS has two main advantages: (1) Real-time inference speed -the separate image and text encoders allow us to compute the features of candidates beforehand for crossmodal retrieval tasks, and only a simple dot product needs to be calculated for each query-candidate pair. (2) Applicability to text-to-video retrieval -without any modification, our COTS can be directly applied to the text-to-video retrieval task, where the video representation can be obtained by averaging frame embeddings obtained by the image encoder. More details are given in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training Objectives</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Token-Level Interaction</head><p>We devise a masked vision-language modeling (MVLM) loss to enhance the token-level interaction in our COTS, which can be further split into two parts: cross-modal masked vision modeling (CMVM) and cross-modal masked language modeling (CMLM). To improve the practice <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28]</ref> of predicting masked image region tags with heavy object detectors, we introduce CMVM based on an image tokenizer inspired by BEIT <ref type="bibr" target="#b3">[4]</ref>. For each image, the objective of CMVM is to predict the labels of masked image tokens with the unmasked image patches and paired text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bert -Transformer Encoder</head><p>Bert -Transformer Encoder Language -Transformer Encoder Vision -Transformer Encoder Word Embedding Linear Embedding "A" "man" "poses" <ref type="bibr">[MASK]</ref> "with"  <ref type="figure">Figure 2</ref>. A schematic illustration of the proposed COTS for cross-modal retrieval.</p><formula xml:id="formula_0">? f ? f f f , f f , f f , ,</formula><formula xml:id="formula_1">Formally, let D = {(v i , l i )} N i=1 denote the training dataset, where (v i , l i ) is the i-th image-text pair.</formula><p>For each raw image v i , we first utilize the pre-trained discrete variational auto-encoder (dVAE) <ref type="bibr" target="#b37">[38]</ref> as the image tokenizer to obtain a sequence of 24 ? 24 discrete image tokens</p><formula xml:id="formula_2">T v i = {t v i,j ? V v } 576 j=1 , where t v i,j</formula><p>is the j-th token of image v i and V v is the vocabulary of discrete image tokens. Meanwhile, the raw image is split into 24 ? 24 patches, which are fed into a vision Transformer <ref type="bibr" target="#b10">[11]</ref> to obtain their embeddings. We then predict the label of each masked token based on the summation of the masked token embedding (which is already fused with unmasked token embeddings) and the global embedding of the paired text. The CMVM loss can thus be formulated as:</p><formula xml:id="formula_3">L CMVM = ?E (vi,li)?D log P (t v i,j |t v i,\j , l i ),<label>(1)</label></formula><p>where t v i,j denotes the target/masked image token, and t v i,\j = T v i \{t v i,j } denotes the unmasked image tokens. Similar to CMVM, for each piece of text l i , the objective of CMLM is to predict the label of each masked word token based on unmasked ones and the paired image:</p><formula xml:id="formula_4">L CMLM = ?E (vi,li)?D log P (t l i,j |t l i,\j , v i ),<label>(2)</label></formula><p>where t l i,j denotes the target/masked text word token, and t l i,\j denotes the unmasked ones. The total loss of our token-level cross-modal interaction is then defined as:</p><formula xml:id="formula_5">L token = L CMVM + L CMLM .<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Instance-Level Interaction</head><p>To model the instance-level interaction of two modalities (i.e., global feature alignment) in our COTS, we adopt a cross-modal momentum contrastive learning (MCL) algorithm inspired by the single-modal MoCo <ref type="bibr" target="#b14">[15]</ref>, which provides a mechanism of dynamically maintaining negative sample queues for contrastive learning. Since the two queues (one for each modality) used in our MCL successfully decouple the queue size from the mini-batch size, the size of negative samples (crucial for contrastive learning) can be much larger than the mini-batch size.</p><p>Concretely, let f v (with parameters ? v ) and f l (with parameters ? l ) denote the image and text encoders, respectively. We adopt two extra momentum encodersf v (with parameters? v ) andf l (with parameters? l ) for the vision and language modalities, respectively. The parameters of momentum encoders are updated by:</p><formula xml:id="formula_6">? v = m ?? v + (1 ? m) ? ? v ,<label>(4)</label></formula><formula xml:id="formula_7">? l = m ?? l + (1 ? m) ? ? l ,<label>(5)</label></formula><p>where m is the momentum hyper-parameter. Further, we maintain two queues Q v = {q v j } Nq j=1 and Q l = {q l j } Nq j=1 , whereq v j /q l j denotes the momentum feature vector, and N q denotes the queue size. Samples in each</p><formula xml:id="formula_8">mini-batch B = {(v i , l i )} N b i=1 ? D (N b = |B|<label>N</label></formula><p>q ) are fed into current momentum encoders to obtain their mo-mentum feature vectors, which are then pushed into corresponding queues after loss calculation. Meanwhile, the earliest N b momentum feature vectors in each queue are popped out. Given each image in a data batch, by regarding its paired text as the positive sample and all samples in Q l as negative ones, we define the image-to-text contrastive loss as (? is the temperature hyper-parameter):</p><formula xml:id="formula_9">L I2T = ? 1 N b (vi,li)?B log pos(f v i ,f l i , ? ) pos(f v i ,f l i , ? )+neg(f v i , Q l , ? ) ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_10">f v i = f v (v i ),f l i =f l (l i ), and pos(f v i ,f l i , ? ) = exp(f v i ?f l i /? ),<label>(7)</label></formula><formula xml:id="formula_11">neg(f v i , Q l , ? ) = q l j ?Q l exp(f v i ?q l j /? ).<label>(8)</label></formula><p>The similarity of two feature vectors is measured by dot product here. Similarly, given each text in a data batch, we define the text-to-image contrastive loss as:</p><formula xml:id="formula_12">L T2I = ? 1 N b (vi,li)?B log pos(f l i ,f v i , ? ) pos(f l i ,f v i , ? )+neg(f l i , Q v , ? ) ,<label>(9)</label></formula><p>where</p><formula xml:id="formula_13">f l i = f l (l i ), andf v i =f v (v i ).</formula><p>The total loss of our instance-level cross-modal interaction is then defined as:</p><formula xml:id="formula_14">L inst = L I2T + L T2I .<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Task-Level Interaction</head><p>As we can see from Eq. (6) that, for each image v i in a mini-batch, the image-to-text contrastive objective is actually maximizing the probability of matching its paired text l i against the unmatched samples in Q l (so does the text side). That is, the instance-level feature alignment only cares about maximizing one particular probability in the whole probability distribution of the image-to-text/text-toimage retrieval task, and fails to capture a higher level interaction between two modalities. To fill the void in the literature, we propose to align the probability distributions of two cross-modal retrieval tasks as our task-level interaction. Concretely, for each image-text pair (v i , l i ) ? B, we define the probability distribution of the image-to-text task as:</p><formula xml:id="formula_15">D I2T = [p(f v i ,f l i ), p(f v i ,q l 1 ), ? ? ? , p(f v i ,q l Nq )],<label>(11)</label></formula><p>where</p><formula xml:id="formula_16">p(f v i ,f l i ) = exp(f v i ?f l i /? ) f ?{f l i }?Q l exp(f v i ?f /? ) ,<label>(12)</label></formula><p>and p(f v i ,q l j ) (q l j ? Q l , j = 1, 2, ? ? ? , N q ) can be calculated in the same way. Similarly, we obtain the probability distribution of the text-to-image task as:</p><formula xml:id="formula_17">D T2I = [p(f l i ,f v i ), p(f l i ,q v 1 ), ? ? ? , p(f l i ,q v Nq )].<label>(13)</label></formula><p>The learning objective of our task-level cross-modal interaction is then formulated as minimizing the symmetric Kullback-Leibler (KL) Divergence between D I2T and D T2I :</p><formula xml:id="formula_18">L task = 1 N b (vi,li)?B (KL(D I2T ||D T2I )+KL(D T2I ||D I2T )). (14)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Adaptive Momentum Filter</head><p>Large-scale web-crawled data inevitably contain noises, which could bring negative effect for pre-training. Therefore, based on the momentum mechanism adopted in our COTS, we propose an adaptive momentum filter (AMF) module to adaptively filter noisy image-text pairs.</p><p>As introduced in the instance-level interaction, our COTS dynamically maintains two sample queues Q v and Q l for momentum contrastive learning. Since paired images and texts are pushed into or popped out of the corresponding queue simultaneously,q v j ? Q v andq l j ? Q l (j = 1, 2, ? ? ? , N q ) are also paired. We can then calculate a similarity score for each pair (q v j ,q l j ) by dot product. In this way, we obtain an extra similarity queue</p><formula xml:id="formula_19">Q s = {q v j ?q l j |q v j ? Q v ,q l j ? Q l } Nq j=1</formula><p>, which is also dynamically maintained along with the two sample queues.</p><p>Note that the similarity queue Q s can be seen as a sampling of the similarity score distribution at the current training iteration. We first calculate its mean ? and standard deviation ? as the estimations of those of the similarity score distribution. We then obtain the threshold value s AMF based on ? and ? (e.g., s AMF = ? ? 2?) for our AMF. Finally, we use this threshold to filter the current data batch B before we compute the losses:</p><formula xml:id="formula_20">B * = {(v i , l i )|f v i ?f l i &gt; s AMF , (v i , l i ) ? B}.<label>(15)</label></formula><p>In this work, s AMF changes in different training iterations as the similarity queue is changing. Specifically, when AMF is adopted in our full COTS, we use B * instead of B in each iteration for loss computation, but we still push all samples in B into Q v and Q l after loss computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Settings</head><p>Pre-Training Datasets. We use two image-text datasets for pre-training our COTS: (1) CC4M contains 4 million images and 5.3 million captions from Conceptual Captions (CC3M) <ref type="bibr" target="#b40">[41]</ref>, SBU <ref type="bibr" target="#b33">[34]</ref>, VG <ref type="bibr" target="#b23">[24]</ref>, MSCOCO <ref type="bibr" target="#b28">[29]</ref> and Flickr30K <ref type="bibr" target="#b35">[36]</ref>. (2) CC14M consists of CC4M and CC12M <ref type="bibr" target="#b5">[6]</ref> (about 2 million urls are now invalid), which contains 14 million images and 15.3 million captions in total. Note that CC14M is much noisier than CC4M. Downstream Datasets. We make downstream evaluation of our COTS on three widely-used benchmark datasets: (1) MSCOCO <ref type="bibr" target="#b28">[29]</ref> is a large image-text dataset of 123,287 images, where each image is annotated with 5 captions. As in <ref type="bibr" target="#b18">[19]</ref>, we adopt the Karpathy split of MSCOCO: 5,000 images for testing, another 5,000 for validation, and the rest 113,287 images for training.</p><p>(2) Flickr30K [36] contains 31,000 images and 158,915 captions totally. Each image is often annotated with 5 captions. Following the split in <ref type="bibr" target="#b11">[12]</ref>, we use 1,000 images for testing, another 1,000 for validation, and the rest for training. (3) To show the general applicability of our COTS, we also conduct experiments on a video-text dataset MSR-VTT <ref type="bibr" target="#b46">[47]</ref>, which has 10K YouTube videos and 200K captions. As in <ref type="bibr" target="#b48">[49]</ref>, we report our results under both the 1KA and 7K splits.</p><p>Text and Image Encoders. In our COTS, we follow <ref type="bibr" target="#b41">[42]</ref> and adopt a BERT-base [10] model as our text encoder, which contains a total of 12 Transformer layers with 768 hidden units and 12 heads. Further, for computation efficiency, we use ViT-B/16 <ref type="bibr" target="#b10">[11]</ref> as our image encoder with the input image resolution of 384?384. Overall, only base text and image encoders are considered in our COTS. Evaluation Metrics. The widely-used R@k (k = 1, 5, 10) in cross-modal retrieval is reported for performance evaluation, which is the proportion of matched samples found in the top-k retrieved results. Following <ref type="bibr" target="#b2">[3]</ref>, we also report the Median Rank (MR) for video-text retrieval. Implementation Details. For our masked vision-language modeling (MVLM), we randomly mask 40% image patches following <ref type="bibr" target="#b3">[4]</ref> and mask word tokens in text with 15% probability. We adopt the Adam <ref type="bibr" target="#b19">[20]</ref> optimizer with a weight decay of 0.02. We select hyper-parameters heuristically due to computational constraint: the momentum hyper-parameter m = 0.99, temperature ? = 0.05, and the queue size N Q is 12,800, 6,400, and 1,200 for pre-training, finetuning on MSCOCO, and finetuning on Flickr30K, respectively. We set the initial learning rate to 5e-5 for the first 5 epochs, and decay the learning rate linearly in the rest epochs. More implementation details can be found in the supp. material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Image-Text Retrieval</head><p>Comparison to the State-of-the-Arts. We compare our COTS with the state-of-the-art methods on two widelyused image-text datasets: Flickr30K and MSCOCO. As shown in <ref type="table" target="#tab_1">Table 1</ref> w.r.t. both single-stream and two-stream methods. On MSCOCO, our COTS ? also achieves higher performance than most single-stream methods and comparable results compared with VinVL <ref type="bibr" target="#b51">[52]</ref> but with a 10,800? faster speed during inference (see Inference Efficiency Analysis). Inference Efficiency Analysis. In real-world application scenarios, inference speed is an important evaluation metric for retrieval methods. In <ref type="figure">Figure 3</ref>, we compare our COTS with recent state-of-the-arts regarding the inference time on the MSCOCO (5K) test set. All methods are evaluated on a single Tesla V100 GPU. Compared with the single-stream VinVL <ref type="bibr" target="#b51">[52]</ref>, our COTS is 10,800? faster on the whole MSCOCOC (5K) test set. This huge gap will even become dramatically larger when the size of test set N grows, as the retrieval time complexity for single-stream models is O(N 2 ) while it is nearly O(N ) for two-stream ones. Although VSE and COOKIE are also two-stream models, our COTS is still significantly faster than them, indicating the extreme high efficiency of our COTS due to its fully tokenized Transformer-based architecture.</p><p>Comparative Retrieval Results without Finetuning. Following ViLT <ref type="bibr" target="#b18">[19]</ref>, we report the comparative retrieval results without finetuning on MSCOCO in <ref type="table">Table 2</ref>. We can observe that: (1) Our COTS outperforms the latest singlestream method ViLT <ref type="bibr" target="#b18">[19]</ref>. <ref type="bibr" target="#b1">(2)</ref> Our COTS also beats the latest two-stream methods CLIP <ref type="bibr" target="#b36">[37]</ref> and ALIGN <ref type="bibr" target="#b17">[18]</ref>, although it is pre-trained with much less data. Ablation Study Results. In <ref type="table">Table 3</ref>, we analyze the contributions of different pre-training objectives and the adaptive momentum filter (AMF) module in our COTS. We randomly sample 200K image-text pairs from CC12M as the pre-training dataset (termed CC200K). Zero-shot retrieval results are reported on the MSCOCO (5K) test set. We start with our instance-level interaction loss L inst (without AMF) and then add other losses successively. We can observe from <ref type="table">Table 3</ref> that: (1) Both CMLM and CMVM bring performance improvements (see L inst + L CMLM vs. L inst , and L inst + L token vs. L inst + L CMLM ), indicating that tokenlevel cross-modal interactions are beneficial to learning the aligned multi-modal representation space. (2) When tasklevel interaction is added (see L inst + L token + L task vs. L inst + L token ), the performance is further improved, which clearly validates the effectiveness of our multi-level crossmodal interactions. (3) Our AMF module works well with either instance-level or multi-level interactions (see L inst (w/ AMF) vs. L inst , and Our Full COTS vs. L inst +L token +L task ).</p><p>(4) Combining all objectives with the AMF module (i.e., Our Full COTS) leads to the best results, indicating that each objective/module is complementary to each other.</p><p>"Five ballet dancers caught mid jump in a dancing studio." "dancers" "five" "jump"</p><p>"Woman is using a baby stroller."</p><p>"Two children , a girl and a boy are practicing their writing." "boy" "girl" "children" (c) "Two young guys with shaggy hair look at their hands while hanging out in the yard." "guys" "hair" "hands"</p><formula xml:id="formula_21">(b) (d) (e)</formula><p>"A girl is playing the violin in the street while her band is talking on her cellphone." "cellphone" "band" "violin" (a) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Video-Text Retrieval</head><p>We further compare our COTS with the state-of-the-art methods on the video-text retrieval task. To directly deploy our COTS, we do not consider utilizing complex methods or additional modules to model the temporal information of videos. Instead, we simply use the mean frame embeddings as video representations and then calculate similarity scores by dot product with text embeddings. We report the textto-video retrieval results on the MSR-VTT dataset in Table 4. Note that only text-to-video retrieval is considered as in the latest work <ref type="bibr" target="#b2">[3]</ref>. It can be seen that: (1) Our COTS significantly outperforms the state-of-the-arts even without modeling the temporal information of videos, which demonstrates the general applicability and the great potentiality of our COTS. (2) Our COTS leads to better results than methods utilizing extra modalities (e.g., motion and audio) or those pre-trained on extra large video data (e.g., the HowTo100M dataset <ref type="bibr" target="#b32">[33]</ref> with more than 100 million video-text pairs), indicating that a well pre-trained visionlanguage model may be the key to video-text retrieval. <ref type="figure" target="#fig_1">Figure 4</ref> shows the visualized attention maps of our COTS on images/video frames responding to individual words. We can see from Figures 4(a)-(b) that our COTS can well locate different objects (even fine-grained ones like "violin" and "cellphone" in <ref type="figure" target="#fig_1">Figure 4</ref>(a), "hair" and "hands" in <ref type="figure" target="#fig_1">Figure 4(b)</ref>) in the same image. <ref type="figure" target="#fig_1">Figure 4</ref>(c) shows how our COTS determines gender information. Given the word "children", COTS focuses on the faces. When recognizing "girl", COTS pays attention to the girl's long hair and pink clothes (and the same for the word "boy"). Interestingly, our COTS can also capture abstract concepts ("five") and actions ("jump") as shown in <ref type="figure" target="#fig_1">Figure 4(d)</ref>. COTS focuses on five dancers for both "five" and "dancers", but pays more attention for the number "five". And it focuses on feet when it comes to "jump". <ref type="figure" target="#fig_1">Figure 4</ref>(e) presents attention maps w.r.t. "stroller" on four frames from the same video, showing that our COTS can also work well for the video modality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Visualization Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have investigated how to improve the performance of the two-stream vision-language pre-training (VLP) while still maintaining its advantage of high efficiency for image-text retrieval. Specifically, we propose a novel COllaborative Two-Stream VLP model termed COTS by leveraging three levels of cross-modal interactions in image-text retrieval. That is, we consider token-level interaction by masked vision-language modeling with both tokenized images and texts, instance-level interaction by cross-modal momentum contrastive learning, and task-level interaction by aligning two task distributions. Extensive experiments validate the effectiveness and high efficiency of our COTS in image-text retrieval. It is also shown to have general applicability as it achieves new state-of-the-art on video-text retrieval without any modification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Visualizations of attention maps of our COTS using GAE [7] on images/video frames responding to individual words. (a) -(d) Image attention maps w.r.t. different words. (e) Video frame attention maps w.r.t. the word "stroller".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Flickr30K (1K)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">MSCOCO (5K)</cell><cell></cell></row><row><cell>Model</cell><cell># PT Pairs</cell><cell></cell><cell cols="2">I2T Retrieval</cell><cell cols="3">T2I Retrieval</cell><cell></cell><cell cols="2">I2T Retrieval</cell><cell cols="3">T2I Retrieval</cell></row><row><cell></cell><cell></cell><cell cols="12">R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10</cell></row><row><cell>Single-Stream:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ViLBERT-Base [31]</cell><cell>3.1M</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">58.2 84.9</cell><cell>91.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Pixel-BERT-R50 [16]</cell><cell>5.6M</cell><cell cols="2">75.7 94.7</cell><cell>97.1</cell><cell cols="2">53.4 80.4</cell><cell>88.5</cell><cell cols="2">59.8 85.5</cell><cell>91.6</cell><cell cols="2">41.1 69.7</cell><cell>80.5</cell></row><row><cell>Pixel-BERT-X152 [16]</cell><cell>5.6M</cell><cell cols="2">87.0 98.9</cell><cell>99.5</cell><cell cols="2">71.5 92.1</cell><cell>95.8</cell><cell cols="2">63.6 87.5</cell><cell>93.6</cell><cell cols="2">50.1 77.6</cell><cell>86.2</cell></row><row><cell>Unicoder-VL [26]</cell><cell>3.8M</cell><cell cols="2">86.2 96.3</cell><cell>99.0</cell><cell cols="2">71.5 91.2</cell><cell>95.2</cell><cell cols="2">62.3 87.1</cell><cell>92.8</cell><cell cols="2">48.4 76.7</cell><cell>85.9</cell></row><row><cell>UNITER-Base [9]</cell><cell>9.6M</cell><cell cols="2">85.9 97.1</cell><cell>98.8</cell><cell cols="2">72.5 92.4</cell><cell>96.1</cell><cell cols="2">64.4 87.4</cell><cell>93.1</cell><cell cols="2">50.3 78.5</cell><cell>87.2</cell></row><row><cell>ERNIE-ViL-base [50]</cell><cell>3.8M</cell><cell cols="2">86.7 97.8</cell><cell>99.0</cell><cell cols="2">74.4 92.7</cell><cell>95.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VILLA-Base [14]</cell><cell>9.6M</cell><cell cols="2">86.6 97.9</cell><cell>99.2</cell><cell cols="2">74.7 92.9</cell><cell>95.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>OSCAR-Base [28]</cell><cell>6.5M</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">70.0 91.1</cell><cell>95.5</cell><cell cols="2">54.0 80.8</cell><cell>88.5</cell></row><row><cell>ViLT [19]</cell><cell>9.9M</cell><cell cols="2">83.5 96.7</cell><cell>98.6</cell><cell cols="2">64.4 88.7</cell><cell>93.8</cell><cell cols="2">61.5 86.3</cell><cell>92.7</cell><cell cols="2">42.7 72.9</cell><cell>83.1</cell></row><row><cell>VinVL-Base [52]</cell><cell>8.9M</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">74.6 92.6</cell><cell>96.3</cell><cell cols="2">58.1 83.2</cell><cell>90.1</cell></row><row><cell>Two-Stream:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VSE?  *  ? [8]</cell><cell>-</cell><cell cols="2">88.7 98.9</cell><cell>99.8</cell><cell cols="2">76.1 94.5</cell><cell>97.1</cell><cell cols="2">68.1 90.2</cell><cell>95.2</cell><cell cols="2">52.7 80.2</cell><cell>88.3</cell></row><row><cell>COOKIE  *  ? [45]</cell><cell>5.9M</cell><cell cols="2">89.0 98.9</cell><cell>99.7</cell><cell cols="2">75.6 94.6</cell><cell>97.2</cell><cell cols="2">71.6 90.9</cell><cell>95.4</cell><cell cols="2">54.5 81.0</cell><cell>88.2</cell></row><row><cell>Frozen in time [3]</cell><cell>5.5M</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">61.0 87.5</cell><cell>92.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>LightningDOT [42]</cell><cell>9.5M</cell><cell cols="2">83.9 97.2</cell><cell>98.6</cell><cell cols="2">69.9 91.1</cell><cell>95.2</cell><cell cols="2">60.1 85.1</cell><cell>91.8</cell><cell cols="2">45.8 74.6</cell><cell>83.8</cell></row><row><cell>COOKIE [45]</cell><cell>5.9M</cell><cell cols="2">84.7 96.9</cell><cell>98.3</cell><cell cols="2">68.3 91.1</cell><cell>95.2</cell><cell cols="2">61.7 86.7</cell><cell>92.3</cell><cell cols="2">46.6 75.2</cell><cell>84.1</cell></row><row><cell>COTS (ours)</cell><cell>5.3M</cell><cell cols="2">88.2 98.5</cell><cell>99.7</cell><cell cols="2">75.2 93.6</cell><cell>96.5</cell><cell cols="2">66.9 88.8</cell><cell>94.0</cell><cell cols="2">50.5 77.6</cell><cell>86.1</cell></row><row><cell>COTS (ours)</cell><cell>15.3M</cell><cell cols="2">90.6 98.7</cell><cell>99.7</cell><cell cols="2">76.5 93.9</cell><cell>96.6</cell><cell cols="2">69.0 90.4</cell><cell>94.9</cell><cell cols="2">52.4 79.0</cell><cell>86.9</cell></row><row><cell>COTS  ? (ours)</cell><cell>15.3M</cell><cell cols="2">91.7 99.0</cell><cell>99.9</cell><cell cols="2">78.3 94.9</cell><cell>97.2</cell><cell cols="2">70.6 91.0</cell><cell>95.3</cell><cell cols="2">53.7 80.2</cell><cell>87.8</cell></row></table><note>Comparative results for image-text retrieval on the Flickr30K (1K) test set and MSCOCO (5K) test set. Notations: # PT Pairs - the number of image-text pairs for pre-training; I2T Retrieval -image-to-text retrieval; T2I Retrieval -text-to-image retrieval.? Ensemble results of two models.* Models that utilize 940M tagged images for visual encoder pre-training.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>48.3 60.0 16.8 37.5 49.6 L inst + L CMLM 24.5 49.3 61.1 16.5 37.8 49.9 L inst + L token 25.6 49.9 61.9 17.1 38.3 50.4 L inst + L token + L task 26.4 50.5 62.9 17.538.5 50.6    Comparison to the state-of-the-arts for text-to-video retrieval on MSR-VTT under two splits: the 7K and 1KA splits. Notations: ? denotes that lower results are better; * denotes that extra modalities (e.g., motion and audio) are used.</figDesc><table><row><cell>Method</cell><cell cols="4">I2T Retrieval R@1 R@5 R@10 R@1 R@5 R@10 T2I Retrieval</cell></row><row><cell cols="5">L inst 24.0 L inst (w/ AMF) 24.7 49.6 61.3 16.6 38.3 50.0</cell></row><row><cell>Our Full COTS</cell><cell cols="4">27.1 51.1 62.9 17.9 39.2 51.1</cell></row><row><cell cols="5">Table 3. Ablation study for our COTS pre-trained on the small</cell></row><row><cell cols="5">CC200K dataset. Zero-shot image-text retrieval results are re-</cell></row><row><cell cols="2">ported on the MSCOCO (5K) test set.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="4"># PT Pairs R@1 R@5 R@10 MR?</cell></row><row><cell>7K split:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>JSFusion [51]</cell><cell cols="2">-10.2 31.2</cell><cell>43.2</cell><cell>13.0</cell></row><row><cell>HT MIL-NCE [33]</cell><cell cols="2">&gt;100M 14.9 40.2</cell><cell>52.8</cell><cell>9.0</cell></row><row><cell>ActBERT [53]</cell><cell cols="2">&gt;100M 16.3 42.8</cell><cell>56.9</cell><cell>10.0</cell></row><row><cell>HERO [27]</cell><cell cols="2">&gt;100M 16.8 43.4</cell><cell>57.7</cell><cell>-</cell></row><row><cell>VidTranslate [23]</cell><cell>&gt;100M 14.7</cell><cell>-</cell><cell>52.8</cell><cell>-</cell></row><row><cell>NoiseEstimation  *  [1]</cell><cell cols="2">&gt;100M 17.4 41.6</cell><cell>53.6</cell><cell>8.0</cell></row><row><cell>UniVL  *  [32]</cell><cell cols="2">&gt;100M 21.2 49.6</cell><cell>63.1</cell><cell>6.0</cell></row><row><cell>ClipBERT [25]</cell><cell cols="2">5.6M 22.0 46.8</cell><cell>59.9</cell><cell>6.0</cell></row><row><cell>TACo  *  [49]</cell><cell cols="2">&gt;100M 24.8 52.1</cell><cell>64.5</cell><cell>5.0</cell></row><row><cell>COTS (ours)</cell><cell cols="2">5.3M 29.0 57.0</cell><cell>67.7</cell><cell>3.0</cell></row><row><cell>COTS (ours)</cell><cell cols="2">15.3M 32.1 60.8</cell><cell>70.2</cell><cell>3.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>4.0</cell></row><row><cell>Support Set  *  [35]</cell><cell cols="2">&gt;100M 30.1 58.5</cell><cell>69.3</cell><cell>3.0</cell></row><row><cell>Frozen in Time [3]</cell><cell cols="2">5.5M 31.0 59.5</cell><cell>70.5</cell><cell>3.0</cell></row><row><cell>COTS (ours)</cell><cell cols="2">5.3M 33.1 61.3</cell><cell>72.8</cell><cell>3.0</cell></row><row><cell>COTS (ours)</cell><cell cols="2">15.3M 36.8 63.8</cell><cell>73.2</cell><cell>2.0</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Noise estimation using density estimation for selfsupervised multimodal learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Amrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Ben Ari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Rotman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6644" to="6652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">G?l Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Bain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1728" to="1738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<title level="m">BEiT: BERT pre-training of image transformers</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3558" to="3568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generic attentionmodel explainability for interpreting bi-modal and encoderdecoder transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hila</forename><surname>Chefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shir</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="397" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning the best pooling strategy for visual semantic embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">UNITER: Universal image-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="104" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">DeViSE: a deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-modal transformer for video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Gabeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="214" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Large-scale adversarial training for visionand-language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Pixel-BERT: Aligning image pixels with text by deep multi-modal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.00849</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manli</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangzhen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhao</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoxing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyuan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baogui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.06561</idno>
		<title level="m">Bridging vision and language by large-scale multi-modal pre-training</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhsuan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duerig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="4904" to="4916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ViLT: Visionand-language transformer without convolution or region supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bokyung</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ildoo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="5583" to="5594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Auto-encoding variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.2539</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07203</idno>
		<title level="m">Video understanding as machine translation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Less is more: ClipBERT for video-and-language learning via sparse sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="7331" to="7341" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unicoder-VL: A universal encoder for vision and language by cross-modal pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuejian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11336" to="11344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">HERO: Hierarchical encoder for video+ language omni-representation pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2046" to="2065" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Object-semantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="121" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<meeting><address><addrLine>RoBERTa</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">ViL-BERT: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">UniVL: A unified video and language pre-training model for multimodal understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaishao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06353</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">HowTo100M: Learning a text-video embedding by watching hundred million narrated video clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2630" to="2640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Im2text: Describing images using 1 million captioned photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1143" to="1151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Support-set bottlenecks for video-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandela</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Yao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno>ICLR, 2021. 7</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">C</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12092,2021.4</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rouditchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angie</forename><surname>Boggust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Harwath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhiraj</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Picheny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09199</idno>
		<title level="m">Learning audio-visual language representations from instructional videos</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2556" to="2565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">LightningDOT: Pre-training visualsemantic embeddings for real-time image-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="982" to="997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning deep structure-preserving image-text embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">COOKIE: Contrastive cross-modal knowledge sharing pre-training for vision-language representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyu</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning fragment self-attention embeddings for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiling</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoli</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM-MM</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2088" to="2096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">MSR-VTT: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5288" to="5296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep correlation for matching images and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3441" to="3450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">TACo: Token-aware cascade contrastive learning for video-text alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Ernie-vil: Knowledge enhanced visionlanguage representations through scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiji</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichong</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3208" to="3216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A joint sequence fusion model for video question answering and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="487" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">VinVL: Revisiting visual representations in vision-language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="5579" to="5588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">ActBERT: Learning globallocal video-text representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8743" to="8752" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
