<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Robust Vision Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Mao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gege</forename><surname>Qi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuefeng</forename><surname>Chen</surname></persName>
							<email>yuefeng.chenyf@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjie</forename><surname>Duan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Swinburne University of Technology</orgName>
								<address>
									<addrLine>3 EPFL</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaokai</forename><surname>Ye</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Xue</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Robust Vision Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances on Vision Transformer (ViT) and its improved variants have shown that self-attention-based networks surpass traditional Convolutional Neural Networks (CNNs) in most vision tasks. However, existing ViTs focus on the standard accuracy and computation cost, lacking the investigation of the intrinsic influence on model robustness and generalization. In this work, we conduct systematic evaluation on components of ViTs in terms of their impact on robustness to adversarial examples, common corruptions and distribution shifts. We find some components can be harmful to robustness. By leveraging robust components as building blocks of ViTs, we propose Robust Vision Transformer (RVT), which is a new vision transformer and has superior performance with strong robustness. Inspired by the findings during the evaluation, we further propose two new plug-and-play techniques called position-aware attention scaling and patch-wise augmentation to augment our RVT, which we abbreviate as RVT * . The experimental results of RVT on ImageNet and six robustness benchmarks demonstrate its advanced robustness and generalization ability compared with previous ViTs and state-ofthe-art CNNs. Furthermore, RVT-S * achieves Top-1 rank on multiple robustness leaderboards including ImageNet-C, ImageNet-Sketch and ImageNet-R.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Following the popularity of transformers in Natural Language Processing (NLP) applications, e.g., BERT <ref type="bibr" target="#b7">[8]</ref> and GPT <ref type="bibr" target="#b29">[30]</ref>, there has sparked particular interest in investigating whether transformer can be a primary backbone for computer vision applications previously dominated by Convolutional Neural Networks (CNNs). Recently, Vision Transformer (ViT) <ref type="bibr" target="#b9">[10]</ref> successfully applies a pure transformer for classification which achieves an impressive speed-accuracy trade-off by capturing long-range dependencies via self-attention. Base on this seminal work, numerous variants have been proposed to improve ViTs from different perspectives containing training data efficiency <ref type="bibr" target="#b39">[40]</ref>, self-attention mechanism <ref type="bibr" target="#b24">[25]</ref>, introducing convolution <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b49">50]</ref> or pooling layers <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b42">43]</ref>, etc. However, these works only focus on the standard accuracy and computation cost, lacking the investigation of the intrinsic influence on model robustness and generalization.</p><p>In this work, we take initiatives to explore a ViT model with strong robustness. To this end, we first give an empirical assessment of existing ViT models in <ref type="figure" target="#fig_0">Figure 1</ref>. Surprisingly, although all ViT variants reproduce the standard accuracy claimed in the paper, some of their modifications may bring devastating damages on the model robustness. A vivid example is PVT <ref type="bibr" target="#b42">[43]</ref>, which achieves a high standard accuracy but suffered with large drop of robust accuracy. We show that PVT-Small obtains only 26.6% robust accuracy, which is 14.1% lower than original DeiT-S in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>To demystify the trade-offs between accuracy and robustness, we analyze ViT models with different patch embedding, position embedding, transformer blocks and classification head whose impact on the robustness that has never been thoroughly studied. Based on the valuable find-ings revealed by exploratory experiments, we propose a Robust Vision Transformer (RVT), which has significant improvement on robustness, but also exceeds most other transformers in accuracy. In addition, we propose two new plug-and-play techniques to further boost the RVT. The first is Position-Aware Attention Scaling (PAAS), which plays the role of position encoding in RVT. PAAS improves the self-attention mechanism by filtering out redundant and noisy position correlation and activating only major attention with strong correlation, which leads to the enhancement of model robustness. The second is a simple and general patch-wise augmentation method for patch sequences which adds rich affinity and diversity to training data. Patch-wise augmentation also contributes to the model generalization by reducing the risk of over-fitting. With the above proposed methods, we can build an augmented Robust Vision Transformer * (RVT * ). Contributions of this paper are three-fold:</p><p>? We give a systematic robustness analysis of ViTs and reveal harmful components. Inspired by it, we reform robust components as building blocks as a new transformer, named Robust Vision Transformer (RVT).</p><p>? To further improve the RVT, we propose two new plug-and-play techniques called position-aware attention scaling and patch-wise augmentation. Both of them can be applied to other ViT models and yield significant enhancement on robustness and standard accuracy.</p><p>? Experimental results on ImageNet and six robustness benchmarks show that RVT exhibits best trade-offs between standard accuracy and robustness compared with previous ViTs and CNNs. Specifically, RVT-S * achieves Top-1 rank on ImageNet-C, ImageNet-Sketch and ImageNet-R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Robustness Benchmarks. The rigorous benchmarks are important for evaluating and understanding the robustness of deep models. Early works focus on the model safety under the adversarial examples with constrained perturbations <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b37">38]</ref>. In real-world applications, the phenomenon of image corruption or out-of-distribution is more commonly appeared. Driven by this, ImageNet-C <ref type="bibr" target="#b16">[17]</ref> benchmarks the model against image corruption which simulates distortions from real-world sources. ImageNet-R <ref type="bibr" target="#b15">[16]</ref> and ImageNet-Sketch <ref type="bibr" target="#b41">[42]</ref> collect the online images consisting of naturally occurring distribution changes such as image style, to measure the generalization ability to new distributions at test time. In this paper, we adopt all the above benchmarks as the fair-minded evaluation metrics.</p><p>Robustness Study for CNNs. The robustness research of CNNs has experienced explosive development in recent years. Numerous works conduct thorough study on the robustness of CNNs and aim to strengthen it in different ways, e.g., stronger data augmentation <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b32">33]</ref>, carefully designed <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b43">44]</ref> or searched <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref> network architecture, improved training strategy <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b46">47]</ref>, quantization <ref type="bibr" target="#b23">[24]</ref> and pruning <ref type="bibr" target="#b48">[49]</ref> of the weights, better pooling <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b52">53]</ref> or activation functions <ref type="bibr" target="#b45">[46]</ref>, etc. Although the methods mentioned above perform well on CNNs, there is no evidence that they also keep the effectiveness on ViTs. A targeted research for improving the robustness of ViTs is still blank.</p><p>Robustness Study for ViTs. Until now, there are several works attempting at studying the robustness of ViTs. Early works focus on the adversarial robustness of ViTs. They find that ViTs are more adversarially robust than CNNs <ref type="bibr" target="#b33">[34]</ref> and the transferability of adversarial examples between CNNs and ViTs is remarkably low <ref type="bibr" target="#b26">[27]</ref>. Follow up works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b28">29]</ref> extend the robustness study on ViTs to much common image corruption and distribution shift, and indicate ViTs are more robust learners. Although some findings are consistent with above works, in this paper, we do not make simple comparison of robustness between ViTs and CNNs, but take a step further by analyzing the detailed robust components in ViT and its variants. Based on the analysis, we design a robust vision transformer and introduce two novel techniques to further reduce the fragility of ViT models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Robustness Analysis of Designed Components</head><p>We give the robustness analysis of four main components in ViTs: patch embedding, position embedding, transformer blocks and classification head. DeiT-Ti <ref type="bibr" target="#b39">[40]</ref> is used as the base model. All the robustness benchmarks mentioned in section 2 are considered comprehensively. There is a positive correlation between these benchmarks in most cases. Due to the limitation of space, we show the robust accuracy under FGSM <ref type="bibr" target="#b10">[11]</ref> adversary in the main body and other results in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Patch Embedding</head><p>F1: Low-level feature of patches helps for the robustness. ViTs <ref type="bibr" target="#b9">[10]</ref> tokenize an image by splitting it into patches with size of 16?16 or 32?32. Such simple tokenization makes the models hard to capture low-level structures such as edges and corners. To extract low-level features of patches, CeiT <ref type="bibr" target="#b49">[50]</ref>, LeViT <ref type="bibr" target="#b11">[12]</ref> and TNT <ref type="bibr" target="#b13">[14]</ref> use a convolutional stem instead of the original linear layer, T2T-ViT <ref type="bibr" target="#b50">[51]</ref> leverages self-attention to model dependencies among neighboring pixels. However, these methods merely focus on the standard accuracy. To answer how is the robustness affected by leveraging low-level features of patches, we compare the original linear projection with two new convolution and tokens-to-tokens embedders, proposed by CeiT and T2T-ViT respectively. As shown in Table 2, low-level patch embedding has a positive effect on the model robustness and standard accuracy as more detailed visual features are exploited. Among them tokens-to-tokens embedder is the best, but it has quadratic complexity with the expansion of image size. We adopt the convolutional embedder with less computation cost.  <ref type="table">Table 1</ref>. Effect of different positional embeddings. We use Deit-Ti as the base model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Position Embedding</head><p>F2: Position encoding is critical for learning shapebias based semantic features which are robust to texture changes. Besides, existing position encoding methods have no big impact on the robustness. We first explore the necessity of position embeddings. Previous work <ref type="bibr" target="#b2">[3]</ref> shows ViT trained without position embeddings has 4% drop of standard accuracy. In this work, we find this gap even can be larger on robustness. In Appendix A, we find with no position encoding, ViT fails to recognize shape-bias objects, which leads to 8% accuracy drop on ImageNet-Sketch. Concerning the ways of positional encoding, learned absolute, sin-cos absolute, learned relative <ref type="bibr" target="#b34">[35]</ref>, input-conditioned <ref type="bibr" target="#b2">[3]</ref> position representations are compared. In <ref type="table">Table 1</ref>, the result suggests that most posi-tion encoding methods have no big impact on the robustness, and a minority even have a negative effect. Especially, CPE <ref type="bibr" target="#b2">[3]</ref> encodes position embeddings conditioned on inputs. Such a conditional position representation makes it changed easily with the input, and causes the poor robustness. The fragility of position embeddings also motivates us to design a more robust position encoding method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Transformer Blocks</head><p>F3: An elaborate multi-stage design is required for constructing robust vision transformers. Modern CNNs always start with a feature of large spatial sizes and a small channel size and gradually increase the channel size while decreasing the spatial size. The different sizes of feature maps constitute the multi-stage convolution blocks. As shown by previous works <ref type="bibr" target="#b3">[4]</ref>, such a design contributes to the expressiveness and generalization performance of the network. PVT <ref type="bibr" target="#b42">[43]</ref>, PiT <ref type="bibr" target="#b19">[20]</ref> and Swin <ref type="bibr" target="#b24">[25]</ref> employ this design principle into ViTs. To measure the robustness variance with changing of stage distribution, we slightly modify the DeiT-Ti architecture to get five variants (V2-V6) in <ref type="table">Table 3</ref>. We keep the overall number of transformer blocks consistent to 12 and replace some of them with smaller or larger spatial resolution. Detailed architecture is shown in Appendix A. By comparing with DeiT-Ti, we find all five variants improve the standard accuracy, benefit from the extraction of hierarchical image features. In terms of robustness, transformer blocks with different spatial sizes show different effects. An experimental conclusion is that the model will get worse on robustness when it contains more transformer blocks with large spatial resolution. On the contrary, reducing the spatial resolution gradually at later transformer blocks contributes to the modest enhancement of robustness. Besides, we also observe that having more blocks with larger input spatial size will increase the number of FLOPs and memory consumption. To achieve the best trade-off on speed and performance, we think V2 is the most compromising choice in this paper.</p><p>F4: Robustness can be benefited from the completeness and compactness among attention heads, by choosing an appropriate head number. ConViT <ref type="bibr" target="#b5">[6]</ref>, Swin <ref type="bibr" target="#b24">[25]</ref> and LeViT <ref type="bibr" target="#b11">[12]</ref> both use more self-attention heads and smaller dimensions of keys and queries to achieve better performance at a controllable FLOPs. To study how does the number of heads affect the robustness, we train DeiT-Ti with different head numbers. Once the number of heads increases, we meanwhile reduce the head dimensions to ensure the overall feature dimensions are unchanged. Similar with generally understanding in NLP <ref type="bibr" target="#b27">[28]</ref>, we find the completeness and compactness among attention heads are important for ViTs. As shown in the <ref type="table" target="#tab_3">Table 4</ref>, the robustness and standard accuracy still gain great improvement with the head increasing till to 8. We think that an appropriate number of heads supplies various aspects of attentive information on the input. Such complete and non-redundant attentive information also introduces more fine-grained representations which are prone to be neglect by model with less heads, thus increases the robustness.  <ref type="table">Table 3</ref>. Effect of stage distribution. We ablate the number of blocks in stages S1, S2, S3, S4 of DeiT-Ti, where S1 is the stage with the largest 56 ? 56 input spatial dimension, and gradually reduced to half of the original in later stages. The GPU memory consumption is tested on input with batch size of 64.</p><p>F5: The locality constraints of self-attention layer may do harm for the robustness. Vanilla self-attention calculates the pair-wise attention of all sequence elements. But for image classification, local region needs to be paid more attention than remoter regions. Swin <ref type="bibr" target="#b24">[25]</ref> limits the self-attention computation to non-overlapping local windows on the input. This hard coded locality of self-attention enjoys great computational efficiency and has linear complexity with respect to image size. Although Swin can also get competitive accuracy, in this work we find such local window self-attention is harmful to the model robustness.</p><p>The result in <ref type="table" target="#tab_1">Table 2</ref> shows after modifying self-attention to the local version, the robust accuracy is getting worse. We think this phenomenon may be partly caused by the destruction of long-range dependencies modeling in ViTs.</p><p>F6: Feed-forward networks (FFN) can be extended to convolutional FFN by encoding multiple tokens in local regions. Such information exchange of local tokens in FFN makes ViTs more robust. LocalViT <ref type="bibr" target="#b22">[23]</ref> and CeiT <ref type="bibr" target="#b49">[50]</ref> introduce connectivity of local regions into ViTs by adding a depth-wise convolution in feed-forward networks (FFN). Our experiment in <ref type="table" target="#tab_1">Table 2</ref> verifies that the convolutional FFN greatly improves both the standard accuracy and robustness. We think the reason lies in two aspects. First, compared with locally self-attention, convolutional FFN will not damage the long-term dependencies modeling ability of ViTs. The merit of ViTs can be inherited. Second, original FFN only encodes single token representation, while convolutional FFN encodes both the current token and its neighbors. Such information exchange within a local region makes ViTs more robust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Classification Head F7: Is the classification token (CLS) important for</head><p>ViTs? The answer is not, and replacing CLS with global average pooling on output tokens even improves the robustness.</p><p>CNNs adopt a global average pooling layer before the classifier to integrate visual features at different spatial locations. This practice also inherently takes advantage of the translation invariance of the image. However, ViTs use an additional classification token (CLS) to perform classification, are not translation-invariant. To get over this shortcoming, CPVT <ref type="bibr" target="#b2">[3]</ref> and LeViT <ref type="bibr" target="#b11">[12]</ref> remove the CLS token and replace it by average pooling along with the last layer sequential output of the Transformer. We compare models trained with and without CLS token in <ref type="table" target="#tab_1">Table 2</ref>. The result shows the adversarial robustness can be greatly improved by removing CLS token. Also we find removing CLS token has slight help for the standard accuracy, which can be benefited from the desired translation-invariance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Combination of Robust Components</head><p>In the above, we separately analyze the effect of each designed component in the ViTs. To make use of these findings, we combine the selected useful components, listed in follows: 1) Extract low-level feature of patches using a convolutional stem; 2) Adopt the multi-stage design of ViTs and avoid blocks with larger spatial resolution; 3) Choose a suitable number of heads; 4) Use convolution in FFN; 5) Replace CLS token with token feature pooling. As we find the effects of the above modifications are superimposed, we adopt all of these robust components into ViTs, the resultant model is called Robust Vision Transformer (RVT). RVT has achieved the new state-of-the-art robustness compared to other ViT variants. To further improve the performance, we propose two novel techniques, position-aware attention scaling and patch-wise data augmentation, to train our RVT. Both of them are also applicable to other ViT models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Position-Aware Attention Scaling</head><p>In this section, we introduce our proposed position encoding mechanism called Position-Aware Attention Scaling (PAAS), which modifies the rescaling operation in the dot product attention to a more generalized version. To start with, we illustrate the scaled dot-product attention in transformer firstly. And then the modification of PAAS will be explained.</p><p>Scaled Dot-product Attention. Scaled dot-product attention is a key component in Multi-Head Self Attention layer (MHSA) of Transformer. MHSA first generates set of queries Q ? R N ?d , keys K ? R N ?d , values V ? R N ?d with the corresponding projection. Then the query vector q ? R d is matched against the each key vector in K. The output is the weighted sum of a set of N value vectors v based on the matching score. This process is called scaled dot-product attention:</p><formula xml:id="formula_0">Attention(Q, K, V ) = Softmax(QK T / ? d)V<label>(1)</label></formula><p>For preventing extremly small gradients and stabilizing the training process, each element in QK T multiplies by a constant 1 ? d to be rescaled into a standard range.</p><p>Position-Aware Attention Scaling. In this work, a more effective position-aware attention scaling method is proposed. To make the original rescaling process of dotproduct attention position-aware, we define a learnable position importance matrix W p ? R N ?N , which presents the importance of each pair of q-k. The oringinal scaled dotproduct attention is modified as follows: where ? is the element-wise product. As W p is input independent and only determined by the position of each q, k in the sequence, our position-aware attention scaling can also serve as a position representation. Thus, we replace the traditional position embedding with our PAAS in RVT. After that the overall self-attention can be decoupled into two parts: the QK T term presents the content-based attention, and W p / ? d term acts as the position-based attention. This untied design offers more expressiveness by removing the mixed and noisy correlations <ref type="bibr" target="#b20">[21]</ref>.</p><formula xml:id="formula_1">Attention(Q, K, V ) = Softmax(QK T ?(W p / ? d))V<label>(</label></formula><p>Robustness of PAAS. As mentioned in section 3.2, most existing position embeddings have no contribution to the model robustness, and some of them even do a negative effect. Differently, our proposed PAAS can improve the model robustness effectively. This superior property relies on the position importance matrix W p , which acts as a soft attention mask on each position pair of q-k. As shown in <ref type="figure">Figure 3</ref>, we visualize the attention map of 3th query patch in 3th transformer block. Without PAAS, an adversarial input can make some unrelated regions activated and produce a noisy self-attention map. To filter out these noises, PAAS suppresses the redundant positions irrelevant for classification in self-attention map, by a learned small multiplier in W p . Finally only the regions important for classification are activated. We experimentally validate that PAAS can provide certain defense power against some white-box adversaries, e.g., FGSM <ref type="bibr" target="#b10">[11]</ref>. Not limited to adversarial attack, it also helps to the corruption and out-of-distribution generalization. Details can be referred to section 6.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Patch-Wise Augmentation</head><p>Image augmentation is a strategy especially important for ViTs since a biggest shortcoming of ViTs is the worse generalization ability when trained on relatively small-size datasets, while this shortcoming can be remedied by sufficient data augmentation <ref type="bibr" target="#b39">[40]</ref>. On the other hand, a rich data augmentation also helps with robustness and generalization, which has been verified in previous works <ref type="bibr" target="#b17">[18]</ref>. For improving the diversity of the augmented training data, we propose the patch-wise data augmentation strategy for ViTs, which imposes diverse augmentation on each input image patches at training time. Our motivation comes from the difference of ViTs and CNNs that ViTs not only extract intra-patch features but also concern the inter-patch relations. We think the traditional augmentation which randomly transforms the whole image could provide enough intra-patch augmentation. However, it lacks the diversity on inter-patch augmentation, as all of patches have the same transformation at one time. To impose more inter-patch diversity, we retain the original image-level augmentation, and then add the following patch-level augmentation on each image patch. For simplicity, only three basic image transformations are considered for patch-level augmentation: random resized crop, random horizontal flip and random gaussian noise.</p><p>Robustness of Patch-Wise Augmentation. Same with the augmentations like MixUp <ref type="bibr" target="#b51">[52]</ref>, AugMix <ref type="bibr" target="#b17">[18]</ref>, Ran-dAugment <ref type="bibr" target="#b4">[5]</ref>, patch-wise augmentation also benefit the model robustness. It effects on the phases after conventional image-level augmentations, and provides the meaningful augmentation on patch sequence input. Different from RandAugment, which adopts augmentations conflicting with ImageNet-C, we only use simple image transform for patch-wise augmentation. It confirms that the most part of robustness improvement is derived from the strategy itself but not the used augmentation. A significant advantage of patch-wise augmentation is that it can be in common use across different ViT models and bring more than 1% and 5% improvement on standard and robust accuracy. Details can be referred to section 6.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Experimental Settings</head><p>Implementation Details. All of our experiments are performed on the NVIDIA 2080Ti GPUs. We implement RVT in three sizes named by RVT-Ti, RVT-S, RVT-B respectively. All of them adopt the best settings investigated in section 2. For RVT * , we add PAAS on multiple transformer blocks. The patch-wise augmentation uses the combination of base augmentation introduced in section 6.4. Other training hyperparameters are same with DeiT <ref type="bibr" target="#b39">[40]</ref>.</p><p>Evaluation Benchmarks. We adopt the ImageNet-1K <ref type="bibr" target="#b6">[7]</ref> dataset for training and standard performance evaluation. No other large-scale dataset is needed for pretraining. For robustness evaluation, we test our RVT in three aspect: 1) for adversarial robustness, we test the adversarial examples generated by white-box attack algorithms FGSM <ref type="bibr" target="#b10">[11]</ref> and PGD <ref type="bibr" target="#b25">[26]</ref> on ImageNet-1K validation set. ImageNet-A <ref type="bibr" target="#b18">[19]</ref> is used for evaluating the model under natural adversarial example. 2) for common corruption robustness, we adopt ImageNet-C <ref type="bibr" target="#b16">[17]</ref> which consists of 15 types of algorithmically generated corruptions with five levels of severity. 3) for out-of-distribution robustness, we evaluate on ImageNet-R <ref type="bibr" target="#b15">[16]</ref> and ImageNet-Sketch <ref type="bibr" target="#b41">[42]</ref>. They contain images with naturally occurring distribution changes. The difference is that ImageNet-Sketch only contains sketch images, which can be used for testing the classification ability when texture or color information is missing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Standard Performance Evaluation</head><p>For standard performance evaluation, we compare our method with state-of-the-art classification methods including Transformer-based models and representative CNNbased models in <ref type="table" target="#tab_4">Table 5</ref>. Compared to CNNs-based models, RVT has surpassed most of CNN architectures with fewer parameters and FLOPs. RVT-Ti * achieves 79.2% Top-1 accuracy on ImageNet-1K validation set, which is competitive with currently popular ResNet and RegNet series, but only has 1.3G FLOPs and 10.9M parameters (around 60% smaller than CNNs). With the same computation cost, RVT-S * obtains 81.9% test accuracy, 2.9% higher than ResNet-50. This result is closed to EfficientNet-B4, however EfficientNet-B4 requires larger 380?380 input size and has much lower throughput.</p><p>Compared to Transformer-based models, our RVT also achieves the comparable standard accuracy. We find just combining the robust components can make RVT-Ti get 78.4% Top-1 accuracy and surpass the existing state-of-theart on ViTs with tiny version. By adopting our newly proposed position-aware attention scaling and patch-wise data augmentation, RVT-Ti * can further improve 0.8% on RVT-Ti with little additional computation cost. For other scales of the model, RVT-S * and RVT-B * also achieve a good promotion compared with DeiT-S and DeiT-B. Although the improvement becomes smaller with the increase of model capacity, we think the advance of our model is still obvious as it strengthen the model ability in various views such as robustness and out-of-domain generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Robustness Evaluation</head><p>We employ a series of benchmarks to evaluate the model robustness on different aspects. Among them, ImageNet-C (IN-C) calculates the mean corruption error (mCE) as metric. The smaller mCE means the more robust of the model  <ref type="table" target="#tab_4">Table 5</ref>. Adversarial Robustness. For evaluating the adversarial robustness, we adopt single-step attack algorithm FGSM <ref type="bibr" target="#b10">[11]</ref> and multi-step attack algorithm PGD <ref type="bibr" target="#b25">[26]</ref> with steps t = 5, step size ? = 0.5. Both attackers perturb the input image with max magnitude ? = 1. <ref type="table" target="#tab_4">Table 5</ref> suggests that the adversarial robustness has a strong correlation with the design of model architecture. With similar model scale and FLOPs, most Transformer-based models have higher robust accuracy than CNNs under adversarial attacks. This conclusion is also consistent with <ref type="bibr" target="#b33">[34]</ref>. Some modifications on ViTs or CNNs will also weaken or strengthen the adversarial robustness. For example, Swin-T <ref type="bibr" target="#b24">[25]</ref> introduces window self-attention for reducing the computation cost but damaging the adversarial robustness, and EfficientNet-B4 <ref type="bibr" target="#b38">[39]</ref> uses smooth activation functions which is helpful with adversarial robustness.</p><p>We summarize the robust design experiences of ViTs in this work. The resultant RVT model achieves superior performance on both FGSM and PGD attackers. In detail, RVT-Ti and RVT-S get over 10% improvement on FGSM, compared with the previous ViT variants. This advance is further expanded by our PAAS and patch-wise augmentation. Adversarial robustness seems unrelated with standard performance. Although models like Swin-T, TNT-S get higher standard accuracy than DeiT-S, their adversarially robust accuracy is well below the baseline. However, our RVT model can achieve the best trade-off between standard performance and adversarial robustness.</p><p>Common Corruption Robustness. To metric the model degradation on common image corruptions, we present the mCE on ImageNet-C (IN-C) in <ref type="table" target="#tab_4">Table 5</ref>. We also list some methods from ImageNet-C Leaderboard, which are built based on ResNet-50. Our RVT-S * gets 49.4 mCE, which has 4.2 improvement on top-1 method DeepAugment <ref type="bibr" target="#b15">[16]</ref> in the leaderboard, and bulids the new state-of-the-art. The result also indicates that Transformer-based models have a natural advantage in dealing with image corruptions. At-tributed to its ability of long-range dependencies modeling, ViTs are easier to learn the shape-bias features. Note that in this work we are not considering RandAugment. As a training augmentation of ViTs, RandAugment adopts conflicted augmentation with ImageNet-C and may cause the unfairness of the comparison proposed by <ref type="bibr" target="#b0">[1]</ref>.</p><p>Out-of-distribution Robustness. We test the generalization ability of RVT on out-of-distribution data by reporting the top@1 accuracy on ImageNet-R (IN-R) and ImageNet-Sketch (IN-SK) in <ref type="table" target="#tab_4">Table 5</ref>. Our RVT and RVT * also beat other ViT models on out-of-distribution generalization. As the superiority of Transformer-based models on capturing shape-bias features mentioned above, our RVT-S also surpasses most CNN and ViT models and get <ref type="bibr" target="#b34">35</ref>   <ref type="table">Table 7</ref>. Ablation experiments on patch-wise augmentation. RC, GN, HF represent random resized crop, random gaussian noise and random horizontal flip respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Ablation Studies</head><p>we conduct ablation studies on the proposed components of PAAS and patch-wise augmentation in this section. Other modifications of RVT are not involved since they have been analyzed in section 2. All of our ablation experiments are based on the RVT-Ti model on ImageNet.</p><p>Single layer PAAS vs. Multiple layer PAAS. We evaluate whether using PAAS on multiple transformer blocks can benefit the performance or robustness. The result is suggested in <ref type="table" target="#tab_6">Table 6</ref>. Learned absolute position embedding in original ViT model is adopted for comparison. With more transformer blocks using PAAS, the standard and robust accuracy gain greater enhancement. After applying PAAS on 5 blocks, the benefit of PAAS gets saturated. There will be the same trend if we replace PAAS with the original position embedding. But the original position embedding is not performed as good as our PAAS on both standard and robust accuracy.</p><p>Different types of basic augmentation. Due to the limited training resources, we only test three basic image augmentations: random resized crop, random horizontal flip and random gaussian noise. For random resized crop, we crop the patch according to the scale sampled from [0.85, 1.0], then resize it to original size with aspect ratio unchanged. We set the mean and standard deviation as 0 and 0.01 for random gaussian noise. For each transformation, we set the applying probability p = 0.1. Other hyperparameters are consistent with the implementation in Kornia <ref type="bibr" target="#b31">[32]</ref>. As shown in <ref type="table">Table 7</ref>, we can see both three augmentations are beneficial of standard and robust accuracy. Among them, random gaussian noise is the better choice as it helps for more robustness improvement.</p><p>Combination of basic augmentations. We further evaluate the combination of basic patch-wise augmentations. For traditional image augmentation, combining multiple basic transformation <ref type="bibr" target="#b4">[5]</ref> can largely improve the standard accuracy. Differently, as shown in <ref type="table">Table 7</ref>, the benefit is marginal for combining basic patch-wise augmentations, but combination of three is still better than using only single augmentation. In this paper, we adopt the combination of all basic augmentations.</p><p>Effect on other ViT architectures. For showing the effectiveness of our proposed position-aware attention scaling and patch-wise augmentation, we apply them to train other ViT models. DeiT-Ti, ConViT-Ti and PiT-Ti are adopted as the base model. The experimental results are shown in <ref type="table" target="#tab_8">Table 8</ref>, with combining the proposed techniques into these base models, all the augmented models achieve significant improvement. Specifically, all the improved models yield more than 1% and 5% promotion on standard and robust accuracy on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vanilla</head><p>Acc Rob. Acc Improved Acc Rob. Acc models models  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We systematically study the robustness of key components in ViTs, and propose Robust Vision Transformer (RVT) by alternating the modifications which would damage the robustness. Furthermore, we have devised a novel patch-wise augmentation which adds rich affinity and diversity to training data. Considering the lack of spatial information correlation in scaled dot-product attention, we present position-aware attention scaling (PAAS) method to further boost the RVT. Experiments show that our RVT achieves outstanding performance consistently on Im-ageNet and six robustness benchmarks. Under the exhaustive trade-offs between FLOPs, standard and robust accuracy, extensive experiment results validate the significance of our RVT-Ti and RVT-S. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Feature Visualization</head><p>In general understanding, intra-class compactness and inter-class separability are crucial indicators to measure the effectiveness of a model to produce discriminative and robust features. We use t-Distributed Stochastic Neighbor Embedding (t-SNE) to visualize the feature sets extracted by ResNet50, DeiT-Ti, Swin-T, PVT-Ti and our RVT respectively. The features are produced on validation set of ImageNet and ImageNet-C. We randomly selected 10  classes for better visualization. As shown in <ref type="figure">Figure 4</ref>, features extracted by our RVT is the closest to the intra-class compactness and inter-class separability. It's confirmed from the side that our RVT does have the stronger robustness and classification performance. We also visualize the feature maps of ResNet50, DeiT-S and our proposed RVT-S in <ref type="figure">Figure 5</ref>. Visualized features are extracted on the 5th layer of the models. The result shows ResNet50 and DeiT-S contain a large part of redundant features, highlighted by red boxes. While our RVT-S reduces the redundancy and ensures the diversity of features, reflecting the stronger generalization ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Loss Landscape Visualization</head><p>Loss landscape geometry has a dramatic effect on generalization and trainability of the model. We visualize the loss surfaces of ResNet50 and our RVT-S in <ref type="figure">Figure 6</ref>. RVT-S has a flatter loss surfaces, which means the stability under input changes.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Comparison between RVT and the baseline transformers. The robust accuracy in figure is recorded under FGSM [11] adversary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Overall architecture of the proposed Robust Vision Transformer (RVT).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2 )Figure 3 .</head><label>23</label><figDesc>Top: visualization of self-attention before and after the position-aware attention scaling. Bottom: visualization of learned scaling factor by our PAAS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .Figure 5 .Figure 6 .</head><label>456</label><figDesc>t-SNE visualization of features produced by different models. Feature visualization of ResNet50, DeiT-S and our proposed RVT-S trained on ImageNet. Red boxes highlight the feature maps with high similarity. Loss landscape of ResNet50 and RVT-S.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablations on other ViT components, where ?indicates the use of the corresponding component.</figDesc><table><row><cell>Patch Emb. Linear Conv. T2T</cell><cell>Local Conv. SA FFN</cell><cell cols="2">CLS Acc</cell><cell>Rob. Acc</cell></row><row><cell>?</cell><cell></cell><cell>?</cell><cell cols="2">72.2 22.3</cell></row><row><cell>?</cell><cell></cell><cell>?</cell><cell cols="2">73.6 23.2</cell></row><row><cell>?</cell><cell></cell><cell>?</cell><cell cols="2">74.9 25.4</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell cols="2">69.1 21.0</cell></row><row><cell>?</cell><cell>?</cell><cell></cell><cell cols="2">73.9 31.9</cell></row><row><cell>?</cell><cell></cell><cell>?</cell><cell cols="2">72.4 28.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>71.7 73.1 73.4 73.9 73.5 Rob. Acc 17.6 21.4 22.8 24.6 25.2 24.7 The performance variance with the number of heads. DeiT-Ti with head number of 1, 2, 4, 6, 8 and 12 are trained for comparison.</figDesc><table><row><cell>Heads</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>12</cell></row><row><cell>Acc</cell><cell>69.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>The performance of RVT and several SOTA CNNs and Transformers on ImageNet and six robustness benchmarks. RVT * represents the RVT model but trained with our proposed PAAS and patch-wise augmentation. Except for different architectures, we also compare some methods such as AugMix, which aims at improving the model robustness based on ResNet-50.</figDesc><table><row><cell>Group</cell><cell>Model</cell><cell cols="2">FLOPs Params (G) (M)</cell><cell cols="8">ImageNet Top-1 Top-5 FGSM PGD IN-C (?) IN-A IN-R IN-SK Robustness Benchmarks</cell></row><row><cell></cell><cell>ResNet-50 [15]</cell><cell>4.1</cell><cell>25.6</cell><cell>76.1</cell><cell>86.0</cell><cell>12.2</cell><cell>0.9</cell><cell>76.7</cell><cell>0.0</cell><cell>36.1</cell><cell>24.1</cell></row><row><cell></cell><cell>ResNet-50  *  [15]</cell><cell>4.1</cell><cell>25.6</cell><cell>79.0</cell><cell>94.4</cell><cell>36.3</cell><cell>12.5</cell><cell>65.5</cell><cell>5.9</cell><cell>42.5</cell><cell>31.5</cell></row><row><cell></cell><cell>Inception v3 [37]</cell><cell>5.7</cell><cell>27.2</cell><cell>77.4</cell><cell>93.4</cell><cell>22.5</cell><cell>3.1</cell><cell>80.6</cell><cell>10.0</cell><cell>38.9</cell><cell>27.6</cell></row><row><cell></cell><cell>RegNetY-4GF [31]</cell><cell>4.0</cell><cell>20.6</cell><cell>79.2</cell><cell>94.7</cell><cell>15.4</cell><cell>2.4</cell><cell>68.7</cell><cell>8.9</cell><cell>38.8</cell><cell>25.9</cell></row><row><cell></cell><cell>EfficientNet-B4 [39]</cell><cell>4.4</cell><cell>19.3</cell><cell>83.0</cell><cell>96.3</cell><cell>44.6</cell><cell>18.5</cell><cell>71.1</cell><cell>26.3</cell><cell>47.1</cell><cell>34.1</cell></row><row><cell>CNNs</cell><cell>ResNeXt50-32x4d [48]</cell><cell>4.3</cell><cell>25.0</cell><cell>79.8</cell><cell>94.6</cell><cell>34.7</cell><cell>13.5</cell><cell>64.7</cell><cell>10.7</cell><cell>41.5</cell><cell>29.3</cell></row><row><cell></cell><cell>DeepAugment [16]</cell><cell>4.1</cell><cell>25.6</cell><cell>75.8</cell><cell>92.7</cell><cell>27.1</cell><cell>9.5</cell><cell>53.6</cell><cell>3.9</cell><cell>46.7</cell><cell>32.6</cell></row><row><cell></cell><cell>ANT [33]</cell><cell>4.1</cell><cell>25.6</cell><cell>76.1</cell><cell>93.0</cell><cell>17.8</cell><cell>3.1</cell><cell>63.0</cell><cell>1.1</cell><cell>39.0</cell><cell>26.3</cell></row><row><cell></cell><cell>AugMix [18]</cell><cell>4.1</cell><cell>25.6</cell><cell>77.5</cell><cell>93.7</cell><cell>20.2</cell><cell>3.8</cell><cell>65.3</cell><cell>3.8</cell><cell>41.0</cell><cell>28.5</cell></row><row><cell></cell><cell>Anti-Aliased CNN [53]</cell><cell>4.2</cell><cell>25.6</cell><cell>79.3</cell><cell>94.6</cell><cell>32.9</cell><cell>13.5</cell><cell>68.1</cell><cell>8.2</cell><cell>41.1</cell><cell>29.6</cell></row><row><cell></cell><cell>Debiased CNN [22]</cell><cell>4.1</cell><cell>25.6</cell><cell>76.9</cell><cell>93.4</cell><cell>20.4</cell><cell>5.5</cell><cell>67.5</cell><cell>3.5</cell><cell>40.8</cell><cell>28.4</cell></row><row><cell></cell><cell>DeiT-Ti [40]</cell><cell>1.3</cell><cell>5.7</cell><cell>72.2</cell><cell>91.1</cell><cell>22.3</cell><cell>6.2</cell><cell>71.1</cell><cell>7.3</cell><cell>32.6</cell><cell>20.2</cell></row><row><cell></cell><cell>ConViT-Ti [6]</cell><cell>1.4</cell><cell>5.7</cell><cell>73.3</cell><cell>91.8</cell><cell>24.7</cell><cell>7.5</cell><cell>68.4</cell><cell>8.9</cell><cell>35.2</cell><cell>22.4</cell></row><row><cell></cell><cell>PiT-Ti [20]</cell><cell>0.7</cell><cell>4.9</cell><cell>72.9</cell><cell>91.3</cell><cell>20.4</cell><cell>5.1</cell><cell>69.1</cell><cell>6.2</cell><cell>34.6</cell><cell>21.6</cell></row><row><cell></cell><cell>PVT-Tiny [43]</cell><cell>1.9</cell><cell>13.2</cell><cell>75.0</cell><cell>92.5</cell><cell>10.0</cell><cell>0.5</cell><cell>79.6</cell><cell>7.9</cell><cell>33.9</cell><cell>21.5</cell></row><row><cell></cell><cell>RVT-Ti</cell><cell>1.3</cell><cell>8.6</cell><cell>78.4</cell><cell>94.2</cell><cell>34.8</cell><cell>11.7</cell><cell>58.2</cell><cell>13.3</cell><cell>43.7</cell><cell>30.0</cell></row><row><cell></cell><cell>RVT-Ti  *</cell><cell>1.3</cell><cell>10.9</cell><cell>79.2</cell><cell>94.7</cell><cell>42.7</cell><cell>18.9</cell><cell>57.0</cell><cell>14.4</cell><cell>43.9</cell><cell>30.4</cell></row><row><cell></cell><cell>DeiT-S [40]</cell><cell>4.6</cell><cell>22.1</cell><cell>79.9</cell><cell>95.0</cell><cell>40.7</cell><cell>16.7</cell><cell>54.6</cell><cell>18.9</cell><cell>42.2</cell><cell>29.4</cell></row><row><cell></cell><cell>ConViT-S [6]</cell><cell>5.4</cell><cell>27.8</cell><cell>81.5</cell><cell>95.8</cell><cell>41.0</cell><cell>17.2</cell><cell>49.8</cell><cell>24.5</cell><cell>45.4</cell><cell>33.1</cell></row><row><cell></cell><cell>Swin-T [25]</cell><cell>4.5</cell><cell>28.3</cell><cell>81.2</cell><cell>95.5</cell><cell>33.7</cell><cell>7.3</cell><cell>62.0</cell><cell>21.6</cell><cell>41.3</cell><cell>29.1</cell></row><row><cell></cell><cell>PVT-Small [43]</cell><cell>3.8</cell><cell>24.5</cell><cell>79.9</cell><cell>95.0</cell><cell>26.6</cell><cell>3.1</cell><cell>66.9</cell><cell>18.0</cell><cell>40.1</cell><cell>27.2</cell></row><row><cell></cell><cell>PiT-S [20]</cell><cell>2.9</cell><cell>23.5</cell><cell>80.9</cell><cell>95.3</cell><cell>41.0</cell><cell>16.5</cell><cell>52.5</cell><cell>21.7</cell><cell>43.6</cell><cell>30.8</cell></row><row><cell>Transformers</cell><cell>TNT-S [14]</cell><cell>5.2</cell><cell>23.8</cell><cell>81.5</cell><cell>95.7</cell><cell>33.2</cell><cell>4.2</cell><cell>53.1</cell><cell>24.7</cell><cell>43.8</cell><cell>31.6</cell></row><row><cell></cell><cell>T2T-ViT t-14 [51]</cell><cell>6.1</cell><cell>21.5</cell><cell>81.7</cell><cell>95.9</cell><cell>40.9</cell><cell>11.4</cell><cell>53.2</cell><cell>23.9</cell><cell>45.0</cell><cell>32.5</cell></row><row><cell></cell><cell>RVT-S</cell><cell>4.7</cell><cell>22.1</cell><cell>81.7</cell><cell>95.7</cell><cell>51.3</cell><cell>26.2</cell><cell>50.1</cell><cell>24.1</cell><cell>46.9</cell><cell>35.0</cell></row><row><cell></cell><cell>RVT-S  *</cell><cell>4.7</cell><cell>23.3</cell><cell>81.9</cell><cell>95.8</cell><cell>51.8</cell><cell>28.2</cell><cell>49.4</cell><cell>25.7</cell><cell>47.7</cell><cell>34.7</cell></row><row><cell></cell><cell>DeiT-B [40]</cell><cell>17.6</cell><cell>86.6</cell><cell>82.0</cell><cell>95.7</cell><cell>46.4</cell><cell>21.3</cell><cell>48.5</cell><cell>27.4</cell><cell>44.9</cell><cell>32.4</cell></row><row><cell></cell><cell>ConViT-B [6]</cell><cell>17.7</cell><cell>86.5</cell><cell>82.4</cell><cell>96.0</cell><cell>45.4</cell><cell>20.8</cell><cell>46.9</cell><cell>29.0</cell><cell>48.4</cell><cell>35.7</cell></row><row><cell></cell><cell>Swin-B [25]</cell><cell>15.4</cell><cell>87.8</cell><cell>83.4</cell><cell>96.4</cell><cell>49.2</cell><cell>21.3</cell><cell>54.4</cell><cell>35.8</cell><cell>46.6</cell><cell>32.4</cell></row><row><cell></cell><cell>PVT-Large [43]</cell><cell>9.8</cell><cell>61.4</cell><cell>81.7</cell><cell>95.9</cell><cell>33.1</cell><cell>7.3</cell><cell>59.8</cell><cell>26.6</cell><cell>42.7</cell><cell>30.2</cell></row><row><cell></cell><cell>PiT-B [20]</cell><cell>12.5</cell><cell>73.8</cell><cell>82.4</cell><cell>95.7</cell><cell>49.3</cell><cell>23.7</cell><cell>48.2</cell><cell>33.9</cell><cell>43.7</cell><cell>32.3</cell></row><row><cell></cell><cell>T2T-ViT t-24 [51]</cell><cell>15.0</cell><cell>64.1</cell><cell>82.6</cell><cell>96.1</cell><cell>46.7</cell><cell>17.5</cell><cell>48.0</cell><cell>28.9</cell><cell>47.9</cell><cell>35.4</cell></row><row><cell></cell><cell>RVT-B</cell><cell>17.7</cell><cell>86.2</cell><cell>82.5</cell><cell>96.0</cell><cell>52.3</cell><cell>27.4</cell><cell>47.3</cell><cell>27.7</cell><cell>48.2</cell><cell>35.8</cell></row><row><cell></cell><cell>RVT-B  *</cell><cell>17.7</cell><cell>91.8</cell><cell>82.7</cell><cell>96.5</cell><cell>53.0</cell><cell>29.9</cell><cell>46.8</cell><cell>28.5</cell><cell>48.7</cell><cell>36.0</cell></row><row><cell cols="5">under corruptions. All other benchmarks use Top-1 accu-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">racy on test data if no special illustration. The results are</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>reported in</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>.0% and 46.9% test accuracy on ImageNet-Sketch and ImageNet-R, buliding the new state-of-the-art.</figDesc><table><row><cell>Layers</cell><cell>Pos. Emb.</cell><cell>Acc</cell><cell>Rob. Acc</cell></row><row><cell>0-1</cell><cell cols="3">Ori. Ours 78.4 34.3 78.2 34.1</cell></row><row><cell>0-5</cell><cell cols="3">Ori. Ours 78.6 35.2 78.4 34.6</cell></row><row><cell>0-10</cell><cell cols="3">Ori. Ours 78.6 35.3 78.4 34.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc></figDesc><table><row><cell cols="3">Augmentations RC GN HF</cell><cell cols="2">Acc Rob. Acc</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell>78.9</cell><cell>41.5</cell></row><row><cell></cell><cell>?</cell><cell></cell><cell>79.0</cell><cell>42.0</cell></row><row><cell></cell><cell></cell><cell>?</cell><cell>79.1</cell><cell>41.3</cell></row><row><cell>?</cell><cell></cell><cell>?</cell><cell>78.8</cell><cell>41.3</cell></row><row><cell></cell><cell>?</cell><cell>?</cell><cell>79.0</cell><cell>41.9</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>79.2</cell><cell>41.7</cell></row><row><cell>Comparison of sin-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>gle and multiple block PAAS.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ori. stands for the learned ab-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>solute position embedding in</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>original ViTs.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 .</head><label>8</label><figDesc>Effect of our proposed PAAS and patch-wise augmentation on other ViT architectures.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 .</head><label>9</label><figDesc>Additional results of robustness analysis on different head number.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Table 10. Additional results of robustness analysis on different position encoding methods.</figDesc><table><row><cell cols="2">Position Embeddings</cell><cell>Acc</cell><cell>FGSM</cell><cell cols="4">PGD IN-C (?) IN-A IN-R</cell><cell>IN-SK</cell></row><row><cell></cell><cell cols="2">none 68.3</cell><cell>15.8</cell><cell>3.6</cell><cell>82.4</cell><cell>5.2</cell><cell>24.3</cell><cell>12.0</cell></row><row><cell cols="3">learned absolute Pos. 72.2</cell><cell>22.3</cell><cell>6.2</cell><cell>71.1</cell><cell>7.3</cell><cell>32.6</cell><cell>20.3</cell></row><row><cell cols="3">sin-cos absolute Pos. 72.0</cell><cell>21.9</cell><cell>5.9</cell><cell>71.9</cell><cell>7.0</cell><cell>31.4</cell><cell>20.2</cell></row><row><cell cols="3">learned relative Pos. [35] 71.8</cell><cell>22.3</cell><cell>6.1</cell><cell>71.6</cell><cell>7.6</cell><cell>32.5</cell><cell>18.6</cell></row><row><cell cols="3">input-conditioned Pos. [3] 72.4</cell><cell>21.5</cell><cell>5.3</cell><cell>72.5</cell><cell>6.8</cell><cell>31.0</cell><cell>18.0</cell></row><row><cell>Patch Emb. Linear Conv. T2T</cell><cell cols="7">Local Conv. CLS PGD IN-C (?) IN-A IN-R SA FFN</cell><cell>IN-SK</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell>?</cell><cell>6.2</cell><cell>71.1</cell><cell>7.3</cell><cell>32.6</cell><cell>20.3</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell>?</cell><cell>6.8</cell><cell>69.2</cell><cell>8.3</cell><cell>33.6</cell><cell>21.1</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell>?</cell><cell>4.7</cell><cell>69.6</cell><cell>10.1</cell><cell>36.7</cell><cell>23.8</cell></row><row><cell>?</cell><cell>?</cell><cell></cell><cell>?</cell><cell>9.0</cell><cell>76.9</cell><cell>4.8</cell><cell>28.7</cell><cell>16.6</cell></row><row><cell>?</cell><cell></cell><cell>?</cell><cell>?</cell><cell>12.7</cell><cell>65.0</cell><cell>8.4</cell><cell>39.0</cell><cell>31.9</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell></cell><cell>12.0</cell><cell>70.0</cell><cell>7.4</cell><cell>32.5</cell><cell>20.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 .Table 12 .Table 13 .</head><label>111213</label><figDesc>Additional results of robustness analysis on different patch embeddings, locality of attention, convolutional FFN and the replacement of CLS token. Additional results of robustness analysis on stage distribution. Additional results of position-aware attention scaling and patch-wise augmentation on other ViT architectures.</figDesc><table><row><cell>Var.</cell><cell cols="2">[S 1 , S 2 , S 3 , S 4 ]</cell><cell cols="2">Acc</cell><cell cols="2">FGSM</cell><cell cols="3">PGD IN-C (?) IN-A IN-R IN-SK</cell></row><row><cell>V1</cell><cell cols="2">[0, 0, 12, 0]</cell><cell cols="2">72.2</cell><cell cols="2">22.3</cell><cell>6.2</cell><cell>71.1</cell><cell>7.3</cell><cell>32.6</cell><cell>20.3</cell></row><row><cell>V2</cell><cell cols="2">[0, 0, 10, 2]</cell><cell cols="2">74.8</cell><cell cols="2">24.3</cell><cell>6.8</cell><cell>66.9</cell><cell>8.8</cell><cell>35.5</cell><cell>21.9</cell></row><row><cell>V3</cell><cell cols="2">[0, 2, 10, 0]</cell><cell cols="2">73.8</cell><cell cols="2">22.0</cell><cell>5.1</cell><cell>76.4</cell><cell>8.2</cell><cell>33.6</cell><cell>21.1</cell></row><row><cell>V4</cell><cell cols="2">[0, 2, 8, 2]</cell><cell cols="2">76.4</cell><cell cols="2">22.3</cell><cell>4.5</cell><cell>71.5</cell><cell>10.3</cell><cell>36.8</cell><cell>23.9</cell></row><row><cell>V5</cell><cell cols="2">[2, 2, 8, 0]</cell><cell cols="2">73.4</cell><cell cols="2">17.0</cell><cell>2.3</cell><cell>76.8</cell><cell>9.0</cell><cell>33.2</cell><cell>20.7</cell></row><row><cell>V6</cell><cell cols="2">[2, 2, 6, 2]</cell><cell cols="2">76.4</cell><cell cols="2">17.5</cell><cell>1.9</cell><cell>71.6</cell><cell>11.2</cell><cell>36.8</cell><cell>23.1</cell></row><row><cell></cell><cell>Models</cell><cell cols="2">Acc</cell><cell cols="4">FGSM PGD</cell><cell cols="2">IN-C (?) IN-A IN-R</cell><cell>IN-SK</cell></row><row><cell></cell><cell>DeiT-Ti</cell><cell cols="2">72.2</cell><cell></cell><cell>22.3</cell><cell cols="2">6.2</cell><cell>71.1</cell><cell>7.3</cell><cell>32.6</cell><cell>20.2</cell></row><row><cell></cell><cell>DeiT-Ti  *</cell><cell cols="2">74.4</cell><cell></cell><cell>29.9</cell><cell cols="2">9.1</cell><cell>67.9</cell><cell>8.1</cell><cell>34.9</cell><cell>23.1</cell></row><row><cell></cell><cell cols="3">ConViT-Ti 73.3</cell><cell></cell><cell>24.7</cell><cell cols="2">7.5</cell><cell>68.4</cell><cell>8.9</cell><cell>35.2</cell><cell>22.4</cell></row><row><cell></cell><cell>ConViT-Ti  *</cell><cell cols="2">74.4</cell><cell></cell><cell>30.7</cell><cell cols="2">9.6</cell><cell>65.6</cell><cell>9.4</cell><cell>37.0</cell><cell>25.2</cell></row><row><cell></cell><cell cols="3">PiT-Ti 72.9</cell><cell></cell><cell>20.4</cell><cell cols="2">5.1</cell><cell>69.1</cell><cell>6.2</cell><cell>34.6</cell><cell>21.6</cell></row><row><cell></cell><cell>PiT-Ti  *</cell><cell cols="2">74.3</cell><cell></cell><cell>27.7</cell><cell cols="2">7.9</cell><cell>66.7</cell><cell>7.1</cell><cell>36.6</cell><cell>24.0</cell></row><row><cell></cell><cell>DeiT-S</cell><cell cols="2">79.9</cell><cell></cell><cell>40.7</cell><cell cols="2">16.7</cell><cell>54.6</cell><cell>18.9</cell><cell>42.2</cell><cell>29.4</cell></row><row><cell></cell><cell>DeiT-S  *</cell><cell cols="2">80.6</cell><cell></cell><cell>42.3</cell><cell cols="2">18.8</cell><cell>53.1</cell><cell>20.5</cell><cell>43.5</cell><cell>31.3</cell></row><row><cell></cell><cell cols="3">ConViT-S 81.5</cell><cell></cell><cell>41.0</cell><cell cols="2">17.2</cell><cell>49.8</cell><cell>24.5</cell><cell>45.4</cell><cell>33.1</cell></row><row><cell></cell><cell>ConViT-S  *</cell><cell cols="2">81.8</cell><cell></cell><cell>42.3</cell><cell cols="2">18.7</cell><cell>49.1</cell><cell>25.6</cell><cell>46.1</cell><cell>34.2</cell></row><row><cell></cell><cell cols="3">PiT-S 80.9</cell><cell></cell><cell>41.0</cell><cell cols="2">16.5</cell><cell>52.5</cell><cell>21.7</cell><cell>43.6</cell><cell>30.8</cell></row><row><cell></cell><cell>PiT-S  *</cell><cell cols="2">81.4</cell><cell></cell><cell>42.2</cell><cell cols="2">18.3</cell><cell>51.4</cell><cell>23.3</cell><cell>44.6</cell><cell>32.3</cell></row><row><cell></cell><cell>Models</cell><cell>Acc</cell><cell></cell><cell cols="2">FGSM</cell><cell>PGD</cell><cell cols="2">IN-C (?)</cell><cell>IN-A</cell><cell>IN-R</cell><cell>IN-SK</cell></row><row><cell></cell><cell cols="2">DeiT-B 83.20</cell><cell></cell><cell cols="2">47.21</cell><cell>24.89</cell><cell></cell><cell>45.50</cell><cell>38.01</cell><cell>52.37</cell><cell>39.54</cell></row><row><cell></cell><cell cols="2">RVT-B 83.57</cell><cell></cell><cell cols="2">53.67</cell><cell>30.45</cell><cell></cell><cell>44.26</cell><cell>41.00</cell><cell>49.67</cell><cell>35.01</cell></row><row><cell></cell><cell>RVT-B  *</cell><cell>83.80</cell><cell></cell><cell cols="2">55.40</cell><cell>33.86</cell><cell></cell><cell>42.99</cell><cell>42.27</cell><cell>52.63</cell><cell>38.43</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 14 .</head><label>14</label><figDesc>RVT pre-trained on ImageNet-22K and finetuned on ImageNet-1K.</figDesc><table><row><cell></cell><cell>Output Size</cell><cell>Layer Name</cell><cell></cell><cell cols="2">DeiT-Ti (V1)</cell><cell></cell><cell>V4</cell><cell></cell><cell></cell><cell>V5</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Patch Embedding</cell><cell></cell><cell cols="2">C1 = 192</cell><cell></cell><cell cols="2">C1 = 96</cell><cell></cell><cell cols="2">C1 = 48</cell></row><row><cell>Stage 1</cell><cell>H 4 ? W 4</cell><cell>Transformer Encoder</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell>? ?</cell><cell>H1 = 48 C1 = 48 N1 = 1</cell><cell>? ? ? 2</cell></row><row><cell></cell><cell></cell><cell>Pooling Layer</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell cols="2">k = 2 ? 2</cell></row><row><cell>Stage 2</cell><cell>H 8 ? W 8</cell><cell>Transformer Encoder</cell><cell></cell><cell>-</cell><cell></cell><cell>? ?</cell><cell>H2 = 48 C2 = 96 N2 = 2</cell><cell>? ? ? 2</cell><cell>? ?</cell><cell>H2 = 48 C2 = 96 N2 = 2</cell><cell>? ? ? 2</cell></row><row><cell></cell><cell></cell><cell>Pooling Layer</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell cols="2">k = 2 ? 2</cell><cell></cell><cell cols="2">k = 2 ? 2</cell></row><row><cell>Stage 3</cell><cell>H 16 ? W 16</cell><cell>Transformer Encoder</cell><cell>? ?</cell><cell>H2 = 64 C2 = 192 N2 = 3</cell><cell>? ? ? 12</cell><cell>? ?</cell><cell>H3 = 64 C3 = 192 N3 = 3</cell><cell>? ? ? 8</cell><cell>? ?</cell><cell>H3 = 64 C3 = 192 N3 = 3</cell><cell>? ? ? 6</cell></row><row><cell></cell><cell></cell><cell>Pooling Layer</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell cols="2">k = 2 ? 2</cell><cell></cell><cell cols="2">k = 2 ? 2</cell></row><row><cell>Stage 4</cell><cell>H 32 ? W 32</cell><cell>Transformer Encoder</cell><cell></cell><cell>-</cell><cell></cell><cell>? ?</cell><cell>H3 = 64 C3 = 384 N3 = 6</cell><cell>? ? ? 2</cell><cell>? ?</cell><cell>H4 = 64 C4 = 384 N4 = 6</cell><cell>? ? ? 2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 15 .</head><label>15</label><figDesc>Detailed architecture of models used in robustness analysis on stage distribution. C, H and N represent the total feature dimension, feature dimension of each head and head number respectively. Only V4 and V5 are listed as examples. The other versions of the model can be generalized by V4 and V5.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional Results of Robustness Analysis on Designed Components</head><p>Here we will show the remaining results of robustness analysis in section 3. As the each component has been discussed detailly, we only give a summary of the results in appendix. We report the additional results of robustness analysis in <ref type="table">Table 10</ref> <ref type="bibr">, 9, 12, 11, 13</ref> and 14 respectively, where each table presents the results of one or some components. The detailed architecture of models used in robustness analysis on stage distribution is shown in <ref type="table">Table 15</ref>. Although each robustness benchmark is consistent on the overall trend, we still find some special cases. For example, in <ref type="table">Table 12</ref>, the V6 version of stage distribution poorly performs on adversarial robustness, but achieves best results on IN-A and IN-R datasets, showing the superior generalization power. Another case is the token-to-token embedder in <ref type="table">Table 11</ref>. Compared with original linear embedder, token-to-token embedder obtains better results on IN-C, IN-A, IN-R and IN-SK datasets. However, under PGD attacker, it only gets the robust accuracy of 4.7%. The above phenomenon also indicates that using only several robustness benchmarks is biased and cannot get a comprehensive assessment result. Therefore, we advocate that the works about model robustness in future should consider multiple benchmarks. For validating the generality of the proposed techniques, we show the robustness evaluation results when trained on other ViT architectures and larger datasets (ImageNet-22k) in <ref type="table">Table 13 and 14.   Heads  1  2  4  6  8  12</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Are transformers more robust than cnns?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieru</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Understanding robustness of transformers for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinadh</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayan</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Glasner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14586</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Do we really need explicit position encodings for vision transformers?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10882</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Inductive bias of deep convolutional networks through pooling geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amnon</forename><surname>Shashua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>St?phane D&amp;apos;ascoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Leavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulio</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Biroli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sagun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10697</idno>
		<title level="m">Convit: Improving vision transformers with soft convolutional inductive biases</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Computer Vision and Pattern Recognition</title>
		<meeting>the Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjing</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.00902</idno>
		<title level="m">Adversarially robust neural architectures</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Levit: a vision transformer in convnet&apos;s clothing for faster inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01136</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">When nas meets robustness: In search of robust architectures against adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhe</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Computer Vision and Pattern Recognition</title>
		<meeting>the Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="631" to="640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Transformer in transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00112</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Computer Vision and Pattern Recognition</title>
		<meeting>the Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The many faces of robustness: A critical analysis of out-of-distribution generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Dorundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samyak</forename><surname>Parajuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16241</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Augmix: A simple data processing method to improve robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representation</title>
		<meeting>the International Conference on Learning Representation</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.07174</idno>
		<title level="m">Natural adversarial examples</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Rethinking spatial dimensions of vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong Joon</forename><surname>Oh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.16302</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Rethinking the positional encoding in language pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.15595</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Shape-texture debiased neural network training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qihang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieru</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05707</idno>
		<title level="m">Localvit: Bringing locality to vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Defensive quantization: When efficiency meets robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rtowards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">On the robustness of vision transformers to adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaleel</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rigel</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marten</forename><surname>Van Dijk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02610</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Are sixteen heads really better than one?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayak</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.07581</idno>
		<title level="m">Vision transformers are robust learners</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Computer Vision and Pattern Recognition</title>
		<meeting>the Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10428" to="10436" />
		</imprint>
	</monogr>
	<note>Kaiming He, and Piotr Doll?r</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Kornia: an open source differentiable computer vision library for pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Riba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ponsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Rublee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Winter Conference on Applications of Computer Vision</title>
		<meeting>the Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3674" to="3683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A simple way to make neural networks robust against diverse image corruptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgenia</forename><surname>Rusak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Schott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Bitterwolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bringmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brendel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rulin</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouxing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15670</idno>
		<title level="m">On the adversarial robustness of visual transformers</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Selfattention with relative position representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Is robustness the cost of accuracy?-a comprehensive study on the robustness of 18 deep image classification models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongge</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yupeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="631" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Computer Vision and Pattern Recognition</title>
		<meeting>the Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">An effective antialiasing approach for residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10675</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning robust global representations by penalizing local predictive power</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songwei</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12122</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinghui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01279</idno>
		<title level="m">Do wider neural networks really help adversarial robustness? arXiv preprint</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15808</idno>
		<title level="m">Cvt: Introducing convolutions to vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.14536</idno>
		<title level="m">Smooth adversarial training</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Computer Vision and Pattern Recognition</title>
		<meeting>the Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Computer Vision and Pattern Recognition</title>
		<meeting>the Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Adversarial robustness vs. model compression, or both?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Shaokai Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Henrik</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Lambrechts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aojun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="111" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Incorporating convolution designs into visual transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaopeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aojun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.11816</idno>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokensto-token vit: Training vision transformers from scratch on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Making convolutional networks shiftinvariant again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
