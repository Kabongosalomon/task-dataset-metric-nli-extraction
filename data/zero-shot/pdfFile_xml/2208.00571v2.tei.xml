<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CLIFF: Carrying Location Information in Full Frames into Human Pose and Shape Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Li</surname></persName>
							<email>zhihao.li@huawei.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhuang</forename><surname>Liu</surname></persName>
							<email>liu.jianzhuang@huawei.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhensong</forename><surname>Zhang</surname></persName>
							<email>zhangzhensong@huawei.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songcen</forename><surname>Xu</surname></persName>
							<email>xusongcen@huawei.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youliang</forename><surname>Yan</surname></persName>
							<email>yanyouliang@huawei.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CLIFF: Carrying Location Information in Full Frames into Human Pose and Shape Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>3D human pose</term>
					<term>human body reconstruction</term>
					<term>2D to 3D</term>
					<term>global rotation</term>
					<term>projection</term>
					<term>global location</term>
					<term>pseudo annotation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Top-down methods dominate the field of 3D human pose and shape estimation, because they are decoupled from human detection and allow researchers to focus on the core problem. However, cropping, their first step, discards the location information from the very beginning, which makes themselves unable to accurately predict the global rotation in the original camera coordinate system. To address this problem, we propose to Carry Location Information in Full Frames (CLIFF) into this task. Specifically, we feed more holistic features to CLIFF by concatenating the cropped-image feature with its bounding box information. We calculate the 2D reprojection loss with a broader view of the full frame, taking a projection process similar to that of the person projected in the image. Fed and supervised by global-locationaware information, CLIFF directly predicts the global rotation along with more accurate articulated poses. Besides, we propose a pseudoground-truth annotator based on CLIFF, which provides high-quality 3D annotations for in-the-wild 2D datasets and offers crucial full supervision for regression-based methods. Extensive experiments on popular benchmarks show that CLIFF outperforms prior arts by a significant margin, and reaches the first place on the AGORA leaderboard (the SMPL-Algorithms track). The code and data are available at https: //github.com/huawei-noah/noah-research/tree/master/CLIFF.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Given a single RGB image, 3D human pose and shape estimation aims to reconstruct human body meshes with the help of statistic models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b59">60]</ref>. It is a fundamentally under-constrained problem due to the depth ambiguity. This problem attracts a lot of research because of its key role in many applications such as AR/VR, telepresence, and action analysis.</p><p>With the popular parametric human model SMPL <ref type="bibr" target="#b31">[32]</ref>, regression-based methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b37">38]</ref> learn to predict the SMPL parameters from image features in a data-driven way, and obtain the meshes from these predictions through a ig. 1. In the upper part, the cropped images look similar and thus get close predictions from a CNN model. However, two of the three predictions are wrong (marked in red). Actually, they have clearly different global rotations relative to the original camera, which can be seen from the bird's eye view. In the lower part, the three global rotations are the same, but again two estimations are wrong. We only show the yaw angles of the global rotations here for simplicity. linear function. Like most tasks in computer vision, there are two approaches to do this: top-down <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref> and bottom-up <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b62">63]</ref>. The former first detects humans, then crops the regions of interest, and processes each cropped image independently. The latter takes a full image as input and gives the results for all individuals at once. The top-down approach dominates this field currently, because it is decoupled from human detection, and has high recall and precision performances thanks to the mature detection technique <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>However, cropping, the first step in the top-down approach, discards location information from the very beginning, which is essential in estimating the global rotation in the original camera coordinate system with respect to the full image. Take <ref type="figure">Fig. 1</ref> as an example, where the original images come from a perspective camera with a common diagonal Field-of-View 55?. After cropping, the input images to the CNN model look similar, and thus get close predictions without surprise (see the upper part of <ref type="figure">Fig. 1</ref>). In fact, the three persons have clearly different global rotations, which can be inferred from the full image. The same problem exists for other 2D evidences such as 2D keypoints. As a result, the 2D reprojection loss calculated in the cropped images is not a proper supervision, which tends to twist articulated poses to compensate the global rotation error. In another word, missing the location information introduces extra ambiguity.</p><p>To fix this problem, we propose CLIFF: Carrying Location Information in Full Frames into 3D human pose and shape estimation, by making two major modifications to the previous top-down approach. First, CLIFF takes more holistic features as input. Besides the latent code of the resolution-fixed cropped image, the bounding box information is also fed to CLIFF, which encodes its discarded location and size in the original full image, providing the model with adequate information to estimate the global rotation. Second, CLIFF calculates the 2D reprojection loss with a broader view of the full frame. We obtain the predicted 3D joints in the original camera coordinate system, and project them onto the full image instead of the cropped one. Consequently, these predicted 2D keypoints have a projection process and perspective distortion similar to those of the person projected in the image, which is important for them to correctly supervise 3D predictions in an indirect way. Fed and supervised by global-location-aware information, CLIFF is able to predict the global rotation relative to the original camera along with more accurate articulated poses.</p><p>On the other hand, the regression-based methods need full supervision for SMPL parameters to boost their performances <ref type="bibr" target="#b24">[25]</ref>. However, it costs a lot of time and effort to obtain these 3D annotations, using multi-view motion capture systems <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b34">35]</ref> or a set of IMU devices <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b33">34]</ref>. Moreover, the lack of diversity in actors and scenes limits the generalization abilities of these data-driven models. On the contrary, 2D keypoints are straightforward and inexpensive to annotate for a wide variety of in-the-wild images with diverse persons and backgrounds <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b1">2]</ref>. Hence, some CNN-based pseudo-ground-truth (pseudo-GT) annotators <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b36">37]</ref> are introduced to lift these 2D keypoints up to 3D poses for full supervision. Since these annotators are based on previous top-down models that are agnostic to the person location in the full frame, they produce inaccurate annotations, especially for the global rotation.</p><p>We propose a novel annotator based on CLIFF with a global perspective of the full frame, which produces high-quality annotations of human model parameters. Specifically, we first pretrain the CLIFF annotator on several datasets with available 3D ground truth, and then test it on the target dataset to predict SMPL parameters. Using these predictions as regularization and ground-truth 2D keypoints as weak supervision, we finetune the pretrained model on the target dataset, and finally test on it to infer SMPL parameters as pseudo-GT. With the implicit prior of the pretrained weights and the explicit prior of the SMPL parameter predictions, the CLIFF annotator alleviates the inherent depth ambiguity to recover feasible 3D annotations from monocular images.</p><p>Our contributions are summarized as follows:</p><p>-We reveal that the global rotations cannot be accurately inferred when only using cropped images, which is ignored by previous methods, and propose CLIFF to deal with this problem by feeding and supervising the model with global-location-aware information. -Based on CLIFF, we propose a pseudo-GT annotator with strong priors to generate high-quality 3D annotations for in-the-wild images, which are demonstrated to be very helpful in performance boost.</p><p>-We conduct extensive experiments on popular benchmarks and show that CLIFF outperforms prior arts by significant margins on several evaluation metrics (e.g., 5.7mm MPJPE and 6.5mm PVE on 3DPW), and reaches the first place on the AGORA leaderboard (the SMPL-Algorithms track).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>3D human pose estimation. 3D human pose is usually represented as a skeleton of 3D joints <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b11">12]</ref>, or a mesh of triangle vertices <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>. These vertex locations are inferred directly by model-free methods <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>, or obtained indirectly from parametric model (e.g., SMPL <ref type="bibr" target="#b31">[32]</ref>) predictions of model-based methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b46">47]</ref>. Optimization-based methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11]</ref> are first proposed to iteratively fit the SMPL model to 2D evidences, while regression-based ones <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b61">62]</ref> make the predictions in a straightforward way that may support real time applications. Both top-down <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b63">64]</ref> and bottomup <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b62">63]</ref> approaches can do the job. Because our method is a top-down framework to regress the SMPL parameters, we only review the most relevant work, and refer the reader to <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b54">55]</ref> for more comprehensive surveys.</p><p>Input and supervision. Previous top-down methods take as input the cropped image <ref type="bibr" target="#b17">[18]</ref> or/and 2D keypoints in the cropped region <ref type="bibr" target="#b7">[8]</ref>, perhaps with additional camera parameters <ref type="bibr" target="#b23">[24]</ref>. Most of them project the predicted 3D joints onto the cropped image to compute the reprojection loss for supervision. Since the location information is lost in the cropped image, it is difficult for them to estimate an accurate global rotation. To solve this problem, Kissos et al. <ref type="bibr" target="#b20">[21]</ref> use the prediction with respect to the cropped image as initialization, and then use SMPLify <ref type="bibr" target="#b3">[4]</ref> to refine the results for better pixel alignment. Since SMPLify computes the reprojection loss in the full image, they obtain a better global rotation in the end. However, as an optimization approach, SMPLify is very slow and may harm the articulated pose estimation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b36">37]</ref>. PCL <ref type="bibr" target="#b60">[61]</ref> warps the cropped image to remove the perspective distortion, and corrects the global rotation via a post-processing, but the articulated poses cannot be corrected. In order to better estimate the root translation in multi-agent aerial applications, AirPose <ref type="bibr" target="#b47">[48]</ref> also provides the additional location information, but in a simpler way without clear geometric meanings. CLIFF exploits the location information in both input and supervision, to predict more accurate global rotation and articulated poses simultaneously, without any post-processing.</p><p>Pseudo-GT annotators. It tends to help regression-based models generalize better to train with diverse in-the-wild images. However, it is hard to obtain the corresponding 3D ground truth, so pseudo-GT annotators are proposed.</p><p>Optimization-based annotators <ref type="bibr" target="#b3">[4]</ref> throw the images away, and fit the human model to 2D keypoints by minimizing the reprojection loss. CNN-based annotators <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b12">13]</ref> are recently introduced to get better results, taking the cropped images as input. They all need priors to deal with the depth ambiguity. An extra model such as GMM <ref type="bibr" target="#b3">[4]</ref>, GAN <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b8">9]</ref> or VAE <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b21">22]</ref> is trained on a large motion capture dataset AMASS <ref type="bibr" target="#b32">[33]</ref> to be an implicit prior. Other methods search for plausible SMPL parameters that may be close to the ground truth to be an explicit prior <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b12">13]</ref>. We propose a novel annotator based on CLIFF, which takes more than the cropped image as input and calculates the 2D supervision in the full image, and use the SMPL parameter predictions by the pretrained CLIFF annotator as an effective explicit prior, without an extra model and human actors mimicking the predefined poses <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b23">24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>In this section, we first briefly review the commonly-used parametric model SMPL and the baseline method HMR, then propose our model CLIFF, and finally present a novel pseudo-GT annotator for in-the-wild images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SMPL Model</head><p>As a parametric human body model, SMPL <ref type="bibr" target="#b31">[32]</ref> provides a differentiable function, which takes dozens of parameters ? = {?, ?} as input, and returns a posed 3D mesh of 6890 vertices V , where ? ? R 24?3 and ? ? R 10 are the pose and shape parameters, respectively. The pose parameters ? consist of the global rotation of the root joint (pelvis) with respect to some coordinate system (e.g., the camera coordinate system in our work), and 23 local rotations of other articulated joints relative to their parents along the kinematic tree. The k joints of interest J 3D ? R k?3 can be obtained by the linear combination of the mesh vertices, J 3D = M V , where M is a pretrained sparse matrix for these k joints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">HMR Model</head><p>HMR <ref type="bibr" target="#b17">[18]</ref> is a simple and widely-used top-down method for 3D human pose and shape estimation. Its architecture is shown in <ref type="figure" target="#fig_6">Fig. 2(a)</ref>. A square cropped image is resized to 224 ? 224 and passed through a convolutional encoder. Then an iterative MLP regressor predicts the SMPL parameters ? = {?, ?} and weakperspective projection parameters P weak = {s, t x , t y } for a virtual camera M crop with respect to the cropped image (see <ref type="figure" target="#fig_2">Fig. 3</ref>), where s is the scale parameter, and t x and t y are the root translations relative to M crop along the X and Y axes, respectively. With a predefined large focal length f HM R = 5000, P weak can be transformed to perspective projection parameters</p><formula xml:id="formula_0">P persp = {f HM R , t crop }, where t crop = [t crop X , t crop Y , t crop Z</formula><p>] denotes the root translations relative to M crop along the X, Y , and Z axes respectively:</p><formula xml:id="formula_1">t crop X = t x , t crop Y = t y , t crop Z = 2 ? f HM R r ? s ,<label>(1)</label></formula><p>where r = 224 denotes the side resolution of the resized square crop. The loss of HMR is defined as:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HMR-main</head><formula xml:id="formula_2">L HM R = ? SM P L L SM P L + ? 3D L 3D + ? 2D L crop 2D ,<label>(2)</label></formula><p>and its terms are calculated by:</p><formula xml:id="formula_3">L SM P L = ? ?? , L 3D = J 3D ?? 3D , L crop 2D = J crop 2D ?? crop 2D ,<label>(3)</label></formula><p>where?,? 3D , and? crop 2D are the ground truth, and the predicted 2D keypoints in the cropped image are obtained by the perspective projection ?:</p><formula xml:id="formula_4">J crop 2D = ?J crop 3D = ?(J 3D + 1t crop ),<label>(4)</label></formula><p>where 1 ? R k?1 is an all-ones matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">CLIFF Model</head><p>As described above, previous top-down methods take only the cropped image as input, and calculate the reprojection loss in the cropped image, which may lead to inaccurate predictions, as emphasized in Section 1. To address this problem, we take HMR as the baseline and propose to make two modifications to build CLIFF, as shown in <ref type="figure" target="#fig_6">Fig. 2</ref> First, CLIFF takes more holistic features as input. Besides the encoded image feature, the additional bounding box information I bbox of the cropped region is also fed to the regressor,</p><formula xml:id="formula_5">(b).</formula><formula xml:id="formula_6">I bbox = [ c x f CLIF F , c y f CLIF F , b f CLIF F ],<label>(5)</label></formula><p>where (c x , c y ) is its location relative to the full image center, b its original size, and f CLIF F the focal length of the original camera M f ull used in CLIFF (see <ref type="figure" target="#fig_2">Fig. 3</ref>). Besides the effect of normalization, taking f CLIF F as the denominator gives geometric meanings to the first two terms in I bbox :</p><formula xml:id="formula_7">tan ? X = c x f CLIF F , tan ? Y = c y f CLIF F ,<label>(6)</label></formula><p>where ? = [? X , ? Y , 0] is the transformation angle from M crop to the original camera M f ull coordinate system with respect to the full image, as shown in <ref type="figure" target="#fig_2">Fig.  3</ref>. Therefore, fed with I bbox as part of the input, the regressor can make the transformation implicitly to predict the global rotation relative to M f ull , which also bring benefits to the articulated pose estimation. As for the focal length, we use the ground truth if it is known; otherwise we approximately estimate its value as f CLIF F = ? w 2 + h 2 , where w and h are the width and height of the full image respectively, corresponding to a diagonal Field-of-View of 55?for M f ull , following the previous work <ref type="bibr" target="#b20">[21]</ref>.</p><p>Second, CLIFF calculates the reprojection loss in the full image instead of the cropped one. The root translation is transformed from M crop to M f ull :</p><formula xml:id="formula_8">t f ull X = t crop X + 2 ? c x b ? s , t f ull Y = t crop Y + 2 ? c y b ? s , t f ull Z = t crop Z ? f CLIF F f HM R ? r b ,<label>(7)</label></formula><p>where</p><formula xml:id="formula_9">t f ull = [t f ull X , t f ull Y , t f ull Z ]</formula><p>denotes the root translations relative to M f ull along the X ? , Y ? , and Z ? axes, respectively. The derivation can be found in our supplementary materials. Then we project the predicted 3D joints onto the full image plane:</p><formula xml:id="formula_10">J f ull 2D = ?J f ull 3D = ?(J 3D + 1t f ull ),<label>(8)</label></formula><p>to compute the 2D reprojection loss:</p><formula xml:id="formula_11">L f ull 2D = J f ull 2D ?? f ull 2D ,<label>(9)</label></formula><p>where the ground truth? f ull 2D is also relative to the full image center. Finally, the total loss of CLIFF is calculated by:</p><formula xml:id="formula_12">L CLIF F = ? SM P L L SM P L + ? 3D L 3D + ? 2D L f ull 2D .<label>(10)</label></formula><p>The predicted 2D keypoints J f ull 2D share a similar projection process and perspective distortion with the person in the image, especially when the focal length f CLIF F is close to its ground truth. Thus, the corresponding loss L f ull 2D can correctly supervise CLIFF to make more accurate prediction of 3D human poses, especially the global rotation, which is demonstrated in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">CLIFF Annotator</head><p>Full supervision from 3D ground truth (particularly the SMPL parameters) is crucial for regression-based methods to improve their performances on 3D human pose and shape estimation <ref type="bibr" target="#b24">[25]</ref>. However, these annotations are scarce for inthe-wild datasets, since they require specialized devices and cost a lot of time and labor <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b33">34]</ref>. Recently, CNN-based pseudo-GT annotators <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b36">37]</ref> are proposed to address this problem. However, their base models are agnostic to the person locations in full frames, and thus they produce inaccurate annotations, especially for the global rotations, as mentioned in Section 1.</p><p>Hence, we propose an annotator based on CLIFF, which is fed and supervised with global-location-aware information, and thus produces better global rotation and articulated pose annotations simultaneously. As shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, there are four steps in our pipeline to annotate an in-the-wild dataset with only 2D keypoint ground truth. 1. We pretrain the CLIFF annotatorH on several datasets with ground-truth SMPL parameters, including 3D datasets and 2D datasets with pseudo-GT generated by EFT <ref type="bibr" target="#b16">[17]</ref>. The pretrained weights serve as an implicit prior from these various datasets for the following optimization in Step 3. 2. Test the pretrained modelH on the target dataset to predict SMPL param-eters?. Although these predictions may not be accurate, they can be an explicit prior to guide the optimization, and cost little without the need for crowd-sourced participants to mimick some predefined poses <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b23">24]</ref>. 3. Finetune the pretrained modelH on the target dataset, using ground-truth 2D keypoints as weak supervision and? as regularization, to get the updated annotator H. Due to the depth ambiguity, these 2D keypoints are insufficient to supervise the optimization to recover their 3D ground truth. Thus, the priors are very important because they can prevent H from overfitting to these 2D keypoints and from offering implausible solutions. 4. Test H on the target dataset to get SMPL parameter predictions ? as the final pseudo-GT. In our experiment, the reconstructed 3D meshes from these pseudo-GT are pixel-aligned to their 2D evidences, and also perceptually realistic which can be confirmed from novel views.</p><p>Compared with other annotators whose priors come from extra models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b42">43]</ref> trained on another large motion capture dataset AMASS <ref type="bibr" target="#b32">[33]</ref>, the CLIFF annotator contains strong priors that are efficient to obtain with no need for an extra model and AMASS. More importantly, based on CLIFF, the annotator produces much better pseudo-GT which is very helpful to boost the training performance as shown in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Implementation details</head><p>We train CLIFF for 244K steps with batch size 256, using the Adam optimizer <ref type="bibr" target="#b19">[20]</ref>. The learning rate is set to 1 ? e ?4 and reduced by a factor of 10 in the middle. The image encoder is pretrained on ImageNet <ref type="bibr" target="#b9">[10]</ref>. The cropped images are resized to 224?224, preserving the aspect ratio. Data augmentation includes random rotations and scaling, horizontal flipping, synthetic occlusion <ref type="bibr" target="#b48">[49]</ref>, and random cropping <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b45">46]</ref>. To annotate an in-the-wild dataset, we train the CLIFF annotator for 30 epochs with learning rate 5 ? e ?5 but no data augmentation. We use MindSpore <ref type="bibr" target="#b0">[1]</ref> and PyTorch <ref type="bibr" target="#b40">[41]</ref> for the implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>Datasets. Following previous work, we train CLIFF on a mixture of 3D datasets (Human3.6M <ref type="bibr" target="#b15">[16]</ref> and MPI-INF-3DHP <ref type="bibr" target="#b34">[35]</ref>), and 2D datasets (COCO <ref type="bibr" target="#b29">[30]</ref> and MPII <ref type="bibr" target="#b1">[2]</ref>) with pseudo-GT provided by the CLIFF annotator. The evaluation is performed on three datasets: 1) 3DPW <ref type="bibr" target="#b33">[34]</ref>, an in-the-wild dataset with 3D annotations from IMU devices; 2) Human3.6M, an indoor dataset providing 3D ground truth from optical markers with a multi-view setup; 3) AGORA <ref type="bibr" target="#b41">[42]</ref>, a synthetic dataset with highly accurate 3D ground truth. We use the 3DPW and AGORA training data when conducting experiments on them respectively. Evaluation metrics. The three standard metrics in our experiments are briefly described below. They all measure the Euclidean distances (in millimeter (mm)) of 3D points between the predictions and ground truth.</p><p>MPJPE (Mean Per Joint Position Error) first aligns the predicted and ground-truth 3D joints at the pelvis, and then calculates their distances, which comprehensively evaluates the predicted poses and shapes, including the global rotations. PA-MPJPE (Procrustes-Aligned Mean Per Joint Position Error, or reconstruction error) performs Procrustes alignment before computing MPJPE, which mainly measures the articulated poses, eliminating the discrepancies in scale and global rotation. PVE (Per Vertex Error, or MVE used in the AGORA evaluation) does the same alignment as MPJPE at first, but calculates the distances of vertices on the human mesh surfaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparisons with State-of-the-art Methods</head><p>Quantitative results. We compare CLIFF with prior arts, including videobased methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b57">58]</ref> that exploit temporal information, and frame-based ones <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b24">25]</ref> that process each frame independently. They could be modelbased <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23]</ref> or model-free <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b27">28]</ref>, and most of them are top-down methods, except for one bottom-up <ref type="bibr" target="#b53">[54]</ref>. As shown in <ref type="table" target="#tab_0">Table 1</ref>, CLIFF outperforms them   by significant margins in all metrics on these three evaluation datasets. With the same image encoder backbone (ResNet-50 <ref type="bibr" target="#b14">[15]</ref>) and similar computation cost, CLIFF beats its baseline HMR-EFT, reducing the errors by more than 13mm on MPJPE and PVE. In the case of similar PA-MPJPE to other methods, CLIFF still has lower MPJPE and PVE, since it has a better global rotation estimation. With HRNet-W48 <ref type="bibr" target="#b52">[53]</ref>, CLIFF has better performance and distinct advantages over previous state-of-the-art, including METRO <ref type="bibr" target="#b27">[28]</ref> and Mesh Graphormer <ref type="bibr" target="#b28">[29]</ref> which have similar image encoder backbones (HRNet-W64) and transformer-based architectures <ref type="bibr" target="#b56">[57]</ref>. CLIFF reaches the first place on the AGORA leaderboard (the SMPL-Algorithms track) way ahead of other methods (whose results are from the leaderboard).</p><p>Qualitative results. As shown in <ref type="figure" target="#fig_4">Fig. 5</ref>  obtained by PARE, we can see clear pixel-misalignment of its results overlaid on the images, suffering from its inferior global rotation estimation. From the novel viewpoints, we can see that the predicted meshes by CLIFF overlay with the ground truth better than those by PARE, thanks to its more accurate global rotation estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>We take HMR as the baseline and make two modifications to build CLIFF: additional input of the bounding box information (CI, denoting the CLIFF Input), and the 2D reprojection loss calculated in the full frame for supervision (CS, denoting the CLIFF Supervision). As shown in <ref type="table" target="#tab_1">Table 2</ref>, we conduct an ablation study on Human3.6M, since it has accurate 3D ground truth. Without CI providing enough information, MPJPE increases significantly, indicating a worse global rotation estimation. It causes larger errors when we also drop CS that can guide CLIFF to better predictions. This study validates our intuition that global-location-aware information helps the model predict the global rotation and obtain more accurate articulated poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Image Front View</head><p>Side View Input Image Front View Side View <ref type="figure">Fig. 6</ref>. Qualitative results of the CLIFF annotator on the MPII dataset. From left to right: input images, the front view and side view of the pseudo-GT. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Annotator Comparisons</head><p>Direct comparison. In <ref type="table" target="#tab_2">Table 3</ref>, we directly compare different pseudo-GT annotators on 3DPW, for it is an in-the-wild dataset with ground-truth SMPL parameters. The CLIFF annotator outperforms other methods in all metrics. Even with similar PA-MPJPE to Pose2Mesh <ref type="bibr" target="#b7">[8]</ref>, CLIFF reduces MPJPE by 12.3mm, and PVE by 20.5mm. Compared to EFT <ref type="bibr" target="#b16">[17]</ref> that finetunes a pretrained model on each example, the CLIFF annotator is trained in a mini-batch manner, which helps it maintain the implicit prior all the way. With the additional explicit prior, there is no need for our annotator to choose a generic stopping criterion carefully. It only takes about 30 minutes for the CLIFF annotator to annotate the whole 35,515 images with 4 Tesla V100 GPUs (the finetuning and final testing steps described in Section 3.4).</p><p>Indirect comparison. We train CLIFF on the COCO dataset with pseudo-GT from different annotators, and show their results on 3DPW and Human3.6M. The training lasts for 110K steps without learning rate decay. As shown in <ref type="table" target="#tab_4">Table  4</ref>, the CLIFF annotator has much better performance than SPIN <ref type="bibr" target="#b24">[25]</ref> and EFT <ref type="bibr" target="#b16">[17]</ref> (more than 13mm margins on MPJPE and PVE). It demonstrates that the CLIFF annotator can generate high-quality pseudo-GT for in-the-wild images with only 2D annotations, which helps to improve performances significantly.</p><p>Qualitative results. In <ref type="figure">Fig. 6</ref>, we show pseudo-GT samples generated by the CLIFF annotator. With good 2D keypoint annotations, the reconstructed meshes are pixel-aligned to the image evidence. From the side view, we can see that they are also perceptually realistic without obvious artifacts, thanks to the strong priors in the CLIFF annotator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>In Section 3, we show how to build CLIFF based on HMR by making two modifications. We believe that the idea can also be applied to many other methods. First, it can benefit regression-based top-down methods that work on the cropped-region features (e.g., image, keypoint, edge, and silhouette). As for bottom-up methods that treat all the subjects without distinction of their different locations, we can take another form to encode the location information, for example, a location map which consists of a normalized coordinate for each pixel. Going beyond 3D human pose estimation, we can apply the idea to other 3D tasks that involve object global rotations (e.g., 3D object detection and 6-DoF object pose estimation). Even when there are perfect 3D annotations and thus no need for the 2D reprojection loss calculated in the full image, it is still important to take the global-location-aware information as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Although translation invariance is a key factor for CNN models to succeed in computer vision, we argue that the global location information in full frames matters in 3D human pose and shape estimation, and the global rotations cannot be accurately inferred when only using cropped images. To address this problem, we propose CLIFF by feeding and supervising the model with globallocation-aware information. CLIFF takes not only the cropped image but also its bounding box information as input. It calculates the 2D reprojection loss in the full image instead of the cropped one, projecting the predicted 3D joints in a way similar to that of the person projected in the image. Moreover, based on CLIFF, we propose a novel pseudo-GT annotator for in-the-wild 2D datasets, which generates high-quality 3D annotations to help regression-based models boost their performances. Extensive experiments on popular benchmarks show that CLIFF outperforms state-of-the-art methods by a significant margin and reaches the first place on the AGORA leaderboard (the SMPL-Algorithms track). Let D (the projection of F) be the image center, and C (the projection of E) be the crop (i.e., detection result) center. Then the root translation of the human body along the X ? axis is calculated by:</p><formula xml:id="formula_13">t f ull X = t x + ?t f ull X , (7.3)</formula><p>where ?t f ull X is the X ? coordinate of point E. Since the two triangles ?OCD and ?OEF in <ref type="figure">Fig. 1 (in yellow)</ref> are similar, we have:</p><formula xml:id="formula_14">?t f ull X t f ull Z = c x f CLIF F . (7.4)</formula><p>Combining Equations 7.2 and 7.4, we obtain:</p><formula xml:id="formula_15">?t f ull X = 2 ? c x b ? s . (7.5)</formula><p>Similarly, it also holds for the root translation along the Y ? axis: The orthogonal projection in the weak-perspective projection omits the Z ? coordinate discrepancy inside the human body, which assumes the human body is far from the camera whose focal length is unrealistically large (corresponding to a very small field-of-view) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">25]</ref>. This is not true for many cases. Thus we use the perspective projection with an appropriate focal length to calculate the 2D reprojection loss, because this is how the original image is captured. However, we still let the model predict the weak-perspective projection parameters, since for most cases, t x ? [?1, 1], t y ? [?1, 1], s ? [0, 1], meaning that they have the normalization property, which makes them suitable to be the CNN predictions.</p><formula xml:id="formula_16">t f ull Y = t y + ?t f ull Y = t y + 2 ? c y b ? s . (7.6)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reconstructed Meshes Input &amp; BBoxes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Impact of the BBox Quality</head><p>The BBox quality is important to our method, just like other top-down methods. However, taking the BBox information as the additional input does not make our method rely more on the BBox quality. As demonstrated in the AGORA evaluation, we use the BBox predicted by Mask R-CNN which is trained on COCO without finetuning on AGORA; yet CLIFF still reaches the first place on the leaderboard, outperforming other top-down and bottom-up methods by large margins. Note that AGORA contains a lot of crowded and severely occluded scenes, as shown in <ref type="figure" target="#fig_6">Fig. 2</ref>. CLIFF is robust to inaccurate BBox detection, mainly thanks to the data augmentation such as random scaling and cropping.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Impact of the Focal Length as Part of the Input</head><p>We conduct this experiment on the 3DPW test set by perturbing the focal length from its GT value f GT . As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, CLIFF is robust (with less than 5% error increase) when the estimated focal length is in [0.4f GT , 3f GT ]. The estimation f CLIF F = ? w 2 + h 2 is within this range for most cases (except for super telephotos). Moreover, in practical applications, f GT is often known, making the performance guaranteed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Smoothness Comparison with Video-Based Methods</head><p>We can apply CLIFF to a video frame by frame, and perform temporal smoothing to reduce jitter, such as OneEuro filtering <ref type="bibr" target="#b4">[5]</ref>. Video-based methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b57">58]</ref> usually make temporally smooth 3D predictions, which is their advantage over frame-based methods. However, they cost much computation by processing additional adjacent frames. Here we compare CLIFF with these video-based methods, especially on the smoothness evaluation, as shown in <ref type="table" target="#tab_0">Table 1</ref>. The metric for evaluating temporal smoothness is acceleration error, which measures the average difference between ground truth 3D acceleration and the predicted 3D acceleration of each joint in mm/s 2 . CLIFF, as a frame-based method, achieves  <ref type="figure" target="#fig_3">Fig. 4</ref>. The curves of training the CLIFF annotator on the 3DPW test data. The evaluation errors do not diverge even for long training, which means there is no need for the CLIFF annotator to choose a generic stopping criterion carefully. comparable smoothness performance with video-based methods. With the additional OneEuro filtering as post-processing which costs negligible extra computation, the smoothness performance is improved significantly with slightly larger pose errors, which are still much smaller than those of the competitors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CLIFF Annotator Training</head><p>In <ref type="figure" target="#fig_3">Fig. 4</ref>, we show the evaluation error curves of training the CLIFF annotator on the 3DPW test data. The learning rate starts from 5 ? e ?5 , and is reduced by a factor of 10 at the 45th epoch. We can obtain a fine model before the 60th epoch, and the evaluation errors do not diverge even for longer training (120 epochs in total), and may decrease for a better performance. It means that the CLIFF annotator is robust in the optimization, because the proposed priors prevent the annotator from overfitting to the 2D keypoints and from producing implausible poses. Consequently, there is no need for our annotator to choose a generic stopping criterion carefully, which is a serious problem for EFT <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Ablation Study of the CLIFF Annotator</head><p>We implement the proposed pseudo-GT annotator based on HMR, and compare it to the CLIFF-based one on 3DPW. As shown in <ref type="table" target="#tab_1">Table 2</ref>, the errors increase when switching the base model from CLIFF to HMR, but the HMR-based annotator is still better than other SOTA methods. Note that Pose2Mesh <ref type="bibr" target="#b7">[8]</ref>, as a model-free method, produces only 3D vertices but no SMPL parameters.</p><p>7 More Qualitative Pseudo-GT Results</p><p>In <ref type="figure" target="#fig_4">Fig. 5</ref>, we show additional qualitative results in the CLIFF annotator experiments. We test the pretrained annotator on the target images to get predictions as the explicit prior, which may not be accurate but usually plausible. The final pseudo-GT achieves better pixel alignment, and maintains the plausibility with the help of the proposed priors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Front View Front View Side View</head><p>Side View of the Explicit Prior of the Pseudo-GT 2D annotation </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2 Fig. 2 .</head><label>22</label><figDesc>(a) HMR architecture where the detail of its main part "HMR-main" is shown in the gray box in (b). (b) CLIFF architecture that takes HMR as its backbone and has two modifications: 1) CLIFF takes as input not only the cropped image but also the bounding box information. 2) CLIFF projects the predicted 3D joints onto the original full image plane to compute the 2D reprojection loss, while HMR does this in the cropped image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The geometric relation between the virtual camera Mcrop for the cropped image (the red rectangle) and the original camera M f ull for the full image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Our pipeline to generate 3D annotations for a 2D dataset. (a) Pretrain CLIFF modelH with full supervision on available datasets. (b) Then testH on the target dataset to predict SMPL parameters?. (c) FinetuneH with 2D keypoints as weak supervision and? as regularization, obtaining the updated model H. (d) Finally, test H on the target dataset to infer SMPL parameters as pseudo-GT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Qualitative comparison on 3DPW. From left to right: input images, CLIFF predictions, PARE predictions, and their visualizations from novel views (green for the ground truth, blue for CLIFF, and red for PARE).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>2 Fig. 1 .</head><label>21</label><figDesc>The transformation from weak-perspective projection to perspective projection (bird's eye view). A weak-perspective projection can be regarded as an orthogonal projection followed by a perspective projection. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 2 .</head><label>2</label><figDesc>Qualitative results on the AGORA test samples. CLIFF works well in crowded and occluded scenes, even when large body parts are missing in the BBoxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 3 .</head><label>3</label><figDesc>Impact of the focal length on estimation errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 5 .</head><label>5</label><figDesc>More pseudo-GT samples from the CLIFF annotator. From left to right: 2D annotation, front view of the explicit prior, side view of the explicit prior, front view of the pseudo-GT, and side view of the pseudo-GT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance comparison between CLIFF and state-of-the-art methods on 3DPW, Human3.6M and AGORA PA-MPJPE ? PVE ? MPJPE ? PA-MPJPE ? MPJPE ? MVE ?</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>3DPW</cell><cell></cell><cell cols="2">Human3.6M</cell><cell cols="2">AGORA</cell></row><row><cell cols="3">Method HMMR [19] TCMR [7] MPJPE ? video 116.5 86.5 VIBE [22] 82.7</cell><cell>72.6 52.7 51.9</cell><cell>-102.9 99.1</cell><cell>--65.6</cell><cell>56.9 -41.1</cell><cell>---</cell><cell>---</cell></row><row><cell></cell><cell>MAED [58]</cell><cell>79.1</cell><cell>45.7</cell><cell>92.6</cell><cell>56.4</cell><cell>38.7</cell><cell>-</cell><cell>-</cell></row><row><cell>model-free</cell><cell>I2L-MeshNet [36] Pose2Mesh [8] HybrIK [27] METRO [28] Graphormer [29]</cell><cell>93.2 89.5 80.0 77.1 74.7</cell><cell>58.6 56.3 48.8 47.9 45.6</cell><cell>110.1 105.3 94.5 88.2 87.7</cell><cell>-64.9 54.4 54.0 51.2</cell><cell>-46.3 34.5 36.7 34.5</cell><cell>-----</cell><cell>-----</cell></row><row><cell></cell><cell>HMR [18]</cell><cell>130.0</cell><cell>81.3</cell><cell>-</cell><cell>-</cell><cell>56.8</cell><cell>180.5</cell><cell>173.6</cell></row><row><cell>model-based</cell><cell>SPIN [25] SPEC [24] HMR-EFT [17] PARE [23] ROMP [54] CLIFF (Res-50)</cell><cell>96.9 96.5 85.1 79.1 76.7 72.0</cell><cell>59.2 53.2 52.2 46.4 47.3 45.7</cell><cell>116.4 118.5 98.7 94.2 93.4 85.3</cell><cell>--63.2 --50.5</cell><cell>41.1 -43.8 --35.1</cell><cell>153.4 112.3 165.4 146.2 116.6 91.7</cell><cell>148.9 106.5 159.0 140.9 113.8 86.3</cell></row><row><cell></cell><cell>CLIFF (HR-W48)</cell><cell>69.0</cell><cell>43.0</cell><cell>81.2</cell><cell>47.1</cell><cell>32.7</cell><cell>81.0</cell><cell>76.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study of CLIFF on Human3.6M</figDesc><table><row><cell>Method</cell><cell cols="2">MPJPE ? PA-MPJPE ?</cell></row><row><cell>CLIFF w/o CI &amp; CS</cell><cell>85.2</cell><cell>54.5</cell></row><row><cell>CLIFF w/o CI</cell><cell>84.0</cell><cell>52.4</cell></row><row><cell>CLIFF</cell><cell>81.4</cell><cell>52.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">Direct comparison among</cell></row><row><cell cols="3">pseudo-GT annotators on 3DPW</cell><cell></cell></row><row><cell>Annotator</cell><cell cols="3">MPJPE ? PA-MPJPE ? PVE ?</cell></row><row><cell>ProHMR [26]</cell><cell>-</cell><cell>52.4</cell><cell>-</cell></row><row><cell>BOA [14]</cell><cell>77.2</cell><cell>49.5</cell><cell>-</cell></row><row><cell>EFT [17]</cell><cell>-</cell><cell>49.3</cell><cell>-</cell></row><row><cell>DynaBOA [13]</cell><cell>65.5</cell><cell>40.4</cell><cell>82.0</cell></row><row><cell>Pose2Mesh [8]</cell><cell>65.1</cell><cell>34.6</cell><cell>-</cell></row><row><cell>CLIFF (Ours)</cell><cell>52.8</cell><cell>32.8</cell><cell>61.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>, we compare CLIFF with PARE [23] on the 3DPW testing data, which is one of the best cropped-image-based methods. We render the reconstructed meshes using the original camera with groundtruth intrinsic parameters. Even though accurate articulated poses can also be</figDesc><table><row><cell>Input Image</cell><cell>CLIFF</cell><cell>PARE</cell><cell>Novel View</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Indirect comparison among pseudo-GT annotators by training on COCO and then evaluating on 3DPW and Human3.6M</figDesc><table><row><cell></cell><cell></cell><cell>3DPW</cell><cell></cell><cell cols="2">Human3.6M</cell></row><row><cell>Annotator</cell><cell cols="5">MPJPE ? PVE ? PA-MPJPE ? MPJPE ? PA-MPJPE ?</cell></row><row><cell>SPIN [25]</cell><cell>101.2</cell><cell>119.2</cell><cell>65.2</cell><cell>115.0</cell><cell>66.9</cell></row><row><cell>EFT [17]</cell><cell>98.8</cell><cell>115.8</cell><cell>62.0</cell><cell>110.9</cell><cell>60.7</cell></row><row><cell>CLIFF (Ours)</cell><cell>85.4</cell><cell>100.5</cell><cell>53.6</cell><cell>96.1</cell><cell>54.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 .</head><label>1</label><figDesc>Comparison between CLIFF and video-based methods on 3DPW</figDesc><table><row><cell>Annotator</cell><cell cols="4">MPJPE ? PA-MPJPE ? PVE ? Accel Error ?</cell></row><row><cell>HMMR [19]</cell><cell>116.5</cell><cell>72.6</cell><cell>-</cell><cell>14.3</cell></row><row><cell>TCMR [7]</cell><cell>86.5</cell><cell>52.7</cell><cell>102.9</cell><cell>7.1</cell></row><row><cell>VIBE [22]</cell><cell>82.7</cell><cell>51.9</cell><cell>99.1</cell><cell>23.4</cell></row><row><cell>MAED [58]</cell><cell>79.1</cell><cell>45.7</cell><cell>92.6</cell><cell>17.6</cell></row><row><cell>CLIFF (Res-50)</cell><cell>72.0</cell><cell>45.7</cell><cell>85.3</cell><cell>24.7</cell></row><row><cell>CLIFF (Res-50) w/ OneEuro</cell><cell>74.0</cell><cell>46.2</cell><cell>87.6</cell><cell>10.6</cell></row><row><cell>CLIFF (HR-W48)</cell><cell>69.0</cell><cell>43.0</cell><cell>81.2</cell><cell>20.5</cell></row><row><cell>CLIFF (HR-W48) w/ OneEuro</cell><cell>70.1</cell><cell>43.1</cell><cell>82.3</cell><cell>11.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 .</head><label>2</label><figDesc>Ablation study of the CLIFF annotator on 3DPW Annotator MPJPE ? PA-MPJPE ? PVE ?</figDesc><table><row><cell>ProHMR</cell><cell>-</cell><cell>52.4</cell><cell>-</cell></row><row><cell>BOA</cell><cell>77.2</cell><cell>49.5</cell><cell>-</cell></row><row><cell>EFT</cell><cell>-</cell><cell>49.3</cell><cell>-</cell></row><row><cell>DynaBOA</cell><cell>65.5</cell><cell>40.4</cell><cell>82.0</cell></row><row><cell>Pose2Mesh</cell><cell>65.1</cell><cell>34.6</cell><cell>-</cell></row><row><cell>HMR-based (Ours)</cell><cell>63.6</cell><cell>38.7</cell><cell>72.6</cell></row><row><cell>CLIFF-based (Ours)</cell><cell>52.8</cell><cell>32.8</cell><cell>61.5</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In these supplementary materials, we first derive Equation 7 given in the main paper, then take a further discussion about the CLIFF input and its performance when applied to videos, and finally provide more details about the CLIFF annotator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Derivation of Equation 7</head><p>Equation 7. CLIFF computes the reprojection loss in the full frame instead of the cropped image, so we need to calculate the root translation </p><p>where s, t x , and t y are the scale and translation parameters of the weak-perspective projection, (c x , c y ) is the crop location relative to the full image center, b is the size of the original crop (detection result), and f CLIF F is the focal length of the original camera. See <ref type="figure">Fig. 1</ref> for the illustration. A weak-perspective projection can be regarded as an orthogonal projection followed by a perspective projection <ref type="bibr" target="#b51">[52]</ref>. As shown in <ref type="figure">Fig. 1</ref>, the human body is first projected (parallel to the Z ? axis) onto the virtual plane Z ? = t f ull Z , and then onto the image plane Z ? = f CLIF F by a perspective projection. A T-pose human body of the mean shape is about 1.8m ? 1.8m (m denoting meters). We enclose it with a slightly enlarged box B of size 2m ? 2m, and align the center of B at the root of the human body (the green point R in <ref type="figure">Fig. 1)</ref>. B is projected to be a square region of size b ? s in the image. Since the two triangles ?OGH and ?OPQ in <ref type="figure">Fig. 1 (in blue)</ref> are similar, we have:</p><p>Note that here b and f CLIF F are in pixels, and t f ull Z is in meters.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mindspore</surname></persName>
		</author>
		<ptr target="https://www.mindspore.cn" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Scape: shape completion and animation of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>SIGGRAPH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Keep it smpl: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">1? filter: A simple speed-based low-pass filter for noisy input in interactive systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Casiez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Roussel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vogel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<title level="m">Hybrid task cascade for instance segmentation</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Beyond static features for temporally consistent 3d human pose and shape from a video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Pose2mesh: Graph convolutional network for 3d human pose and mesh recovery from a 2d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davydov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Remizova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.04203</idno>
		<title level="m">Adversarial parametric pose prior</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Reconstructing 3d human pose by watching humans in the mirror</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Poseaug: A differentiable pose augmentation framework for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Out-of-domain human mesh reconstruction via dynamic bilevel online adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.04017</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Bilevel online adaptation for out-ofdomain human mesh reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Exemplar fine-tuning for 3d human model fitting towards in-the-wild 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning 3d human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Beyond weak perspective for monocular 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kissos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Meir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kliger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Vibe: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Pare: Part attention regressor for 3d human body estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Spec: Seeing people in the wild with an estimated camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning to reconstruct 3d human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Probabilistic modeling for human mesh recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Hybrik: A hybrid analyticalneural inverse kinematics solution for 3d human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">End-to-end human pose and mesh reconstruction with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Mesh graphormer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.11536</idno>
		<title level="m">Recent advances in monocular 2d and 3d human pose estimation: A deep learning perspective</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Smpl: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Amass: Archive of motion capture as surface shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">F</forename><surname>Troje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">I2l-meshnet: Image-to-lixel prediction network for accurate 3d human pose and mesh estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.11232</idno>
		<title level="m">Neuralannot: Neural annotator for in-the-wild expressive 3d human pose and mesh training sets</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.11534</idno>
		<title level="m">Pose2pose: 3d positional pose-guided 3d rotational pose prediction for expressive 3d human pose and mesh estimation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">On self-contact and human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Star: Sparse trained articulated human body regressor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Agora: Avatars in geography optimized for regression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Expressive body capture: 3d hands, face, and body from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Full-body awareness from partial observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rockwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Delving deep into hybrid annotations for 3d human recovery in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Airpose: Multi-view fusion network for aerial 3d human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Saini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bonetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>S?r?ndi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.09316</idno>
		<title level="m">How robust is 3d human pose estimation to occlusion</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Synthetic training for accurate 3d human pose and shape estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Budvytis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Monocular 3d human pose estimation by generation and ordinal ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Varigonda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bindal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">A geometric interpretation of weak-perspective motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shimshoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rivlin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Monocular, one-stage, regression of multiple 3d people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.01923</idno>
		<title level="m">Recovering 3d human mesh from monocular images: A survey</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Total capture: 3d human pose estimation fusing video and inertial sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trumble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Malleson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collomosse</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Encoder-decoder with multi-level attention for 3d human shape and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Probabilistic monocular 3d human pose estimation with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wehrbein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wandt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Ghum &amp; ghuml: Generative 3d human shape and articulated pose models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Bazavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Pcls: Geometry-aware neural reconstruction of 3d pose with perspective crop layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Pymaf: 3d human pose and shape regression with pyramidal mesh alignment feedback loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Body meshes as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Object-occluded human shape and pose estimation from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
