<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Residual Fourier Transformation for Single Image Deblurring</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintian</forename><surname>Mao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">East China Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">East China Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingli</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">East China Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">East China Normal University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Residual Fourier Transformation for Single Image Deblurring</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Code: https://github.com/INVOKERer/DeepRFT</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>It has been a common practice to adopt the ResBlock, which learns the difference between blurry and sharp image pairs, in end-to-end image deblurring architectures. Reconstructing a sharp image from its blurry counterpart requires changes regarding both low-and high-frequency information. Although conventional ResBlock may have good abilities in capturing the high-frequency components of images, it tends to overlook the low-frequency information. Moreover, ResBlock usually fails to felicitously model the long-distance information which is non-trivial in reconstructing a sharp image from its blurry counterpart. In this paper, we present a Residual Fast Fourier Transform with Convolution Block (Res FFT-Conv Block), capable of capturing both long-term and short-term interactions, while integrating both low-and high-frequency residual information. Res FFT-Conv Block is a conceptually simple yet computationally efficient, and plug-and-play block, leading to remarkable performance gains in different architectures. With Res FFT-Conv Block, we further propose a Deep Residual Fourier Transformation (DeepRFT) framework, based upon MIMO-UNet, achieving state-of-the-art image deblurring performance on GoPro, HIDE, RealBlur and DPDD datasets. Experiments show our DeepRFT can boost image deblurring performance significantly (e.g., with 1.09 dB improvement in PSNR on GoPro dataset compared with MIMO-UNet), and DeepRFT+ even reaches 33.23 dB in PSNR on GoPro dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image deblurring aims at removing blurring artifacts to recover sharp images <ref type="bibr" target="#b9">[10]</ref>. The blurring/degradation of an image can be caused by many factors, e.g., irregular camera and/or objects movement, out-of-focus optics, etc. The blurry image leads to visually low quality and hampers subsequent high-level vision tasks, ranging from security, medical imaging to object recognition <ref type="bibr" target="#b5">[6]</ref>.</p><p>Substantial research works have been done over the past decades to explore accurate and efficient image de-   <ref type="figure">Figure 2</ref>. Image deblurring on the GoPro dataset <ref type="bibr" target="#b26">[27]</ref>. PSNR vs. number of parameters and PNSR vs. FLOPs. Our method performs much better than the baseline method MIMO-UNet <ref type="bibr" target="#b9">[10]</ref> and state-of-the-arts.</p><p>blurring approaches. But, due to the ill-posed nature of the subject, image deblurring is still an unsolved problem. Kernel-based methods try to estimate the latent sharp image and the blur operator simultaneously, given a blurry image <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b42">43]</ref>. But kernel-related problems usually hamper its usage in effective image deblurring, e.g., kernel estimation is sensitive to noise, and there often exist various constraints to model characteristics of blur which are not practical in real scenarios. DeepDeblur <ref type="bibr" target="#b26">[27]</ref>, pioneers the technique of end-to-end trainable methods, directly mapping a blurry image to its paired sharp image by a Convolutional Neural Network (CNN). It designs a multi-scale architecture, and uses a modified residual network structure <ref type="bibr" target="#b13">[14]</ref> called ResBlock to focus on learning the difference between blurry and sharp image pairs. Thereafter, ResBlock is proven to be effective in image deblurring, and becomes a widely accepted module in various end-to-end image deblurring architectures <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b55">56]</ref>. Though these methods have furthered the efficacy compared with most kernel-based methods, there are still severe limitations yet to be tackled: <ref type="bibr" target="#b0">(1)</ref> ResBlock is commonly instantiated by CNNs, whose effective receptive field size is limited, especially in early layers. Thereon, ResBlock techniques proposed in literature corpus usually fail to felicitously model the global information which is non-trivial in learning discrepancy between blurry and sharp image pairs, e.g., the context for understanding the global structure of the blur. (2) These methods rarely delicately design the ResBlock tailored to capture the true discrepancy between blurry and sharp image pairs from the perspective of frequency domain, which should be paid more attention to for image deblurring. Concretely, we notice that compared with the blurry image, the sharp image contains much less low-frequency information ? and more high-frequency information, (see <ref type="figure" target="#fig_0">Fig. 1</ref>). As is known, CNNs are prone to capture elementary visual features from edges or contours, especially in lower layers, while the higher layers tend to combine these features <ref type="bibr" target="#b49">[50]</ref>. As also suggested by a more recent work <ref type="bibr" target="#b43">[44]</ref>, we conclude that ResBlock may be armed with good abilities in learning high-frequency components, and might lack a powerful representation ability in modeling low-frequency information.</p><p>To alleviate the problems mentioned above, in this work, we propose a Residual Fast Fourier Transform with Convolution Block (Res FFT-Conv Block), a conceptually simple, effective and plug-and-play block, leading to remarkable performance gains in different image deblurring architectures. The Res FFT-Conv Block contains two streams for residual learning: (1) The FFT-Conv stream uses a 1 ? 1 convolution after converting the spatial feature map to the frequency domain, enabling both the low and high frequency learning. Due to the properties of FFT, it also allows the image-wide receptive field that covers the entire image starting from early layers. It captures the global discrepancy between blurry and sharp image pairs effortlessly, which is crucial for high resolution image deblurring. (2) The normal convolution stream focuses on the local details and prone to learn high-frequency differences.</p><p>We further present a Deep Residual Fourier Transformation (DeepRFT) framework, by plugging our Res FFT-Conv Block into MIMO-UNet <ref type="bibr" target="#b9">[10]</ref>. MIMO-UNet is one of the most recent ResBlock-based models with ? Compared with sharp image, the low-frequency information in blurry image can be generated by situations such as severe motion. remarkable speed advantages among existing networks. In order to achieve noticeable deblurring performance improvement compared with state-of-the-arts, we additionally replace convolution operation by Depthwise Over-parameterized Convolution (DO-Conv) <ref type="bibr" target="#b2">[3]</ref> in Deep-RFT. Over-parameterization has been proven as a way to accelerate the training of deep networks. We show that DO-Conv can introduce some performance gains for image deblurring without incurring extra parameters for inference.</p><p>The effectiveness of Res FFT-Conv Block is compared and verified by plugging in different architectures. We also verify our proposed DeepRFT on GoPro <ref type="bibr" target="#b26">[27]</ref>, HIDE <ref type="bibr" target="#b36">[37]</ref> RealBlur <ref type="bibr" target="#b32">[33]</ref>, and DPDD <ref type="bibr" target="#b0">[1]</ref> datasets. Without bells and whistles, DeepRFT achieves 32.82 dB in terms of PSNR on GoPro datasets, with 1.09 dB improvement compared with MIMO-UNet, and DeepRFT+ reaches 33.23 dB. Our method noticeably improves MIMO-UNet without introducing too many parameters, while maintaining a low computational complexity. The PSNR vs. Params (M) and PSNR vs. FLOPs (G) compared with state-of-the-art methods are shown in <ref type="figure">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Deep Image Deblurring</head><p>Deep learning methods have achieved significant success in image deblurring <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b55">56]</ref> as well as other low-level vision tasks such as image denoise <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b47">48]</ref>, image deraining <ref type="bibr" target="#b15">[16]</ref> and image super-resolution <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b53">54]</ref>. Sun et al. <ref type="bibr" target="#b39">[40]</ref> propose to estimate the spatially-varying kernels of motion blur by a CNN. But, since the characteristics of blur are complex, the blur kernel estimation method is not practical in real scenarios. Later, DeepDeblur <ref type="bibr" target="#b26">[27]</ref>, without estimating the blur kernel, directly maps a blurry image to its sharp counterpart. Scalerecurrent network <ref type="bibr" target="#b41">[42]</ref> proposes an encoder-decoder structure to yield training feasibility. Adversarial training has also been extensively studied <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. Most of these networks perform CNNs on the spatial domain to recover the sharp image from its blurry image. But, after MPRNet <ref type="bibr" target="#b48">[49]</ref>, performances of methods based upon the spatial domain fall into the bottleneck. The performance of MPRNet on Go-Pro dataset <ref type="bibr" target="#b26">[27]</ref> is hard to be surpassed. To address this problem, we propose to learn the knowledge from both the spatial and the frequency domain, which is able to achieve 0.57 dB improvement in PSNR with much less FLOPs and runtime compared with MPRNet.</p><p>Transformer/non-local has strong global context modeling ability and has shown its great promise in various computer vision tasks. A few transformer-based image restoration methods have been proposed, such as SwinIR <ref type="bibr" target="#b22">[23]</ref>. But its considerable computational complexity usually hampers its usage in efficient image restoration. We test the model of SwinIR <ref type="bibr" target="#b22">[23]</ref> on GoPro dataset, which takes 1.99s per image, even two times slower than MPRNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">End-to-end Deblur Model with ResBlock</head><p>DeepDeblur <ref type="bibr" target="#b26">[27]</ref> designs a residual block (ResBlock) based on Conv-ReLU-Conv structure. Thereafter, Res-Block has become one fundamental block in image deblurring <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b50">51]</ref>. Various efforts have been devoted to modifying the ResBlock, e.g., the content-aware processing module proposed by SAPHN <ref type="bibr" target="#b38">[39]</ref>, the channel attention block proposed by MPRNet <ref type="bibr" target="#b48">[49]</ref>, the HIN block proposed by HINet <ref type="bibr" target="#b4">[5]</ref>, and the dilated conv block proposed by SD-WNet <ref type="bibr" target="#b55">[56]</ref>. All mentioned methods focus on extracting information from the spatial domain, and overlook important discrepancies in the frequency domain. We propose a Res FFT-Conv Block from the perspective of image's frequency, capturing both high-and low-frequency discrepancies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Applications of Fourier Transform</head><p>In recent years, some methods extracted information from the frequency domain to fulfill different tasks are proposed <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b54">55]</ref>. FDA <ref type="bibr" target="#b45">[46]</ref> swaps the lowfrequency spectrum between images to mitigate the influence caused by the images' style change for image segmentation. GFNet <ref type="bibr" target="#b30">[31]</ref> learns long-term spatial dependencies in the frequency domain for image classification. LaMa <ref type="bibr" target="#b40">[41]</ref> applies the structure of fast Fourier convolution <ref type="bibr" target="#b7">[8]</ref> to image inpainting. In image deblurring, SDWNet <ref type="bibr" target="#b55">[56]</ref> introduces wavelet transform into deep networks. Inspired by the success of Fast Fourier Transform (FFT), we propose a Res FFT-Conv Block which can effectively model the frequency information to deblur an image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">ResBlock</head><p>Before introducing our proposed method, we first revisit the residual building block in image deblurring tasks, which is called ResBlock. Specifically, ResBlock consists of two 3 ? 3 convolutional layers and one ReLU layer in between, as shown in <ref type="figure" target="#fig_2">Fig. 3</ref> (a). ResBlock enables deeper architecture, larger receptive field size, and fast convergence speed at training time.</p><p>The success of ResBlock suggests that properly learning the difference of blurry and sharp image pairs improves model performance. As discussed in Sec. 1 and <ref type="figure" target="#fig_0">Fig. 1</ref>, both the high-frequency and low-frequency discrepancy should be modeled in ResBlock. A convolutional operator has good abilities in learning high-frequency details, as it usually captures informative features from edges. Thus, it may lack a powerful representation ability in modeling lowfrequency information. Moreover, to reconstruct a high quality sharp image from its blurry counterpart, the ability of understanding the content globally is non-trivial. Though the effective receptive field size can be enlarged by stacking more ResBlocks, a big receptive field size in the early layer is still missing, and the computation complexity can be largely increased by stacking. In addition, techniques which capture long-range dependencies such as non-local has been explored in low level vision tasks <ref type="bibr" target="#b25">[26]</ref>, the efforts to reduce high memory requirements may lead to suboptimal solutions, as discussed in <ref type="bibr" target="#b38">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Residual Fast Fourier Transform Block</head><p>We propose a Residual Fast Fourier Transform with Convolution Block (Res FFT-Conv Block) to replace the ResBlock. Such building block enjoys benefits from modeling both high-frequency and low-frequency discrepancies between blurry image and sharp image pairs, while capturing both long-term and short-term interactions. As shown in <ref type="figure" target="#fig_2">Fig. 3 (b)</ref>, besides a normal spatial residual stream, we simply add another stream based on a channel-wise FFT <ref type="bibr" target="#b1">[2]</ref> to account for the global context in the frequency domain. Discrete Fourier Transform (DFT) is widely used in modern signal processing algorithms, whose 1D version can be derived by:</p><formula xml:id="formula_0">X[k] = N ?1 n=0 x[n]e ?j 2? N kn ,<label>(1)</label></formula><p>where x[n] is a sequence of N complex numbers, X[k] indicates the spectrum at the frequency ? k = 2?k/N , and j represent the imaginary unit. It is clear that the spectrum at any frequency has global information. Noted that the DFT of a real signal x[n] is conjugate symmetric, i.e.,</p><formula xml:id="formula_1">X[N ? k] = N ?1 n=0 x[n]e ?j 2? N (N ?k)n = X * [k]. (2)</formula><p>The same applies to 2D DFT, which performs sequential row and column 1D DFT on a 2D signal whose size is</p><formula xml:id="formula_2">M ? N , i.e., X[M ? u, N ? v] = X * [u, v].</formula><p>Since the results of a real array's DFT has symmetric properties, the right half of the results can be derived from the left half. The FFT algorithms reduce the complexity and calculates the DFT in a more efficient way. Let Z ? R H?W ?C be the input feature volume, where H, W , and C indicate the height, width and channel of the feature volume. The left most stream in Res FFT-Conv Block (see <ref type="figure" target="#fig_2">Fig. 3 (b)</ref>) is processed as follows:</p><p>(1) computes 2D real FFT of Z and obtain F(Z) ? C H?W/2?C ; (2) concatenates the real part R(F(Z)) and the imaginary part I(F(Z)) along the channel dimension to acquir? Z = R(F(Z)) C I(F(Z)) ? R H?W/2?2C , where C represents concatenation through the channel dimension;</p><p>(3) uses two stacks of 1 ? 1 convolution layers (convolution operator * ) with a ReLU layer in between:</p><formula xml:id="formula_3">f (Z; ? (1) , ? (2) ) = ReLU(Z * ? (1) ) * ? (2) ? R H?W/2?2C , and f (Z; ? (1) , ? (2)</formula><p>) will be termed as f for simplicity;</p><formula xml:id="formula_4">(4) applies inverse 2D real FFT to transform f = f real C f img back to spatial domain: Y fft = F ?1 f real + jf img ? R H?W ?C , where f real , f img ? R H?W/2?C .</formula><p>Here, all frequencies share the identical convolution kernel, enabling the modeling of all frequencies' information and correlations. Then the final output of Res FFT-Conv Block is calculated via Y = Y fft + Y res + Z, where Y res uses the same computation as that in the original ResBlock.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Deep Residual Fourier Transform Framework</head><p>Our Deep Residual Fourier Transformation (DeepRFT) framework is designed based upon MIMO-UNet <ref type="bibr" target="#b9">[10]</ref>, a multi-input multi-output U-Net <ref type="bibr" target="#b34">[35]</ref> architecture for efficient multi-scale image deblurring. The overall framework of our DeepRFT is depicted in <ref type="figure" target="#fig_3">Fig. 4</ref>. We replace all Res-Blocks in MIMO-UNet by our Res FFT-Conv Blocks. In order to achieve noticeable deblurring performance gains compared with all state-of-the-art methods, we additionally replace convolution operation by DO-Conv <ref type="bibr" target="#b2">[3]</ref>.</p><p>Depthwise over-parameterized convolution DO-Conv has shown its potential in many high-level vision tasks such as image classification, semantic segmentation and object detection <ref type="bibr" target="#b2">[3]</ref>. DO-Conv speeds up the training with more parameters and achieves better performance by augmenting a convolutional layer with a depthwise convolution. We will show in the experiment that adding DO-Conv to image deblurring can help converge to a slightly lower loss. Meanwhile, DO-Conv is two adjacent linear operations, which can be combined into a traditional convolution operation during inference time. Inspired by this, we replace all non 1 ? 1 convolutions with DO-Conv.</p><p>Loss Function Let k ? {0, ..., K ? 1},? k , S k and ? denote the kth level in DeepRFT, the kth reconstructed image, the kth groundtruth sharp image, and a constant value 10 ?3 , respectively. In order to train DeepRFT, three kinds of loss functions are adopted: (1) Multi-Scale Charbonnier (MSC) loss <ref type="bibr" target="#b48">[49]</ref>:</p><formula xml:id="formula_5">L msc = K?1 k=0 ||? k ? S k || 2 + ? 2 .<label>(3)</label></formula><p>(2) Multi-Scale Edge (MSED) loss <ref type="bibr" target="#b48">[49]</ref>:</p><formula xml:id="formula_6">L msed = K?1 k=0 ||?(? k ) ? ?(S k )|| 2 + ? 2 ,<label>(4)</label></formula><p>where ? denotes the Laplacian operator.</p><p>(3) Multi-Scale Frequency Reconstruction (MSFR) loss <ref type="bibr" target="#b9">[10]</ref>, which evaluates the difference of the multi-scale reconstructed images and the groundtruth sharp images in the frequency domain:</p><formula xml:id="formula_7">L msf r = K?1 k=0 ||FT (? k ) ? F T (S)|| 1 ,<label>(5)</label></formula><p>where FT represents the FFT operation. Finally, the loss function for DeepRFT is: L = L msc + ? 1 L msed + ? 2 L msf r , where ? 1 and ? 2 are tradeoff-parameters and are empirically set to 0.05 as in <ref type="bibr" target="#b48">[49]</ref> and 0.01, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and Implementation Details</head><p>In this section, we evaluate DeepRFT on four datasets: GoPro <ref type="bibr" target="#b26">[27]</ref>, HIDE <ref type="bibr" target="#b36">[37]</ref>, RealBlur <ref type="bibr" target="#b32">[33]</ref> and DPDD <ref type="bibr" target="#b0">[1]</ref> datasets. Since existing methods adopt different experimental settings, we summarize them and report four groups of results: (I) train on 2,103 pairs of blurry and sharp images in GoPro dataset, and test on 1,111 image pairs in GoPro (follow <ref type="bibr" target="#b9">[10]</ref>), 2,025 image pairs in HIDE (follow <ref type="bibr" target="#b48">[49]</ref>), 980 image pairs in RealBlur-R test set, and 980 image pairs in RealBlur-J test set (follow <ref type="bibr" target="#b48">[49]</ref>), respectively; (II) train on 3,758 image pairs in RealBlur-R, and test on 980 image pairs in RealBlur-R (follow <ref type="bibr" target="#b48">[49]</ref>), and train on 3,758 image pairs in RealBlur-J, and test on 980 image pairs in RealBlur-J (follow <ref type="bibr" target="#b48">[49]</ref>); (III) train on GoPro and RealBlur-J training sets, and test on RealBlur-J test set <ref type="bibr" target="#b9">[10]</ref> to compare with the results reported in MIMO-UNet <ref type="bibr" target="#b9">[10]</ref>; (IV) train and test on the defocus deblurring dataset: DPDD <ref type="bibr" target="#b0">[1]</ref>, where 500 dualpixel images are split into training, validation, and testing sets, each of which contains 350, 74 and 76 scenes (follow <ref type="bibr" target="#b21">[22]</ref>).</p><p>We adopt the training strategy used in MPRNet <ref type="bibr" target="#b48">[49]</ref> unless otherwise specified. I.e., the network training hyperparameters (and the default values we use) are patch size (256? 256), batch size <ref type="formula" target="#formula_0">(16)</ref>  learning rate is steadily decreased to 1?10 ?6 using the cosine annealing strategy <ref type="bibr" target="#b24">[25]</ref>. Following <ref type="bibr" target="#b48">[49]</ref>, horizontal and vertical flips are randomly applied on patches for data augmentation. For testing, we adopt the same dataset slicing crop method as used in SDWNet <ref type="bibr" target="#b55">[56]</ref>, where we utilize a step of 256 to perform 256?256 size sliding window slicing, and compensate slicing on the edge part (for the overlap part, we directly use the non-edge patch).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation metric:</head><p>The average performance of PSNR and SSIM over all testing sets are computed and compared by using the official software released by <ref type="bibr" target="#b48">[49]</ref> unless otherwise specified. We also report number of parameters, FLOPs, and testing time per image on on a workstation with Intel Xeon Gold 6240C CPU, NVIDIA GeForce RTX 3090 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation of Res FFT-Conv Block:</head><p>Our Res FFT-Conv Block is a plug-and-play block. In order to show its effectiveness in improving image deblurring performance in various architectures, we plug it into U-Net and ORSNet (two backbone networks used in MPR-Net <ref type="bibr" target="#b48">[49]</ref>); MPRNet-small <ref type="bibr" target="#b48">[49]</ref>, whose number of channels is three times smaller than original MPRNet due to limited computation resource; and MIMO-UNet <ref type="bibr" target="#b9">[10]</ref>. For U-Net, ORSNet and MPRNet-small, we train 1000 epochs with <ref type="table" target="#tab_1">Table 1</ref>. Evaluation of Res FFT-Conv Block on GoPro <ref type="bibr" target="#b26">[27]</ref> and HIDE <ref type="bibr" target="#b36">[37]</ref> (Group I setting). FFT indicates whether the ResBlock in the original architecture is replaced by our Res FFT-Conv Block. ? means the results are obtained by the original architecture. All models are trained by ourselves for fair comparison.</p><p>GoPro <ref type="bibr" target="#b26">[27]</ref> HIDE <ref type="bibr" target="#b36">[37]</ref> Model FFT PSNR SSIM PSNR SSIM  Next, we show Res FFT-Conv Block captures more lowfrequency discrepancy than ResBlock. We visualize the average magnitudes of 2D FFT of the residual features in <ref type="figure" target="#fig_4">Fig. 5</ref>. The features are extracted from the last ResBlock in the well-trained MIMO-UNet ( <ref type="figure" target="#fig_4">Fig. 5(a)</ref>), and the last Res FFT-Conv Block in the well-trained MIMO-UNet w/ Res FFT-Conv Block model ( <ref type="figure" target="#fig_4">Fig. 5(b)</ref>) on GoPro dataset <ref type="bibr" target="#b26">[27]</ref>. Y fft learns much more low-frequency information than Y res . It is also interesting to observe that the learned Y res in MIMO-UNet w/ Res FFT-Conv Block is armed with more low-frequency information than Y res in MIMO-UNet. The reason may be that the former Y res contains lowfrequency information learned via Res FFT-Conv Blocks in lower layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation of DeepRFT:</head><p>We compare our DeepRFT with state-of-the-art methods. MIMO-UNet has three variants, i.e., MIMO-UNet (8 residual blocks for each EB and DB), MIMO-UNet+ (20 residual blocks for each EB and DB), and MIMO-UNet++ (estimate MIMO-UNet+ with the geometric self-ensemble method <ref type="bibr" target="#b23">[24]</ref>). Since we cannot access the specific augmentations which are exploited in MIMO-UNet++, we only design DeepRFT based upon the first two variants, acquiring DeepRFT and DeepRFT+. Besides, we reduce the number of Res FFT-Conv block to 4 for each EB and DB, and obtain an extra model termed as DeepRFT-small. PSNR, SSIM and number of parameters in inference on GoPro and HIDE datasets of Group I setting are shown in <ref type="table">Table 2</ref>. Our DeepRFT-small outperforms MIMO-UNet by 0.57 dB PSNR with 1.7M less parameters. DeepRFT+ achieves the best performance, and even outperforms an eight-time assembled model MIMO-UNet++ with a clear margin (0.55 dB PSNR). For a fair comparison with SD-WNet <ref type="bibr" target="#b55">[56]</ref>, we use the same PSNR/SSIM computation codes released by <ref type="bibr" target="#b55">[56]</ref>, and report the results in <ref type="table">Table 3</ref>.</p><p>Following the experimental setting used in <ref type="bibr" target="#b48">[49]</ref>, we directly test the model (trained on GoPro dataset) on RealBlur-R and RealBlur-J datasets (Group I setting). The quantitative comparisons on RealBlur datasets are shown in top regions in <ref type="table">Table 4</ref>. DeepRFT and DeepRFT+ achieve <ref type="table">Table 2</ref>. Performance comparison on GoPro <ref type="bibr" target="#b26">[27]</ref> and HIDE <ref type="bibr" target="#b36">[37]</ref> (Group I setting). The best and 2nd best results are highlighted and underlined. ? means testing an image for 8 times by test time augmentation. * denotes results tested by using released models.</p><p>GoPro <ref type="bibr" target="#b26">[27]</ref> HIDE <ref type="bibr">[</ref>  <ref type="bibr" target="#b48">[49]</ref> yields the 2nd best result in RealBlur-R dataset. It is worth mentioning that the FLOPs number of MPRNet is 777.01G, which is much larger than ours (80.21G for DeepRFT and 187.04G for DeepRFT+, see <ref type="table">Table 6</ref>).</p><p>We also train and test on RealBlur dataset <ref type="bibr" target="#b48">[49]</ref> (Group II setting). Results are compared in <ref type="table">Table 4</ref>   <ref type="table">Table 4</ref> with symbol ?. Our DeepRFT+ achieves improvement of 0.23 dB PSNR compared with MIMO-UNet++.</p><p>To show the effectiveness of DeepRFT in single image defocus deblurring, we train and test our model on the DPDD dataset. Results computed using the codes released by IFAN <ref type="bibr" target="#b21">[22]</ref> are shown in <ref type="table">Table 5</ref>. Our DeepRFT surpasses all state-of-the-arts in PSNR, SSIM and MAE with less number of parameters. DeepRFT achieves comparable LPIPS compared with IFAN <ref type="bibr" target="#b21">[22]</ref>. It is worth mentioning that IFAN is specifically designed for dealing with defocus deblur, and uses dual-pixel stereo images for training. We only use blur images.</p><p>Some of the predicted sharp images from the GoPro datast are shown in <ref type="figure" target="#fig_5">Fig. 6</ref>. Results from competitors are tested by using their released models. DeepRFT is more successful in deblurring local details and structures compared with others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FLOPs and runtime comparison:</head><p>We compare FLOPs and runtime with the state-of-thearts (see <ref type="table">Table 6</ref>). FLOPs number is calculated using ptflops ? with the input size of 256?256 <ref type="bibr" target="#b55">[56]</ref>. Runtime is measured by using the released test code of each method to run the entire GoPro <ref type="bibr" target="#b26">[27]</ref> testing dataset, and obtaining the average runtime (second) per image on our environment ? https://github.com/sovrasov/flops-counter.pytorch with a single NVidia 3090 GPU. Our DeepRFT+ is about 3 times faster than MIMO-UNet++ and about 2 times faster than MPRNet, while achieving 0.55 dB and 0.57 dB improvements in terms of PSNR, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation study and discussion:</head><p>We conduct ablation experiments on the GoPro dataset <ref type="bibr" target="#b26">[27]</ref> to show the effectiveness of each module in DeepRFT. As shown in <ref type="table">Table 7</ref>, Res FFT-Conv Block significantly boosts PSNR by 0.74 dB. Replacing convolutional layers by DO-Conv further improves the results by 0.08 dB, without introducing more parameters or runtime in inference. Using only the FFT stream in Res FFT-Conv Block leads to performance drop (see * * in <ref type="table">Table 7</ref>). We also try to replace Res FFT-Conv Block by a recently developed fast Fourier convolutions <ref type="bibr" target="#b7">[8]</ref> operator which has shown its potential in image inpainting <ref type="bibr" target="#b40">[41]</ref>, but it does not help in improving image deblurring performance. <ref type="figure" target="#fig_6">Fig. 7</ref> shows training curves corresponding to the first (i.e., MIMO), second and fourth row in the result part of <ref type="table">Table 7</ref>. Res FFT-Conv Block and DO-Conv converge faster and to lower losses, especially the Res FFT-Conv Block. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we present Deep Residual Fourier Transformation (DeepRFT) for image deblurring, where we propose a plug-and-play residual block called Res FFT-Conv Block, integrating both spatial and frequency residual information. Res FFT-Conv Block allows the image-wide receptive field which is able to capture the long-term interaction. By plugging Res FFT-Conv Block into MIMO-UNet, our DeepRFT achieves remarkable superior perfor- <ref type="table">Table 6</ref>. FLOPs and runtime comparison. PSNR is shown as a reference. Runtime shows the average testing time per image in GoPro <ref type="bibr" target="#b26">[27]</ref> dataset on a single NVidia 3090 GPU. PSNR calculated using codes released by <ref type="bibr" target="#b55">[56]</ref> are shown with ?, whose details are illustrated in <ref type="table">Table 3</ref>  <ref type="table">Table 7</ref>. Ablation on GoPro dataset <ref type="bibr" target="#b26">[27]</ref>. MIMO indicates original MIMO-UNet <ref type="bibr" target="#b9">[10]</ref>, but trained with our strategy (Sec. 4.1) and loss functions (Eq. 3 -5); FFT indicates Res FFT-Conv Block; DC denotes DO-Conv <ref type="bibr" target="#b2">[3]</ref>. * * indicates removing the middle stream in Res FFT-Conv Block ( <ref type="figure" target="#fig_2">Fig. 3 (b)</ref>  Limitations and broader impacts. The proposed method has accomplished a primary exploration of the frequency domain. The Res FFT-Conv Block introduces slightly more parameters than ResBlock. A great many studies are worth exploring by following our work, to acquire even faster speed and better performance. Besides, the proposed model may generate details used to pry into other people's privacy. Thus, the model should be used in a right way.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The magnitudes of 2-dimensional discrete Fourier transform (denoted as |F(?)|) of two pairs of blurry vs. sharp images (upper and bottom). The last column shows the absolute value of the magnitude subtraction. The difference in frequency domain lies in both low and high frequency (see the right-most column).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>(a) ResBlock. (b) Proposed Res FFT-Conv Block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>The framework of DeepRFT. DeepRFT is designed based upon MIMO-UNet<ref type="bibr" target="#b9">[10]</ref>, which takes multi-scale input blurry images and generates multi-scale output sharp images. The ResBlocks are replaced by Res FFT-Conv Blocks. Non 1?1 convolutional layers are replaced by DO-Conv. AFF, SCM, EB and DB are short for asymmetric feature fusion, shallow convolutional module, encoder block and decoder block, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Average magnitudes of 2D FFT of learned last residual features in well-trained models on GoPro dataset [27]. (a) Res-Block in MIMO-UNet and (b) Res FFT-Conv Block in MIMO-UNet w/ Res FFT-Conv Block model, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Examples on the GoPro test dataset. The original blur image and the zoom-in patches are shown. From left-top to right-bottom are Blurry image, results obtained through DMPHN, MIMO+, HINet, MPRNet, DeepRFT, DeepRFT+, and the Groundtruth sharp image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Training loss curves. 120 epochs are shown. mance compared with all state-of-the-arts. Experiments are evaluated on four image deblurring datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>UNet, we use the same training strategy as illustrated in Sec. 4.1, with a combination of MSC loss, MSED loss and MSFR loss as the loss function. PSNR/SSIM on GoPro and HIDE datasets in Group I setting are summarized in</figDesc><table><row><cell>U-Net [49]</cell><cell>?</cell><cell>29.87</cell><cell>0.930</cell><cell>28.45 0.906</cell></row><row><cell></cell><cell></cell><cell>31.13</cell><cell>0.946</cell><cell>29.51 0.924</cell></row><row><cell>ORSNet [49]</cell><cell>?</cell><cell>29.34</cell><cell>0.924</cell><cell>28.07 0.899</cell></row><row><cell></cell><cell></cell><cell>31.77</cell><cell>0.953</cell><cell>30.10 0.930</cell></row><row><cell>MPRNet-small [49]</cell><cell>?</cell><cell>29.97</cell><cell>0.932</cell><cell>28.68 0.910</cell></row><row><cell></cell><cell></cell><cell>31.75</cell><cell>0.952</cell><cell>30.27 0.933</cell></row><row><cell>MIMO-UNet [10]</cell><cell>?</cell><cell>31.92</cell><cell>0.953</cell><cell>29.65 0.925</cell></row><row><cell></cell><cell></cell><cell>32.74</cell><cell>0.960</cell><cell>30.90 0.939</cell></row><row><cell cols="5">a patch size of 256. The loss function is a combination</cell></row><row><cell cols="5">of Charbonnier loss and Edge loss, followed by [49]. For</cell></row><row><cell>MIMO-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>. Replacing ResBlock by our Res FFT-Conv Block leads to remarkable performance gains in various ar- chitectures. E.g., for ORSNet, w/ Res FFT-Conv Block out- performs w/ ResBlock by 2.43 dB in PSNR on the GoPro dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>with symbol ?. 0.53 dB and 0.43 dB PSNR performance gains are achieved on RealBlur-R and RealBlur-J compared with MPRNet. Besides, to compare with MIMO-UNet [10] who uses the GoPro and RealBlur-J training datasets for training models and RealBlur-J test datasts for testing, we conduct experiments following the Group III setting. Detailed comparisons are illustrated in the bottom region in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>Performance comparison on RealBlur datasets<ref type="bibr" target="#b32">[33]</ref> (Group I, II, III settings). Group II and Group III results are with ? and ?, respectively. DeepRFT, DeepRFT+ are proposed based on MIMO-UNet and MIMO-UNet+, respectively. DeepRFT-small reduces the number of Res FFT-Conv Block to 4. * denotes the results tested by using released models. Performance comparison on the DPDD dataset<ref type="bibr" target="#b0">[1]</ref> (Group IV setting). DeepRFT surpasses all other methods in PSNR, SSIM and MAE. It achieves comparable LPIPS with IFAN<ref type="bibr" target="#b21">[22]</ref>, which is specifically designed for image defocus deblurring.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">RealBlur-R</cell><cell cols="2">RealBlur-J</cell></row><row><cell>Model</cell><cell></cell><cell cols="2">PSNR</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SSIM</cell></row><row><cell>Hu et al. [15]</cell><cell></cell><cell cols="2">33.67</cell><cell>0.916</cell><cell>26.41</cell><cell>0.803</cell></row><row><cell cols="2">DeepDeblur [27]</cell><cell cols="2">32.51</cell><cell>0.841</cell><cell>27.87</cell><cell>0.827</cell></row><row><cell cols="2">DeblurGAN [19]</cell><cell cols="2">33.79</cell><cell>0.903</cell><cell>27.97</cell><cell>0.834</cell></row><row><cell>Pan et al. [28]</cell><cell></cell><cell cols="2">34.01</cell><cell>0.916</cell><cell>27.22</cell><cell>0.790</cell></row><row><cell>Xu et al. [45]</cell><cell></cell><cell cols="2">34.46</cell><cell>0.937</cell><cell>27.14</cell><cell>0.830</cell></row><row><cell cols="2">DeblurGan-v2 [20]</cell><cell cols="2">35.26</cell><cell>0.944</cell><cell>28.70</cell><cell>0.866</cell></row><row><cell cols="2">Zhang et al. [52]</cell><cell cols="2">35.48</cell><cell>0.947</cell><cell>27.80</cell><cell>0.847</cell></row><row><cell>SRN [42]</cell><cell></cell><cell cols="2">35.66</cell><cell>0.947</cell><cell>28.56</cell><cell>0.867</cell></row><row><cell>DMPHN [51]</cell><cell></cell><cell cols="2">35.70</cell><cell>0.948</cell><cell>28.42</cell><cell>0.860</cell></row><row><cell>MPRNet [49]</cell><cell></cell><cell cols="2">35.99</cell><cell>0.952</cell><cell>28.70</cell><cell>0.873</cell></row><row><cell cols="4">MIMO-UNet [10] 35.47  MIMO-UNet+ [10] 35.54  DeepRFT-small 35.89</cell><cell>0.952</cell><cell>28.60</cell><cell>0.868</cell></row><row><cell>DeepRFT</cell><cell></cell><cell cols="2">36.06</cell><cell>0.954</cell><cell>28.90</cell><cell>0.880</cell></row><row><cell>DeepRFT+</cell><cell></cell><cell cols="2">35.86</cell><cell>0.950</cell><cell>28.97</cell><cell>0.884</cell></row><row><cell cols="2">DeblurGan-v2  ? [20]</cell><cell cols="2">36.44</cell><cell>0.935</cell><cell>29.69</cell><cell>0.870</cell></row><row><cell>SRN  ? [42]</cell><cell></cell><cell cols="2">38.65</cell><cell>0.965</cell><cell>31.38</cell><cell>0.909</cell></row><row><cell cols="2">MPRNet  ? [49]</cell><cell cols="2">39.31</cell><cell>0.972</cell><cell>31.76</cell><cell>0.922</cell></row><row><cell>DeepRFT+  ?</cell><cell></cell><cell cols="2">39.84</cell><cell>0.972</cell><cell>32.19</cell><cell>0.931</cell></row><row><cell cols="2">MIMO-UNet+  ? [10]</cell><cell>N/A</cell><cell></cell><cell>N/A</cell><cell>31.92</cell><cell>0.919</cell></row><row><cell cols="2">MIMO-UNet++  ? [10]</cell><cell>N/A</cell><cell></cell><cell>N/A</cell><cell>32.05</cell><cell>0.921</cell></row><row><cell>DeepRFT+  ?</cell><cell></cell><cell>N/A</cell><cell></cell><cell>N/A</cell><cell>32.28</cell><cell>0.929</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Params</cell></row><row><cell>Model</cell><cell cols="2">PSNR? SSIM?</cell><cell cols="2">MAE (?10 ?1 ) ?</cell><cell>LPIPS?</cell><cell>(M)</cell></row><row><cell>Input</cell><cell>23.89</cell><cell>0.725</cell><cell></cell><cell>0.471</cell><cell>0.349</cell><cell>N/A</cell></row><row><cell>JNB [38]</cell><cell>23.69</cell><cell>0.707</cell><cell></cell><cell>0.480</cell><cell>0.442</cell><cell>N/A</cell></row><row><cell>EBDB [17]</cell><cell>23.94</cell><cell>0.723</cell><cell></cell><cell>0.468</cell><cell>0.402</cell><cell>N/A</cell></row><row><cell>DMENet [21]</cell><cell>23.90</cell><cell>0.720</cell><cell></cell><cell>0.470</cell><cell>0.410</cell><cell>26.94</cell></row><row><cell>DPDNet S [1]</cell><cell>24.03</cell><cell>0.735</cell><cell></cell><cell>0.461</cell><cell>0.279</cell><cell>35.25</cell></row><row><cell>DPDNet D [1]</cell><cell>25.23</cell><cell>0.787</cell><cell></cell><cell>0.401</cell><cell>0.224</cell><cell>35.25</cell></row><row><cell>IFAN [22]</cell><cell>25.37</cell><cell>0.789</cell><cell></cell><cell>0.394</cell><cell>0.217</cell><cell>10.48</cell></row><row><cell>DeepRFT</cell><cell>25.71</cell><cell>0.801</cell><cell></cell><cell>0.389</cell><cell>0.218</cell><cell>9.6</cell></row></table><note>* 0.946* 27.76* 0.836** 0.947* 27.63* 0.837*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>.</figDesc><table><row><cell>Model</cell><cell>PSNR</cell><cell cols="2">FLOPs (G) Runtime (s)</cell></row><row><cell>DMPHN [51]</cell><cell>31.20</cell><cell>N/A</cell><cell>0.307</cell></row><row><cell>DBGAN [53]</cell><cell>31.10</cell><cell>759.85</cell><cell>1.298</cell></row><row><cell>MPRNet [49]</cell><cell>32.66</cell><cell>777.01</cell><cell>1.002</cell></row><row><cell>MIMO-UNet [10]</cell><cell>31.37</cell><cell>67.17</cell><cell>0.153</cell></row><row><cell>MIMO-UNet+ [10]</cell><cell>32.45</cell><cell>154.41</cell><cell>0.309</cell></row><row><cell>MIMO-UNet++ [10]</cell><cell>32.68</cell><cell>1235.26</cell><cell>2.467</cell></row><row><cell>SDWNet [56]</cell><cell>31.36 ?</cell><cell>189.68</cell><cell>0.533</cell></row><row><cell>DeepRFT-small</cell><cell>32.30</cell><cell>44.60</cell><cell>0.204</cell></row><row><cell>DeepRFT</cell><cell>32.82</cell><cell>80.21</cell><cell>0.345</cell></row><row><cell>DeepRFT+</cell><cell>33.23</cell><cell>187.04</cell><cell>0.786</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>). FFC denotes replacing Res FFT-Conv Block in DeepRFT by fast Fourier convolutions<ref type="bibr" target="#b40">[41]</ref>.</figDesc><table><row><cell>MIMO</cell><cell>FFT</cell><cell>DC</cell><cell cols="3">PSNR Params Runtime FLOPs</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(M)</cell><cell>(s)</cell><cell>(G)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>31.92</cell><cell>6.8</cell><cell>0.134</cell><cell>67.03</cell></row><row><cell></cell><cell></cell><cell></cell><cell>32.74</cell><cell>9.6</cell><cell>0.345</cell><cell>80.21</cell></row><row><cell></cell><cell></cell><cell></cell><cell>32.18</cell><cell>6.8</cell><cell>0.134</cell><cell>67.03</cell></row><row><cell></cell><cell></cell><cell></cell><cell>32.82</cell><cell>9.6</cell><cell>0.345</cell><cell>80.21</cell></row><row><cell></cell><cell>*  *</cell><cell></cell><cell>31.00</cell><cell>3.4</cell><cell>0.235</cell><cell>22.17</cell></row><row><cell></cell><cell>FFC</cell><cell></cell><cell>31.11</cell><cell>7.7</cell><cell>0.508</cell><cell>72.10</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Defocus deblurring using dual-pixel data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdullah</forename><surname>Abuolaim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The fast fourier transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">O</forename><surname>Brigham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Morrow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Spectrum</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="63" to="70" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Do-conv: Depthwise over-parameterized convolutional layer. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinming</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhe</forename><surname>Tu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A neural approach to blind motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayan</forename><surname>Chakrabarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hinet: Half instance normalization network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengpeng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">One-shot generative adversarial learning for MRI segmentation of craniomaxillofacial bony structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinggang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><forename type="middle">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><forename type="middle">H</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim-Han</forename><surname>Thung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pew-Thian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gateno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="787" to="796" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Nbnet: Noise basis learning for image denoising with subspace projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqiang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Fast fourier convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<editor>Hugo Larochelle, Marc&apos;Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin</editor>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast non-local neural networks with spectral residual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiyu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rethinking coarse-to-fine approach in single image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung-Jin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seo-Won</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Pyo</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seung-Won</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung-Jea</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dynamic scene deblurring with parameter selective sharing and nested skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Closedloop matters: Dual regression networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeshuai</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deblurring low-light images with light streaks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-scale progressive fusion network for single image deraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baojin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yimin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjun</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Edge-based defocus blur estimation with adaptive scale selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Karaali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?udio</forename><forename type="middle">R</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1126" to="1137" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Yoshua Bengio and Yann LeCun</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deblurgan: Blind motion deblurring using conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orest</forename><surname>Kupyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Budzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykola</forename><surname>Mykhailych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deblurgan-v2: Deblurring (orders-of-magnitude) faster and better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orest</forename><surname>Kupyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tetiana</forename><surname>Martyniuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junru</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep defocus map estimation using domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungyong</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Jaesung Rim, Sunghyun Cho, and Seungyong Lee. Iterative filter adaptive network for single image defocus deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongseok</forename><surname>Son</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Swinir: Image restoration using swin transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SGDR: stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Image superresolution with non-local sparse attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqun</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqian</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep multi-scale convolutional neural network for dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Tae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Blind image deblurring using dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Jin-Shan Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanspeter</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-temporal recurrent neural networks for progressive non-uniform single image deblurring with incremental temporal training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongwon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Un</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jisoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Se Young</forename><surname>Chun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Region-adaptive dense network for efficient motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuldeep</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Rajagopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Global filter networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Neural blind deconvolution using deep priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Dongwei Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Real-world blur dataset for learning and benchmarking deblurring algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesung</forename><surname>Rim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haeyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jucheol</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghyun</forename><surname>Cho</surname></persName>
		</author>
		<editor>Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm</editor>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Spectral representations for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning to deblur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Christian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Harmeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1439" to="1451" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Human-aware motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiankai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingfa</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Just noticeable defocus blur detection and estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Spatially-attentive patch-hierarchical network for adaptive motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maitreya</forename><surname>Suin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuldeep</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Rajagopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning a convolutional neural network for non-uniform motion blur removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenfei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongben</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Kiwoong Park, and Victor Lempitsky. Resolution-robust large mask inpainting with fourier convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Suvorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizaveta</forename><surname>Logacheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Mashikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasia</forename><surname>Remizova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsenii</forename><surname>Ashukha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksei</forename><surname>Silvestrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naejin</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harshith</forename><surname>Goka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Scale-recurrent network for deep image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Explore image deblurring via encoded blur kernel space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phong</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quynh</forename><surname>Phung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>Hoai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">High-frequency component helps explain the generalization of convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xindi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unnatural L0 sparse representation for natural image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shicheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">FDA: fourier domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Efficient dynamic scene deblurring using spatially variant deconvolution network with optical flow guided training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandan</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning enriched features for real image restoration and enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Syed Waqas Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><forename type="middle">H</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Multi-stage progressive image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Syed Waqas Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><forename type="middle">H</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep stacked hierarchical multi-patch network for image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Dynamic scene deblurring using spatially variant recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">S J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Rynson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deblurring by realistic blurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Stenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Joint sub-bands learning with clique structures for wavelet domain super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisheng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Sdwnet: A straight dilated network with wavelet transformation for image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
