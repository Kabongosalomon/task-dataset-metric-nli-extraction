<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">All Labels Are Not Created Equal: Enhancing Semi-supervision via Label Grouping and Co-training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Islam</forename><surname>Nassar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept of Data Science and AI</orgName>
								<orgName type="department" key="dep2">Faculty of IT</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samitha</forename><surname>Herath</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept of Data Science and AI</orgName>
								<orgName type="department" key="dep2">Faculty of IT</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Abbasnejad</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Australian Institute for Machine Learning</orgName>
								<orgName type="institution">University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wray</forename><surname>Buntine</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept of Data Science and AI</orgName>
								<orgName type="department" key="dep2">Faculty of IT</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept of Data Science and AI</orgName>
								<orgName type="department" key="dep2">Faculty of IT</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">All Labels Are Not Created Equal: Enhancing Semi-supervision via Label Grouping and Co-training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pseudo-labeling is a key component in semi-supervised learning (SSL). It relies on iteratively using the model to generate artificial labels for the unlabeled data to train against. A common property among its various methods is that they only rely on the model's prediction to make labeling decisions without considering any prior knowledge about the visual similarity among the classes. In this paper, we demonstrate that this degrades the quality of pseudolabeling as it poorly represents visually similar classes in the pool of pseudo-labeled data. We propose SemCo, a method which leverages label semantics and co-training to address this problem. We train two classifiers with two different views of the class labels: one classifier uses the onehot view of the labels and disregards any potential similarity among the classes, while the other uses a distributed view of the labels and groups potentially similar classes together. We then co-train the two classifiers to learn based on their disagreements. We show that our method achieves stateof-the-art performance across various SSL tasks including 5.6% accuracy improvement on Mini-ImageNet dataset with 1000 labeled examples. We also show that our method requires smaller batch size and fewer training iterations to reach its best performance. We make our code available at https://github.com/islam-nassar/semco.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep neural models require large amounts of labeled data to achieve their high performance. This quickly becomes prohibitive and non-scalable especially when labeling data is expensive and/or non practical. Semi-supervised learning (SSL) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b34">35]</ref> has hence emerged to explore a diverse set of methods which aim to leverage unlabeled data to enable learning from a smaller set of labeled data.</p><p>In the context of image classification, recent methods use unlabeled data to guide learning in different ways. * corresponding author: islam.nassar@monash.edu <ref type="figure">Figure 1</ref>: A conceptual diagram of our co-training solution Some methods primarily focus on consistency regularization <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b17">18]</ref>, where the model is enforced to produce consistent predictions for different perturbed versions of the same unlabeled input image. While others focus on pseudolabeling <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b13">14]</ref>, where the model is used to produce artificial labels for the unlabeled data that are then used to further train the model. Evidently, combining the two approaches has shown the state-of-the-art results on various image classification tasks <ref type="bibr" target="#b30">[31]</ref>.</p><p>When it comes to pseudo-labeling, a common problem which hinders the SSL performance is the so-called confirmation bias <ref type="bibr" target="#b33">[34]</ref>. This takes place when the model reassures its wrong predictions by retraining on them, leading to an accumulation of the error from which the model can not recover. To mitigate this behaviour, some methods use a warm-up phase until the model becomes more reliable <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b33">34]</ref>, or limit the number of pseudo-labeled samples in each mini-batch <ref type="bibr" target="#b0">[1]</ref>. Other strategies include using a confidence threshold whereby a sample is only considered for pseudo-labeling if the model is highly confident about its prediction <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b18">19]</ref>. One property shared by all these methods, however, is that they only rely on the model's output and disregard any prior knowledge about potential similarities among the classes. As we show in Section 2, visually similar classes are expected to confuse the model and therefore get poorly represented in the pseudo-labeled data pool. This fact is even more exacerbated in confidence-based methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b30">31]</ref> as it leads to discarding most of the visually similar samples simply because the model is rarely confident about their predictions. We show that this leads to a class imbalance in the pseudo-labeled pool, and thereby, misguides the training.</p><p>In this paper, we demonstrate that by exploiting class labels semantics, we can account for such similarity among the classes. We draw inspiration from few-shot learning methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b42">43]</ref> where we use distributed embeddings to represent class labels. We present two methods to generate label embeddings in a way which encodes a weak prior on the visual similarity among the classes. One such method is based on knowledge graph embeddings <ref type="bibr" target="#b32">[33]</ref>, while another is based on visual attributes annotation <ref type="bibr" target="#b36">[37]</ref>. Having such embeddings provides basis to group the class labels into visually similar concepts and allow considering such grouping while making pseudo-labeling decisions.</p><p>The benefit of using label embeddings goes beyond label grouping. Earlier work <ref type="bibr" target="#b9">[10]</ref> has shown that using embeddings as training targets (as opposed to one-hot labels) allows the model to map the image features to a more meaningful semantic space, and thereby, enables few shot transfer. In our work, we leverage this idea in a co-training <ref type="bibr" target="#b3">[4]</ref> style approach to improve SSL performance. We propose to train two classifiers with the two different views of the class labels, i.e. one-hot and distributed. One of the classifiers makes use of the label grouping during pseudo-labelling, while the other does not. We then allow the two classifiers to learn from their disagreements via a shared consistency regularization loss on the unlabeled data.</p><p>We show that our method achieves new state-of-the-art results across five different datasets, while using smaller batch size with fewer training iterations. To summarize, our contributions are: 1. We propose an approach which leverages the semantic similarity among the classes to improve pseudolabeling quality by addressing the confusion events.</p><p>2. We present a co-training-based SSL method which involves two classifiers co-operating via pseudo-labels obtained using their different views of the class label.</p><p>3. We show our approach outperforms the state-of-theart in SSL by a large margin on 5 different datasets including 5.6% on Mini-Imagenet with 1000 labeled point, i.e. 10 labels per class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>We are interested in a K-way semi-supervised image classification problem, where we train a model using batches of both labelled and unlabelled examples. Specifically, each batch comprises labeled examples, X = {(x i , y i )} n i=1 and unlabeled examples, U = {u j } ??n j=1 , where the scalar ? denotes the ratio between the number of unlabeled and labeled examples in a given batch, and y i denotes the one-hot representation of the label. But before we introduce our approach, we begin by reviewing three key concepts underpinning our method. Consistency Regularization These methods exploit the assumption that predictions for different perturbed versions of a sample should be consistent <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b29">30]</ref>. One way to operationalise this idea is to produce several augmented versions of a given unlabeled image, then apply a loss to ensure that the predictions for all such versions are consistent. Inspired by recent methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b2">3]</ref>, we make use of two types of augmentations, namely, weak augmentations A w (.) and strong augmentations A s (.), where the notion of intensity relates to how perturbing an augmentation is to an image. Pseudo-labeling These methods rely on producing synthetic labels for unlabeled data which are then used to retrain the model. Recent alternative variations of pseudolabeling <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b30">31]</ref> can broadly be formalized as methods trying to account for unlabeled data by minimizing the following objective,</p><formula xml:id="formula_0">L(?) = ?1 ? ? n ?n j=1 ? j log p(y =? j | u j , ?),<label>(1)</label></formula><p>where ? represents learnable model parameters,? denotes the pseudo-label, and ? is an arbitrary function. The choice of? and ? gives rise to different variations of pseudolabeling methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b18">19]</ref>. We are particularly interested in confidence-based methods wher?</p><formula xml:id="formula_1">y j = arg max y p(y = y | u j , ?)<label>(2)</label></formula><p>and ? j = 1(p(y =? j | u j , ?) ? ? ), with 1 denoting the Indicator function. In such methods, the unlabeled sample is only retained for pseudo-labeling if the model's maximum confidence score exceeds a predefined threshold, ? , and the pseudo-label is then selected to be the class with the maximum score 1 . This approach mitigates confirmation bias (see Sec. 1) by only retaining high confidence samples. Simultaneously, it encourages entropy minimization <ref type="bibr" target="#b10">[11]</ref> whereby the model is encouraged to produce high confidence predictions on the unlabeled data.</p><p>Recently, Sohn et al. <ref type="bibr" target="#b30">[31]</ref> combined such approach with consistency regularization to propose FixMatch, a method which achieves state-of-the-art results on several SSL image classification benchmarks.</p><p>Co-training The idea of co-training <ref type="bibr" target="#b3">[4]</ref> is to train two models with different views of the data, where each model is trained on the other's most confident predictions. Given sufficiently diverse views of the data, this approach was shown to improve learning, as it allows the two models to  <ref type="figure">Figure 2</ref>: Confidence-based pseudo-labeling comparison between the baseline (left) and our method (right). Accuracy values show how much, on average, pseudo-labels for a given class match the true label, while Ratio values show the percentage of samples of a given class which are retained for pseudo-labeling (i.e. with confidence score above the threshold). The two metrics are calculated for the 4 most (red) and least (green) visually similar classes over the first 10 epochs of training.</p><p>learn based on their disagreements <ref type="bibr" target="#b34">[35]</ref>. We adopt a similar strategy, albeit, we use two different views of the label rather than the data. We use the regular one-hot view as well as a distributed view (i.e. label embedding). As we introduced in Sec. 1, using a distributed view of the label grants the ability to map from the image feature space to another meaningful semantic space. This is under the assumption that the label embeddings are learnt in a way that captures semantic similarities among the labels. In <ref type="bibr" target="#b9">[10]</ref>, authors show how semantic information gleaned from text, in form of word embeddings <ref type="bibr" target="#b21">[22]</ref>, can be exploited to enable prediction of labels never observed during training. In this work, we combine the above ideas to propose our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem Statement and Motivation</head><p>While achieving great results, approaches that rely on pseudo-labeling share a limitation. As Eqn. (1) suggests, they solely rely on the model's prediction to decide about pseudo-labeling, while disregarding any prior information about possible similarities among the classes. We find in our work that visually similar classes often produce low-confidence predictions, hence are either discarded (for methods which use confidence thresholds such as Fix-Match) or confused with others. This leads to class imbalance among the pseudo-labeled instances which potentially misguides SSL training. In <ref type="figure">Fig. 2</ref> -left, we demonstrate such behaviour by examining the pseudo-labeling statistics of FixMatch method. We use the true labels of the unlabeled data 2 to calculate the true accuracy of pseudo-labeling for each class. Further, we calculate the ratio of samples retained for pseudo-labeling (i.e. where the classifier confidence exceeds the threshold). We plot these two metrics for the 4 most and least visually similar classes 3 . We observe that visually similar concepts are chosen less frequently (i.e. less ratio) for pseudo-labeling and are often mislabeled (i.e. less accuracy) as opposed to visually distinct concepts. Motivated by this observation, we consider the label similarities as a particularly essential prior that is easy to obtain. In the subsequent sections, we will discuss how to obtain and incorporate such a prior for an improved pseudo-labeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Our Method (SemCo)</head><p>We aim to address the issues demonstrated for visually similar classes. We build on top of recent approaches, but we additionally propose to condition pseudo-labeling on our prior knowledge of class similarities. Effectively, we enhance the model by incorporating knowledge about potential confusions based on semantic and visual similarities <ref type="bibr" target="#b3">4</ref> . To that end, we encode the notion of similarity among the classes using a label embeddings matrix M ? R K?d where each row represents a d-dimensional label embedding of class k ? {1, ? ? ? , K} 5 . We further group the labels using a density-based clustering approach such as <ref type="bibr" target="#b6">[7]</ref> using a hyperparameter so that the number of groups are not pre-defined. Subsequently, we obtain Q class groups.</p><p>Thereafter, we train two classifiers sharing the same backbone network (see <ref type="figure">Fig. 1</ref>). The Semantic Classifier f sc : R h?w ? R d maps an input image closer to its corresponding label in the embedding space spanned by the rows of M ; and the One-Hot Classifier f oh : R h?w ? R K maps input images to a one-hot view of the label. Note that, for brevity, we define the classifiers f sc and f oh to implicitly include the shared backbone network and its parameters.</p><p>For each of the two classifiers we minimize a supervised loss on the labeled data and an unsupervised consistency loss on the unlabeled data. Additionally, we add a loss term for co-training to allow the two classifiers to co-operate on pseudo-labeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Semantic Classifier</head><p>For the supervised loss, we minimize the cosine loss between the true label embedding and the predicted label embedding,</p><formula xml:id="formula_2">L sc s = 1 n n i=1 C(M T y i , f sc (x i )),<label>(3)</label></formula><p>with C(z, z ) = 1 ? CosineSim(z, z ).</p><p>For the unsupervised loss (see <ref type="figure">Fig. 3</ref>), we draw inspiration from FixMatch <ref type="bibr" target="#b30">[31]</ref>, where we use a weakly augmented version of the image to obtain a pseudo-label and enforce that against the model's prediction for a strongly augmented version of the same image. Specifically, for an unlabeled image u j , we obtain the predicted embedding for a weakly augmented version of the image: q j = f sc (A w (u j )). Then we calculate class scores, p j = p(y j |u j ) by normalizing the vector-wise cosine similarity between q j and M .</p><p>Unlike FixMatch, we consider an unlabeled sample for pseudo labeling if the prediction score for one of the class groups (as opposed to the individual classes) exceeds a predefined threshold (? e ). To elaborate, referring to <ref type="figure">Fig. 3</ref>, due to the visual similarity between "bicycle" and "motorbike", the class scores for each of them, individually, is falling below the threshold. However, since they are both identified as "visually similar" based on clustering their embeddings, their scores are added first before applying the threshold. The combined score exceeds the threshold so the sample is retained for pseudo-labeling where the pseudo-label is calculated as the average of the "bicycle" and "motorbike" embeddings weighted by their normalized class prediction scores. To put it formally, to obtain the score for a given class label group, we sum the normalized class scores of all its members (where the membership is defined based on clustering M ). This gives rise to our group scores g j 6 . Thereafter, we apply our mask to select samples for pseudolabeling as per,</p><formula xml:id="formula_3">? sc j = 1(max(g j ) ? ? e ).<label>(4)</label></formula><p>If a sample is selected for pseudo-labeling, we obtain a pseudo-label embedding (? j ) 7 for such sample as a weighted average of the group members embedding, where we weigh the average based on the original class scores p j . Consequently, we apply the loss against the embedding prediction of a strongly augmented version of u j as per,</p><formula xml:id="formula_4">L sc u = 1 ? ? n ??n j=1 C(? j , f sc (A s (u j ))) ? ? sc j .<label>(5)</label></formula><p>6 g j is calculated as the inner product of p j with the cluster assignment matrix 7? j is calculated as the inner product of M with the normalised class scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">One-Hot Classifier</head><p>For the One-Hot Classifier, we follow the same procedure as the Semantic Classifier with two crucial differences: 1) we use cross-entropy loss instead of cosine loss, and 2) we don't apply label grouping before comparing with the confidence threshold. We note here that this classifier operates in a similar way to FixMatch, yet we include the loss equations for completeness. By analogy, the supervised loss is calculated as,</p><formula xml:id="formula_5">L oh s = 1 n n i=1 H(y i , f oh ((x i ))).<label>(6)</label></formula><p>Here, we use H to represent the cross-entropy loss function.</p><p>To this end, the unsupervised loss is formulated as,</p><formula xml:id="formula_6">L oh u = 1 ? ? n ??n j=1 H(? j , f oh (A s (u j ))) ? ? oh j ,<label>(7)</label></formula><formula xml:id="formula_7">where? j = arg max A w (u j ); ? oh j = 1(max(? j ) ? ? o )</formula><p>. We note that our motivation behind using two different types of loss functions is related to the concept of co-training. An assumption underlying the success of cotraining is to ensure that the learners are sufficiently diverse so that they learn better based on their different views <ref type="bibr" target="#b34">[35]</ref>. We validate such choice in our ablations (Sec. 5.4)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Co-training Loss</head><p>This loss is meant to enable both classifiers to learn from each other. The intuition is that due to each classifier's different view of the labels, they will each be confident about different samples of the unlabeled data. We exploit that by retaining a sample for pseudo-labeling if either of the classifiers is confident about its prediction. In case the two classifiers disagree about a sample (i.e. they are both confident about two different labels), the sample is included twice in the loss, once with each of the two pseudo-labels. We experimented with another approach, where in such event, the sample gets discarded but it degraded the performance. We conjecture that it's because the former approach encourages the two classifiers to be consistent while the latter completely ignores the confusion event. Formally, we define the co-training loss as,</p><formula xml:id="formula_8">L co = 1 ? ? n ??n j=1 C(M T? j , f sc (A s (u j ))) ? ? oh j + H(arg max(p j ), f oh (A s (u j ))) ? ? sc j (8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Total Loss</head><p>We now define our final training loss function by combining all five losses (Eqns. 3, and 5 to 8) as per, <ref type="figure">Figure 3</ref>: Unsupervised loss for the Semantic Classifier -A weakly augmented image is used (upper path) to obtain a predicted embedding, which is then used to obtain class scores. The class scores are summed for each label group (as identified by our grouping method) to obtain group scores. If one of the group scores exceeds the threshold, it is retained for pseduo-labeling.</p><formula xml:id="formula_9">L total = L sc s + L oh s + ? u (L sc u + L oh u ) + ? co L co . (9)</formula><p>The pseudo-label is then calculated as an average of the group members embeddings weighted by their class scores. The loss is then enforced against the predicted embedding for a strongly augmented image (lower path).</p><p>Here, ? u and ? co are fixed scalar weights to modulate the contribution of the unsupervised loss and co-training loss, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Extracting Label Semantics</head><p>In this section, we propose two alternatives to obtain the label embedding matrix M which establishes our prior on the visual similarity among the classes. Using Knowledge Graphs In cases where the class labels are semantically meaningful, we make use of the Concept-Net knowledge graph <ref type="bibr" target="#b32">[33]</ref> together with GloVe <ref type="bibr" target="#b25">[26]</ref> and word2vec <ref type="bibr" target="#b21">[22]</ref> distributional embeddings as the basis for obtaining the distributed label embeddings. ConceptNet is a multilingual knowledge graph that connects words of natural language with labeled, weighted relations. Since our main goal is to obtain label embeddings which capture visual similarity, we filter the graph to only retain relations which imply such similarity. Specifically, we retain any nodes which share the following relations: Sim-ilarTo, InstanceOf, IsA, FormOf, Synonym, Etymological-lyRelatedTo, DefinedAs. A detailed description of such relations and examples thereof can be found in the ConceptNet documentation <ref type="bibr" target="#b7">8</ref> . On the other hand, GloVe and word2vec are two prominent sets of word embeddings, the former is trained on 840 billion words of the Common Crawl <ref type="bibr" target="#b25">[26]</ref>, while the latter is trained on 100 billion words of Google News <ref type="bibr" target="#b21">[22]</ref>. The two sets capture the distributional similarity among the different words but don't necessarily capture visual similarity. For example, "cat" and "dog" usually appear in similar contexts (being both animal pets) so they would have a relatively similar GloVe (or word2vec) word embedding even though they are not visually similar. Combining the distributional embeddings with the ConceptNet 8 https : / / github . com / commonsense / conceptnet5 / wiki/Relations filtered graph allows us to address this problem: we follow a procedure similar to the authors in <ref type="bibr" target="#b32">[33]</ref> to retrofit the distributional embeddings with the filtered knowledge graph. Retrofitting <ref type="bibr" target="#b7">[8]</ref> is a process which adjusts a word embedding matrix based on a knowledge graph by optimizing an objective function which tries to find for each term a new vector close to the vector's original value but also close to the term neighbours in the graph. Since the retained relations in the graph are those which implies visual similarity, this retrofitting results in a new hybrid set of embeddings which captures distributional similarity but also correlates well with visual similarity. Finally, to handle class labels which are not present in the embeddings vocabulary, we implement a fall-out strategy to find the most reasonable alternative. We provide further description of the retrofitting process and we show a qualitative comparison to demonstrate its effectiveness in the supplementary material. We also provide further details of the fall-out strategy. Using Class Attributes Annotations In cases where the class labels are not semantically meaningful, a viable alternative is to use manually annotated class attributes. We demonstrate (Sec. 5) that by using attributes annotation of CUB-200 <ref type="bibr" target="#b36">[37]</ref> fine-grained dataset as our M matrix, we achieve significant gains against the baseline. Considering that the cost of annotating attributes is expected to be cheaper than annotating data instances, we propose class attributes annotation as a possible alternative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>To evaluate SemCo, we compare it to various recent SSL baselines on 3 standard benchmarks (CIFAR-10 <ref type="bibr" target="#b15">[16]</ref>, CIFAR-100 <ref type="bibr" target="#b15">[16]</ref>, Mini-ImageNet <ref type="bibr" target="#b27">[28]</ref>). Further, we experiment on 2 other datasets: CUB-200 <ref type="bibr" target="#b36">[37]</ref>, to test SemCo on fine-grained tasks; and DomainNet <ref type="bibr" target="#b24">[25]</ref>, to verify its performance on larger more complex images. <ref type="table">Table 1</ref>: Error rates for CIFAR-10, CIFAR-100 and Mini-ImageNet. We report results for two different values of ? -i.e. ratio between unlabeled and labeled data in a mini-batch, for our method and FixMatch. ? denotes that the results reported are using the same codebase. * denotes that the result is based on using CNN-13 model. We report the mean and standard deviation across 3 different splits of labeled data for each experiment.  <ref type="table">Table 1</ref>.</p><p>Mini-ImageNet A subset of the well-known Ima-geNet <ref type="bibr" target="#b28">[29]</ref>. It consists of 100 classes with 600 images per class (84x84 each). We use the same train/test split used by <ref type="bibr" target="#b13">[14]</ref> and we create splits for 40 and 100 labeled images per class to enable comparing with the baseline systems. However, we also experiment with 10 images per class to test SemCo in the low data regime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CUB-200</head><p>A fine-grained image classification dataset comprising 11k images from 200 different types of birds annotated with 312 attributes per class. We experiment with 5 and 10 images per class corresponding to 1000 and 2000 total labeled data.</p><p>DomainNet The dataset contains 345 classes of images coming from six domains: Clipart, Infograph, Painting, Quickdraw, Real, and Sketch. We report results only on the Real domain to evaluate how our method works on larger more complex datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Experimental Setup</head><p>Across all experiments, we follow the standard approach where we randomly select a certain number of samples to represent our labeled set and ignore the labels of the remaining samples and use them to form our unlabeled set. For the standard benchmarks, we compare our results to various existing baselines <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b30">31]</ref>, which employ consistency regularization and/or pseudo-labeling (see Sec. 6). For the two other datasets, we only compare with FixMatch, being the most similar to our solution and the closest in performance.</p><p>Since SemCo bears the most resemblance with <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b2">[3]</ref>, and <ref type="bibr" target="#b1">[2]</ref>, we follow the recommendation of Oliver et al. <ref type="bibr" target="#b23">[24]</ref> for a realistic comparison: we integrate the implementation of their methods 9 into our codebase and use the unified codebase to conduct all the experiments. As for the other baselines, we report the results as mentioned in the original papers, provided that the result is based on the same model architecture we use. We use WideResnet-28-2 <ref type="bibr" target="#b43">[44]</ref> for CIFAR-10/100, Resnet-18 <ref type="bibr" target="#b11">[12]</ref> for Mini-ImageNet, and Resnet50 for CUB-200 and DomainNet. Additionally, we attach a fully connected layer to the encoder output to act as our Semantic Classifier (see <ref type="figure">Fig. 1</ref>). We train our model end-to-end along with the backbone network.</p><p>Unless otherwise specified, we use the same hyperparameters for all our experiments. These were tuned on a validation set for a single experiment (CIFAR100 -2500 labels) and then fixed across all other experiments. In general, we found that our model is not sensitive to the values of ? u and ? co . Values between 0.5 -1 all yielded similar performance, albeit, smaller values of ? u slightly slowed convergence. Further, we found that our model is mostly sensitive to -the label grouping parameter and hence it was the only parameter we tuned separately for each dataset (see supplements for the full list of hyperparameters).</p><p>Since the labels of all datasets are semantically meaningful, we use their retrofitted embeddings (see Sec. 4.5) as targets for our Semantic Classifier. The only exception is for CUB-200 where we use human annotated attributes as targets to test our alternative proposal in Sec. 4.5. We start from the 312 dimensional class attributes given in <ref type="bibr" target="#b36">[37]</ref> and reduce their dimensionality to 128 using PCA <ref type="bibr" target="#b37">[38]</ref>. We, then, use the obtained class attributes matrix as targets for our Semantic Classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Main Results</head><p>We report standard benchmarks results in <ref type="table">Table 1</ref> and CUB-200 and DomainNet results in <ref type="table" target="#tab_2">Table 2</ref>. We observe that SemCo outperforms all the baselines with a large margin across the different datasets and amounts of labeled data (except for one case). Notably, SemCo achieves an average error rate of 55.35% on Mini-ImageNet with 1000 labels (i.e. 10 samples per class). This is almost 5.6% improvement compared to the closest baseline. We note here that Mini-ImageNet classes include 13 different species of dogs which share many visual similarities. SemCo grouped 7 of these classes into a single group based on clustering their label embeddings. To understand why the performance degrades on CIFAR10 (250 labeled samples), we looked into the clustering results for CIFAR-10 class embeddings. We observed that the 10 classes were deemed visually distinct by our clustering component, leading to one-member clusters for all 10 classes. The two above results align with our original hypothesis that SemCo is particularly useful when there are visually similar concepts among the classes. Evidently, in such cases, using label grouping in conjunction with our co-training routine helps improving the pseudolabeling quality. This is also consistent with the pseudolabeling statistics shown in <ref type="figure">Fig. 2</ref> -right, where we can see that SemCo significantly improves both the quality and the quantity of pseudo-labeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ratio of unlabeled data</head><p>We observe that SemCo achieves better results with less batch size as opposed to the baseline. As shown in <ref type="table">Table 1</ref>, we experiment with different values of ?, which defines the ratio between unlabeled and labeled data in each training batch. We find that our method consistently achieves better results even when using less unlabeled data. For example, for CIFAR-100 (4000 labels), we achieve less average error rate (29.4%) with ? = 3 than FixMatch does with ? = 7 (31.27%). More notably, we achieve 13% improvement on Mini-ImageNet (4000 labels) when fixing ? to 3.</p><p>Co-training Analysis Further, we investigate the effectiveness of our co-training routine. We use the same experimental setup of capturing pseudo-label metrics (see <ref type="figure">Fig. 2</ref>), but this time, we monitor the rate of disagreement on pseudolabels among the two classifiers f sc and f oh (i.e. percentage of time the two classifiers are confident about different pseudo-labels for the same unlabeled sample). In <ref type="figure" target="#fig_0">Fig. 4</ref> c, we report disagreement curves for the same 8 classes shown in <ref type="figure">Fig. 2</ref>. As the training progresses, we track the pseudo-labeling accuracy <ref type="figure" target="#fig_0">(Fig. 4 d)</ref> for each of: 1) our classifier ensemble, 2) our one-hot classifier f oh , and 3) the baseline (FixMatch). Note that f oh is using the same method for pseudo-labeling (i.e. confidence threshold on non-grouped labels) as FixMatch. We find that at the beginning of the training, both classifiers highly disagree about pseudo-labeling, especially for visually similar classes. As the training progresses, we witness a sharp reduction in disagreements coupled with an increase in accuracy for both classifiers. Interestingly, we find that the accuracy of our f oh is considerably higher than FixMatch although both are using the same basis for pseudo-labeling. This demonstrates the success of co-training in making pseudo-labeling consistent and accurate by leveraging the co-operation between the two classifiers. Convergence Speed In <ref type="figure" target="#fig_0">Fig. 4 a and b</ref>, we study the convergence plots on Mini-ImageNet and CIFAR-100 (1000 labels). We observe that SemCo achieves the best performance of the baseline with significantly less training iterations. This can be explained through <ref type="figure" target="#fig_0">Fig. 4 d:</ref> the higher accuracy of pseudo-labeling in the early phase of the training helps better guide the learning and thereby translates to faster convergence. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ablation</head><p>We are interested in isolating the contribution of each of the three key components of SemCo towards the witnessed performance gain. Label Grouping &amp; Co-training In <ref type="table" target="#tab_3">Table 3</ref>, we investigate the effect of label grouping (by controlling our clustering hyperparameter ), and co-training (by toggling ? co ). We observe that both components are almost equally important towards the witnessed performance gain. However, co-training seems to provide a slight advantage over label grouping in both experiments. Label Embeddings as Training Targets We experiment on CIFAR-100 and Mini-ImageNet in another setting where we use the one-hot target for both our classifiers. In such case, the only difference between the two classifiers is that the Semantic Classifier implements label grouping while the One-Hot Classifier does not. In <ref type="table" target="#tab_4">Table 4</ref>, we observe   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related Work</head><p>Since the seminal "?-model" <ref type="bibr" target="#b26">[27]</ref>, consistency regularization and pseudo-labelling SSL solutions have seen improvements in the consistency propagation <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b17">18]</ref>, and approaches for generating diverse views <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23]</ref>. For instance, the Mean-Teacher <ref type="bibr" target="#b33">[34]</ref> proposes a teacher model, where its parameters are updated according to an exponential moving average (EMA) rule. Temporal Ensembling <ref type="bibr" target="#b17">[18]</ref>, maintains an EMA over the predictions for the consistency loss computation. French et al. <ref type="bibr" target="#b8">[9]</ref> explore a masking based approach for generating diverse views. The interpolation training given in <ref type="bibr" target="#b35">[36]</ref> computes the consistency between interpolated views of unlabelled instances. Miyato et al. <ref type="bibr" target="#b22">[23]</ref> explores using adversarial methods for perturbation to create diverse views.</p><p>Our solution bears a lot of similarity to FixMatch and ReMixMatch <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b1">2]</ref> where the main idea is to use a weakly augmented image to obtain a pseudo-label then enforce it against the model's prediciton for a strongly augmented one. ReMixMatch uses a soft pseudo-label via sharpening, while FixMatch uses a hard label based on confidence. Our method compliments theirs by also conditioning on label semantics while pseudo-labeling.</p><p>Using label semantics to benefit learning is not a new idea, prior knowledge from language models <ref type="bibr" target="#b25">[26]</ref>, graph embeddings <ref type="bibr" target="#b32">[33]</ref>, and attribute vector representaions <ref type="bibr" target="#b36">[37]</ref> has helped pushing the performance of computer vision models. In the pioneering work, DeViSE <ref type="bibr" target="#b9">[10]</ref> showed distributed label representations derived from unannotated text are helpful for image classification. They also extend their solution to Zero-Shot Learning (ZSL) <ref type="bibr" target="#b40">[41]</ref>. Ye et al. <ref type="bibr" target="#b42">[43]</ref> proposes distributed labels for Few-shot learning. Such label representations are informative to even generate descriptive representations for classification <ref type="bibr" target="#b39">[40]</ref> and has become the backbone representation for ZSL <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21]</ref>. To this end, literature provides explorations on learning visualsemantic embedding spaces with better discriminative properties <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b12">13]</ref>. However, to our best knowledge the capacity of such label representations has not been explored for SSL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we have introduced a novel semisupervised learning approach leveraging class label semantics and co-training for more effective and efficient learning. We operationalize this approach for image classification, and demonstrate that it leads to significant gains. We believe the key ingredients of our approach are general and can be extended to supervised and unsupervised learning settings, which we will explore in the future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Probabilistic Interpretation of SemCo</head><p>In this section, we provide a probabilistic interpretation of our method described in Sec. 4. <ref type="bibr" target="#b9">10</ref> We start by recalling the general form of recent pseudolabeling methods captured by the below formalization for the unsupervised loss:</p><formula xml:id="formula_10">L(?) = ?1 ? ? n ?n j=1 ? j log p(y =? j | u j , ?),<label>(1)</label></formula><p>which is typically added to the loss for the labeled data. To reiterate, we use ? to represent learnable model parameters.</p><p>For an unlabelled input u i ,? denotes the pseudo-label and ? is an arbitrary function. The choice of? and ? gives rise to three distinct variations of pseudo-labeling that can be written as,</p><formula xml:id="formula_11">y j = arg max y p(y = y | u j , ?) with ? j = 1,<label>(2)</label></formula><formula xml:id="formula_12">y j = exp(f (u j )/T ) k exp(f k (u j )/T ) with ? j = 1, and<label>(3)</label></formula><formula xml:id="formula_13">y j = arg max y p(y = y | u j , ?) (4) with ? j = 1(p(y =? j | u j , ?) ? ? ).</formula><p>The first approach (Eqn. 2) corresponds to naive pseudolabeling where the class with the highest confidence is used as a pseudo-label regardless of its score, while the second (Eqn. 3) improves on that by employing temperature sharpening <ref type="bibr" target="#b2">[3]</ref> via T . For brevity, we use f is the onehot logit for class . Sharpening the pseudo-label implicitly encourages entropy minimizaton <ref type="bibr" target="#b10">[11]</ref> whereby the classifier is encouraged to produce high confidence predictions on the unlabeled data. The third approach (Eqn. 4) adopted in <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b18">19]</ref> where the unlabeled sample is only retained for pseudolabeling if the max confidence score exceeds a predefined threshold, ? . This simultaneously encourages entropy minimization <ref type="bibr" target="#b10">11</ref> and decrease confirmation bias (see Sec. 1) by only retaining high confidence samples. As opposed to above methods, we additionally propose to take a multi-view approach in which we have y as different representation of the label as well as a grouping of the similar labels potentially (but not necessarily) obtained from it. We consider this additional label to be conditionally independent of the one-hot representation of the label. Specifically, instead of Eqn. 1, we propose to minimize the <ref type="bibr" target="#b9">10</ref> All section, table, and figure references are following the original paper numbering. <ref type="bibr" target="#b10">11</ref> Note that it is equivalent to sharpening with T ? 0 objective,</p><formula xml:id="formula_14">L(?) = ?1 ? ? n j log p(y =?j | uj, ?)p(y =? j | uj, ?) ? j , p(y =? j | uj, ?) = c p(y =? j | c, uj, ?) additional classification p(c | Y , ?) grouped semantics ,<label>(5)</label></formula><p>where Y denotes the collection of the labels which we consider to be conditionally independent of the conventional one-hot label. We use the density-based clustering to calculate p(c | Y , ?). Then by using Jensen's inequality, we have the following as the upper-bound on the loss in Eqn. 5:</p><formula xml:id="formula_15">L(?) ? ?1 ? ? n j ? j log ( p(y =? j | u j , ?) ) + c log p(y =? j | c, u j , ?) Sec. 4.1 p(c | Y , ?) Sec. 4.5 .<label>(6)</label></formula><p>which indicates the log-likelihood of the additional labels are weighted by the grouping of their semantic relationships. We use the f sc in the paper to denote the classifier head that predicts these additional labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Obtaining Label Embeddings using Con-ceptNet Knowledge Graph</head><p>In this section, we elaborate on the process described in Sec. 4.5 which aims to obtain class label embeddings which correlate well with visual similarity. We start by describing the procedure and then we present some qualitative examples to demonstrate its effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Procedure</head><p>We follow a similar procedure to that described in <ref type="bibr" target="#b32">[33]</ref> with one crucial difference. Instead of using the entire Con-ceptNet graph, we use the graph after filtering it to retain only the relations which imply visual similarity (see Sec. 5 for more details).</p><p>We start with the filtered graph, the GloVe word embeddings matrix, and the word2vec word embeddings matrix. The process comprises two main steps: 1) retrofitting each of the GloVe and word2vec embeddings using the Concept-Net filtered graph to obtain two new sets of embeddings, and 2) combining the two retrofitted sets to obtain our final hybrid embeddings set.</p><p>Retrofitting Given the filtered graph and a matrix of word embeddings, the aim is to infer for each term/word a new embedding vector v i which is close to the original vector v i but also close to the term neighbors in the graph with edges E. This can be achieved by minimizing the following objective function.</p><formula xml:id="formula_16">E(v) = n i=1 ? ? ? i v i ?v i 2 + (i,j)?E ? ij v i ? v j 2 ? ? ;</formula><p>with ? i = 1 if term i is present in the embeddings vocabulary and zero otherwise; and ? ij denoting the weight of the edge connecting term i and term j. Note that the use of ? allows optimizing the above objective for terms which appear in the knowledge graph even if it is not present in the vocabulary of the word embeddings <ref type="bibr" target="#b31">[32]</ref>. To minimize the above function, we follow the iterative algorithm originally suggested by Faruqui et al. <ref type="bibr" target="#b7">[8]</ref> and later extended by Speer et al. <ref type="bibr" target="#b31">[32]</ref>. We perform such optimization twice: once for the GloVe embeddings and another for the word2vec.</p><p>Combining the Two Sets After applying retrofitting to both matrices, we combine them by finding a globally linear projection that aligns the results based on their common vocabulary. As inspired by <ref type="bibr" target="#b44">[45]</ref> and <ref type="bibr" target="#b32">[33]</ref>, to find such projection, we concatenate the columns of the two matrices and use SVD to reduce their dimensionality to 128. Such alignment allows us to deduce compatible embeddings for terms which appear in one of the vocabularies but not the other. This alignment and merging give rise to a hybrid set of embeddings which combines all three sources: GloVe, word2vec, and ConceptNet filtered graph. We use this set as the basis for establishing the prior on visual similarity among a given set of class labels (see Sec. 4).</p><p>Handling Out-of-Vocabulary Labels Our obtained embeddings vocabulary consists of approximately 500k different terms and hence provides a reasonable coverage for most of the class labels. However, it might sometimes be the case that one or more of the class labels are missing from the vocabulary. In such event, we employ a fall-out strategy to find the most reasonable alternative. We present the flowchart for our fall-out strategy in <ref type="figure">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Label Grouping Examples</head><p>As mentioned in Sec. 4, we apply density-based clustering on the class labels embeddings to group the labels into visually similar concepts. To demonstrate the effectivness of the retrofitted embeddings in capturing said similarity, we compare the clustering output if we use the retrofitted embeddings as opposed to if we use the GloVe distributional embeddings without retrofitting. We perform this comparison for Mini-ImageNet (see <ref type="table">Table 6</ref>,7), CIFAR-100 (see <ref type="table">Table 8</ref> <ref type="bibr">,9)</ref>, and DomainNet classes (see <ref type="table">Table 10</ref> <ref type="bibr">,11)</ref>. Note that we only report the groups having more than one member and we omit single-member groups. We observe that in all three cases, clustering the retrofitted embeddings produces groups which largely match our intuition about visual similarity. On the other hand, we notice that clustering the non-retrofitted GloVe embeddings results in grouping labels which usually appear in similar context, even if they are not visually similar. For example, in <ref type="table">Table 9</ref>, we observe that "sea" was grouped with other classes which are contextually related to "sea", yet bear no visual similarity to it. This is due to the fact that GloVe embeddings are learnt in a way that captures distributional semantics rather than visual semantics. However, when the GloVe embeddings are retrofitted with the ConceptNet filtered graph, we witness an improved grouping which aligns better with visual semantic similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Details</head><p>Hyperparameters In our preliminary experiments, we mostly found that our method is not sensitive to the hyperparameters, so we tuned their values via a validation set on a single experiment (CIFAR100 -2500 labels) then fixed them across all other experiments to the values shown in Table 5. The only exception is the density-based clustering parameter . The number of clusters (i.e. label groups) is automatically decided based on , which denotes the maximum cosine distance between two embedding vectors for one to be considered in the same neighbourhood as the other. The larger the value of , the more aggressive the grouping is (i.e. the more members in each group). Accordingly, we tune individually for each dataset. We find that = 0.2 works well for all datasets except Mini-ImageNet where we use = 0.3 instead. In <ref type="figure" target="#fig_3">Fig. 8</ref>, we demonstrate the effect of varying on the error rate using a single split of CIFAR-100 (2500 labeled instances) when training for 100 epochs. Semantic Classifier Loss We use two different loss functions for our two classifiers, i.e. cosine loss for the Semantic Classifier, and cross-entropy for the One-Hot Classifier (see Sec. 4). It is, hence, important to consider the scale of both losses so that one doesn't dominate over the other. Cosine loss values are bounded between 0 and 2 while crossentropy values are not. Accordingly, we multiply the Semantic Classifier loss by a factor of 3 before applying the back propagation step. We obtained such value by using a held-out validation set on CIFAR-100 (1000 labeled examples) and we fixed it across all other experiments and datasets. Augmentations As described in Sec. 2, we make use of two types of augmentations, i.e. weak and strong. For weak augmentations, we use random cropping and padding, and random horizontal flips. As for the strong augmentations, we use the RandAugment <ref type="bibr" target="#b5">[6]</ref> list of transformations for both our system and the FixMatch baseline. Inference Since we train two classifiers in our method, during inference time, we can choose one of three options for inference: 1) use the One-Hot Classifier prediction, 2) use the Semantic Classifier prediction, 3) Average the softmax <ref type="figure">Figure 5</ref>: A flowchart describing our label embedding lookup strategy aiming to find the most reasonable embedding for a given class label. We include demonstrative examples for each of the fall-out cases.  <ref type="figure">Figure 6</ref>: CIFAR-100 confidence-based pseudo-labeling comparison between the baseline (left) and our method (right). Accuracy values show how much, on average, pseudo-labels for a given class match the true label, while Ratio values show the percentage of samples of a given class which are retained for pseudo-labeling (i.e. with confidence score above the threshold). The two metrics are calculated for the 4 most (red) and least (green) visually similar classes over the first 10 epochs of training.</p><p>scores of the two classifiers and use the combined score for prediction. During our validations, we found that the former approach always yields marginally better results, so we use it as our basis for inference. Finally, We also use an exponential moving average of model weights with a decay rate of 0.999. The caption next to each image group denotes the true class to which the image group belongs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Further Pseudo-labeling Analysis</head><p>In <ref type="figure">Fig. 2</ref> in the main text, we present a comparison between pseudo-labeling statistics (on Mini-ImageNet dataset) of our method versus the baseline. In this section, we elaborate about the experimental setup for obtaining these statistics. Additionally, we provide similar analysis on CIFAR-100 dataset.</p><p>For a given dataset, we run our algorithm for 10 epochs of unlabeled data and we capture a highly granular view of the pseudo-labeling statistics for each mini-batch. Consequently, we calculate two metrics: 1) we use the true labels of the unlabeled data samples (which we originally ignore to emulate an SSL setting) to measure the true pseudo-labeling accuracy for each of the classes in the dataset, and 2) we use the classifier confidence scores to calculate the pseudolabeling ratio for each class, which represents the amount of unlabeled samples exceeding the confidence threshold and thereby are retained for pseudo-labeling. We repeat the same procedure and measure the same metrics for our baseline <ref type="bibr" target="#b30">[31]</ref>. We, then, display those metrics for the 4 classes which were deemed by our clustering method as the most visually similar concepts. Conversely, we also display them for the 4 classes which are deemed most visually distinct. In <ref type="figure">Fig. 6</ref>, we report these metrics for CIFAR-100 dataset (see <ref type="figure">Fig. 2</ref> for Mini-ImageNet). Additionally, through the same experimental setup described above, we keep track of pseudo-labeling statistics for each individual unlabeled image. We report in <ref type="figure" target="#fig_1">Fig. 7</ref> the most confused images among the 4 most visually similar classes for both datasets. We define confusion as the average number of times a given image is incorrectly pseudo-labeled as another class within the 4 classes (e.g. "boy" pseudo-labeled as "girl").  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>Experimental analysis plots showing: (a,b): Convergence trends of our method and the baseline for CIFAR-100 (a) and Mini-ImageNet (b) with 1000 labeled examples. (c,d): Co-training analysis plots showing the disagreements between our two classifiers for visually similar and distinct classes (c) and the associated pseudo-labeling accuracies (d). The co-training plots are spanning only the first 10 epochs of training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 7 :</head><label>7</label><figDesc>The most confused images for the 4 most visually similar classes of Mini-ImageNet (left) and CIFAR-100 (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 :</head><label>8</label><figDesc>Error rates for different values of . = 0 corresponds to no label grouping, while &gt; 0.7 corresponds to grouping all labels into a single cluster.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Visually similar classes Visually distinct classes Visually similar classes Visually distinct classes FixMatch Ours</head><label></label><figDesc></figDesc><table><row><cell>60%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>60%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>40%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>40%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>20%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>20%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ibizan</cell><cell>golden</cell><cell>wok</cell><cell>frying</cell><cell>orange theater</cell><cell>school</cell><cell>rock</cell><cell>ibizan</cell><cell>golden</cell><cell>wok</cell><cell>frying</cell><cell>orange theater</cell><cell>school</cell><cell>rock</cell></row><row><cell>hound</cell><cell>retriever</cell><cell></cell><cell>pan</cell><cell>curtain</cell><cell>bus</cell><cell>beauty</cell><cell>hound</cell><cell>retriever</cell><cell></cell><cell>pan</cell><cell>curtain</cell><cell>bus</cell><cell>beauty</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Error rates on CUB-200 dataset and DomainNet Real. Errors are reported based on 1 split for each of the amounts of labeled data. Poor baseline results are omitted.</figDesc><table><row><cell>CUB-200</cell><cell cols="2">Total Labeled Samples</cell></row><row><cell>Method</cell><cell>1000</cell><cell>2000</cell></row><row><cell>Supervised baseline</cell><cell>-</cell><cell>70.11</cell></row><row><cell>FixMatch</cell><cell>84.35</cell><cell>72.15</cell></row><row><cell>Ours (SemCo)</cell><cell>79.44</cell><cell>66.76</cell></row><row><cell>DomainNet Real</cell><cell cols="2">Total Labeled Samples</cell></row><row><cell>Method</cell><cell>6900</cell><cell>10350</cell></row><row><cell>Supervised baseline</cell><cell>47.9</cell><cell>45.2</cell></row><row><cell>FixMatch</cell><cell>41.34</cell><cell>39.04</cell></row><row><cell>Ours (SemCo)</cell><cell>35.32</cell><cell>32.89</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Error Rates for different settings of Co-training and Label Grouping</figDesc><table><row><cell></cell><cell></cell><cell>Mini-ImageNet</cell><cell>CIFAR-100</cell></row><row><cell></cell><cell></cell><cell>1000</cell><cell>2500</cell></row><row><cell cols="2">Label Grouping Co-training</cell><cell cols="2">Error Rate</cell></row><row><cell></cell><cell></cell><cell>55.35</cell><cell>31.93</cell></row><row><cell>-</cell><cell></cell><cell>59.60</cell><cell>33.09</cell></row><row><cell></cell><cell>-</cell><cell>60.39</cell><cell>33.19</cell></row><row><cell>-</cell><cell>-</cell><cell>62.16</cell><cell>34.25</cell></row></table><note>a significant decrease in performance when using the one- hot view for both the classifiers. This strongly supports our hypothesis that co-training with different views of the label does indeed help the learning.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Error Rates when using Embedding Targets versus One-Hot Targets for our Semantic Classifier, reported on CIFAR-100 and Mini-ImageNet</figDesc><table><row><cell></cell><cell cols="2">Embeddings Target One-Hot Target</cell></row><row><cell>CIFAR-100 (2500)</cell><cell>31.93</cell><cell>33.33</cell></row><row><cell>Mini-ImageNet (1000)</cell><cell>55.35</cell><cell>60.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :similar classes Visually distinct classes Visually similar classes Visually distinct classes FixMatch Ours</head><label>5</label><figDesc>Hyper-parameters values across all our experiments Dimensionality of the label embeddings 128 DBSCAN clustering coefficient denoting the maximum distance between two samples for one to be considered as in the neighborhood of the other 0.20</figDesc><table><row><cell></cell><cell cols="2">Hyper-parameter</cell><cell></cell><cell cols="2">Description</cell><cell></cell><cell></cell><cell></cell><cell>Value</cell></row><row><cell></cell><cell>?u</cell><cell></cell><cell>Unlabeled loss coefficient</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.00</cell></row><row><cell></cell><cell>?co</cell><cell></cell><cell>Co-training loss coefficient</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.00</cell></row><row><cell></cell><cell>?e</cell><cell></cell><cell cols="6">Pseudo-labeling confidence threshold for the Semantic Classifier</cell><cell>0.70</cell></row><row><cell></cell><cell>?o</cell><cell></cell><cell cols="6">Pseudo-labeling confidence threshold for the One-Hot Classifier</cell><cell>0.95</cell></row><row><cell></cell><cell>batch size</cell><cell></cell><cell cols="2">Number of labeled images per batch</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>64</cell></row><row><cell></cell><cell>?</cell><cell></cell><cell cols="6">Ratio between number of unlabeled and labeled images in each batch</cell><cell>3</cell></row><row><cell></cell><cell cols="2">images per epoch</cell><cell cols="2">Number of labeled images per epoch</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>64 ? 1024</cell></row><row><cell></cell><cell cols="2">num epochs</cell><cell>Number of epochs of training</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>300</cell></row><row><cell></cell><cell>lr</cell><cell></cell><cell cols="5">learning rate max value (10 epochs warmup then cosine decay)</cell><cell></cell><cell>0.03</cell></row><row><cell></cell><cell cols="2">weight decay</cell><cell cols="2">Weight decay regualrization coefficient</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5.00 ? 10 ?4</cell></row><row><cell></cell><cell>momentum</cell><cell></cell><cell cols="2">Nesterov momentum for SGD optimizer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.90</cell></row><row><cell></cell><cell>emb dim</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>60% 40% 20%</cell><cell>boy Visually girl</cell><cell>crab</cell><cell>lobster sunflower orange wardrobe plain</cell><cell>60% 40% 20%</cell><cell>girl</cell><cell>boy</cell><cell>crab</cell><cell cols="2">lobster sunflower orange wardrobe plain</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For notation simplicity, we assume here that the arg max in Eqn. 2 produces a one-hot probability distribution</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that we have access to the true labels but they are discarded during training to emulate an SSL setting<ref type="bibr" target="#b2">3</ref> We elaborate on how we identify similarity in Sec. 4</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We present a probabilistic interpretation of our method in the supplementary material<ref type="bibr" target="#b4">5</ref> We defer the discussion on how to obtain M to Sec. 4.5.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">we don't report results for<ref type="bibr" target="#b1">[2]</ref> due to adaptation difficulties</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was partly supported by DARPA's Learning with Less Labeling (LwLL) program under agreement FA8750-19-2-0501 and by the Australian Government Research Training Program (RTP) Scholarship.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>'oak tree', 'pine tree'   'jacket', 'pants', 'shorts', 'sweater', 'underwear' Group 20 'raccoon', 'squirrel' Group 21 'radio', 'television' Group 22</p><p>'snowflake', 'snowman' Group 23 'toothbrush', 'toothpaste'</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pseudo-labeling and confirmation bias in deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E O&amp;apos;</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcguinness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09785</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh annual conference on Computational learning theory</title>
		<meeting>the eleventh annual conference on Computational learning theory</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="92" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semi-supervised learning (chapelle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<editor>o. et al.</editor>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="542" to="542" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>book reviews</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A density-based algorithm for discovering clusters in large spatial databases with noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rg</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Xu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Retrofitting word vectors to semantic lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sujay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4166</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Milking cowmask for semi-supervised image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.12022</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning robust visual-semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Kang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3571" to="3580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Label propagation for deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmet</forename><surname>Iscen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>Yannis Avrithis, and Ondrej Chum</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semantic autoencoder for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elyor</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3174" to="3183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Featmatch: Feature-based augmentation for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Wen</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08505</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Temporal ensembling for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02242</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on challenges in representation learning, ICML</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Decoupled certaintydriven consistency loss for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiting</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robby</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.05657</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hyperbolic visual embedding learning for zero-shot recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoteng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangming</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Shin-Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1979" to="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Realistic evaluation of deep semi-supervised learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekin</forename><surname>Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3235" to="3246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Moment matching for multi-source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinxun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xide</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1406" to="1415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with ladder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikko</forename><surname>Honkala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapani</forename><surname>Raiko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3546" to="3554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Regularization with stochastic transformations and perturbations for deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehran</forename><surname>Javanmardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Tasdizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1163" to="1171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Fixmatch: Simplifying semisupervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07685</idno>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">An ensemble method to produce high-quality word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robyn</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Chin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.01692</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Conceptnet 5.5: An open multilingual graph of general knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robyn</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03975</idno>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A survey on semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jesper E Van Engelen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Holger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="373" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Interpolation consistency training for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 28th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3635" to="3641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Caltech-ucsd birds 200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Principal component analysis. Chemometrics and intelligent laboratory systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svante</forename><surname>Wold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Esbensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Geladi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="37" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unified visual-semantic embeddings: Bridging vision and language with structured meaning representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6609" to="6618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Feature generating networks for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5542" to="5551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Zero-shot learning-the good, the bad and the ugly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4582" to="4591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12848</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Learning embedding adaptation for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Han-Jia Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>De-Chuan Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03664</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Wide residual networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning translation models from monolingual continuous representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hany</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
