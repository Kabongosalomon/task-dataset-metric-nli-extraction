<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Global Context-Aware Progressive Aggregation Network for Salient Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuyao</forename><surname>Chen</surname></persName>
							<email>chenzuyao17@mails.ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianqian</forename><surname>Xu</surname></persName>
							<email>xuqianqian@ict.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Key Lab. of Intelligent Information Processing</orgName>
								<orgName type="institution">ICT, CAS</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runmin</forename><surname>Cong</surname></persName>
							<email>rmcong@bjtu.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">Institute of Information Science</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
							<email>qmhuang@ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Key Lab. of Intelligent Information Processing</orgName>
								<orgName type="institution">ICT, CAS</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Key Lab. of Big Data Mining and Knowledge Management</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<region>Guangdong</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Global Context-Aware Progressive Aggregation Network for Salient Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep convolutional neural networks have achieved competitive performance in salient object detection, in which how to learn effective and comprehensive features plays a critical role. Most of the previous works mainly adopted multiplelevel feature integration yet ignored the gap between different features. Besides, there also exists a dilution process of high-level features as they passed on the top-down pathway. To remedy these issues, we propose a novel network named GCPANet to effectively integrate low-level appearance features, high-level semantic features, and global context features through some progressive context-aware Feature Interweaved Aggregation (FIA) modules and generate the saliency map in a supervised way. Moreover, a Head Attention (HA) module is used to reduce information redundancy and enhance the top layers features by leveraging the spatial and channel-wise attention, and the Self Refinement (SR) module is utilized to further refine and heighten the input features. Furthermore, we design the Global Context Flow (GCF) module to generate the global context information at different stages, which aims to learn the relationship among different salient regions and alleviate the dilution effect of high-level features. Experimental results on six benchmark datasets demonstrate that the proposed approach outperforms the state-of-the-art methods both quantitatively and qualitatively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Salient object detection aims to detect interesting regions that attract human attention in an image <ref type="bibr" target="#b2">(Cong et al. 2018a)</ref>. As an efficient preprocessing technique, salient object detection benefits a wide range of applications such as image understanding <ref type="bibr" target="#b36">(Zhang, Du, and Zhang 2014)</ref>, image retrieval <ref type="bibr" target="#b9">(Gao et al. 2015)</ref>, and object tracking <ref type="bibr" target="#b11">(Hong et al. 2015)</ref>.</p><p>In recent years, the development of deep learning, especially the emergence of Fully Convolutional Network <ref type="bibr" target="#b21">(Long, Shelhamer, and Darrell 2015)</ref>, has greatly boosted the progress of salient object detection <ref type="bibr" target="#b37">(Zhao et al. 2015;</ref><ref type="bibr" target="#b15">Li and Yu 2016;</ref><ref type="bibr" target="#b27">Wang et al. 2016)</ref>. Fully Convolutional Network (FCN) stacks multiple convolution layers and pooling  <ref type="bibr" target="#b20">(Liu, Han, and Yang 2018)</ref>; (e) BASNet <ref type="bibr" target="#b25">(Qin et al. 2019</ref>). layers to gradually enlarge the receptive fields of network and extracts high-level semantic information. As pointed out in previous works <ref type="bibr" target="#b22">(Luo et al. 2017;</ref><ref type="bibr" target="#b33">Zhang et al. 2017</ref>), due to the pyramid-like CNNs structure, low-level features usually have larger spatial size and more fine-grained details, while high-level features tend to gain more semantic knowledge and discard some meaningless or irrelevant detail information. Generally speaking, the high-level features are beneficial to the coarse localization of salient objects, whereas the low-level features that contain the spatial structural details are suitable to refine boundaries. However, there remains several problems for the FCN-based methods: (1) Due to the gap between different level features, the simple combination of semantic information and appearance information is insufficient and lacks consideration of the different contribution of different features for salient object <ref type="bibr">detection;</ref> (2) Most of the previous works ignored the global context information, which benefits for deducing the relationship among multiple salient regions and producing more complete saliency result.</p><p>To remedy the above mentioned issues, we propose a novel network named Global Context-Aware Progressive Aggregation Network (GCPANet), which consists of four modules: Feature Interweaved Aggregation (FIA) module, Self Refinement (SR) module, Head Attention (HA) module, and Global Context Flow (GCF) module. Considering the characteristics difference between multiple level features, we design the FIA module to fully integrate the high-level semantic features, low-level detail features, and global context features, which is expected to suppress the noises but recover more structural and detail information. Before the first FIA module, we add a HA module on the top layer of the backbone to strengthen the spatial and channel-wise response on the salient object. After aggregation, features will be fed into a SR module to refine the feature maps via leveraging the inner characteristics within features. Taken into account that the context information can benefit for capturing the relationship among multiple salient objects or different parts of salient object, we design a GCF module to exploit the relationship from global perspective, which is conducive to improving the completeness of salient object detection. Besides, as pointed out in <ref type="bibr" target="#b19">(Liu et al. 2019)</ref>, the high-level features will be diluted as they passed on the topdown pathway. By introducing GCF, the features containing global semantics are delivered to feature maps at different stages, which alleviates the effect of features dilution. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, the proposed method can handle some challenging scenarios, such as complex scene understanding (the high-luminance ceiling interference), or multiple objects relationship reasoning (the ping-pong bat and ball).</p><p>From the above, the contributions of our work can be summarized as follows: 1. A global context-aware progressive aggregation network is proposed to achieve saliency detection, which includes the Feature Interweaved Aggregation (FIA) module, the Self Refinement (SR) module, the Head Attention (HA) module, and the Global Context Flow (GCF) module. 2. The FIA module integrates the low-level detail information, high-level semantic information, and global context information in an interweaved way, where the global context information is produced by the GCF module to capture the relationship among different salient regions and improve the completeness of the generated saliency map. 3. Compared with 12 state-of-the-art methods on six public benchmark datasets, the proposed network GCPANet achieves best performance in quantitative and qualitative evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>In this section, we will review the related works on deep learning based salient object detection methods, which have achieved remarkable progress on saliency detection thanks to its powerful representation capability. Inspired by image semantic segmentation, Zhao et al. <ref type="bibr" target="#b37">(Zhao et al. 2015)</ref> proposed a fully connected CNN to integrate local and global features to predict the saliency map. <ref type="bibr" target="#b27">Wang et al. (Wang et al. 2016</ref>) adopted a recurrent CNN to refine the predicted saliency map step by step. For further enhance the saliency map, several recent works <ref type="bibr" target="#b33">Zhang et al. 2017;</ref><ref type="bibr" target="#b6">Deng et al. 2018;</ref><ref type="bibr" target="#b13">Hu et al. 2018;</ref><ref type="bibr" target="#b17">Li et al. 2018;</ref><ref type="bibr" target="#b34">Zhang et al. 2018a;</ref> integrate features in multiple layers of CNN to exploit the context information at different semantic levels. Among them, Hou et al. ) introduced short connections to the skiplayer structure for capturing fine details. <ref type="bibr" target="#b33">Zhang et al. (Zhang et al. 2017</ref>) concatenated multi-level feature maps based on multiple resolution and introduced a boundary refinement strategy. <ref type="bibr" target="#b6">Deng et al. (Deng et al. 2018)</ref> proposed an iterative method to optimize the saliency map, leveraging features generated by deep and shallow layers. Hu et al. ) recurrently concatenated multi-layer features for saliency detection. <ref type="bibr" target="#b17">Li et al. (Li et al. 2018</ref>) proposed a contour-to-saliency transferring method that simultaneously predict the contours and saliency maps. Zhang et al. <ref type="bibr" target="#b34">(Zhang et al. 2018a</ref>) built a bi-directional message passing model for better integrating multi-level features. Zhang et al. <ref type="bibr" target="#b35">(Zhang et al. 2018b</ref>) designed an attention guided network that selectively integrates multi-level contextual information in a progressive manner. Lately, Wu et al. <ref type="bibr" target="#b30">(Wu, Su, and Huang 2019)</ref> proposed a cascade partial decoder that utilizes attention mechanism to refine high-level features. Qin et al. <ref type="bibr" target="#b25">(Qin et al. 2019</ref>) proposed a boundary-aware model to segment salient object regions and predict the boundaries simultaneously. <ref type="bibr" target="#b19">Liu et al. (Liu et al. 2019</ref>) extended the FPN structure equipped with pyramid pooling module to fuse the coarselevel semantic features and fine-level features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology</head><p>In this section, we first outline the proposed network. Then, we elucidate how each component made up and illustrate its effect for saliency detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overview of the Proposed Network</head><p>As <ref type="figure" target="#fig_1">Fig. 2</ref> shows, the proposed network is a symmetrical encoder-decoder architecture, where the encoder component is based on ResNet-50 to extract the multi-level features, and the decoder component progressively integrates the multilevel comprehensive features to generate the saliency map in a supervised way. Specifically, we first use a HA module to strengthen the spatial regions and feature channels with high response on salient objects, and a SR module to generate the first-stage high-level features through the feature refinement and enhancement. Then, we progressively cascade a FIA module and a SR module in three times to learn more discriminative features and generate more accurate saliency map. In the FIA module, the low-level detail information, high-level semantic information, and global context information are fused in an interweaved way. The SR module successive to each FIA module is to refine the coarse aggregation features. Note that, the global context information is produced by the proposed GCF module, which captures the relationship among different salient regions and constrains more complete saliency prediction. To facilitate the optimization, we combine auxiliary loss branches of each sub-stage with dominant loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Interweaved Aggregation Module</head><p>As we all know, low-level features include more detail information, such as texture, boundary, and spatial structure, but they also contain more background noises. By contrast, high-level features can provide abstract semantic information, which is beneficial to locate the salient object and suppress the noises. Thus, these two level features are always combined together to generate the complementary features. In addition to these two level features, the global context information is very useful to infer the relationship among different salient objects or parts from the global perspective, which is conducive to generate more complete and accurate saliency map. Moreover, using the context features can alleviate the effect of feature dilution. Hence, we develop the FIA module to fully integrate these three level features, which in turn produces a discriminative and comprehensive feature with global perception. Specifically, as shown in <ref type="figure" target="#fig_2">Fig.  3</ref>, the FIA module receives three parts input, i.e., the highlevel features from the output of the previous layer, the lowlevel features from the corresponding bottom layer, and the global context feature generated by the GCF module. Note that, the production of global context feature will be introduced in the latter subsection.</p><p>We first introduce the aggregation strategy for high-level features and low-level features. Different from previous works <ref type="bibr" target="#b25">(Qin et al. 2019;</ref><ref type="bibr" target="#b19">Liu et al. 2019</ref>) that often simply fuse the high-level features after up-sampling with the low-level features by concatenation or addition operation, we adopt a more aggressive yet efficient operation, i.e., multiplication. The multiplication operation can strengthen the response of salient objects, meanwhile suppress the background noises. Specifically, for the consistency of multiplication operation, the low-level feature maps f t l (t = 1, 2, 3) are firstly fed into a 1 ? 1 convolution layer conv 1 , which compress the features to have the same number of channels as of the highlevel features f t h . Then, a 3 ? 3 convolution layer is applied to high-level features f t h to obtain a semantic mask W t h after up-sampling. Further, we multiply the mask W t h to the compressed low-level featuresf t l . Besides, considering high-level features will discard some detail information relevant to salient objects, we apply the above fusion strategy in a mirror way. The mirror path different from the above mentioned is that a detail mask W t l is generated by lowlevel features through a 3 ? 3 convolution layer and then, the mask W t l is multiplied to the high-level featuresf t h af- ter up-sampling. The mirror path is supposed to add finegrained detail information to the predicted saliency maps. The above process can be described as</p><formula xml:id="formula_0">f h conv upsample f hl con? f l f l upsample f lhf l conv f g conv upsample f glf l c conv f a</formula><formula xml:id="formula_1">W t h = upsample(conv 2 (f t h )) (1) f t hl = ?(W t h f t l ) (2) W t l = conv 3 (f t l ) (3) f t lh = ?(W t l upsample(f t h ))<label>(4)</label></formula><p>wheref t l = conv 1 (f t l ) denotes the compressed low-level features, denotes element-wise multiplication, ? denotes the ReLU activation function, upsample is the up-sampling operation via bilinear interpolation, and t is the stage index. Further, to model the relationship between different parts of salient objects and alleviate the dilution process of highlevel features, we introduce the global context features f t g at each stage. We employ the global context features f t g to generate a context mask W t g . Then, the mask W t g is multiplied conv conv conv f out f in <ref type="figure">Figure 4</ref>: Structure of the SR module. to the compressed low-level featuresf t l .</p><formula xml:id="formula_2">W t g = upsample(conv 4 (f t g )) (5) f t gl = ?(W t g f t l )<label>(6)</label></formula><p>Finally, these three level features are concatenated and then passed through a 3 ? 3 convolution layer to obtain the final fusion features:</p><formula xml:id="formula_3">f t a = conv 5 (concat(f t hl , f t lh , f t gl ))<label>(7)</label></formula><p>Each of the above mentioned convolution layers except conv 2 , conv 3 , and conv 4 is equipped with a batch normalization layer and the ReLU activation function. The output of FIA module is then passed to the SR module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self Refinement Module</head><p>In FIA module, we combine the complementary characteristics between different level features and obtain the comprehensive feature expression. As a simple and intuitionistic way, one can directly apply a softmax layer after FIA module to obtain the saliency maps, while it still exists some defects. For instance, there are some holes in the predicted salient objects, which are caused by the contradictory response of different layers. Hence, we develop a SR module to further refine and enhance the feature maps after passing the HA module and FIA modules by utilizing the multiplication and addition operation (see <ref type="figure">Fig. 4</ref>). In detail, we firstly apply a 3 ? 3 convolution layer to squeeze the input features f in into feature vectorf with the channel dimension of 256, meanwhile remaining useful information. Then, the featuref is fed into two convolution layers to obtain the mask W and bias b for multiplication and addition operation. The main process can be described as</p><formula xml:id="formula_4">f = conv 6 (f in )<label>(8)</label></formula><formula xml:id="formula_5">f out = ?(W f + b) (9)</formula><p>where f out is the refined feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Head Attention Module</head><p>Since the top layers features of the encoder component usually are redundant for salient object detection, we design a HA module following the top layer to learn more selective and representative features by leveraging the spatial and channel-wise attention mechanisms. Specifically, we first apply a convolution layer to the input feature maps F to obtain a compressed feature representa-tionF with 256 channels. Then, we generate a mask W and bias b as similar as the way used in the SR module. The output of the first stage is obtained by</p><formula xml:id="formula_6">F 1 = ?(W F + b)<label>(10)</label></formula><p>Further, the input feature F is down-sampled into a channel-wise feature vector f through average pooling, which has strong consistency and invariance. Then, two successive fully connected layers f c 1 (?), f c 2 (?) are applied to project the feature vector f into an output vector y. The final output feature maps F out will be obtained via weighting with vector y. The second stage can be described as the following equations,</p><formula xml:id="formula_7">y = ? ? f c 2 ? ? ? f c 1 ? (f ) (11) F out = F 1 y<label>(12)</label></formula><p>where f c i (?) denotes i-th FC layers, ? denotes the ReLU activation function, ? is the sigmoid operation, and ? denotes function composition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global Context Flow Module</head><p>For the challenging scenarios in salient object detection, such as cluttered background, foreground disturbance, and multiple salient objects, simple integration of high-level and low-level features may fail to completely detect the salient regions due to lacking the global semantic relationship among different parts of salient object or multiple salient objects. Besides, since the top-down pathway is built upon the bottom-up backbone, the high-level features will be gradually diluted as they are transmitted to lower layers. To remedy these issues, we design the GCF module to capture the global context information embedded into the FIA module at each stage. Different from <ref type="bibr" target="#b19">(Liu et al. 2019)</ref>, we take into account the different contributions at different stages. We firstly employ global average pooling <ref type="bibr" target="#b18">(Lin, Chen, and Yan 2013)</ref> to obtain the global contextual information and then reassign different weights to different channels of the global contextual feature maps for each stage. More specifically, for each stage, the process can be described as</p><formula xml:id="formula_8">y t = ? ? f c 4 ? ? ? f c 3 (f gap ) (13) f t = conv 10 (f top ) (14) f t g =f t y t<label>(15)</label></formula><p>where f top refers to the top layer features, and f gap refers to the features generated by the top layer features through global average pooling, which includes global contextual information. Then, the output f t g is fed into the FIA module, which has been elaborated in the previous section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Function</head><p>In saliency detection, binary cross-entropy loss is often used as the loss function to measure the relation between the generated saliency map and the ground truth, whcih can be formulated as</p><formula xml:id="formula_9">= ? 1 H ? W H i=1 W j=1 [G ij log(S ij ) + (1 ? G ij ) log(1 ? S ij )]<label>(16)</label></formula><p>where H, W denote the height and width of the image, respectively, G ij is the ground truth label of the pixel (i, j), and S ij represents the corresponding probability of being salient objects in position (i, j). To facilitate the optimization of the proposed network, we add auxiliary loss at three decoder stages. Specifically, a 3 ? 3 convolution operation is applied for each stage to squeeze the channel of the output feature maps to 1. Then these maps are up-sampled to the same size as the ground truth via bilinear interpolation and sigmoid function is used to normalize the predicted values into [0, 1]. The total loss consists of two parts, i.e., the dominant loss corresponding to the output and the auxiliary loss of each sub-stage.</p><formula xml:id="formula_10">total = dom + 3 i=1 ? i i aux (17)</formula><p>where ? i denotes the weight of different loss, and dom , i aux denote the dominant and auxiliary loss, respectively. The auxiliary loss branches only exist during the training stage, whereas they are abandoned when inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>In this section, we first describe the implementation details, introduce the benchmark datasets, evaluation metrics. Then, we conduct experiments on these datasets to evaluate the effectiveness of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>We adopt ResNet-50 <ref type="bibr" target="#b10">(He et al. 2016</ref>) pretrained on ImageNet <ref type="bibr" target="#b5">(Deng et al. 2009</ref>) as our network backbone. In the training stage, we resize each image to 320?320 with random horizontal flipping, then randomly crop a patch with the size of 288 ? 288 for training. During the inference stage, images are simply resized to 320 ? 320 then fed into the network to obtain prediction without any other post-processing (e.g., CRF). We use Pytorch <ref type="bibr" target="#b24">(Paszke et al. 2017</ref>) to implement our model. Mini-batch Stochastic gradient descent (SGD) is used to optimize the whole network with the batch size of 32, the momentum of 0.9, and the weight decay of 5e-4. We use the warm-up and linear decay strategies with the maximum learning rate 5e-3 for the backbone and 0.05 for other parts to train our model and stop training after 30 epochs. The inference of a 320 ? 320 image takes about 0.02s (over 50 fps) with the acceleration of one NVIDIA Titan-Xp GPU card. The code is now available. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>We conduct experiments on six public saliency detection benchmark datasets, and the detailed introduction is provided as follows:</p><p>? ECSSD  consists of 1,000 natural images which are manually collected from the Internet;</p><p>? PASCAL-S <ref type="bibr" target="#b16">(Li et al. 2014</ref>) has 850 natural images that are carefully selected from the PASCAL VOC dataset (Everingham et al. 2010);</p><p>? HKU-IS <ref type="bibr" target="#b14">(Li and Yu 2015)</ref> includes 4,447 images and most of them have low contrast or more than one salient object; 1 https://github.com/JosephChenHub/GCPANet.git</p><p>? DUT-OMRON <ref type="bibr" target="#b32">(Yang et al. 2013)</ref> contains 5,168 high quality images. Images of this dataset have one or more salient objects and relatively cluttered background, and thus salient object detection on this dataset is very challenging;</p><p>? SOD (Movahedi and Elder 2010) is composed of 300 images, many of which contain multiple objects either with low contrast or touching the image boundary;</p><p>? DUTS ) is currently the largest saliency detection benchmark dataset, which consists of 10,553 training images (DUTS-TR) and 5,019 testing images (DUTS-TE).</p><p>As with other works in salient object detection <ref type="bibr" target="#b25">(Qin et al. 2019;</ref><ref type="bibr" target="#b19">Liu et al. 2019)</ref>, we employ DUTS-TR as our training dataset and evaluate our model on other datasets.</p><p>Evaluation Metrics To quantitatively evaluate the effectiveness of our proposed model, we adopt precision-recall (PR) curves , F-measure (F ? ) score and curves, Mean Absolute Error (MAE), and structural similarity measure (S m ) as our performance measures. With different thresholds, pairs of precision and recall value can be computed by comparing the binarized map with the ground truth. Then, we can plot the precision-recall curve <ref type="bibr" target="#b4">(Cong et al. 2019)</ref>. The second metric F-measure score takes both precision and recall into account, which is defined as</p><formula xml:id="formula_11">F ? = (1+? 2 )?P recision?Recall ? 2 ?P recision+Recall</formula><p>where ? 2 is set to 0.3 to emphasize the precision over recall, as suggested in the previous work <ref type="bibr" target="#b3">(Cong et al. 2018b</ref>). Larger F-measure score indicates better performance. For precision-recall pairs, we calculated each corresponding Fmeasure score and choose the maximum as the evaluation score on the whole dataset. Another metric MAE is defined as the average pixel-wise absolute difference between the prediction map and the ground truth <ref type="bibr" target="#b1">(Cong et al. 2017)</ref>, i.e., M AE = 1 H?W H y=1 W x=1 |S(x, y) ? G(x, y)| where S denotes the predicted saliency map, G indicates the corresponding ground truth, and H, W are the height and width of the saliency map respectively. The smaller MAE indicates better performance. Since the F ? and MAE are based on pixel-wise errors and ignore the structural similarities, we adopt the structural similarity measure proposed by <ref type="bibr" target="#b8">(Fan et al. 2017</ref>) as one of our metrics. The structural similarity measure is defined as S m = ? * S o + (1 ? ?) * S r , where ? is set to 0.5 to balance the object-aware structural similarity (S o ) and region-aware structural similarity (S r ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compared with the State-of-the-arts</head><p>We compare the proposed model with 12 state-of-the-art methods, including Amulet <ref type="bibr" target="#b33">(Zhang et al. 2017)</ref>, C2S <ref type="bibr" target="#b17">(Li et al. 2018)</ref>, RANet <ref type="bibr" target="#b0">(Chen et al. 2018)</ref>, PAGR <ref type="bibr" target="#b35">(Zhang et al. 2018b</ref>), PiCANet-R <ref type="bibr" target="#b20">(Liu, Han, and Yang 2018)</ref>, DGRL , R 3 Net <ref type="bibr" target="#b6">(Deng et al. 2018)</ref>, BMPM <ref type="bibr" target="#b34">(Zhang et al. 2018a)</ref>, RADF , CPD-R <ref type="bibr" target="#b30">(Wu, Su, and Huang 2019)</ref>, BASNet <ref type="bibr" target="#b25">(Qin et al. 2019)</ref> and Pool-Net <ref type="bibr" target="#b19">(Liu et al. 2019)</ref>. For fair comparison, the saliency maps of different methods are provided by authors or obtained by running their released codes under the default parameters.  <ref type="bibr" target="#b33">(Zhang et al. 2017)</ref> 0  <ref type="table" target="#tab_0">Table 1</ref> shows the quantitative comparison results in terms of F-measure, S-measure, and MAE score. It's obvious that the proposed method achieves the best performance in terms of different measures, which demonstrates the effectiveness of the proposed model. In addition, as shown in <ref type="figure" target="#fig_3">Fig. 5</ref>, the PR curves and F-measure curves by our approach (the red curves) are outstanding in most cases compared with other previous methods under different thresholds, which is consistent with the measures reported in <ref type="table" target="#tab_0">Table 1</ref>.</p><formula xml:id="formula_12">Methods ECSSD HKU-IS PASCAL-S DUT-OMRON DUTS-TE SOD F ? ? S m ? MAE? F ? ? S m ? MAE? F ? ? S m ? MAE? F ? ? S m ? MAE? F ? ? S m ? MAE? F ? ? S m ? MAE? Amulet</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantitative Evaluation</head><p>Qualitative Evaluation To further illustrate the advantages of the proposed method, we provide some visual examples of different methods. As <ref type="figure">Fig. 6</ref> shows, our proposed method can handle various challenging scenarios, including fine-grained structures, cluttered background, foreground disturbance, objects concurrency, and multiple salient objects, etc. Compared with other previous methods, the saliency maps generated by our approach are more complete and accurate. Note that our approach is more robust to background/foreground disturbance (the second/third row) and can capture the relationship among multiple objects (the fifth row), which illustrates the power of the feature interweaved aggregation strategy and the introducing of global context information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>In this part, we conduct the ablation study to verify the effectiveness of each key components designed in the proposed model. The ablation experiments are conducted on the EC-SSD dataset and ResNet-50 is adopted as the backbone. As shown in <ref type="table" target="#tab_2">Table 3</ref>, the proposed model containing all components (i.e., FIA, SR, HA and GCF) achieves the best performance, which demonstrates the necessity of each component for the proposed model to obtain the best saliency detection results. We adopt the model like U-Net <ref type="bibr" target="#b26">(Ronneberger, Fischer, and Brox 2015)</ref> that only concatenates high-level features after up-sampling and low-level features as the baseline model, then add each module progressively. From <ref type="table" target="#tab_2">Table 3</ref>, the FIA module largely improves the baseline from 0.0456 to 0.0390 in terms of MAE. Furthermore, the MAE score is improved by 14% compared with the basic model after <ref type="figure">Figure 6</ref>: Qualitative comparison of the proposed model with other state-of-the-art methods. Obviously, saliency maps generated by our approach are more accurate and much close to the ground truth in various challenging scenarios.  adding the SR module. The combination of FIA and SR has already achieved well performance, while the addition of HA has a slight enhancement. Finally, we add the GCF to the model and obtain the best result. Moreover, we evaluate the effectiveness of the GCF module compared to another setting, in which the global context features are shared at all stages. From <ref type="table" target="#tab_1">Table 2</ref>, the proposed GCF module outperforms the shared one. The potential reason behind this phenomenon is that the parallel scheme of the GCF modules can provide distinct features for different stages, which benefits to learn the comprehensive and discriminative features for salient objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we propose a Global Context-Aware Progressive Aggregation Network (GCPANet) to achieve salient object detection. Considering different characteristics of different level features, we design a simple yet effective aggregation module to fully integrate different level features. We introduce global context information at different stages to capture the relationship among multiple salient objects or multiple regions of salient object and alleviate the dilution effect of features. Experimental results on six benchmark datasets demonstrate that the proposed network outperforms other 12 state-of-the-art methods under different evaluation metrics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Sample results of our method compared with others. (a) Image; (b) Ground truth; (c) GCPANet (Ours); (d) PiCANet-R</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The overall pipeline of the proposed network GCPANet. f t l , f t h , f t g (t = 1, 2, 3) denote the low-level detail, high-level semantic, and global context features, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of the FIA module, where symbol "c" denotes concatenation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Illustration of PR curves (the first row), F-measure curves (the second row) on four datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison with 12 state-of-the-art methods on 6 benchmark datasets. The best results on each dataset are highlighted in boldface.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>MAE Comparison of the GCF with the shared one.</figDesc><table><row><cell></cell><cell cols="5">ECSSD HKU-IS PASCAL-S DUT-OMRON DUTS-TE</cell><cell>SOD</cell></row><row><cell cols="2">with the Shared 0.0361</cell><cell>0.0313</cell><cell>0.0628</cell><cell>0.0590</cell><cell>0.0388</cell><cell>0.0915</cell></row><row><cell>with GCF</cell><cell>0.0348</cell><cell>0.0309</cell><cell>0.0614</cell><cell>0.0563</cell><cell>0.0380</cell><cell>0.0874</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablation study with different components combinations on ECSSD dataset.</figDesc><table><row><cell>Baseline FIA SR HA GCF M AE ?</cell></row><row><cell>0.0456</cell></row><row><cell>0.0390</cell></row><row><cell>0.0365</cell></row><row><cell>0.0364</cell></row><row><cell>0.0348</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Reverse attention for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="234" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An iterative co-saliency framework for rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TC</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="233" to="246" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Review of visual saliency detection with comprehensive information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>IEEE TCSVT</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Hscs: Hierarchical sparsity based co-saliency detection for rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>IEEE TMM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Video saliency detection via sparsity-based reconstruction and propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IEEE TIP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">R 3 Net: Recurrent residual refinement network for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="684" to="690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Structure-measure: A new way to evaluate foreground maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4548" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Database saliency for fast image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMM</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="359" to="369" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Online tracking by learning discriminative saliency map with convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="597" to="606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3203" to="3212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recurrently aggregating deep features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6943" to="6950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visual saliency based on multiscale deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5455" to="5463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep contrast learning for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="478" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The secrets of salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="280" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Contour knowledge transfer for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="355" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<title level="m">Network in network</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A simple pooling-based design for real-time salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3917" to="3926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">PiCANet: Learning pixel-wise contextual attention for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3089" to="3098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Non-local deep features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Achkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eichel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-M</forename><surname>Jodoin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6609" to="6617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Design and perceptual validation of performance measures for salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Movahedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Elder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">BASNet: Boundary-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jagersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7479" to="7489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Saliency detection with recurrent fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="825" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to detect salient objects with image-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="136" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Detect globally, refine locally: A novel approach to saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3127" to="3135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cascaded partial decoder for fast and accurate salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3907" to="3916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hierarchical saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1155" to="1162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Saliency detection via graph-based manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3166" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Amulet: Aggregating multi-level convolutional features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="202" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A bi-directional message passing model for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1741" to="1750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Progressive attention guided recurrent network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="714" to="722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Saliency-guided unsupervised feature learning for scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TGRS</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2175" to="2184" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Saliency detection by multi-context deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1265" to="1274" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
