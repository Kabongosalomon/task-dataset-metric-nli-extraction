<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Twin Contrastive Learning for Online Clustering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfan</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Mouxing</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><forename type="middle">?</forename><surname>Dezhong Peng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taihao</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><forename type="middle">Jiantao</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
							<email>pengdz@scu.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><forename type="middle">Y</forename><surname>Peng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
							<email>jthuang@zhejianglab.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Zhejiang Lab. Hangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Twin Contrastive Learning for Online Clustering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Received: date / Accepted: date</note>
					<note>Noname manuscript No. (will be inserted by the editor) Corresponding author:</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Deep Clustering ? Online Clustering ? Unsupervised Learning ? Contrastive Learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes to perform online clustering by conducting twin contrastive learning (TCL) at the instance and cluster level. Specifically, we find that when the data is projected into a feature space with a dimensionality of the target cluster number, the rows and columns of its feature matrix correspond to the instance and cluster representation, respectively. Based on the observation, for a given dataset, the proposed TCL first constructs positive and negative pairs through data augmentations. Thereafter, in the row and column space of the feature matrix, instance-and cluster-level contrastive learning are respectively conducted by pulling together positive pairs while pushing apart the negatives. To alleviate the influence of intrinsic false-negative pairs and rectify cluster assignments, we adopt a confidence-based criterion to select pseudolabels for boosting both the instance-and cluster-level contrastive learning. As a result, the clustering performance is further improved. Besides the elegant idea of twin contrastive learning, another advantage of TCL is that it could independently predict the cluster assignment for each instance, thus effortlessly fitting online scenarios. Extensive experiments on six widely-used image and text benchmarks demonstrate the effectiveness of TCL. The code will be released on GitHub.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig. 1</ref> <p>Our key observation and basic idea. By projecting data into a feature space with the dimensionality of cluster number, the element in the i-th row and k-th column of the feature matrix represents the probability of instance i belonging to cluster k. Namely, rows correspond to the cluster assignment probabilities, which are special representations of instances. More interestingly, if we look at the feature matrix from the column view, each column actually corresponds to the cluster distribution over the data, which could be seen as a special representation of the cluster. As a result, the instance-and cluster-level representation learning (e.g., contrastive learning) could be conducted in the row and column space, respectively.</p><p>into different clusters without label information, such that the within-cluster data come from the same class or share similar semantics. Besides facilitating general representation learning <ref type="bibr" target="#b4">(Caron et al., 2018</ref><ref type="bibr" target="#b5">(Caron et al., , 2020</ref>, clustering is also helpful in a variety of real-world applications, such as face recognition <ref type="bibr" target="#b65">(Shen et al., 2021)</ref>, medical analysis <ref type="bibr" target="#b69">(Thanh et al., 2017)</ref>, and gene sequencing <ref type="bibr" target="#b39">(Kiselev et al., 2019)</ref>.</p><p>During past years, most clustering methods <ref type="bibr" target="#b9">(Chen and Lerman, 2009;</ref><ref type="bibr" target="#b46">Liu et al., 2017</ref><ref type="bibr" target="#b47">Liu et al., , 2016</ref><ref type="bibr" target="#b50">Nie et al., 2011</ref><ref type="bibr" target="#b51">Nie et al., , 2019</ref><ref type="bibr" target="#b68">Tang et al., 2019;</ref><ref type="bibr" target="#b87">Zhang et al., 2012)</ref> mainly focus on developing different similarity metrics and clustering strategies. Though grounded in theory, their performance is limited by the adopted shallow models. <ref type="bibr">Recently, deep clustering (Ghasedi Dizaji et al., 2017;</ref><ref type="bibr" target="#b44">Li et al., 2020)</ref> has shown promising results on various benchmarks by extracting representative features to facilitate downstream clustering. Early deep clustering methods <ref type="bibr" target="#b4">(Caron et al., 2018;</ref><ref type="bibr" target="#b58">Peng et al., 2016;</ref><ref type="bibr" target="#b77">Xie et al., 2016;</ref><ref type="bibr" target="#b80">Yang et al., 2016)</ref> iteratively perform representation learning and clustering to bootstrap each other. However, this kind of method usually needs the entire dataset to perform offline clustering, which is less attractive for large-scale data and even impractical for streaming data. Luckily, the offline limitation could be solved by the idea of "label as representation" <ref type="bibr" target="#b57">(Peng et al., 2015</ref><ref type="bibr" target="#b60">(Peng et al., , 2019</ref>. By directly and independently predicting cluster assignment for each instance, large-scale and online clustering could be achieved <ref type="bibr" target="#b30">(Hu et al., 2017;</ref><ref type="bibr" target="#b31">Huang et al., 2020;</ref><ref type="bibr" target="#b33">Ji et al., 2019)</ref>. Very recently, the rapid growth of contrastive learning <ref type="bibr" target="#b10">(Chen et al., 2020a;</ref><ref type="bibr" target="#b82">Zbontar et al., 2021)</ref> significantly improves the performance of unsupervised representation learning. Motivated by their successes, some contrastive learning based clustering methods <ref type="bibr" target="#b86">Li et al., 2021b;</ref><ref type="bibr" target="#b52">Niu and Wang, 2021;</ref><ref type="bibr" target="#b70">Van Gansbeke et al., 2020)</ref> are proposed, which achieve state-of-the-art results.</p><p>In this work, we propose an end-to-end online deep clustering method by conducting twin contrastive learning (TCL) based on the observation shown in <ref type="figure">Fig. 1</ref>. In brief, the rows and columns of the feature matrix correspond to the instance and cluster representations, respectively. Under this observation, TCL conducts contrastive learning in the row and column space of the feature matrix to jointly learn the instance and cluster representation. Specifically, TCL first constructs contrastive pairs through data augmentations. Different from most existing contrastive learning methods that use weak augmentations proposed in SimCLR <ref type="bibr" target="#b10">(Chen et al., 2020a)</ref>, we provide a new effective augmentation strategy by mixing weak and strong transformations. With the constructed pairs, TCL performs contrastive learning at both the instance and cluster level. The instance-level contrastive learning aims to pull withinclass instances together while pushing between-class instances apart. And the cluster-level contrastive learning aims to distinguish distributions of different clusters while attracting distributions of the same cluster under different augmentations. To relieve the influence of intrinsic false-negative pairs and rectify cluster assignments, we progressively select the confident predictions (i.e., those with cluster assignment probability close to one-hot) to fine-tune the twin contrastive learning. Such a fine-tuning strategy is based on the observation that the predictions with high confidence are more likely to be correct and thus could be used as pseudo labels. Once the model converges, it could independently make cluster assignments for each instance in an end-to-end manner to achieve clustering. The major contributions of this work are summarized as follows:</p><p>-We reveal that the rows and columns of the feature matrix intrinsically correspond to the instance and cluster representations. On top of that, we propose TCL that achieves clustering by simultaneously conducting contrastive learning at the instance and cluster level; -We provide a new data augmentation strategy by mixing weak and strong transformations, which naturally fits our TCL framework and is proved to be effective for both image and text data in our experiments; -To alleviate the influence of intrinsic false negatives and rectify cluster assignments, we adopt a confidence-based criterion to generate pseudo-labels for fine-tuning both the instance-and cluster-level contrastive learning. Experiments show that such a fine-tuning strategy could further boost the clustering performance; -The proposed TCL clusters data in an end-to-end and online manner, which only needs batch-wise optimization and thus could handle large-scale datasets. Moreover, TCL could handle streaming data since it could timely make cluster assignments for new coming data without accessing the whole dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we give a brief review on contrastive learning and deep clustering, followed by a discussion on the connection between these two topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Contrastive Learning</head><p>Recently, the contrastive learning paradigm shows its power in unsupervised representation learning <ref type="bibr" target="#b5">(Caron et al., 2020;</ref><ref type="bibr" target="#b10">Chen et al., 2020a;</ref><ref type="bibr" target="#b12">Chen and He, 2020;</ref><ref type="bibr" target="#b21">Grill et al., 2020;</ref><ref type="bibr" target="#b28">He et al., 2020;</ref><ref type="bibr" target="#b29">Hu et al., 2020;</ref><ref type="bibr" target="#b82">Zbontar et al., 2021)</ref>. It first constructs positive and negative pairs for each instance and then projects them into a subspace to maximize the similarities of positive pairs and minimize those of the negatives <ref type="bibr" target="#b23">(Hadsell et al., 2006)</ref>. The most straightforward solution is to use labels to guide the pair construction <ref type="bibr" target="#b35">(Khosla et al., 2020)</ref>. However, in the unsupervised setting, other strategies are needed to construct and utilize contrastive pairs. For example, SimCLR <ref type="bibr" target="#b10">(Chen et al., 2020a)</ref> constructs positive and negative pairs through augmentations within mini-batch. MoCo  recasts contrastive learning as a dictionary look-up task by building a dynamic dictionary with a queue and a moving-averaged encoder. To avoid the efforts in building negative pairs, BYOL <ref type="bibr" target="#b21">(Grill et al., 2020)</ref> and SimSiam <ref type="bibr" target="#b12">(Chen and He, 2020)</ref> replace negative pairs with an online predictor that prevents the network from collapsing into trivial solutions. As an alternative, AdCo <ref type="bibr" target="#b29">(Hu et al., 2020)</ref> directly learns negative samples in an adversarial manner. Lately, Barlow Twins <ref type="bibr" target="#b82">(Zbontar et al., 2021)</ref> performs contrastive learning from a redundancy-reduction perspective and achieves comparable results. There are two major differences between our work and these contrastive learning methods. First, our method concurrently conducts row-and column-wise contrastive learning at both the instance and cluster level while most existing methods solely perform row-wise contrastive learning at the instance level. Such an elegant idea is based on our observation that rows and columns of the feature matrix correspond to the instance and cluster representations respectively. Second, the aforementioned methods adopt the weak augmentation strategy proposed in SimCLR <ref type="bibr" target="#b10">(Chen et al., 2020a)</ref> because strong augmentations (RandAugment <ref type="bibr" target="#b15">(Cubuk et al., 2020)</ref> to be specific) experimentally show inferior performance <ref type="bibr" target="#b73">(Wang and Qi, 2021)</ref>. Though there are some works (Van Gansbeke et al., 2020) that use strong augmentations to fine-tune the network, it is still unclear how to directly facilitate contrastive learning with strong augmentations. From this perspective, this work could shed some light on how to effectively utilize weak and strong transformations by using the proposed TCL framework (more details in <ref type="table" target="#tab_6">Table 6</ref>). The proposed augmentation strategy is suitable for various types of data, such as images and texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep Clustering</head><p>Both effective clustering strategies and discriminative features are essential in achieving good clustering. Thanks to the powerful representability of deep neural networks, deep clustering methods have attracted more and more attention in recent <ref type="bibr" target="#b0">(Asano et al., 2019;</ref><ref type="bibr" target="#b4">Caron et al., 2018;</ref><ref type="bibr" target="#b22">Guo et al., 2017;</ref><ref type="bibr" target="#b42">Li et al., 2021a;</ref><ref type="bibr" target="#b44">Li et al., 2020;</ref><ref type="bibr" target="#b58">Peng et al., 2016</ref><ref type="bibr" target="#b59">Peng et al., , 2018</ref><ref type="bibr" target="#b61">Peng et al., , 2022</ref><ref type="bibr" target="#b77">Xie et al., 2016;</ref><ref type="bibr" target="#b80">Yang et al., 2016)</ref>. To name a few, JULE <ref type="bibr" target="#b80">(Yang et al., 2016)</ref> iteratively learns data representation and performs hierarchical clustering. DeepCluster <ref type="bibr" target="#b4">(Caron et al., 2018)</ref> clusters data using the prior representation and uses the cluster assignment of each sample as a classification target to learn the new representation. Though representation learning and clustering could bootstrap each other to some extent, this kind of two-stage method might suffer from errors accumulated during alternations. Another weakness of these methods is that they could not be applied in the online scenario, where data is presented in streams and only a batch of samples are accessible at one time. Specifically, JULE needs the global similarity to decide which sub-clusters should be merged, while DeepCluster and SL <ref type="bibr" target="#b0">(Asano et al., 2019)</ref> need to perform offline k-means or solve a global optimal transport problem to acquire cluster assignments. To overcome the offline limitation, some online deep clustering methods are proposed <ref type="bibr" target="#b16">(Dang et al., 2021;</ref><ref type="bibr" target="#b31">Huang et al., 2020;</ref><ref type="bibr" target="#b33">Ji et al., 2019;</ref><ref type="bibr" target="#b86">Li et al., 2021b;</ref><ref type="bibr" target="#b88">Zhong et al., 2020)</ref>. For example, IIC <ref type="bibr" target="#b33">(Ji et al., 2019)</ref> discovers clusters by maximizing mutual information between the cluster assignments of data pairs. PICA <ref type="bibr" target="#b31">(Huang et al., 2020)</ref> learns the most semantically plausible data separation by maximizing the partition confidence of the clustering solution. Very recently, some studies <ref type="bibr" target="#b52">(Niu and Wang, 2021;</ref><ref type="bibr" target="#b54">Park et al., 2020;</ref><ref type="bibr" target="#b70">Van Gansbeke et al., 2020)</ref> use pseudo-labels generated by preliminary clustering, namely, self-labeling, to further improve the clustering performance in a multi-stage manner.</p><p>Unlike most of the above works that perform representation learning and clustering in multiple stages, our method unifies these two tasks into the twin contrastive learning framework. Such a one-stage learning paradigm helps the model to learn more clusteringfavorable representations compared with previous works that solely conduct instance-level contrastive learning <ref type="bibr" target="#b52">(Niu and Wang, 2021;</ref><ref type="bibr" target="#b70">Van Gansbeke et al., 2020)</ref>. In the boosting stage, despite rectifying the cluster assignments based on the features extracted in the early stage <ref type="bibr" target="#b52">(Niu and Wang, 2021)</ref>, we could also fine-tune the instancelevel contrastive learning to alleviate the influence of false-negative pairs thanks to our one-stage learning paradigm. <ref type="figure">Fig. 2</ref> The pipeline of Twin Contrastive Learning (TCL). First, it constructs data pairs through weak and strong augmentations. A shared backbone is used to extract features from augmented samples. Then, two independent MLPs (? denotes the ReLU activation and ? denotes the Softmax operation to produce soft labels) project the features into the row and column space wherein the instance-and cluster-level contrastive learning are conducted, respectively. Finally, pseudo labels are selected based on the confidence of cluster predictions to alleviate the influence of false-negative pairs and rectify previous predictions, which further boosts the clustering performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Connection between contrastive learning and deep clustering</head><p>Both representation learning and deep clustering share a common goal, namely, extracting discriminative features. Recently, a variety of works have shown that contrastive learning and deep clustering could bootstrap each other. On the one hand, the performance of representation learning could be enhanced by integrating the clustering property . For example, instead of constructing pairs between samples, SwAV <ref type="bibr" target="#b5">(Caron et al., 2020)</ref> first performs clustering by solving an optimal transport problem and then contrasts the instances and the learned cluster centers. Similarly, PCL  pulls each sample to its corresponding cluster center with a prototypical contrastive loss. In addition, <ref type="bibr" target="#b71">Van Gansbeke et al. (2021)</ref> and <ref type="bibr" target="#b18">Dwibedi et al. (2021)</ref> show that leveraging the local neighborhood can be effective for contrastive learning. On the other hand, recent deep clustering methods such as SCAN (Van Gansbeke et al., 2020), CC <ref type="bibr" target="#b86">(Li et al., 2021b)</ref>, and SPICE <ref type="bibr" target="#b52">(Niu and Wang, 2021)</ref> have achieved state-of-the-art performance thanks to the contrastive learning paradigm.</p><p>Though the combination of contrastive learning and deep clustering has brought promising results, most existing works still treat these two tasks separately. Different from existing works, this study elegantly unifies contrastive learning and deep clustering into the twin contrastive learning framework, which might bring some insights to both communities. Notably, this study is a significant extension of <ref type="bibr" target="#b86">(Li et al., 2021b)</ref> with the following improvements:</p><p>-In this paper, we propose a confidence-based boosting strategy to fine-tune both the instance-and clusterlevel contrastive learning. Specifically, most confident predictions are selected as pseudo labels based on the observation that they are more likely to be correct. Upon that, we use the generated pseudo labels to alleviate the influence of false-negative pairs (composed of within-class samples) in instance-level contrastive learning, and adopt cross-entropy loss to rectify cluster assignments in cluster-level contrastive learning. Notably, such a twin self-training paradigm is benefited from our TCL framework since the cluster assignments (from CCH) of instance features (from ICH) could be obtained in an online manner. -In this paper, we propose a data augmentation strategy by mixing weak and strong transformations. Though such an augmentation strategy is seemingly simple, its effectiveness is closely correlated with the proposed TCL framework. Previous works have shown that directly introducing strong augmentation into the contrastive learning framework could lead to sub-optimal performance <ref type="bibr" target="#b73">(Wang and Qi, 2021)</ref>. Different from such a conclusion, we show that the mixed augmentation strategy naturally fits the proposed TCL framework (see <ref type="table" target="#tab_6">Table 6</ref> for more details). -To investigate the generalization ability of the proposed method, we verify the effectiveness of our method in text clustering despite the difference in data augmentation. Experimental results demonstrate the superiority of the proposed TCL framework, mixed augmentation strategy, and confidence-based boosting strategy. A comparable performance gain is achieved by this journal extension compared with the previous conference paper <ref type="bibr" target="#b86">(Li et al., 2021b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>The pipeline of the proposed TCL is illustrated in <ref type="figure">Figure 2</ref>. The model consists of three parts, namely, the contrastive pair construction (CPC), the instance-level contrastive head (ICH), and the cluster-level contrastive head (CCH), which are jointly optimized through twin contrastive learning and confidence-based boosting. Specifically, in the twin contrastive learning stage, CPC first constructs contrastive pairs through data augmentations and then projects the pairs into a latent feature space. After that, ICH and CCH conduct instance-and cluster-level contrastive learning at the row and column space of the feature matrix respectively by minimizing the proposed twin contrastive loss. To alleviate the influence of intrinsic false-negative pairs in contrastive learning and to rectify cluster assignments, we propose a confidence-based boosting strategy (CB). In detail, some confident predictions are selected as pseudo labels to fine-tune the instance-and cluster-level contrastive learning with the self-supervised contrastive loss and self-labeling loss, which further improves the clustering performance.</p><p>Once the model converges, CCH could make cluster assignments for each instance to achieve online clustering. Notably, though the twin contrastive learning can be directly conducted on the same contrastive head as indicated in our basic idea, we experimentally find that decoupling it into two independent subspaces improves the clustering performance (see Section 4.6.4 for detailed discussion).</p><p>In this section, we first introduce the construction of contrastive pairs in CPC, then present the twin contrastive loss for training, and finally elaborate on our confidence-based boosting strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Contrastive Pair Construction</head><p>Inspired by recent developments of contrastive learning <ref type="bibr" target="#b5">(Caron et al., 2020;</ref><ref type="bibr" target="#b10">Chen et al., 2020a)</ref>, the proposed TCL constructs contrastive pairs through data augmentation. Specifically, for each instance x i , CPC stochastically samples and applies two groups of transformations t and t from two augmentations families T and T respectively, resulting in two correlated samples (i.e., a data pair) denoted asx 2i?1 = t(x i ) and</p><formula xml:id="formula_0">x 2i = t (x i ).</formula><p>Recent studies suggest that data augmentation is essential in contrastive learning methods <ref type="bibr" target="#b5">(Caron et al., 2020;</ref><ref type="bibr" target="#b10">Chen et al., 2020a)</ref>, and most of them adopt weak augmentations <ref type="bibr" target="#b10">(Chen et al., 2020a;</ref><ref type="bibr" target="#b21">Grill et al., 2020;</ref><ref type="bibr" target="#b82">Zbontar et al., 2021)</ref> since directly using strong augmentations experimentally leads to inferior performance <ref type="bibr" target="#b73">(Wang and Qi, 2021)</ref>. In this work, we provide a novel data augmentation strategy by mixing weak and strong transformations, which achieves superior performance on both image and text data. To be specific, for image data, we adopt the transformations proposed by SimCLR <ref type="bibr" target="#b10">(Chen et al., 2020a)</ref> and RandAugment <ref type="bibr" target="#b15">(Cubuk et al., 2020)</ref> as the weak T and strong T augmentation, respectively. For text data, we employ the synonym replacement strategy <ref type="bibr" target="#b85">(Zhang et al., 2021a)</ref> as the weak augmentation T and use the sentence operations <ref type="bibr" target="#b75">(Wei and Zou, 2019)</ref> as the strong augmentation T .</p><p>Given the constructed pairs, a shared backbone f (?) is used to extract features h from the augmented samples through h 2i?1 = f (x 2i?1 ) and h 2i = f (x 2i ). Specific backbones are used to handle different types of data. In this work, we adopt ResNet <ref type="bibr" target="#b27">(He et al., 2016)</ref> and Sentence Transformer <ref type="bibr" target="#b64">(Reimers and Gurevych, 2019)</ref> as the backbone for image and text data, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Twin Contrastive Learning</head><p>In the training stage, the backbone, the instance-level contrastive head (ICH), and the cluster-level contrastive head (CCH) are jointly optimized according to the following twin contrastive loss, i.e.,</p><formula xml:id="formula_1">L train = L ins + L clu ,<label>(1)</label></formula><p>where L ins is the instance-level contrastive loss which is computed on ICH and L clu denotes the cluster-level contrastive loss computed on CCH.</p><p>In general, one may add a dynamic weight parameter to balance the two losses across the training process, but explicitly tuning the weight could violate the unsupervised constraint. In practice, we find a simple addition of the two contrastive losses already works well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Instance-level contrastive loss</head><p>The instance-level contrastive learning aims to maximize the similarities of positive pairs while minimizing those of negative ones. To achieve clustering, ideally, one could define pairs of within-class instances to be positive and those of between-class instances to be negative. However, since no prior label information is given, we construct instance pairs based on data augmentations as a compromise. To be specific, the positive pairs consist of samples augmented from the same instance, and the negative pairs otherwise.</p><p>Formally, for a mini-batch of size N , TCL performs two types of data augmentations on each instance x i , resulting in 2N augmented samples {x 1 ,x 2 , . . . ,x 2i?1 ,x 2i , . . . ,x 2N }. Each samplex 2i?1 forms 2N ? 1 pairs with others, among which we choose the pair with its corresponding augmented sample {x 2i?1 ,x 2i } to be positive and define other 2N ? 2 pairs to be negative.</p><p>As directly conducting contrastive learning on the feature matrix may cause information loss <ref type="bibr" target="#b10">(Chen et al., 2020a)</ref>, we stack a two-layer nonlinear MLP g I (?) to map the features into a subspace via z i = g I (h i ), where the instance-level contrastive learning is applied. The pair-wise similarity is measured using cosine distance, namely,</p><formula xml:id="formula_2">s(z i , z j ) = z i z j z i z j , i, j ? [1, 2N ].<label>(2)</label></formula><p>The InfoNCE loss <ref type="bibr" target="#b53">(Oord et al., 2018</ref>) is adopted to optimize pair-wise similarities defined by Eq. 2. Without loss of generality, the loss for a given augmented samplex i (suppose it forms a positive pair withx j ) is defined as</p><formula xml:id="formula_3">i = ? log exp(s(z i , z j )/? I ) 2N k=1 1 [k =i] exp (s(z i , z k )/? I ) ,<label>(3)</label></formula><p>where ? I is the instance-level temperature parameter to control the softness, and 1 [k =i] is an indicator function evaluating to 1 iff k = i. To identify the positive counterpart for each augmented sample, the instancelevel contrastive loss is computed across all augmented samples, i.e.,</p><formula xml:id="formula_4">L ins = 1 2N 2N k=1 k .<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Cluster-level contrastive loss</head><p>When a sample is projected into a subspace whose dimensionality equals the cluster number, the i-th element of its feature represents its probability of belonging to the i-th cluster. In other words, the feature vector corresponds to its cluster assignment probability. Suppose the target cluster number is M , similar to the instance-level contrastive head, we use another twolayer MLP g C (?) to project the features into an Mdimensional space via y i = g C (h i ). Here y i corresponds to the cluster assignment probability of the augmented samplex i . Formally, let Y = [y 1 , . . . , y 2i?1 , . . . , y 2N ?1 ] ? R N ?M be the cluster assignment probabilities of the mini-batch under the weak augmentation T (and Y = [y 2 , . . . , y 2i , . . . , y 2N ] for those under the strong augmentation T ). Based on the observation shown in <ref type="figure">Fig. 1</ref>, the columns of Y and Y correspond to the cluster distributions over the mini-batch and could be interpreted as special cluster representations. We would like to point out that this observation still holds even when the dimension is larger than the ground-truth cluster number. In that case, a more fine-grained cluster structure is considered and its effectiveness is verified in Barlow Twins <ref type="bibr" target="#b82">(Zbontar et al., 2021)</ref>.</p><p>For clarity, we denote the i-the column of Y as? 2i?1 (and? 2i for the i-the column of Y ), namely, the representation of cluster i under the weak (and strong) data augmentation. The representations of the same cluster under two augmentations form positive cluster pairs</p><formula xml:id="formula_5">{? 2i?1 ,? 2i }, i ? [1, M ]</formula><p>, while other pairs are defined to be negative. Again, we use cosine distance to measure the similarity between cluster? i and? j , that is</p><formula xml:id="formula_6">s(? i ,? j ) =? i? j ? i ? j , i, j ? [1, 2M ]<label>(5)</label></formula><p>Without loss of generality, the following loss function is adopted to identify cluster? i from all other 2M ? 2 clusters except its counter part? j , i.e.,</p><formula xml:id="formula_7">i = ? log exp(s(? i ,? j )/? C ) 2M k=1 1 [k =i] exp (s(? i ,? k )/? C ) ,<label>(6)</label></formula><p>where ? C is the cluster-level temperature parameter to control the softness, and 1 [k =i] is an indicator function evaluating to 1 iff k = i. By traversing all clusters, the cluster-level contrastive loss is computed through</p><formula xml:id="formula_8">L clu = 1 2M M k=1? k .<label>(7)</label></formula><p>As simply optimizing the above cluster-level contrastive loss might lead to trivial solution where most samples are assigned to a few clusters, we add a cluster entropy to prevent the model from collapsing and achieve more balanced clustering <ref type="bibr" target="#b19">(Ghasedi Dizaji et al., 2017;</ref><ref type="bibr" target="#b31">Huang et al., 2020)</ref>. Formally, let P (? 2i?1 ) = 1 N N k=1 Y ki be the assignment probability of cluster i within a mini-batch under the weak augmentation and P (? 2i ) = 1 N N k=1 Y ki be that under the strong augmentation, then the cluster entropy is computed by</p><formula xml:id="formula_9">H clu = ? 2M i=1 [P (? i ) log P (? i )].<label>(8)</label></formula><p>To sum up, the cluster-level contrastive loss is finally defined as </p><formula xml:id="formula_10">L clu = 1 2M 2M k=1? k ? H clu .<label>(9</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Confidence-based Boosting</head><p>As the train progresses, we notice that the model tends to make more confident predictions (i.e., with cluster assignment probability close to one-hot). Those confident predictions are more likely to be correct (see <ref type="figure">Fig. 4</ref>). Based on this observation, in the boosting stage, we progressively select the most confident predictions as pseudo labels to fine-tune both the instance-and cluster-level contrastive learning. The pseudo labels are selected by the following criterion. Namely, for a minibatch of size N , we use the raw data x as input to compute prediction pred with confidence conf for each instance by</p><formula xml:id="formula_11">y i = g C (f (x i )), conf i = max(y i ), pred i = arg max(y i ).<label>(10)</label></formula><p>In every mini-batch, we select the top ? confident predictions from each cluster as pseudo labels, where ? is the confident ratio and we fix it to 0.5. To be specific, a prediction pred i will be selected as pseudo label if it meets the following criteria, i.e.,</p><formula xml:id="formula_12">n = ? ? N/M, CONF k = sort({conf i | i ? [1, N ], pred i = k})[n], conf i ? CONF pred i ,<label>(11)</label></formula><p>where CONF k is the n-th largest confidence of predictions on cluster k ? [0, M ? 1]. Notably, selecting most confident predictions from each cluster leads to more class-balanced pseudo labels compared with thresholdbased criterion <ref type="bibr" target="#b52">(Niu and Wang, 2021;</ref><ref type="bibr" target="#b54">Park et al., 2020;</ref><ref type="bibr" target="#b70">Van Gansbeke et al., 2020)</ref>. We store the pseudo labels for all instances, denoted as P , in the memory.</p><p>With the generated pseudo labels, we fine-tune the model with the following loss to further boost the clustering performance, namely,</p><formula xml:id="formula_13">L boost = L scl + L sl ,<label>(12)</label></formula><p>where L scl is the self-supervised contrastive loss used to alleviate the influence of false negative pairs in instancelevel contrastive learning and L sl is the self-labeling loss for rectifying cluster assignments made by CCH.</p><p>Recall that in the instance-level contrastive learning, we treat pairs of samples augmented from different instances to be negative, since no label information is given. However, for downstream tasks such as classification and clustering, within-class samples should not be pushed apart. To this end, with the help of pseudo labels, we remove within-class samples from negatives pairs (i.e., the denominator in Eq. 4) and adopt a selfsupervised contrastive loss to fine-tune the instancelevel contrastive learning. Specifically, for each augmented sample x i with pseudo label pred i , the self-supervised contrastive loss is defined as</p><formula xml:id="formula_14">l i = ? log exp (s(z i , z j )/? I ) 2N k=1 1 [pred k =pred i ] ? exp (s(z i , z k )/? I ) ,<label>(13)</label></formula><p>where 1 is the indicator. Notably, inspired by the negative learning paradigm <ref type="bibr" target="#b36">(Kim et al., 2019)</ref>, here we only remove potential within-class pairs from negative ones without treating them as positive, considering that the latter strategy could be too strong when the pseudo labels are of inferior quality. By traversing all augmented samples, the self-supervised instance-level contrastive loss is computed through</p><formula xml:id="formula_15">L scl = 1 2N 2N i=1 l i .<label>(14)</label></formula><p>For the cluster-level contrastive head, the self-labeling strategy is adopted to rectify previous predictions. Specifically, we define the self-labeling loss as the weighted cross-entropy on the strongly augmented samples x = t (x), i.e.,</p><formula xml:id="formula_16">y i = g C (f (x i )), L sl = ? 1 N p N i=1,i?P w pred i log exp(y i [pred i ]) M ?1 k=0 exp(y i [k]) ,<label>(15)</label></formula><p>where N p denotes the number of instances that have pseudo-labels in the mini-batch, and w c ? 1/N c is the weight parameter for cluster c of size N c . The weighted loss could prevent large clusters from dominating the optimization.</p><p>Though the confident ratio is fixed to ? = 0.5, it does not mean that at most 50% of predictions will be selected as pseudo labels. As the boosting progresses and batch shuffles, more pseudo labels would be selected progressively. Furthermore, considering that the model might make some mistakes in selecting pseudo labels, we remove the pseudo labels from P once their confidences decrease to below a certain threshold, namely,</p><formula xml:id="formula_17">conf i &lt; ?,<label>(16)</label></formula><p>where ? is the lower confidence bound of pseudo labels and is set to 0.99. This weeding out mechanism keeps the high quality of pseudo labels and gives the model a chance of rectifying previous predictions. The choice of ? and ? is fixed across all the experiments, and we provide parameter analysis on them in Section 4.6.2. The overall training, boosting, and test process of the proposed TCL is summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, the clustering performance of the proposed TCL is evaluated on five image and two text datasets. A series of qualitative analyses and ablation studies are carried out to help comprehensively and intuitively understand the method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>A brief description of the used datasets is summarized in <ref type="table" target="#tab_1">Table 1</ref>. More specifically, the image datasets include CIFAR-10, CIFAR-100 <ref type="bibr" target="#b40">(Krizhevsky and Hinton, 2009</ref>), STL-10 (Coates et al., 2011), ImageNet-10, and ImageNet-Dogs <ref type="bibr" target="#b6">(Chang et al., 2017a)</ref>. For CIFAR-100, its 20 super-classes rather than 100 fine-grained classes are taken as the ground truth. The text datasets include StackOverflow <ref type="bibr" target="#b78">(Xu et al., 2017a)</ref> and Biomedical <ref type="bibr" target="#b78">(Xu et al., 2017a)</ref>. The former is a subset of the challenge data published by Kaggle, and the latter is a subset of the PubMed data distributed by BioASQ. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Settings</head><p>Different backbones could be used to handle different types of data, including but not limited to images and texts. For a fair comparison with previous works <ref type="bibr" target="#b31">(Huang et al., 2020;</ref><ref type="bibr" target="#b33">Ji et al., 2019;</ref><ref type="bibr" target="#b86">Zhang et al., 2021b)</ref>, we adopt ResNet34 <ref type="bibr" target="#b27">(He et al., 2016)</ref> as the backbone for images and the distilbert-base-nli-stsb-mean-tokens model from the Sentence Transformer library <ref type="bibr" target="#b64">(Reimers and Gurevych, 2019)</ref> for texts. For ResNet34, its output dimension of the fully-connected classification layer (i.e., the dimension of h) is set to 512. For STL-10, its 100,000 unlabeled images are additionally used to compute the instance-level contrastive loss in the training stage. The ResNet34 is randomly initialized, while the Sentence Transformer is pre-trained to produce meaningful word embeddings to keep consistent with previous works <ref type="bibr" target="#b78">(Xu et al., 2017a;</ref><ref type="bibr" target="#b86">Zhang et al., 2021b)</ref>. For simplicity, instead of customizing the network to handle images of different sizes, we simply resize all images to 224 ? 224, and no additional modification is made on the standard ResNet34.</p><p>For images, we adopt data transformations proposed in SimCLR <ref type="bibr" target="#b10">(Chen et al., 2020a)</ref> as weak augmentation, including ResizedCrop, ColorJitter, Grayscale, Horizon-talFlip, and GaussianBlur. Notably, as small images already become blurred after up-scaling, we leave the GaussianBlur augmentation out for CIFAR-10/100 in the weak augmentation T . The strong augmentation T is composed of four randomly selected transformations from RandAugment <ref type="bibr" target="#b15">(Cubuk et al., 2020)</ref> with parameters listed in <ref type="table" target="#tab_2">Table 2</ref>, followed by one Cutout (DeVries and Taylor, 2017) operation with a size of 75 ? 75.</p><p>For text data, we randomly substitute 20% words of each text with their top-n suitable words found by the pre-trained Roberta from the Contextual Augmenter Library (Ma, 2019) as weak augmentation, following the setting in SCCL <ref type="bibr" target="#b85">(Zhang et al., 2021a)</ref>. The four operations proposed by EDA <ref type="bibr" target="#b75">(Wei and Zou, 2019)</ref> with a probability of 0.2 each are adopted as strong augmentations, including SynonymReplacement, RandomInsertion, RandomSwap, and RandomDeletion. For the two contrastive heads, the dimensionality of ICH is set as 128 to keep more discriminative information (see ablation study in Section 4.6.4), and the dimensionality of CCH is naturally set to the target cluster number. The temperature parameters are empirically set as ? I = 0.5, ? C = 1.0 for all datasets. The Adam optimizer with an initial learning rate of 1e?4 and a weight decay of 1e?4 is adopted to jointly optimize the two contrastive heads and the backbone network on image datasets. Since the backbone is pretrained for text data, we set the learning rate of the optimizer as 5e?6 for the backbone and 5e?4 for two contrastive heads. The model is trained for 1000/500 epochs, followed by 200/100 boosting epochs for the image/text dataset with a batch size of 256. Experiments are carried out on Nvidia TITAN RTX 24G and RTX 3090 on the Ubuntu 18.04 platform with CUDA 11.0 and PyTorch 1.8.0 <ref type="bibr" target="#b55">(Paszke et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Compared Methods</head><p>The proposed TCL is evaluated on five image datasets and two text datasets. For image clustering, we take 21 representative state-of-the-art approaches for comparisons, including k-means <ref type="bibr">(MacQueen et al., 1967), SC (Zelnik-Manor and</ref><ref type="bibr" target="#b84">Perona, 2005)</ref>, AC <ref type="bibr" target="#b20">(Gowda and Krishna, 1978)</ref>, NMF <ref type="bibr" target="#b3">(Cai et al., 2009)</ref>, AE <ref type="bibr" target="#b1">(Bengio et al., 2007)</ref>, DAE <ref type="bibr" target="#b72">(Vincent et al., 2010)</ref>, <ref type="bibr">DCGAN (Radford et al., 2015)</ref>, DeCNN <ref type="bibr" target="#b83">(Zeiler et al., 2010)</ref>, VAE <ref type="bibr" target="#b37">(Kingma and Welling, 2013)</ref>, JULE <ref type="bibr" target="#b80">(Yang et al., 2016)</ref>, DEC <ref type="bibr" target="#b77">(Xie et al., 2016)</ref>, DAC <ref type="bibr" target="#b7">(Chang et al., 2017b)</ref>, ADC <ref type="bibr" target="#b24">(Haeusser et al., 2018)</ref>, DDC <ref type="bibr" target="#b8">(Chang et al., 2019)</ref>, DCCM <ref type="bibr" target="#b76">(Wu et al., 2019)</ref>, IIC <ref type="bibr" target="#b33">(Ji et al., 2019)</ref>, PICA <ref type="bibr" target="#b31">(Huang et al., 2020)</ref>, CC <ref type="bibr" target="#b86">(Li et al., 2021b)</ref>, SPICE <ref type="bibr" target="#b52">(Niu and Wang, 2021)</ref>, <ref type="bibr">SCAN (Van Gansbeke et al., 2020)</ref>, and PCL . For those representation-based methods, namely SC, NMF, AE, DAE, DCGAN, DeCNN, and VAE, clustering is achieved by applying the vanilla kmeans on the learned features. To ensure the backbone is the same across all recent deep clustering methods, we reproduce SCAN with ResNet34 using its official released code. We would like to point out that SPICE further boosts the clustering performance under a semisupervised framework, and it uses a deeper and wider ResNet backbone (e.g., WRN37-2) which enjoys a much better feature extraction ability <ref type="bibr" target="#b11">(Chen et al., 2020b)</ref>. Thus, for a fair comparison, here we compare it with its self-trained results which are achieved on ResNet34. Besides, SPICE uses the model pre-trained on ImageNet for ImageNet-10/Dogs (denoted by "()" in <ref type="table">Table 3</ref>), while all other methods including ours train the model from scratch.</p><p>For text clustering, we compare the proposed TCL with 11 benchmarks, including TF/TF-IDF <ref type="bibr" target="#b34">(Jones, 1972)</ref>, BagOfWords (BOW) <ref type="bibr" target="#b26">(Harris, 1954)</ref>, SkipVec <ref type="bibr" target="#b38">(Kiros et al., 2015)</ref>, Para2Vec <ref type="bibr" target="#b41">(Le and Mikolov, 2014)</ref>, GSDPMM , RecNN <ref type="bibr" target="#b66">(Socher et al., 2011)</ref>, STCC <ref type="bibr" target="#b79">(Xu et al., 2017b)</ref>, HAC-SD <ref type="bibr" target="#b63">(Rakib et al., 2020)</ref>, ECIC <ref type="bibr" target="#b63">(Rakib et al., 2020)</ref>, and SCCL <ref type="bibr" target="#b85">(Zhang et al., 2021a)</ref>. Similarly, the vanilla k-means is conducted on the extracted features to cluster data for those representation-based methods, including BOW, TF/TF-IDF, SkipVec, Para2Vec, and RecNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation Metrics</head><p>Three widely-used clustering metrics including Normalized Mutual Information (NMI) <ref type="bibr" target="#b67">(Strehl and Ghosh, 2002)</ref>, Clustering Accuracy (ACC) <ref type="bibr" target="#b43">(Li and Ding, 2006)</ref>, and Adjusted Rand Index (ARI) <ref type="bibr" target="#b32">(Hubert and Arabie, 1985)</ref> are utilized to evaluate our method. Higher scores indicate better clustering performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Results</head><p>Both quantitative and qualitative studies are carried out to evaluate the proposed method. Specifically, we compare TCL with state-of-the-art baselines on image and text benchmarks, and visualize the clustering results across the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Comparisons with state of the arts</head><p>The clustering results on five image benchmarks shown in <ref type="table">Table 3</ref> demonstrate the promising performance of <ref type="table">Table 3</ref> The clustering performance on five object image benchmarks. The first and second best results are shown in bold and underline, respectively. "()" denotes that extra training data is used. "TCL ICH " refers to clustering results achieved by conducting k-means on the ICH features.  TCL. It is worth noting that our method even outperforms SPICE <ref type="bibr" target="#b52">(Niu and Wang, 2021)</ref> on the ImageNet-Dogs dataset without ImageNet pre-training, which proves the effectiveness of TCL. <ref type="table" target="#tab_4">Table 4</ref> shows the clustering results on two commonlyused text datasets. Because existing works seldom use the ARI to evaluate text clustering, here we just adopt NMI and ACC for comparisons. The results show that TCL achieves promising performance on both datasets. We would like to point out that SCCL <ref type="bibr" target="#b85">(Zhang et al., 2021a)</ref> is also a contrastive learning based method, which achieves clustering by conducting DEC <ref type="bibr" target="#b77">(Xie et al., 2016)</ref> on the representation learned by the instance-level contrastive learning. The dominance of TCL over SCCL <ref type="bibr" target="#b85">(Zhang et al., 2021a)</ref> proves the effectiveness of the proposed twin contrastive learning and confidence-based boosting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Evolution of instance features and cluster assignments</head><p>The instance-and cluster-level contrastive learning ought to help the model to learn discriminative representations and predict accurate cluster assignments, respectively. To experimentally study the convergence of TCL, we perform t-SNE (Van der Maaten and Hinton, 2008) on representations learned by ICH at four timestamps throughout the training and boosting stage, where the cluster assignments predicted by CCH are denoted in different colors. As shown in <ref type="figure" target="#fig_0">Fig. 3</ref>, features are all mixed and most instances are assigned to a few clusters at the beginning. As the training progresses, features scatter more distinctly and cluster assignments become more balanced. Finally, more compact and wellseparated clusters are achieved with the help of the confidence-based boosting. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Ablation Study</head><p>Five ablation studies are carried out to further show the importance of each component in the proposed method.</p><p>To be specific, the effectiveness of the boosting strategy, the decoupling strategy, and the two contrastive heads are studied. In addition, we investigate the influence of the hyper-parameters in the boosting stage. Besides, different combinations of weak and strong augmentation are tried to verify the effectiveness of the proposed mixed augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.1">Effectiveness of the boosting strategy</head><p>To verify the effectiveness of fine-tuning at both the instance and cluster level, we conduct ablation studies by removing one and both of the boosting losses and report the results in <ref type="table" target="#tab_5">Table 5</ref>. The self-labeling loss on CCH is essential in performance boosting because it directly affects the cluster assignments and thus cannot be removed. The results show that both losses improve the clustering performance. The success of fine-tuning relies on the quality of pseudo labels. As shown in <ref type="figure">Fig. 4</ref>, the confident predictions are more likely to be correct after a period of training, and the model makes more confident predictions as the training progresses. The number of confident predictions significantly increases at the boosting stage due to the self-labeling loss. However, it does not mean that all the confident predictions are selected as pseudo labels since the selection is based on rating instead of an absolute threshold.</p><p>Note that as the following ablation studies only influence the training stage, we report their clustering performances without boosting for simplicity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.2">Hyper-parameters for pseudo label selection</head><p>Recall that in the boosting stage, we select most confident cluster assignments as pseudo labels based on the threshold ? and the ratio ?. To investigate the influence of two hyper-parameters, we evaluate different choices of them in <ref type="figure" target="#fig_1">Fig. 5. From Fig. 5</ref>(a) and 5(b), one could see that the boosting performance is stable across a reasonable range of ? and ?, but degrades when one of the criteria is abandoned (i.e., ? = 0.0 or ? = 1.0). Such a result indicates the robustness and necessity of two pseudo label selection criteria. Besides, since pseudo labels are selected in a batch-wise manner, we also investigate the influence of batch size. To avoid the influence of batch size on contrastive learning, pseudo labels are selected at the beginning of each epoch within mini-batch of different sizes, and the batch size for optimization remains the default (i.e., 256). <ref type="figure" target="#fig_1">Fig. 5(c)</ref> shows that the boosting performance is stable in general, but it slightly decreases for large batch sizes. A possible explanation is that for fixed ? and ?, a larger N intrinsically tightens the pseudo label selection. In practice, a smaller ? is preferred when the batch size is small and vice versa.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.3">Different combinations of data augmentation</head><p>As discussed above, we provide a new effective augmentation strategy by mixing weak and strong transformations. To further show its superiority, we validate our model with different combinations of the weak and strong augmentation. The results shown in Table 7 suggest that for both image and text data, such a mixed augmentation strategy results in the best clustering performance. Notably, though such an augmentation strategy is seemingly simple, its effectiveness is closely correlated with the proposed TCL framework. Previous works have shown that directly introducing strong augmentation into the contrastive learning framework could lead to sub-optimal performance <ref type="bibr" target="#b73">(Wang and Qi, 2021)</ref>. Different from such a conclusion, we show that the mixed augmentation strategy naturally fits the proposed TCL framework. To support the claim, we compare the performance gain obtained by introducing mixed augmentation to the "SimCLR+k-means" paradigm and our TCL framework. <ref type="table" target="#tab_6">Table 6</ref> shows the clustering performance on CIFAR-10, which demonstrates that our TCL benefits more from the mixed augmentation strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.4">Effectiveness of the decoupling strategy.</head><p>Although the instance-and cluster-level contrastive learning could be directly conducted in the same subspace, in practice, we find that decoupling them into two separate subspaces leads to better clustering performance. <ref type="table">Table 8</ref> shows the ablation results, where "Yes" denotes that the twin contrastive learning is conducted in two separate subspaces and "No" denotes that both contrastive losses are computed in CCH.</p><p>Though decoupling the twin contrastive learning into two subspaces improves the performance, it does not contradict our motivation. No matter the two contrastive losses are computed jointly or separately, they act on the same representation h, and jointly optimize the network. In fact, it is consistent with the common contrastive learning framework <ref type="bibr" target="#b10">(Chen et al., 2020a;</ref><ref type="bibr" target="#b28">He et al., 2020)</ref> by stacking projection heads on the representation to compute contrastive loss. The inferior performance of computing two contrastive losses in the same subspace could be attributed to two facts: i ) the dimension of rows that equals the cluster number is not high enough to contain much information for instancelevel contrastive learning (see ablation studies in Table 9); and ii ) it would lead to a sub-optimal solution since the instance-and cluster-level contrast will influence each other in the same subspace. In brief, the instance-level contrastive learning aims at discriminating different instances instead of clusters, whose optimal solution is thus inferior to the cluster-level contrastive learning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.5">Importance of twin contrastive heads</head><p>To investigate the effectiveness of the twin contrastive heads (i.e., ICH and CCH), we conduct ablation studies by removing one of them. Since the cluster assignments can no longer be obtained when CCH is removed, we perform k-means on the features learned by ICH instead.</p><p>The results in <ref type="table" target="#tab_1">Table 10</ref> prove the effectiveness of two contrastive heads and show that superior performance is obtained by jointly conducting instance-and clusterlevel contrastive learning. Despite the performance improvement brought by CCH, we would like to emphasize that CCH is essential in achieving online clustering as it could directly and independently make cluster assignments for each instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.6">Over-clustering experiments</head><p>It is highly expected that the clustering methods could be robust to different choices of the target cluster number. Therefore, we conduct over-clustering experiments by doubling the ground-truth cluster number (and even 100 classes for CIFAR-100). In the experiments, all parameters except the CCH dimension remain the same as the default. As the Hungarian matching is not practical to find a many-to-one mapping for over-clustering evaluation, we adopt the majority voting mechanism as an alternative. Namely, we assign all samples of one cluster to the majority ground-truth class among the cluster, being consistent with the criterion used in SCAN (Van Gansbeke et al., 2020). The results in <ref type="table" target="#tab_1">Table 11</ref> demonstrate that TCL is robust against different target cluster numbers. Essentially, the effect of over-clustering is to increase the intra-class variance. Such behavior is more favorable when the data is intrinsically fine-grained (i.e., composed of several subclasses). Forcing the model to produce more fine-grained partitions could break the cluster structure when the data is intrinsically coarsegrained, which explains the performance drop on CIFAR-10. On CIFAR-100 instead, the over-clustering significantly improves the clustering performance. However, one may note that over-clustering with 100 clusters is slightly worse than that with 40 clusters. Such a performance drop could attribute to the insufficiently large batch size. To correctly represent clusters in CCH, it is necessary to include a reasonable number of samples in each cluster. With a batch size of 256, there are only two samples in each cluster on average. And some clusters might even be empty in some mini-batches, which would harm the cluster-level contrastive learning, leading to inferior clustering performance.</p><p>Note that in the proposed TCL framework, the target cluster number needs to be manually set. In practice, when the intrinsic cluster number is unknown, one could adopt some cluster number searching or community detection methods like X-means <ref type="bibr" target="#b56">(Pelleg et al., 2000)</ref> and Louvain <ref type="bibr" target="#b2">(Blondel et al., 2008)</ref> on the ICH output to estimate the cluster number. Moreover, as our TCL is robust to different choices of cluster numbers, the estimation does not necessarily need to be very precise. And one could adjust the dimensionality of CCH to cluster data under different resolutions depending on the practical needs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.7">Scaling-up to ImageNet</head><p>To see how TCL scales to large datasets with much more instances and clusters, we further conduct experiments on the ImageNet dataset. The training split with 1,281,167 images is used for both training and evaluation. As discussed above, cluster-level contrastive learning requires a large batch size to ensure a reasonable number of samples exist in each cluster. Thus, we set the batch size to 4,096 on ImageNet with 1,000 classes. However, we encounter the "out of GPU memory" problem even on eight RTX 3090 GPUs with such a large batch size. As a solution, we inherit the ResNet50 model learned by MoCo v2 <ref type="bibr" target="#b13">Chen et al. (2020c)</ref> and freeze the first two blocks. In other words, we only optimize the last two blocks of ResNet50 and two contrastive heads. Due to the heavy computational burden, we train and boost the model for 100 and 20 epochs, respectively. We set ? I = ? C = 0.2 for training and use the default ? = 0.99 and ? = 0.5 for boosting. In the evaluation, we choose MoCo v2 (the model we inherited) as a baseline by conducting k-means on the extracted features. The clustering performance is shown in <ref type="table" target="#tab_1">Table 12</ref>, which proves the effectiveness of the twin contrastive learning framework, the boosting strategy, and the scalability of our method.  <ref type="figure">Fig. 6</ref> Ablation study of different choices of temperature ? I and ? C in TCL, trained for 50 epochs on ImageNet. One of the hyper-parameters is ablated from ? I = ? C = 0.2.</p><p>(a) threshold ? (b) ratio ? <ref type="figure">Fig. 7</ref> Ablation study of different choices of ? and ? during the boosting stage on ImageNet. One of the hyper-parameters is ablated from the default setting ? = 0.99 and ? = 0.5.</p><p>As suggested in SimCLR <ref type="bibr" target="#b10">(Chen et al., 2020a)</ref>, a smaller temperature ? I in instance-level contrastive loss could speed up convergence with a large batch size. To investigate the influence of ? I and ? C in our TCL framework when scaling up to large batch size and cluster number, we ablate these two parameters in <ref type="figure">Fig. 6</ref>. As shown, a proper choice of ? I could slightly improve the performance. More importantly, a smaller temperature ? C is also preferred in cluster-level contrastive learning with a large cluster number, since it could sharpen the cluster assignments to obtain a more discriminative cluster representation. We further conduct ablation studies on two boosting hyper-parameters ? and ?. The results in <ref type="figure">Fig. 7</ref> show that the threshold ? plays a more important role on ImageNet compared with the ratio ?. This is because all samples are likely to meet the ratio criterion eventually, as there are only four samples in each cluster on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Based on the observation that the rows and columns of the feature matrix intrinsically correspond to the instance and cluster representations, we propose an online deep clustering method termed Twin Contrastive Learning (TCL). By dually conducting contrastive learning at the instance and cluster level, TCL simultaneously learns representations and performs clustering. In addition, to alleviate the influence of intrinsic false negative pairs in the instance-level contrastive learning and to rectify cluster assignments, we propose a confidence-based boosting strategy to further improve the performance by selecting some pseudo labels to finetune the twin contrastive learning. Extensive experiments demonstrate the effectiveness of TCL on five image benchmarks and two text datasets.</p><p>In the future, we plan to take a deeper look at the influence of different augmentations on contrastive learning. Furthermore, it is worthwhile to explore how to better utilize pseudo labels to identify false negative pairs for improving contrastive learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3</head><label>3</label><figDesc>200 epoch (NMI=0.699) (c) 1000 epoch (NMI=0.790) (d) 1200 epoch (NMI=0.819) The evolution of instance features and cluster assignments across the training and boosting stage on CIFAR-10. We perform t-SNE on the features learned by ICH and use different colors to indicate the cluster assignment predicted by CCH.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5</head><label>5</label><figDesc>Ablation study of different choices of hyper-parameters during the boosting stage on STL-10. One of the hyperparameters is ablated from the default setting ? = 0.99, ? = 0.5, N = 256 each time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>)</head><label></label><figDesc>Algorithm 1 Twin Contrastive LearningInput: dataset X ; training iterations E 1 ; boosting iterations E 2 ; batch size N ; cluster number M ; temperature parameter ? I and ? C ; network f , g I , and g C ; augmentation strategies T , T . Output: cluster assignments.// Training for epoch = 1 to E 1 do sample a mini-batch {x i } N i=1 from X sample two augmentations t ? T , t ? Tcompute instance and cluster representations compute instance-level contrastive loss L ins through Eq. 2-4 compute cluster-level contrastive loss L clu through Eq. 5-9 compute training loss L train by Eq. 1 update f, g I , g C through gradient descent to minimize L train end // Boosting for epoch = 1 to E 2 do sample a mini-batch {x i } N i=1 from X sample two augmentations t ? T , t ? T update pseudo labels with Eq. 10-11 and Eq. 16 compute self-supervised contrastive loss L scl by Eq. 14 compute self-labeling loss L sl by Eq. 15 compute boosting loss L boost by Eq. 12 update f, g I , g C through gradient descent to minimize L</figDesc><table /><note>boost end // Test for x in X do extract features by h = f (x) compute cluster assignment by c = arg max g C (h) end</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>A summary of datasets used for evaluation.</figDesc><table><row><cell>Dataset</cell><cell>Split</cell><cell cols="2">Samples Classes</cell></row><row><cell>CIFAR-10</cell><cell>Train+Test</cell><cell>60,000</cell><cell>10</cell></row><row><cell>CIFAR-100</cell><cell>Train+Test</cell><cell>60,000</cell><cell>20</cell></row><row><cell>STL-10</cell><cell>Train+Test</cell><cell>13,000</cell><cell>10</cell></row><row><cell>ImageNet-10</cell><cell>Train</cell><cell>13,000</cell><cell>10</cell></row><row><cell>ImageNet-Dogs</cell><cell>Train</cell><cell>19,500</cell><cell>15</cell></row><row><cell>StackOverflow</cell><cell>-</cell><cell>20,000</cell><cell>20</cell></row><row><cell>Biomedical</cell><cell>-</cell><cell>20,000</cell><cell>20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>List of strong augmentations for images.</figDesc><table><row><cell cols="2">Transformations Parameter</cell><cell>Range</cell></row><row><cell>AutoContrast</cell><cell>-</cell><cell>-</cell></row><row><cell>Equalize</cell><cell>-</cell><cell>-</cell></row><row><cell>Identity</cell><cell>-</cell><cell>-</cell></row><row><cell>Brightness</cell><cell>B</cell><cell>[0.05, 0.95]</cell></row><row><cell>Color</cell><cell>C</cell><cell>[0.05, 0.95]</cell></row><row><cell>Contrast</cell><cell>C</cell><cell>[0.05, 0.95]</cell></row><row><cell>Posterize</cell><cell>B</cell><cell>[4, 8]</cell></row><row><cell>Rotate</cell><cell>?</cell><cell>[-30, 30]</cell></row><row><cell>Sharpness</cell><cell>S</cell><cell>[0.05, 0.95]</cell></row><row><cell>Shear X, Y</cell><cell>R</cell><cell>[-0.3, 0.3]</cell></row><row><cell>Solarize</cell><cell>T</cell><cell>[0, 256]</cell></row><row><cell>Translate X, Y</cell><cell>?</cell><cell>[-0.3, 0.3]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>The clustering performance on two text datasets. The first and second best results are shown in bold and underline, respectively. "TCL ICH " refers to clustering results achieved by conducting k-means on the ICH features.</figDesc><table><row><cell>Dataset</cell><cell cols="2">StackOverflow</cell><cell cols="2">Biomedical</cell></row><row><cell>Metrics</cell><cell>NMI</cell><cell>ACC</cell><cell>NMI</cell><cell>ACC</cell></row><row><cell>TF</cell><cell>0.078</cell><cell>0.135</cell><cell>0.093</cell><cell>0.152</cell></row><row><cell>BOW</cell><cell>0.140</cell><cell>0.185</cell><cell>0.092</cell><cell>0.143</cell></row><row><cell>SkipVec</cell><cell>0.027</cell><cell>0.009</cell><cell>0.107</cell><cell>0.163</cell></row><row><cell>TF-IDF</cell><cell>0.156</cell><cell>0.203</cell><cell>0.254</cell><cell>0.280</cell></row><row><cell>Para2Vec</cell><cell>0.279</cell><cell>0.326</cell><cell>0.348</cell><cell>0.413</cell></row><row><cell>GSDPMM</cell><cell>0.306</cell><cell>0.294</cell><cell>0.320</cell><cell>0.281</cell></row><row><cell>RecNN</cell><cell>0.402</cell><cell>0.405</cell><cell>0.338</cell><cell>0.367</cell></row><row><cell>STCC</cell><cell>0.490</cell><cell>0.511</cell><cell>0.381</cell><cell>0.436</cell></row><row><cell>HAC-SD</cell><cell>0.595</cell><cell>0.648</cell><cell>0.335</cell><cell>0.401</cell></row><row><cell>ECIC</cell><cell>0.734</cell><cell>0.787</cell><cell>0.413</cell><cell>0.478</cell></row><row><cell>SCCL</cell><cell>0.745</cell><cell>0.755</cell><cell>0.415</cell><cell>0.462</cell></row><row><cell>TCL</cell><cell>0.786</cell><cell cols="2">0.882 0.429</cell><cell>0.498</cell></row><row><cell>TCL ICH</cell><cell>0.788</cell><cell>0.807</cell><cell>0.423</cell><cell>0.470</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc>Effectiveness of the boosting strategy. "SL" refers to the self-labeling loss, and "SCL" refers to the self-supervised contrastive loss. Evolution of clustering performance and confident predictions w.r.t. training epochs on CIFAR-10, where confident predictions are those with CONF ? 0.99.</figDesc><table><row><cell>Dataset</cell><cell>Boost</cell><cell>NMI</cell><cell>ACC</cell><cell>ARI</cell></row><row><cell></cell><cell>None</cell><cell>0.790</cell><cell>0.865</cell><cell>0.752</cell></row><row><cell>CIFAR-10</cell><cell>SL</cell><cell>0.805</cell><cell>0.878</cell><cell>0.770</cell></row><row><cell></cell><cell cols="4">SL+SCL 0.819 0.887 0.780</cell></row><row><cell></cell><cell>None</cell><cell>0.518</cell><cell>0.549</cell><cell>0.381</cell></row><row><cell>ImageNet-Dogs</cell><cell>SL</cell><cell>0.562</cell><cell>0.600</cell><cell>0.441</cell></row><row><cell></cell><cell cols="4">SL+SCL 0.623 0.644 0.516</cell></row><row><cell></cell><cell>None</cell><cell>0.751</cell><cell>0.860</cell><cell>0.731</cell></row><row><cell>StackOverflow</cell><cell>SL</cell><cell>0.780</cell><cell>0.877</cell><cell>0.761</cell></row><row><cell></cell><cell cols="4">SL+SCL 0.786 0.882 0.771</cell></row><row><cell>Fig. 4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6</head><label>6</label><figDesc>Clustering performance gain on CIFAR-10 by introducing the mixed augmentation strategy. T denotes the weak augmentation, and T denotes the strong augmentation.</figDesc><table><row><cell>Method</cell><cell>Augmentation</cell><cell>NMI</cell><cell>ACC</cell><cell>ARI</cell></row><row><cell>SimCLR+k-means</cell><cell>T + T T + T</cell><cell>0.699 0.734 (+0.035)</cell><cell>0.782 0.821 (+0.039)</cell><cell>0.616 0.675 (+0.059)</cell></row><row><cell>TCL</cell><cell>T + T T + T</cell><cell cols="3">0.705 0.790 (+0.085) 0.865 (+0.075) 0.752 (+0.115) 0.790 0.637</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7</head><label>7</label><figDesc>Importance of data augmentation. T denotes the weak augmentation, and T denotes the strong augmentation.</figDesc><table><row><cell>Dataset</cell><cell>Augmentation</cell><cell>NMI</cell><cell>ACC</cell><cell>ARI</cell></row><row><cell></cell><cell>T + T</cell><cell>0.712</cell><cell>0.805</cell><cell>0.648</cell></row><row><cell>CIFAR-10</cell><cell>T + T</cell><cell cols="3">0.790 0.865 0.752</cell></row><row><cell></cell><cell>T + T</cell><cell>0.678</cell><cell>0.738</cell><cell>0.614</cell></row><row><cell></cell><cell>T + T</cell><cell>0.441</cell><cell>0.413</cell><cell>0.262</cell></row><row><cell>ImageNet-Dogs</cell><cell>T + T</cell><cell cols="3">0.518 0.549 0.381</cell></row><row><cell></cell><cell>T + T</cell><cell>0.071</cell><cell>0.147</cell><cell>0.027</cell></row><row><cell></cell><cell>T + T</cell><cell>0.546</cell><cell>0.580</cell><cell>0.420</cell></row><row><cell>StackOverflow</cell><cell>T + T</cell><cell cols="3">0.751 0.860 0.731</cell></row><row><cell></cell><cell>T + T</cell><cell>0.647</cell><cell>0.715</cell><cell>0.569</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 Table 9</head><label>89</label><figDesc>Effectiveness of the decoupling strategy. Difference choices of the dimensionality of ICH.</figDesc><table><row><cell>Dataset</cell><cell>Decoupling</cell><cell>NMI</cell><cell>ACC</cell><cell>ARI</cell></row><row><cell>CIFAR-10</cell><cell>Yes No</cell><cell cols="3">0.770 0.854 0.730 0.601 0.640 0.481</cell></row><row><cell>ImageNet-Dogs</cell><cell>Yes No</cell><cell cols="3">0.518 0.535 0.385 0.343 0.368 0.214</cell></row><row><cell>StackOverflow</cell><cell>Yes No</cell><cell cols="3">0.747 0.857 0.725 0.687 0.745 0.616</cell></row><row><cell>Dataset</cell><cell>Dimension</cell><cell>NMI</cell><cell>ACC</cell><cell>ARI</cell></row><row><cell></cell><cell>10</cell><cell>0.770</cell><cell>0.854</cell><cell>0.730</cell></row><row><cell></cell><cell>32</cell><cell>0.789</cell><cell>0.867</cell><cell>0.754</cell></row><row><cell>CIFAR-10</cell><cell>64</cell><cell cols="3">0.798 0.871 0.760</cell></row><row><cell></cell><cell>128</cell><cell>0.790</cell><cell>0.866</cell><cell>0.752</cell></row><row><cell></cell><cell>256</cell><cell>0.789</cell><cell>0.867</cell><cell>0.753</cell></row><row><cell></cell><cell>15</cell><cell>0.518</cell><cell>0.535</cell><cell>0.385</cell></row><row><cell></cell><cell>32</cell><cell>0.535</cell><cell>0.557</cell><cell>0.400</cell></row><row><cell>ImageNet-Dogs</cell><cell>64</cell><cell>0.529</cell><cell>0.551</cell><cell>0.389</cell></row><row><cell></cell><cell>128</cell><cell>0.518</cell><cell>0.549</cell><cell>0.381</cell></row><row><cell></cell><cell>256</cell><cell cols="3">0.541 0.562 0.411</cell></row><row><cell></cell><cell>20</cell><cell>0.747</cell><cell>0.857</cell><cell>0.725</cell></row><row><cell></cell><cell>32</cell><cell>0.749</cell><cell>0.858</cell><cell>0.726</cell></row><row><cell>StackOverflow</cell><cell>64</cell><cell>0.748</cell><cell>0.858</cell><cell>0.728</cell></row><row><cell></cell><cell>128</cell><cell cols="3">0.751 0.860 0.731</cell></row><row><cell></cell><cell>256</cell><cell>0.727</cell><cell>0.821</cell><cell>0.688</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10</head><label>10</label><figDesc>Effectiveness of two contrastive heads.</figDesc><table><row><cell>Dataset</cell><cell>Contrastive Head</cell><cell>NMI</cell><cell>ACC</cell><cell>ARI</cell></row><row><cell></cell><cell>ICH + CCH</cell><cell cols="3">0.790 0.865 0.752</cell></row><row><cell>CIFAR-10</cell><cell>w/o ICH</cell><cell>0.633</cell><cell>0.658</cell><cell>0.522</cell></row><row><cell></cell><cell>w/o CCH</cell><cell>0.734</cell><cell>0.821</cell><cell>0.675</cell></row><row><cell></cell><cell>ICH + CCH</cell><cell cols="3">0.518 0.549 0.381</cell></row><row><cell>ImageNet-Dogs</cell><cell>w/o ICH</cell><cell>0.376</cell><cell>0.366</cell><cell>0.221</cell></row><row><cell></cell><cell>w/o CCH</cell><cell>0.504</cell><cell>0.535</cell><cell>0.336</cell></row><row><cell></cell><cell>ICH + CCH</cell><cell cols="3">0.751 0.860 0.731</cell></row><row><cell>StackOverflow</cell><cell>w/o ICH</cell><cell>0.735</cell><cell>0.842</cell><cell>0.706</cell></row><row><cell></cell><cell>w/o CCH</cell><cell>0.666</cell><cell>0.732</cell><cell>0.516</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11</head><label>11</label><figDesc>The robustness of TCL against different choices of the target cluster number.</figDesc><table><row><cell>Dataset</cell><cell>Target cluster number</cell><cell>NMI</cell><cell>ACC</cell><cell>ARI</cell></row><row><cell>CIFAR-10</cell><cell>Standard (10) Over-cluster (20)</cell><cell cols="3">0.790 0.865 0.752 0.759 0.846 0.718</cell></row><row><cell></cell><cell>Standard (20)</cell><cell>0.477</cell><cell>0.481</cell><cell>0.303</cell></row><row><cell>CIFAR-100</cell><cell>Over-cluster (40)</cell><cell cols="3">0.520 0.579 0.380</cell></row><row><cell></cell><cell>Over-cluster (100)</cell><cell>0.493</cell><cell>0.566</cell><cell>0.359</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12</head><label>12</label><figDesc>TCL scales up to ImageNet-1K with much more images and clusters. ? means without the boosting stage.</figDesc><table><row><cell>Metrics</cell><cell>NMI</cell><cell>ACC</cell><cell>ARI</cell></row><row><cell>MoCo (inherited model)</cell><cell>0.6186</cell><cell>0.3047</cell><cell>0.1428</cell></row><row><cell>TCL  ?</cell><cell>0.6332</cell><cell>0.3160</cell><cell>0.1901</cell></row><row><cell>TCL</cell><cell cols="3">0.6711 0.3789 0.2656</cell></row><row><cell>(a) ICH temperature ? I</cell><cell cols="3">(b) CCH temperature ? C</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements The authors would like to thank the Associate editor and reviewers for the constructive comments and valuable suggestions that remarkably improve this study. This work was supported in part by the National Key R&amp;D Program of China under Grant 2020YFB1406702; in part by NFSC under Grant 62176171, U21B2040, and U19A2078; and in part by Open Research Projects of Zhejiang Lab under Grant 2021KH0AB02.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Self-labelling via simultaneous clustering and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations 3</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast unfolding of communities in large networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">D</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lambiotte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lefebvre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of statistical mechanics: theory and experiment</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="10008" to="10022" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Locality preserving nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: IJCAI</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1010" to="1015" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtyfourth Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep adaptive image clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5879" to="5887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep adaptive image clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5879" to="5887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<idno>arXiv:190501681 9</idno>
		<title level="m">Deep discriminative clustering analysis</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spectral curvature clustering (scc)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="317" to="330" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint arXiv</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22243" to="22255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno>arXiv:201110566 3</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno>arXiv:200304297 14</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An analysis of singlelayer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth international conference on artificial intelligence and statistics</title>
		<meeting>the fourteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<idno>arXiv:210305484 3</idno>
		<title level="m">Doubly contrastive deep clustering</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno>arXiv:170804552 8</idno>
		<title level="m">Improved regularization of convolutional neural networks with cutout</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dwibedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>arXiv:210414548 4</idno>
		<title level="m">With a little help from my friends: Nearest-neighbor contrastive learning of visual representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep clustering via joint convolutional autoencoder embedding and relative entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghasedi</forename><surname>Dizaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Herandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Agglomerative clustering using the concept of mutual nearest neighbourhood</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Gowda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="105" to="112" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<idno>arXiv:200607733 3</idno>
		<title level="m">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improved deep embedded clustering with local structure preservation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="1753" to="1759" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Associative deep clustering: Training a classification network with no labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haeusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Plapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aljalbout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="18" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mitigating embedding and class assignment mismatch in unsupervised image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-08-23" />
			<biblScope unit="page" from="768" to="784" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XXIV 16</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">S</forename><surname>Harris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Distributional structure. Word</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="146" to="162" />
			<date type="published" when="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Adco: Adversarial contrast for efficient learning of unsupervised representations from self-trained negative adversaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Qi</surname></persName>
		</author>
		<idno>arXiv:201108435 3</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning discrete representations via information maximizing self-augmented training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tokui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, PMLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1558" to="1567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep semantic clustering by partition confidence maximisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Comparing partitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arabie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of classification</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="193" to="218" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Invariant information clustering for unsupervised image classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A statistical interpretation of term specificity and its application in retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of documentation</title>
		<imprint>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Supervised contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Nlnl: Negative learning for noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="101" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>arXiv:13126114 9</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<idno>arXiv:150606726 9</idno>
		<title level="m">Skip-thought vectors</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Challenges in unsupervised clustering of single-cell rnaseq data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">Y</forename><surname>Kiselev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hemberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Genetics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="273" to="282" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Toronto 8</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning, PMLR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Prototypical contrastive learning of unsupervised representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The relationships among various nonnegative matrix factorization methods for clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth International Conference on Data Mining (ICDM&apos;06)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="362" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Autoencoder constrained clustering with adaptive neighbors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Neural Networks and Learning Systems pp 1-7</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note>2021b) Contrastive clustering 35 2, 3, 4</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Sparse embedded kmeans clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3319" to="3327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Multiple kernel k-means clustering with matrix-induced regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirtieth AAAI conference on artificial intelligence</title>
		<meeting>the thirtieth AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">Nlp</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth Berkeley symposium on mathematical statistics and probability</title>
		<meeting>the fifth Berkeley symposium on mathematical statistics and probability<address><addrLine>Oakland, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1967" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Spectral embedded clustering: A framework for in-sample and out-of-sample spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1796" to="1808" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">K-multiple-means: A multiple-means clustering method with specified k clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="959" to="967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Spice: Semantic pseudo-labeling for image clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>arXiv preprint arXiv:210309382 2, 3, 4, 7, 9</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oord</forename><surname>Avd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>arXiv:180703748 6</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Improving unsupervised image clustering with robust learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cha</surname></persName>
		</author>
		<idno>arXiv:201211150 3</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<idno>arXiv:191201703 9</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">X-means: Extending k-means with efficient estimation of the number of clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pelleg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: Icml</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="727" to="734" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Robust subspace clustering via thresholding ridge regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>AAAI</publisher>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="3827" to="3833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep subspace clustering with sparsity prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Yau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Structured autoencoders for subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Yau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5076" to="5086" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deep clustering with sample-assignment invariance prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4857" to="4868" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Xai beyond classification: Interpretable neural clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Mach Learn Res</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno>arXiv:151106434 9</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Enhancement of short text clustering by iterative classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrh</forename><surname>Rakib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jankowska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Milios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Applications of Natural Language to Information Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="105" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<idno>arXiv:190810084 5</idno>
		<title level="m">Sentence-bert: Sentence embeddings using siamese bert-networks</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<idno>arXiv:210313225 2</idno>
		<title level="m">Structure-aware face clustering on a large-scale graph with 10 7 nodes</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 conference on empirical methods in natural language processing</title>
		<meeting>the 2011 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Cluster ensembles-a knowledge reuse framework for combining multiple partitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Strehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="583" to="617" />
			<date type="published" when="2002-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Kernel cuts: Kernel and spectral clustering meet regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">B</forename><surname>Ayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="477" to="511" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Neutrosophic recommender system for medical diagnosis based on algebraic similarity measure and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Thanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Fuzzy Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Scan: Learning to classify images without labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno>arXiv:210605967 4</idno>
		<title level="m">Revisiting contrastive methods for unsupervised learning of visual representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Qi</surname></persName>
		</author>
		<idno>arXiv:210407713 3</idno>
		<title level="m">Contrastive learning with stronger augmentations</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning by cross-level instance-group discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12586" to="12595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zou</surname></persName>
		</author>
		<idno>arXiv:190111196</idno>
		<title level="m">Easy data augmentation techniques for boosting performance on text classification tasks</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Deep comprehensive correlation mining for image clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8150" to="8159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Self-taught convolutional neural networks for short text clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="22" to="31" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Self-taught convolutional neural networks for short text clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="22" to="31" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Joint unsupervised learning of deep representations and image clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">A model-based approach for text clustering with outlier detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 32nd International Conference on Data Engineering</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="625" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Barlow twins: Self-supervised learning via redundancy reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Deny</surname></persName>
		</author>
		<idno>arXiv:210303230 2</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Deconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2528" to="2535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Self-tuning spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1601" to="1608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<idno>arXiv:210312953 5</idno>
		<title level="m">Supporting clustering with contrastive learning</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<idno>arXiv:210312953 8</idno>
		<title level="m">Supporting clustering with contrastive learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Graph degree linkage: Agglomerative clustering on a directed graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="428" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">Z</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename></persName>
		</author>
		<idno>arXiv:200803030 3</idno>
		<title level="m">Deep robust clustering by contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
