<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">R3L: CONNECTING DEEP REINFORCEMENT LEARNING TO RECURRENT NEURAL NETWORKS FOR IMAGE DENOISING VIA RESIDUAL RECOVERY</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongkai</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Zha</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Dauwels</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Microelectronics</orgName>
								<orgName type="institution">Delft University of Technology</orgName>
								<address>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bihan</forename><surname>Wen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">R3L: CONNECTING DEEP REINFORCEMENT LEARNING TO RECURRENT NEURAL NETWORKS FOR IMAGE DENOISING VIA RESIDUAL RECOVERY</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Recurrent Neural Network</term>
					<term>Deep Reinforce- ment Learning</term>
					<term>Image Denoising</term>
					<term>Residual Recovery</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>State-of-the-art image denoisers exploit various types of deep neural networks via deterministic training. Alternatively, very recent works utilize deep reinforcement learning for restoring images with diverse or unknown corruptions. Though deep reinforcement learning can generate effective policy networks for operator selection or architecture search in image restoration, how it is connected to the classic deterministic training in solving inverse problems remains unclear. In this work, we propose a novel image denoising scheme via Residual Recovery using Reinforcement Learning, dubbed R3L. We show that R3L is equivalent to a deep recurrent neural network that is trained using a stochastic reward, in contrast to many popular denoisers using supervised learning with deterministic losses. To benchmark the effectiveness of reinforcement learning in R3L, we train a recurrent neural network with the same architecture for residual recovery using the deterministic loss, thus to analyze how the two different training strategies affect the denoising performance. With such a unified benchmarking system, we demonstrate that the proposed R3L has better generalizability and robustness in image denoising when the estimated noise level varies, comparing to its counterparts using deterministic training, as well as various state-ofthe-art image denoising algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Image denoising is one of the most fundamental inverse problems, which aims to estimate the underlying clean x from its noisy observation y, which is corrupted with noise n as:</p><formula xml:id="formula_0">y = x + n .<label>(1)</label></formula><p>Assuming n to be the additive white Gaussian noise (AWGN), it follows the normal distribution, i.e., n ? N (0, ? 2 ). Besides improving the image visual quality, it is also a necessary preprocessing step for many high-level vision tasks such as classification <ref type="bibr" target="#b0">[1]</ref>, segmentation <ref type="bibr" target="#b1">[2]</ref>, object detection <ref type="bibr" target="#b2">[3]</ref> and tracking <ref type="bibr" target="#b3">[4]</ref>. * Bihan Wen (bihan.wen@ntu.edu.sg) is the corresponding author. Copyright 2021 IEEE. Published in the IEEE 2021 International Conference on Image Processing (IEEE ICIP 2021), scheduled for <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref> September 2021 in Anchorage, Alaska, United States. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtained from the IEEE. Contact: Manager, Copyrights and Permissions / IEEE Service Center / 445 Hoes Lane / P.O. Box 1331 / Piscataway, NJ 08855-1331, USA. Telephone: + Intl. 908-562-3966.</p><p>Classic image denoising algorithms are based on analytical models, e.g., image non-local similarity <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>, transform-domain sparsity <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, etc. More recently, deep learning has demonstrated remarkable results in image denoising by training the highly flexible neural networks with deterministic loss functions using an end-toend approach <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. While most deep denoisers exploit feed-forward convolutional neural networks (CNNs) <ref type="bibr" target="#b11">[12]</ref>, the models usually involve a huge amount of trainable parameters leading to high memory complexity. Alternatively, some recent works exploit recurrent neural networks (RNNs) with shared module parameters. For example, the non-local recurrent network (NLRN) <ref type="bibr" target="#b12">[13]</ref> achieved both high parameter efficiency and denoising performance.</p><p>Comparing to the end-to-end supervised deep learning, few works to date exploited deep reinforcement learning (DRL) for image denoising. Some pilot works trained a separate policy network for operator selection <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> or architecture search <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> to assist image denoising. However, it is unclear how DRL can be "directly" applied to inverse problems, e.g., how effective is the denoising network trained via DRL? To the best of our knowledge, no work to date has benchmarked DRL with supervised deep learning with deterministic loss functions in image denoising.</p><p>To this end, we propose a novel image denoising scheme via Residual Recovery using Reinforcement Learning (R3L) for image denoising. We show that the proposed R3L is equivalent to a RNN denoiser trained using a stochastic reward, which provides a unified framework to compare DRL to other RNN-based image denoising schemes. To benchmark the effectiveness of DRL, we train a recurrent neural network with the same architecture as our R3L model for residual recovery using supervised learning with a deterministic mean square error, called R3N. The experiments show that the proposed R3L achieved more reliable denoising results when the estimated noise levels (i.e., noise standard deviation ?) of degraded images deviate from the oracle. The average denoising PSNRs (over varied noise estimations) using our R3L outperform those by R3N as well as many state-of-the-art denoising algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Image denoising methods are classified into two categories: priorbased methods and learning based methods. Many classical methods, such as BM3D <ref type="bibr" target="#b5">[6]</ref> and WNNM <ref type="bibr" target="#b6">[7]</ref>, are based on effective priors, and some of them applied the denoising operator recursively <ref type="bibr" target="#b6">[7]</ref>. On the other hand, learning-based methods utilized more flexible models such as deep neural networks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18]</ref>. Though deep denoising models lead to superior image restoration, most of them involve a huge amount of parameters. One solution to enhance memory efficiency is applying a lighter neural network recursively, which results in many successful frameworks based on RNN or DRL. We <ref type="table">Table 1</ref>. Comparison between various image denoisers, including the proposed R3L and other existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Trainable kernels</head><p>Residual learning Recursion DRL BM3D <ref type="bibr" target="#b5">[6]</ref> WNNM <ref type="bibr" target="#b6">[7]</ref> DnCNN <ref type="bibr" target="#b11">[12]</ref> NLRN <ref type="bibr" target="#b12">[13]</ref> pixelRL <ref type="bibr" target="#b14">[15]</ref> R3L provide a summary of RNN and DRL algorithms for image denoising. <ref type="table">Table 1</ref> summarizes the representatives of image denoising methods of different categories, as well as the proposed R3L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">RNN for Image Denoising</head><p>Deep RNNs have been widely applied for image denoising. Chen et al. <ref type="bibr" target="#b18">[19]</ref> first used a deep RNN which exploits temporal-spatial information for video denoising. Putzky et al. <ref type="bibr" target="#b19">[20]</ref> proposed a learning framework, dubbed Recurrent Inference Machines (RIM), in which they train a RNN to learn an inference algorithm for solving inverse problems. Liu et al. <ref type="bibr" target="#b12">[13]</ref> proposed NLRN which incorporates the non-local operations into an RNN for image restoration achieving the state-of-the-art results. However, most of the RNN-based denoising models are trained over a corpus of images containing the similar noise distribution, using the deterministic loss function (e.g., mean square error), thus hard to generalize to complex and inaccurately estimatesd noise in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">DRL for Image Denoising</head><p>DRL has recently gathered considerable interest showing great promise in many applications <ref type="bibr" target="#b20">[21]</ref>, including image processing tasks. Yu et al. <ref type="bibr" target="#b13">[14]</ref> firstly attempt to apply DRL to learn a policy to select suitable operators from a pre-defined toolbox to progressively restore corrupted images. Their improved version <ref type="bibr" target="#b15">[16]</ref> can dynamically select an appropriate route for different image regions in a multi-path CNN, to perform spatial-varying image denoising. Furuta et al. <ref type="bibr" target="#b14">[15]</ref> proposes pixelRL, the first framework to do a pixelwise restoration. Most of DRL based methods still rely on manually designed filters. What the DRL agent learns is the order to apply filters instead of directly modifying the pixel values, i.e., residual recovery. Therefore, it remains unclear how DRL approaches to image denoising relate to other learning based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PROPOSED R3L METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Residual Recovery as Markov Decision Process</head><p>Residual recovery is commonly used in deep image denoising, which aims to obtain the residual image of the noisy input relative to the ground truth. As removing a residual can be considered as sequentially removing several inter-residuals, residual recovery is a sequential decision problem. Therefore, we modeled the denoising problem via residual recovery as a Markov Decision Process (MDP), which can be solved using DRL. At each state t (t = 0 denotes the initial state) of denoising, taking the noisy image (t = 0) or the denoised estimate (t ? 1) from the previous state as the input I t ? R N , the DRL agent follows a policy ? to output the probability P (a t i |I t ) ?i. Here a t i denotes the estimated residual of the ith pixel (1 ? i ? N ) at the state t. We apply a deep neural network to construct the policy ?, denoted as the policy network with the trainable parameter ??. A is the action set, which consists of all discrete values in a predefined range, and a t i ? A. The estimated image is updated to I t+1 by applying the output actions, and the agent can obtain a reward r t i for each pixel. The denoising process repeats until the termination stage n, and outputs the final denoised image. The probability of an action trajectory Ji for each pixel i, denoted as P (Ji|I 0 , ??), is calculated as:</p><formula xml:id="formula_1">P (Ji|I 0 , ??) = P (a 1 i |I 0 , ??)P (a 2 i |a 1 i , I 0 , ??) ...P (a t i |a t?1 i ..., a 1 i , I 0 , ??) = T t=1 P (a t i |J t?1 i , I 0 , ??)<label>(2)</label></formula><p>where Ji a 1 i , a 2 i , ... , a n i . Following the common setting in DRL, we use the long-term discounted reward R t i (Ji) to evaluate a policy at the state i, which is defined as:</p><formula xml:id="formula_2">R t i (Ji) r t i + ?r t+1 i + ? 2 r t+2 i + ... + ? n?t r n i<label>(3)</label></formula><p>Here, ? j denotes the jth power of the discount factor 0 &lt; ? &lt; 1, and r t i denotes the reward for pixel i at stage t. The DRL agent can explore different trajectories towards learning the optimal policy ? * . Following ? * , the agent selects the optimal action at each state with the highest probability by maximizing the expectation of R 0 i (Ji) as:</p><formula xml:id="formula_3">? * = argmax ? P (Ji|I 0 , ??)R 0 i (Ji)<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Proposed R3L Framework</head><p>Inspired by <ref type="bibr" target="#b14">[15]</ref>, we apply the fully convolutional network (FCN) based asynchronous advantage actor-critic (A3C) <ref type="bibr" target="#b21">[22]</ref> framework in the proposed residual recovery reinforcement learning (R3L) scheme. We apply FCN as the encoder which is widely used and effective in image processing tasks for the pixel-level modification. We apply A3C with a policy network ? and a value network V to make the training more stable and efficient <ref type="bibr" target="#b22">[23]</ref>. The FCN-based encoder is denoted as EFCN, which is shared by both ? and V . EFCN extracts the features of the input image I t and outputs s t , as the representation of state t. Taking s t , the policy network ? outputs the probability of selecting a certain residual value a t i for each pixel, and the value network outputs V (s t |?v), which is the estimation of the long term discounted rewards R t i for each pixel. The reward r t i used in R3L for image denoising is defined as:</p><formula xml:id="formula_4">r t i (xi ? I t?1 i ) 2 ? (xi ? I t i ) 2<label>(5)</label></formula><p>where x denotes the clean image, and xi denotes its i pixel. Without loss of generality, for convenience, we consider the one-stage learning case (n = 1) here. The gradients of the parameters of these two networks ??,?v are calculated as: During training, the residual value a t i is sampled from a predefined range, i.e. [?13, 13], according to the output from the policy network. In the testing phase, only the well-trained policy network is deployed and the residual value with the highest probability is greedily selected. The inference process is formulated as:</p><formula xml:id="formula_5">R t i = r t i + ?V (s t+1 |?v) d?v = ? ?v 1 N N i=1 (R t i ? V (s t |?v)) 2 d?? = ?? ?? 1 N N i=1 log P (a t i |s t , ??)(R t i ? V (s t |?v))<label>(6)</label></formula><formula xml:id="formula_6">s t = EFCN(I t )</formula><p>a t = Greedy(?(s t |? * ? )) t = 0, 1, 2, ..., T</p><formula xml:id="formula_7">I t+1 = I t + a t<label>(7)</label></formula><p>where Greedy(?) denotes the deterministic greedy sampling operator <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>, a t denotes the residual image built by a t i (1 ? i ? N ), and T denotes the number of total stages. Here, we use T = 5 as a hyperparameter to balance the processing time and the performance. The inference process of R3L at state t is illustrated as <ref type="figure" target="#fig_0">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Connection of R3L and RNNs</head><p>Theorem 1. Greedily selecting the action with the highest policy from the output of policy network reduces the inference process of R3L to a RNN.</p><p>Proof. In general, the recurrent inference process in an RNN is:</p><formula xml:id="formula_8">I t+1 = f ? (I t ) ?t ,<label>(8)</label></formula><p>where f ? is the recurrent module which is parameterized by ?. Based on <ref type="bibr" target="#b6">(7)</ref>, the inference process of R3L follows <ref type="bibr" target="#b7">(8)</ref>, with the corresponding module f ? as the following form: <ref type="figure">(EFCN(I t ))</ref>).</p><formula xml:id="formula_9">f ? (I t ) = I t + Greedy(? ?</formula><p>Theorem 1 shows that the inference process of R3L follows an RNN. However, the R3L model is trained using DRL using the stochastic reward with no hidden states. In RNNs, the same network will be applied recursively until the termination. Our R3L also exploits the same recursive property and benefits a high parameter efficiency from it. However, most RNN based methods mainly focus on learning the final residual in an end-to-end manner, which makes the learning outcome a deterministic one-to-one mapping from the noisy input to the residual. R3L makes the solution a stochastic combination of different inter-residuals in a certain order rather than a deterministic mapping, and therefore has more flexibility. Moreover, RNNs use hidden states to summarize the modifications in the previous stages. In R3L, we assume that the action only depends on the current state input, so no hidden states are needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Benchmarking R3L with R3N</head><p>Although R3L is connected to the existing learning based methods, since inference in the R3L is equivalent to applying an RNN, there is a lack of RNN based methods to do a fair comparison, because usually RNNs are combined with some other techniques and involve hidden states. To achieve a fair comparison and verify how the different training methods can help R3L, we propose a simplified RNN based benchmark named residual recovery RNN (R3N) for image denoising.</p><p>In the R3N, the input image I t is encoded via EFCN to s t . Taking s t as input, a RNN block RNN(?|?R) outputs the residual res t , and I t is updated to I t+1 by adding the residual to it. The whole process is formulated as:</p><formula xml:id="formula_11">s t = EFCN(I t )</formula><p>res t = RNN(s t |?R) t = 0, 1, 2, . . . , T</p><formula xml:id="formula_12">I t+1 = I t + res t<label>(10)</label></formula><p>and the gradient for the parameters of R3N is formulated as:</p><formula xml:id="formula_13">d?R = ? ? R 1 N N i=1 (I T +1 i ? xi) 2<label>(11)</label></formula><p>where T = 5 follows the same setting as R3L.</p><p>The inference process of R3N is basically the same as R3L shown in <ref type="figure" target="#fig_0">Fig.1</ref>, but with the policy network replaced by the RNN block. The specific design of the layers in R3N and R3L is summarized in <ref type="table" target="#tab_0">Table 2</ref>. The numbers in the table denote the filter size, dilation factor, and output channels, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Settings</head><p>We use the BSD400 dataset as training images and BSD68 <ref type="bibr" target="#b25">[26]</ref> dataset as test images to verify the performance of R3L and R3N on image denoising. We train the models with additional Gaussian noise and the noise levels are selected as ? = 25 and ? = 35. However, in practice, it is difficult to estimate the noise level exactly accurately, and the noise level can vary in a range. Hence, besides testing the performance when the estimated noise level is accurate, we also test the cases when the noise level is estimated wrongly.     <ref type="table" target="#tab_1">Table 3</ref> and <ref type="table" target="#tab_2">Table 4</ref> summarize the PSNR results of our proposed frameworks and several state-of-the-art denoising methods. It shows that though our proposed R3L does not perform the best when the estimation is accurate, it can outperform the baselines when the estimation is inaccurate. More specifically, for the cases when the estimation error is relatively large, for instance ? 10, R3L can still maintain a good denoising performance with a higher PSNR. It should be emphasised that R3L achieves these performance using far fewer parameters than DnCNN. <ref type="figure" target="#fig_2">Fig.2 and Fig.3</ref> demonstrate the visual quality of the denoised image for the different methods. It shows that when the noise level is overestimated oversmoothing is a critical issue, however, the images processed by R3L have more detailed textures remaining. More-over, when the noise level is underestimated, the denoised images from R3N and the other baselines still have obvious noise remaining and may also involve some artifacts, but R3L can remove most of the noise with no additional artifacts resulting in a more natural and better visual quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>The experimental results show that R3L is a more robust denoiser with high parameter efficiency. We explain the robustness from two points. First, compared with very deep end-to-end frameworks, R3L has less complexity, which endows R3L more generalization ability. Second, R3L is trained using a stochastic state-wise reward. The stochastic training process help R3L explore more different states and generate a more general policy than R3N, which is trained using a deterministic end-to-end loss. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In this paper, we propose a novel DRL based framework, namely R3L, to learn residual recovery for image denoising, and eventually close the gap of directly applying DRL for image denoising. We position R3L well by showing that R3L reduces to a deep RNN that is trained using the stochastic reward, and thus build the connection among R3L and the other methods. With the help of the proposed R3N, we benchmark R3L and verify how the different training method benefits R3L. The extensive experiment results reveals that R3L is a more robust denoiser with high parameter efficiency. Trained for a specific noise level, R3L can still be applied for a range of noise levels, which makes R3L a suitable framework for real-life scenarios, where the noise level estimation can be inaccurate.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The inference process of R3L at state t.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>For the model trained with noise level ? = 25, we test their performance when the noise levels are ? = 15, 20, 25, 30 and 35. For the model trained with noise level ? = 35, we test their performance when the noise levels are ? = 25, 30, 35, 40 and 45. Following the same setting, we also test the performance of BM3D, WNNM and DnCNN as baselines. The results are measured in terms of peak signal-to-noise ratio (PSNR) and shown in the next section.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Example of of denoising results using (b) BM3D<ref type="bibr" target="#b5">[6]</ref>, (c) DnCNN<ref type="bibr" target="#b11">[12]</ref>, (d) WNNM<ref type="bibr" target="#b6">[7]</ref>, (e) proposed R3N and (f) proposed R3L, with the zoom-in region highlighted. All the methods are set/trained with the estimated noise level ? = 25. The first row are results for noisy images with ? = 15 and the second row are results for noisy images with ? = 35.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Example of of denoising results using (b) BM3D<ref type="bibr" target="#b5">[6]</ref>, (c) DnCNN<ref type="bibr" target="#b11">[12]</ref>, (d) WNNM<ref type="bibr" target="#b6">[7]</ref>, (e) proposed R3N and (f) proposed R3L, with the zoom-in region highlighted. All the methods are set/trained with the estimated noise level ? = 35. The first row are results for noisy images with ? = 25 and the second row are results for noisy images with ? = 45.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Specific design of the layers</figDesc><table><row><cell></cell><cell>EFCN(I t )</cell><cell cols="4">Conv+ReLU Conv+ReLU Conv+ReLU Conv+ReLU</cell></row><row><cell cols="2">(in both R3L and R3N)</cell><cell>3x3, 1, 64</cell><cell>3x3, 2, 64</cell><cell>3x3, 3, 64</cell><cell>3x3, 4, 64</cell></row><row><cell>R3L</cell><cell>Policy Value</cell><cell cols="2">Conv+ReLU Conv+ReLU 3x3, 3, 64 3x3, 2, 64 Conv+ReLU Conv+ReLU 3x3, 3, 64 3x3, 2, 64</cell><cell cols="2">Conv+ReLU+Softmax 3x3, 1, |A| Conv 3x3, 1, 1</cell></row><row><cell></cell><cell>R3N</cell><cell cols="2">Conv+ReLU Conv+ReLU 3x3, 3, 64 3x3 ,2, 64</cell><cell cols="2">Conv+tanh 3x3, 1, 1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>The average PSNR (dB) results of different methods. All the methods are set/trained with ? = 25. The best and the second best results are highlighted in red and blue respectively.</figDesc><table><row><cell>?</cell><cell cols="3">BM3D [6] WNNM [7] DnCNN [12]</cell><cell>R3N</cell><cell>R3L</cell></row><row><cell>15</cell><cell>29.05</cell><cell>28.15</cell><cell>29.17</cell><cell cols="2">28.83 29.64</cell></row><row><cell>20</cell><cell>28.87</cell><cell>28.57</cell><cell>29.42</cell><cell cols="2">29.07 29.30</cell></row><row><cell>25</cell><cell>28.56</cell><cell>28.80</cell><cell>29.23</cell><cell cols="2">28.95 28.73</cell></row><row><cell>30</cell><cell>27.48</cell><cell>26.94</cell><cell>26.40</cell><cell cols="2">26.70 27.44</cell></row><row><cell>35</cell><cell>24.88</cell><cell>23.77</cell><cell>22.86</cell><cell cols="2">23.15 25.16</cell></row><row><cell>Average</cell><cell>27.77</cell><cell>27.25</cell><cell>27.41</cell><cell cols="2">27.34 28.05</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>The average PSNR (dB) results of different methods.All the methods are set/trained with ? = 35. The best and the second best results are highlighted in red and blue respectively.</figDesc><table><row><cell>?</cell><cell cols="3">BM3D [6] WNNM [7] DnCNN [12]</cell><cell>R3N</cell><cell>R3L</cell></row><row><cell>25</cell><cell>27.54</cell><cell>26.80</cell><cell>27.68</cell><cell cols="2">27.61 28.00</cell></row><row><cell>30</cell><cell>27.37</cell><cell>27.14</cell><cell>27.85</cell><cell cols="2">27.74 27.67</cell></row><row><cell>35</cell><cell>27.09</cell><cell>27.29</cell><cell>27.69</cell><cell cols="2">27.44 27.18</cell></row><row><cell>40</cell><cell>26.32</cell><cell>26.08</cell><cell>25.68</cell><cell cols="2">25.58 26.24</cell></row><row><cell>45</cell><cell>24.39</cell><cell>23.58</cell><cell>22.53</cell><cell cols="2">22.56 24.60</cell></row><row><cell>Average</cell><cell>26.54</cell><cell>26.18</cell><cell>26.28</cell><cell cols="2">26.19 26.74</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">When image denoising meets high-level vision tasks: a deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bihan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 27th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="842" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Connecting image denoising and high-level vision tasks via deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bihan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3695" to="3706" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards reliable object detection in noisy images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Milyaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition and Image Analysis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="713" to="722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mixture of preprocessing experts model for noise robust deep learning on resource constrained platforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesik</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minah</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Burhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyabrata</forename><surname>Mudassar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Hwan</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saibal</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mukhopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Nonlocal similarity image filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifei</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Bertozzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Analysis and Processing</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="62" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3-d transformdomain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostadin</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on image processing</title>
		<imprint>
			<biblScope unit="page" from="2080" to="2095" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Weighted nuclear norm minimization with application to image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2862" to="2869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image restoration approach using a joint sparse representation in 3d-transform domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinzheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Signal Processing</title>
		<imprint>
			<biblScope unit="page" from="307" to="323" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Structured overcomplete sparsifying transform learning with convergence guarantees and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bihan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saiprasad</forename><surname>Ravishankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Bresler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="137" to="167" />
		</imprint>
	</monogr>
	<note>International Journal of Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Toward convolutional blind denoising of real photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifei</forename><surname>Shi Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1712" to="1722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Trainable nonlinear reaction diffusion: A flexible framework for fast and effective image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1256" to="1272" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="page" from="3142" to="3155" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Non-local recurrent network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bihan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1680" to="1689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Crafting a toolchain for image restoration by deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2443" to="2452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fully convolutional network with multi-step reinforcement learning for image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryosuke</forename><surname>Furuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoto</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3598" to="3605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Path-restore: Learning network path selection for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-step reinforcement learning for single image superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Vassilo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cory</forename><surname>Heatwole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tarek</forename><surname>Taha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asif</forename><surname>Mehmood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="512" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ffdnet: Toward a fast and flexible solution for cnn-based image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="page" from="4608" to="4622" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep rnns for video denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Digital Image Processing</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">99711</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Recurrent inference machines for solving inverse problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Putzky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04008</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veda</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adria</forename><forename type="middle">Puigdomenech</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1928" to="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Policy gradient methods for reinforcement learning with function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satinder</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishay</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page" from="1057" to="1063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention, learn to solve routing problems!</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wouter</forename><surname>Kool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Herke Van Hoof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for traveling salesman problem with time windows and rejections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anatolii</forename><surname>Prokhorchuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Dauwels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fields of experts: a framework for learning image priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="860" to="867" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
