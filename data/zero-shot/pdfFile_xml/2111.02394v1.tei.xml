<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FAST: Searching for a Faster Arbitrarily-Shaped Text Detector with Minimalist Kernel Representation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
							<email>wangwenhai362@smail.nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
							<email>xieenze@hku.hk</email>
							<affiliation key="aff1">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Yang</surname></persName>
							<email>yangzhibo450@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="department">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
							<email>lutong@nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
							<email>pluo@cs.hku.hk</email>
							<affiliation key="aff1">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FAST: Searching for a Faster Arbitrarily-Shaped Text Detector with Minimalist Kernel Representation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose an accurate and efficient scene text detection framework, termed FAST (i.e., faster arbitrarily-shaped text detector). Different from recent advanced text detectors that used hand-crafted network architectures and complicated post-processing, resulting in low inference speed, FAST has two new designs. (1) We search the network architecture by designing a network search space and reward function carefully tailored for text detection, leading to more powerful features than most networks that are searched for image classification. <ref type="formula">(2)</ref> We design a minimalist representation (only has 1-channel output) to model text with arbitrary shape, as well as a GPU-parallel post-processing to efficiently assemble text lines with negligible time overhead. Benefiting from these two designs, FAST achieves an excellent trade-off between accuracy and efficiency on several challenging datasets. For example, FAST-A0 yields 81.4% F-measure at 152 FPS on Total-Text, outperforming the previous fastest method by 1.5 points and 70 FPS in terms of accuracy and speed. With Ten-sorRT optimization, the inference speed can be further accelerated to over 600 FPS.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Scene text detection is a fundamental task in computer vision with wide practical applications, such as image understanding, instant translation, and autonomous driving. With the remarkable progress of deep learning, a considerable amount of methods <ref type="bibr" target="#b20">(Long et al. 2018;</ref><ref type="bibr">Wang et al. 2019a,b;</ref><ref type="bibr" target="#b14">Liao et al. 2020)</ref> have been proposed to detect text with arbitrary shape, and the performance on public datasets is constantly being refreshed. However, we argue that the above methods still have room to improve due to two main suboptimal designs: (1) hand-crafted network architecture and (2) low-efficient post-processing.</p><p>First, most existing text detectors adopt a heavy handcrafted backbone (e.g., ResNet50 ) to achieve excellent performance at the expense of inference speed. For high efficiency, some methods <ref type="bibr" target="#b42">(Wang et al. 2019b;</ref><ref type="bibr" target="#b14">Liao et al. 2020</ref>) developed text detector based on ResNet18 , but the backbone is originally designed for image classification and may not be the best choice for text detection. Although many auto-searched lightweight networks <ref type="bibr" target="#b3">(Cai, Zhu, and Han 2018;</ref><ref type="bibr" target="#b2">Cai et al. 2019;</ref>  2019) have been presented, they only focus on image classification or general object detection, and the application to text detection is rarely considered. Therefore, how to design an efficient and powerful network specific for text detection, is a topic worth exploring. Second, the post-processing of previous works usually takes over 30% of the whole inference time <ref type="bibr">(Wang et al. 2019a,b;</ref><ref type="bibr" target="#b14">Liao et al. 2020)</ref>. Moreover, these post-processing approaches are designed to run on the CPU (see <ref type="figure">Figure 2</ref>), which are difficult to parallel with GPU resources, resulting in relatively low efficiency. Consequently, it is also important to develop a GPU-parallel post-processing for the realtime text detector.</p><p>In this work, we propose an efficient and powerful text detection framework, termed FAST (Faster Arbitrary-Shaped Text detector). As illustrated in <ref type="figure">Figure 2</ref>, the proposed FAST contains the following two main improvements to achieve high efficiency.</p><p>(1) We carefully design a NAS search space and reward function for the text detection task. The searched efficient networks are termed TextNAS, which can provide more powerful features for text detection than the net-  <ref type="bibr">(Wang et al. 2019b) 84.3 13.7</ref> 3.5 57.1 DB-R18 <ref type="bibr" target="#b14">(Liao et al. 2020)</ref> 82.8 15.3 4.7 50.0 FAST-A1-512 (Ours) 84.9 7.5 1.1 115.5 <ref type="figure">Figure 2</ref>: Overall pipelines of representative arbitrarilyshaped text detectors. Our FAST achieves significantly faster speed than previous methods, benefiting from (1) the network searched for text detection, and (2) the minimalist kernel representation with a GPU-parallel post-processing.</p><p>work searched on image classification (e.g., OFA <ref type="bibr" target="#b2">(Cai et al. 2019)</ref>).</p><p>(2) We propose a minimalist kernel representation that formulates a text line as an eroded text region surrounded by peripheral pixels. Compared to existing kernel representations <ref type="bibr">(Wang et al. 2019a,b;</ref><ref type="bibr" target="#b14">Liao et al. 2020)</ref>, our representation method not only benefits the network to predict a 1-channel output, but also enjoys a GPU-parallel postprocessing (i.e., text dilation). Combining the advantages of these designs, our method achieves the excellent trade-off between accuracy and inference speed.</p><p>To demonstrate the effectiveness of our FAST, we conduct extensive experiments on four challenging benchmarks, including Total-Text (Ch'ng and Chan 2017), CTW1500 <ref type="bibr" target="#b18">(Liu et al. 2019b</ref>), ICDAR 2015 <ref type="bibr" target="#b10">(Karatzas et al. 2015)</ref> and MSRA-TD500 <ref type="bibr" target="#b49">(Yao et al. 2012)</ref>. According to the model size, we name our text detectors from FAST-A0 to FAST-A2. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, on Total-Text, FAST-A0-448, which means scaling the shorter side of input images to 448, achieves 81.4% F-measure at 152.8 FPS, being 1.5% higher and 70 FPS faster than the previous fastest method PAN-320. Our best model FAST-A2-800 achieves 86.3% F-measure, while still keeping a real-time speed (46.0 FPS).</p><p>In summary, our contributions are as follows:</p><p>(1) We develop an accurate and efficient arbitrarilyshaped text detector, termed FAST, which is completely GPU-parallel, including network and post-processing.</p><p>(2) We design a search space and reward function specifically for text detection, and search for a series of networks friendly to text detection with different inference speeds.</p><p>(3) We propose a minimalist kernel representation with a GPU-parallel post-processing, significantly reducing its time consumption.</p><p>(4) Our FAST achieves an astonishing speed of 152.8 FPS while maintaining competitive accuracy on Total-Text. With TensorRT, it can be further accelerated to over 600 FPS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Scene Text Detection. Inspired by general object detection methods <ref type="bibr" target="#b17">Liu et al. 2016</ref>), many methods <ref type="bibr" target="#b13">(Liao et al. 2017;</ref><ref type="bibr" target="#b53">Zhou et al. 2017;</ref><ref type="bibr" target="#b28">Shi, Bai, and Belongie 2017;</ref><ref type="bibr" target="#b23">Ma et al. 2018;</ref> have been proposed to detect horizontal and multi-oriented text. However, most of them fail to locate curved text accurately. To remedy this defect, recent methods cast the text detection task as a segmentation problem. PixelLink <ref type="bibr" target="#b5">(Deng et al. 2018</ref>) separated adjacent text lines by performing text/non-text prediction and link prediction at the pixel level. SPCNet <ref type="bibr" target="#b45">(Xie et al. 2019</ref>) and Mask TextSpotter <ref type="bibr" target="#b21">(Lyu et al. 2018a)</ref> proposed to detect arbitrarily-shaped text in an instance segmentation manner. SAE <ref type="bibr" target="#b35">(Tian et al. 2019</ref>) introduced a shape-aware loss and new cluster postprocessing to distinguish adjacent text lines with various aspect ratios and small gaps. PSENet <ref type="bibr" target="#b40">(Wang et al. 2019a</ref>) proposed the progressive scale expansion (PSE) algorithm to merge multi-scale text kernels. Although the above methods achieve excellent performance, most of them run at a slow inference speed due to the complicated network architecture and cumbersome post-processing.</p><p>Real-time Text Detection. With the growing demand of real-time applications, efficient text detection attracts increasing attention. EAST <ref type="bibr" target="#b53">(Zhou et al. 2017</ref>) applied a fully convolutional network (FCN) to directly produce rotated rectangles or quadrangles for text regions, which is the first text detector that runs at 20 FPS. PAN <ref type="bibr" target="#b42">(Wang et al. 2019b)</ref> and DB <ref type="bibr" target="#b14">(Liao et al. 2020</ref>) are two representative real-time text detectors, both of which adopted a lightweight backbone (i.e., ResNet18 ) to speedup inference. For post-processing, PAN developed a learnable post-processing algorithm, namely pixel aggregation (PA), to improve the accuracy by using the predicted similarity vectors. DB proposed the box formation process, which utilized the Vatti clipping algorithm <ref type="bibr" target="#b36">(Vatti 1992)</ref> to dilate the predicted text kernels. Although these methods have simplified the text detection pipeline compared to previous methods <ref type="bibr" target="#b40">(Wang et al. 2019a;</ref><ref type="bibr" target="#b20">Long et al. 2018</ref>), real-time text detection is still room for improvement, due to sub-optimal hand-crafted network architecture and CPU-based post-processing.</p><p>Neural Architecture Search. Recently, owing to the Neural Architecture Search (NAS) techniques, there has been a significant change in designing neural networks. Many autosearched efficient networks, such as Proxyless <ref type="bibr" target="#b3">(Cai, Zhu, and Han 2018)</ref>, EfficientNet (Tan and Le 2019), OFA <ref type="bibr" target="#b2">(Cai et al. 2019)</ref>, and MobileNetV3 , play increasingly important roles in industry and the research community. Despite these developments, these NAS-based models are mainly limited to a few tasks such as image classification and general object detection, leading to the weak generalization ability in other tasks. To compensate for these drawbacks, many researchers explore applying NAS methods to their specific domains, including semantic segmentation <ref type="bibr" target="#b16">(Liu et al. 2019a</ref>), pose estimation <ref type="bibr" target="#b46">(Xu et al. 2021)</ref>, and scene text recognition <ref type="bibr" target="#b8">Hong, Kim, and Choi 2020)</ref>. However, there is still rare to extend NAS approaches to text detection.  <ref type="figure">Figure 3</ref>: Overall architecture of FAST. The over-parameterized network is divided into four stages, each of which contains L i learnable blocks, for architecture search of text detection. The multi-level features from the network are upsampled and concatenated as the final feature map F , which is used to predict text kernels. The GPU-parallel post-processing (i.e., text dilation) is applied to reconstruct complete text lines.</p><formula xml:id="formula_0">(a) Input Image ? (e) Text Kernel (f) Text Region ? !"# ? $"% (d) Stage i Conv 3?3 Input Output Conv 3?3 Conv 1?3 Conv 3?1 Identity (c) Learnable Block ( ' ?) 2 ! ? 2 ! ? !"# 2 !$# ? 2 !$# ? ! Text Dilation Forward Backward ? Upsample &amp; Concat (b) Over-parameterized</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed Method</head><p>Overall Architecture</p><p>As illustrated in <ref type="figure">Figure 3</ref>, the proposed FAST contains <ref type="formula" target="#formula_1">(1)</ref> an over-parameterized network (see <ref type="figure">Figure 3</ref>(b)) with multiple learnable blocks (see <ref type="figure">Figure 3</ref>(c)) for architecture search of text detection; and (2) a GPU-parallel differentable postprocessing to rebuild complete text line (see <ref type="figure">Figure 3</ref>(f)) from text kernel (see <ref type="figure">Figure 3</ref>(e)).</p><p>In the inference phase, we first feed the input image of H ?W ?3 into the searched network, and obtain multi-level features, which are 1/4, 1/8, 1/16, 1/32 of the original image resolution. Then, we reduce the dimension of each feature map to 128 via 3?3 convolution, and these feature maps are upsampled and concatenated via the function C(?), to obtain the final feature map F , whose shape is H/4?W/4? 512 (see <ref type="figure">Figure 3</ref>(d)). After that, the final feature map F passes through 2-layer convolutions to perform text kernel segmentation. Finally, we rebuild the complete text regions via the text dilation process with negligible time overhead, as shown in <ref type="figure">Figure 3</ref>(e)(f).</p><p>During training, we perform architecture search for text detection based on ProxylessNAS <ref type="bibr" target="#b3">(Cai, Zhu, and Han 2018)</ref>. We stochastically sample subnets (i.e., binary gates) from the over-parameterized network for each batch of data, and use loss functions L ker and L tex to optimize the text kernel predicted by network and the text region generated by post-processing, respectively. During search, we resample the subnets, calculate rewards according to the accuracy and inference speed, and then use a reinforce-based strategy to update the architecture parameters. The two updating steps are performed in an alternative manner. Once the search of architecture is finished, we can prune redundant paths and obtain the final architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neural Architecture Search for Text Detection</head><p>Search Space. Following ProxylessNAS <ref type="bibr" target="#b3">(Cai, Zhu, and Han 2018)</ref>, we build an over-parameterized network for text detection architecture search. As shown in <ref type="figure">Figure 3</ref>(c), each stage of the network is comprised of a stride-2 convolution and L i learnable blocks, where the 3?3 convolution with stride 2 is used for downsampling feature maps, and each learnable block consists of a set of candidate operations, from which the most appropriate one is selected as the final operation after NAS.</p><p>Specifically, we present a layer-level candidate set, defined as {conv3?3, conv1?3, conv3?1, identity}.</p><p>As the 1?3 and 3?1 convolutions have asymmetric kernels, they can help capture the features of extreme aspect-ratio text lines. Besides, the identity operator indicates that a layer is skipped, which is used to control the depth and inference speed of the model. In summary, because there are a total of L = L 1 + L 2 + L 3 + L 4 learnable blocks, and each of them has four candidates, the size of the search space is 4 L .</p><p>Reward Function. In addition to the search space, we design a customized reward function R(?), to search network architectures for efficient text detection. Specifically, given a model m, we define the reward function as:</p><formula xml:id="formula_1">R(m) = (IoU k (m) + ?IoU t (m)) ? FPS(m) T w ,<label>(1)</label></formula><p>where IoU k (m) and IoU t (m) denote the intersection-overunion (IoU) metric of the predicted text kernels and text regions, respectively. ? is the weight of IoU t (m), which is empirically set to 0.5. Besides that, FPS(m) means the inference speed of the entire text detector measured on the GPU with batch size 1, and T is the target inference speed. w is a hyper-parameter to balance the accuracy and inference speed, which is set to 0.1 following ProxylessNAS <ref type="bibr" target="#b3">(Cai, Zhu, and Han 2018)</ref>.</p><p>Discussion. Our method is different from existing works on NAS in two main aspects:</p><p>(1) We introduce asymmetric convolution  into the search space, to capture the features of ex- treme aspect-ratio text lines, while most existing NAS methods adopt MBConv <ref type="bibr" target="#b27">(Sandler et al. 2018)</ref> as the building block, do not consider the geometric characteristics of text.</p><p>(2) We propose a specialized reward function, which considers both the performance of text kernel and text region, achieving effective architecture search for text detection. Because most previous reward functions <ref type="bibr" target="#b9">Howard et al. 2019;</ref><ref type="bibr" target="#b37">Wang et al. 2020a</ref>) are designed for image classification or general object detection, are not suitable for arbitrarily-shaped text detection.</p><p>In addition, we adopt ProxylessNAS <ref type="bibr" target="#b3">(Cai, Zhu, and Han 2018)</ref> as the search algorithm in this work. We consider that whether the NAS algorithm needs to be redesigned for text detection, is an interesting topic that can be further explored in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Minimalist Kernel Representation</head><p>Definition. As illustrated in <ref type="figure" target="#fig_1">Figure 4</ref>, our representation method formulates a given text line as an eroded text region (i.e., text kernel) with peripheral pixels. Compared to the existing kernel representations <ref type="bibr">(Wang et al. 2019a,b;</ref><ref type="bibr" target="#b14">Liao et al. 2020)</ref>, our representation method has two main differences as follows:</p><p>(1) Because our text kernel is generated by the morphological erosion operation, it can be approximatively restored to complete text region by its reverse operation (i.e., dilation). Moreover, both erosion and dilation can be easily implemented in PyTorch with GPU acceleration.</p><p>(2) Our representation method only requires the network to predict a 1-channel output, which is simpler than previous methods that need multi-channel output, as shown in <ref type="figure">Figure  2</ref>. To our knowledge, it may be the simplest kernel representation for arbitrarily-shaped text detection.</p><p>Label Generation. To learn this representation, we need to generate labels for text kernels and text regions. Specifically, for a given text image, the label of text regions can be directly produced by filling the bounding boxes, which is denoted as G tex (see <ref type="figure" target="#fig_1">Figure 4(b)</ref>). Note that G tex is a binary image, applying an erosion operator with s?s kernel to G tex , the peripheral pixels of text regions will be converted to non-text pixels. We take this result as the label for text kernels and denote it as G ker (see <ref type="figure" target="#fig_1">Figure 4(c)</ref>).</p><p>Post-Processing. Based on the proposed representation method, we develop a GPU-parallel post-processing, termed text dilation, to recover complete text lines with negligible time overhead. The pseudo code is shown in Algorithm 1, in which we utilize the max-pooling function with s ? s kernel to implement the dilation operator equivalently. During training, for a given prediction of text kernels, we directly apply the dilation operator to rebuild whole text regions. Since this step is differentiable, we can supervise both text kernels and text regions, as shown in <ref type="figure">Figure 3</ref>. In the inference phase, we first binarize the predicted text kernels, and implement a GPU-accelerated Connected Components Labeling (CCL) algorithm <ref type="bibr" target="#b0">(Allegretti, Bolelli, and Grana 2019)</ref> to distinguish different text kernels. Finally, we apply the dilation operator to reconstruct the complete text lines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Function</head><p>The loss function of our FAST can be formulated as:</p><formula xml:id="formula_2">L = L ker + ?L tex ,<label>(2)</label></formula><p>where L ker and L tex are losses for text kernels and text regions. Following common practices <ref type="bibr">(Wang et al. 2019a,b)</ref>, we apply Dice loss <ref type="bibr" target="#b24">(Milletari et al. 2016</ref>) to supervise the network. Therefore, L ker and L tex can be expressed as follows:</p><p>L ker = 1 ? 2 x,y P ker (x, y) G ker (x, y)</p><p>x,y P ker (x, y) 2 + x,y G ker (x, y) 2 , (3)</p><formula xml:id="formula_3">L tex = 1 ? 2 x,y P tex (x, y) G tex (x, y)</formula><p>x,y P tex (x, y) 2 + x,y G tex (x, y) 2 , (4) where P (x, y) and G(x, y) represent the value of position (x, y) in the prediction and the ground-truth, respectively. In addition, we apply Online Hard Example Mining (Shrivastava, Gupta, and Girshick 2016) to L tex to ignore simple non-text regions. ? balances the importance of L ker and L tex , which is set to 0.5 in all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Datasets</head><p>Total-Text (Ch'ng and Chan 2017) is a challenging dataset for arbitrarily-shaped text detection, including horizontal, multi-oriented, and curved text lines. It contains 1,255 training images and 300 testing images, all of which are labeled with polygons at the word level. CTW1500 <ref type="bibr" target="#b18">(Liu et al. 2019b</ref>) is also a widely used dataset for arbitrarily-shaped text detection. It consists of 1,000 training images and 500 testing images. In this dataset, text lines are labeled with 14 points as polygons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv 3?3</head><p>Conv 1?3 Conv 3?1 Identity Stride-2 Conv Stage 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 1 2 3 4</p><p>Layer Conv 3?3 Conv 1?3 Conv 3?1 Identity Stride-2 Conv Stage 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9  MSRA-TD500 <ref type="bibr" target="#b49">(Yao et al. 2012</ref>) is a multi-lingual dataset that contains multi-oriented and long text lines. It has 300 training images and 200 testing images. Following the previous works <ref type="bibr" target="#b53">(Zhou et al. 2017;</ref><ref type="bibr" target="#b20">Long et al. 2018;</ref><ref type="bibr" target="#b22">Lyu et al. 2018b)</ref>, we include the 400 images of HUST-TR400 <ref type="bibr" target="#b48">(Yao, Bai, and Liu 2014)</ref> as training data. IC17-MLT <ref type="bibr" target="#b25">(Nayef et al. 2017</ref>) is a multi-language dataset that consists of 7,200 training images, 1,800 validation images, and 9,000 testing images. In this dataset, text lines are annotated with word-level quadrangles.</p><p>Implementation Details NAS Settings. During search, we consider a total of L = 36 learnable blocks in the network. Following the common practice of doubling the number of channels when halving the size of feature maps, we set C 1 , C 2 , C 3 , C 4 to 64, 128, 256, and 512, respectively. We adopt a widely-used Proxy-lessNAS <ref type="bibr" target="#b3">(Cai, Zhu, and Han 2018)</ref> as the search algorithm, and Adam <ref type="bibr" target="#b11">(Kingma and Ba 2014)</ref> with an initial learning rate of 0.01 as the optimizer. We set our target inference speed T as 100, 80, and 60 FPS, for searching TextNAS-A0, A1, and A2 respectively. To keep generalization ability, we take IC17-MLT as the training set during NAS, and construct a validation set that concludes the training sets of ICDAR 2015 and Total-Text for NAS. The entire network is trained and searched for 200 epochs with batch size 16 on 4 GPUs, which takes around 200 GPU hours on 1080Ti.</p><p>Training Settings. Following previous methods <ref type="bibr" target="#b45">(Xie et al. 2019;</ref><ref type="bibr" target="#b40">Wang et al. 2019a;</ref><ref type="bibr" target="#b6">Feng et al. 2019;</ref><ref type="bibr" target="#b44">Xie et al. 2021)</ref>, we pre-train our models on IC17-MLT for 300 epochs, in which images are cropped and resized to 640 ? 640 pixels. We then finetune the models for 600 epochs. The dilation size s is set to 9 in our experiments unless explicitly stated. All models are optimized by Adam with batch size 16 on 4 GPUs. We adopt a "poly" learning rate schedule with an initial learning rate of 1?10 ?3 . Training data augmentations include random scale, random flip, random rotation, random crop, and random blur.</p><p>Inference Settings. In the inference phase, we scale the shorter side of images to different sizes, and report the performance on each dataset. For fair comparison, we evaluate   <ref type="figure">Figure 6</ref>: Text detection F-measure and inference speed of different networks on Total-Text, where we scale the shorter side of images to 640 pixels. Our TextNAS models significantly outperform existing hand-crafted and autosearched networks.</p><p>all testing images and calculate the average speed. All results are tested with a batch size of 1 on one 1080Ti GPU and a 2.20GHz CPU in a single thread unless explicitly stated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>Comparison with Hand-Crafted Networks. We first study the differences between our TextNAS and representative hand-crafted networks, such as ResNets ) and VGG16 <ref type="bibr" target="#b30">(Simonyan and Zisserman 2014)</ref>. Without loss of generality, we show the searched architecture of TextNAS-A2 in <ref type="figure" target="#fig_2">Figure 5</ref>, from which we can make the following observations: (1) Asymmetric convolutions are the dominant operators in our network, which facilitate the detection of text lines with extreme aspect ratios. (2) TextNAS-A2 tends to stack more convolutions in the shallow stages (i.e., stage 1 and 2), which helps to capture richer low-level characteristics, such as colors, textures, and edges.</p><p>As shown in <ref type="figure">Figure 6</ref>, TextNAS achieves a better trade-off between accuracy and inference speed than previous models by a significant margin. In addition, our TextNAS-A0, A1, and A2 have 6.8M, 8.0M, and 8.9M parameters respectively, which are more parameter-efficient than ResNets ) and VGG16 (Simonyan and Zisserman 2014), but slightly larger than PVTv2-B0 <ref type="figure" target="#fig_0">(Wang et al. 2021b,a)</ref>. These results demonstrate that TextNAS models are effective for text detection on the GPU device.</p><p>Comparison with Other NAS Networks. For fair comparison, all models are pre-trained on IC17-MLT and finetuned on Total-Text. As shown in <ref type="figure">Figure 6</ref>, our TextNAS models outperform existing NAS networks in terms of both accuracy and inference speed, including Proxyless-GPU <ref type="bibr" target="#b3">(Cai, Zhu, and Han 2018)</ref>, OFA-1080Ti-12ms <ref type="bibr" target="#b2">(Cai et al. 2019</ref>), MobileNetV3 , and EfficientNet-B0 (Tan and Le 2019). For example, TextNAS-A1 achieves 85.2% F-measure at 85.3 FPS, being 1.0% more accurate and 1.5? faster than OFA-1080Ti-12ms. A major reason is that these NAS networks are mainly searched for image classification, and the generalization ability on other tasks is not robust. Therefore, designing the search space and reward function for text detection is meaningful and necessary.</p><p>Upper Bound of Minimalist Representation. We verify the upper bound of our text representation method by calculating the F-measure of text lines recovered from the groundtruth text kernels. The verification results of text kernels under different erosion sizes s are shown in <ref type="table" target="#tab_5">Table 1</ref>, where we see that the best F-measure is approaching 1 when s ? 7, indicating that this concern is unnecessary.</p><p>Influence of the Dilation Size. In this experiment, we study the effect of the dilation size s (equal to the erosion size) based on our FAST-A2 model. We scale the shorter sides of images in Total-Text and ICDAR 2015 to 640 and 736 pixels, respectively. As shown in <ref type="table" target="#tab_5">Table 1</ref>, the F-measure on both datasets drops when the dilation size is too small. Empirically, we set the dilation size s to 9 by default. Note that if the size of the shorter side (denoted as S) is changed, the dilation size s should be updated proportionally to obtain the best performance:</p><formula xml:id="formula_4">s new = Round(S new ? s default /S default ).<label>(5)</label></formula><p>Here, Round(?) is a function to round the decimal portion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparisons with State-of-the-Art Methods</head><p>Curve Text Detection. To show the advantages of FAST in detecting curved text, we compare it with existing stateof-the-art methods on the Total-Text and CTW1500 datasets, and report the results in <ref type="table" target="#tab_6">Table 2 and Table 3</ref>. On Total-Text, FAST-A0-448 yields an F-measure of 81.4% at 152.8 FPS, which is faster than all previous methods. Our FAST-A1-512 outperforms the real-time text detector DB-R18 <ref type="bibr" target="#b14">(Liao et al. 2020</ref>) by 2.1% in F-measure and  <ref type="bibr">Total-Text (Ch'ng and Chan 2017)</ref>. The suffix of our method means the size of the shorter side. "*" indicates the results from <ref type="bibr" target="#b20">(Long et al. 2018)</ref>. "Ext." denotes external data. "P", "R", and "F" indicate precision, recall, and F-measure, respectiely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Ext. P R F FPS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-real-time Methods</head><p>TextSnake <ref type="bibr" target="#b20">(Long et al. 2018</ref><ref type="bibr">) 82.7 74.5 78.4 12.4 TextField (Xu et al. 2019</ref> 81.2 79.9 80.6 -CRAFT <ref type="bibr" target="#b1">(Baek et al. 2019)</ref> 87.6 79.9 83.6 4.8 LOMO  88.6 75.7 81.6 4.4 SPCNet <ref type="bibr" target="#b45">(Xie et al. 2019)</ref> 83.0 82.8 82.9 4.6 PSENet <ref type="bibr" target="#b40">(Wang et al. 2019a)</ref> 84.0 78.0 80.9 3.9 ContourNet <ref type="bibr">(Wang et al. 2020b) -86.9 83.9 85.4 3.8</ref> Real-time Methods EAST <ref type="bibr" target="#b53">(Zhou et al. 2017)</ref> -50.0 * 36.2 * 42.0 * -DB-R50 <ref type="bibr" target="#b14">(Liao et al. 2020)</ref> 87.1 82.5 84.7 32.0 DB-R18 <ref type="bibr" target="#b14">(Liao et al. 2020)</ref> 88.3 77.9 82.8 50.0 PAN <ref type="bibr" target="#b42">(Wang et al. 2019b)</ref> 89  <ref type="table">Table 3</ref>: Detection results on CTW1500 .</p><p>The suffix of our method means the length of the shorter side. Results with "*" are collected from . "Ext." denotes external data. "P", "R", and "F" indicate precision, recall, and F-measure, respectiely.   <ref type="bibr" target="#b10">(Karatzas et al. 2015)</ref>. The suffix of our method means the size of the shorter side. "Ext." denotes external data. "P", "R", and "F" indicate precision, recall, and F-measure, respectiely. while still keeping a fast inference speed (46.0 FPS). Similar conclusions are also on CTW1500. For example, the inference speed of FAST-A0-512 is 129.1 FPS, which is at least 2.3? faster than prior works, while the F-measure is still very competitive (81.5%). The best F-measure of our method is 83.7%, which is as same as PAN <ref type="bibr" target="#b42">(Wang et al. 2019b</ref>), but our method runs at a faster speed (66.5 FPS vs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="39.8">FPS).</head><p>Oriented Text Detection. We evaluate the effectiveness of FAST in detecting oriented text lines on ICDAR 2015. From <ref type="table" target="#tab_8">Table 4</ref>, we can observe that our fastest model FAST-A0-736 reaches 60.9 FPS and maintains a competitive F-measure of 81.9%. Compared with PAN <ref type="bibr" target="#b42">(Wang et al. 2019b</ref>), FAST-A2-896 surpasses it by 2.4% in F-measure and is more efficient (31.8 FPS vs. 26.1 FPS). Because ICDAR 2015 contains many small text lines, previous methods <ref type="bibr" target="#b45">(Xie et al. 2019;</ref><ref type="bibr" target="#b40">Wang et al. 2019a</ref>) always adopt high-resolution images to ensure detection performance. With this setting, FAST-A2-1280 achieves an F-measure of 87.0%, which outperforms PSENet <ref type="bibr" target="#b40">(Wang et al. 2019a</ref>) by 1.3% and runs 9.8? faster.</p><p>Long Straight Text Detection. FAST is also robust for long straight text detection. As shown in <ref type="table" target="#tab_10">Table 5</ref>, on MSRA-TD500, FAST-A1-736 and FAST-A2-736 achieve the Fmeasure of 86.1% and 86.7% respectively, outperforming all previous works with a significant margin. More notably, FAST-A0-512 runs at 137.2 FPS with 84.4% F-measure, being 75 FPS faster and 1.6% better than the previous fastest method DB-R18 <ref type="bibr" target="#b14">(Liao et al. 2020)</ref>. Some qualitative text detection results are shown in <ref type="figure" target="#fig_5">Figure 7</ref>.  <ref type="bibr" target="#b49">(Yao et al. 2012)</ref>. The suffix of our method means the size of the shorter side. "Ext." denotes external data. "P", "R", and "F" indicate precision, recall, and F-measure, respectiely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Ext. P R F FPS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-real-time Methods</head><p>PixelLink <ref type="bibr" target="#b5">(Deng et al. 2018</ref>) -83.0 73.2 77.8 3.0 Corner <ref type="bibr" target="#b22">(Lyu et al. 2018b)</ref> 87.6 76.2 81.5 5.7 TextSnake <ref type="bibr" target="#b20">(Long et al. 2018)</ref> 83.2 73.9 78.3 1.1 CRAFT <ref type="bibr" target="#b1">(Baek et al. 2019)</ref> 88.2 78.2 82.9 8.6 DRRG  88 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this work, we proposed FAST, a faster arbitrarily-shaped text detector. To achieve high efficiency, we designed a search space and reward function tailored for text detection, and searched for a series of efficient networks friendly to text detection. Moreover, we presented a minimalist kernel representation, as well as a GPU-parallel post-processing, making our models can completely run on the GPU. Equipped with the two designs, our FAST achieves a significantly better trade-off between accuracy and inference speed than prior arts. We hope our method could serve as a cornerstone for text-related applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Text detection F-measure and inference speed of different text detectors on Total-Text (Ch'ng and Chan 2017). Our FAST models enjoy faster inference speed and better accuracy than counterparts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Label generation of the minimalist kernel representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Searched architecture of TextNAS-A2. The four nodes in each column represent a learnable block, and the black arrows indicate the selected operations. TextNAS-A0 and TextNAS-A1 are shown in the supplementary materials.ICDAR 2015<ref type="bibr" target="#b10">(Karatzas et al. 2015)</ref> is one of the challenges of the ICDAR 2015 Robust Reading Competition. It focuses on multi-oriented text in natural scenes and contains 1,000 training images and 500 testing images. The text lines are labeled by quadrangles at the word level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>.3 81.0 85.0 39.6 PAN++ (Wang et al. 2021c) 89.9 81.0 85.3 38.3 FAST-A0-448 (Ours) 88.2 75.5 81.4 152.8 FAST-A1-512 (Ours) 90.4 80.0 84.9 115.5 FAST-A2-512 (Ours) 89.3 81.8 85.4 93.2 FAST-A2-640 (Ours) 90.5 81.4 85.7 67.5 FAST-A2-800 (Ours) 90.5 82.5 86.3 46.0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative text detection results of FAST on Total-Text (Ch'ng and Chan 2017), CTW1500, ICDAR 2015<ref type="bibr" target="#b10">(Karatzas et al. 2015)</ref> and MSRA-TD500<ref type="bibr" target="#b49">(Yao et al. 2012)</ref>. More examples are provided in the supplementary materials.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 :</head><label>1</label><figDesc>Ablation studies of the erosion/dilation size s.</figDesc><table><row><cell></cell><cell>Dataset</cell><cell cols="2">s = 3 s = 5 s = 7 s = 9 s = 11</cell></row><row><cell>Upper</cell><cell>Total-Text</cell><cell>95.9 97.8 99.2 99.3</cell><cell>99.0</cell></row><row><cell cols="3">Bound ICDAR 2015 81.5 92.4 99.6 99.6</cell><cell>99.5</cell></row><row><cell>Actual</cell><cell>Total-Text</cell><cell>82.1 84.5 85.5 85.7</cell><cell>85.6</cell></row><row><cell cols="3">F-measure ICDAR 2015 56.8 77.1 83.0 83.7</cell><cell>83.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Detection results on</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Detection results on ICDAR 2015</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Detection results on MSRA-TD500</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Optimized blockbased algorithms to label connected components on GPUs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Allegretti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bolelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Grana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="423" to="438" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Character region awareness for text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9365" to="9374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Once-for-All: Train One Network and Specialize it for Efficient Deployment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICML</title>
		<meeting>the International Conference on Learning Representations (ICML</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Total-text: A comprehensive dataset for scene text detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Ch&amp;apos;ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Document Analysis and Recognition (ICDAR)</title>
		<meeting>the International Conference on Document Analysis and Recognition (ICDAR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="935" to="942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pixellink: Detecting scene text via instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">TextDragon: An end-to-end framework for arbitrary shaped text spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9076" to="9085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Memory-efficient models for scene text recognition via neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-K</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (CACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (CACV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="183" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Searching for mobilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<title level="m">Proceedings of the International Conference on Document Analysis and Recognition (ICDAR)</title>
		<meeting>the International Conference on Document Analysis and Recognition (ICDAR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1156" to="1160" />
		</imprint>
	</monogr>
	<note>ICDAR 2015 competition on robust reading</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Textboxes++: A single-shot oriented scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on image processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3676" to="3690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Textboxes: A fast text detector with a single deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Realtime scene text detection with differentiable binarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11474" to="11481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rotationsensitive regression for oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5909" to="5918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="82" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Curved scene text detection via transverse and longitudinal sequence connection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="337" to="345" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02170</idno>
		<title level="m">Detecting curve text in the wild: New dataset and new solution</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Textsnake: A flexible representation for detecting text of arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mask textspotter: An end-to-end trainable neural network for spotting text with arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="67" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multioriented scene text detection via corner localization and region segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7553" to="7563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Arbitrary-oriented scene text detection via rotation proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="3111" to="3122" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahmadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V-Net</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Conference on 3D Vision (3DV)</title>
		<meeting>the Fourth International Conference on 3D Vision (3DV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Icdar2017 robust reading challenge on multi-lingual scene text detection and script identification-rrc-mlt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nayef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bizid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rigaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chazalon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Document Analysis and Recognition (ICDAR)</title>
		<meeting>the International Conference on Document Analysis and Recognition (ICDAR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1454" to="1459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Detecting oriented text in natural images by linking segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2550" to="2558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Training regionbased object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2820" to="2828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning shape-aware embedding for scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4234" to="4243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A generic solution to polygon clipping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Vatti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="56" to="63" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">NAS-FCOS: Fast neural architecture search for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11943" to="11951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13797</idno>
		<title level="m">Pvtv2: Improved baselines with pyramid vision transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12122</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Shape robust text detection with progressive scale expansion network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9336" to="9345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">PAN++: Towards Efficient and Accurate Endto-End Spotting of Arbitrarily-Shaped Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Efficient and accurate arbitrary-shaped text detection with pixel aggregation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8440" to="8449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Contournet: Taking a further step toward accurate arbitraryshaped scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11753" to="11762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Polar-Mask++: Enhanced Polar Representation for Single-Shot Instance Segmentation and Beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Scene text detection with supervised pyramid context network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9038" to="9045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">ViPNAS: Efficient Video Pose Estimation via Neural Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16072" to="16081" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Textfield: Learning a deep direction field for irregular scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5566" to="5579" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A unified framework for multioriented text detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4737" to="4749" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Detecting texts of arbitrary orientations in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1083" to="1090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Look more than once: An accurate detector for text of arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10552" to="10561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Au-toSTR: Efficient backbone search for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="751" to="767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep relational reasoning graph network for arbitrary shape text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-C</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page" from="9699" to="9708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">East: an efficient and accurate scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5551" to="5560" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
