<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">End-to-end Trainable Deep Neural Network for Robotic Grasp Detection and Semantic Segmentation from RGB</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Ainetter</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Friedrich</forename><surname>Fraundorfer</surname></persName>
						</author>
						<title level="a" type="main">End-to-end Trainable Deep Neural Network for Robotic Grasp Detection and Semantic Segmentation from RGB</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we introduce a novel, end-to-end trainable CNN-based architecture to deliver high quality results for grasp detection suitable for a parallel-plate gripper, and semantic segmentation. Utilizing this, we propose a novel refinement module that takes advantage of previously calculated grasp detection and semantic segmentation and further increases grasp detection accuracy. Our proposed network delivers state-of-the-art accuracy on two popular grasp dataset, namely Cornell and Jacquard. As additional contribution, we provide a novel dataset extension for the OCID dataset, making it possible to evaluate grasp detection in highly challenging scenes. Using this dataset, we show that semantic segmentation can additionally be used to assign grasp candidates to object classes, which can be used to pick specific objects in the scene. Source code and dataset extension are available at https:// github.com/stefan-ainetter/grasp_det_seg_cnn. arXiv:2107.05287v2 [cs.CV]  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Automated grasping is a very active field of research in robotics. The process of having a robot manipulator to successfully grasp objects in a cluttered environment is still a challenging problem. Recent research on robotic grasping led to highly accurate grasp detection in scenes with multiple objects. Yang et al. <ref type="bibr" target="#b0">[1]</ref> combine grasp detection with a CRFbased semantic model for task-oriented grasping and object detection. The works of <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> combine grasp detection and object detection to predict grasp candidates that are assigned to specific objects in the scene. They showed that object detection provides accurate information about which objects are in the scene, however little information about the object's shape is provided. This can be especially critical in highly stacked scenes, where multiple objects overlap each other. In this work, we address grasp detection for parallel-plate grippers as well as semantic segmentation, by proposing an end-to-end trainable multi-task deep neural network. We show that one single network is able to provide high quality results for grasp detection and semantic segmentation in challenging scenes. Furthermore, we propose a novel refinement module which combines information about previously calculated grasp candidates and semantic segmentation to further increase the grasp detection accuracy. At last, we propose an extension to the OCID dataset <ref type="bibr" target="#b3">[4]</ref>, by adding hand-annotated grasp candidates and class labels to each object. Evaluation of our proposed model on this OCID <ref type="bibr" target="#b0">1</ref> All authors are with the Institute of Computer Graphics and Vision, Graz University of Technology, 8010 Graz, Austria {stefan.ainetter,fraundorfer}@icg.tugraz.at</p><p>The research leading to these results has received funding from the Austrian Ministry for Transport, Innovation and Technology (BMVIT) within the ICT of the Future Programme (4th call) of the Austrian Research Promotion Agency (FFG) under grant agreement n. 864807. <ref type="figure">Fig. 1</ref>. Grasp detection and segmentation results of our proposed model for an image containing multiple graspable objects. Top left shows all predicted grasp candidates in the scene (blue lines denote parallel plates of the gripper, red lines denote opening width), top right shows the predicted semantic segmentation. Bottom left shows that combining both leads to valid grasp candidates for each object in the scene, which makes it possible to decide which object to pick. Bottom right shows the results for object detection represented by axis-aligned bounding boxes. Compared to the semantic segmentation, the bounding box representation is neither suitable to determine which grasp candidate belongs to which object, nor to determine the correct relationship between objects, as boxes highly overlap each other. dataset extension indicates high accuracy for grasp detection and segmentation in complex scenes with multiple objects. We also show that semantic segmentation can be used to assign grasp candidates to objects, which makes it possible to pick specific objects in the scene. <ref type="figure">Figure 1</ref> shows results for our novel OCID dataset extension. The main contributions of our paper are the following:</p><p>1) An end-to-end trainable deep neural network architecture for joint grasp detection and dense, pixel-wise semantic segmentation, which yields state-of-the-art performance for grasp detection. 2) A novel grasp refinement module, combining results from grasp detection and segmentation to further improve the overall grasp accuracy. 3) An extension of the OCID dataset <ref type="bibr" target="#b3">[4]</ref> for robotic grasping, by adding ground truth grasp candidates as oriented bounding boxes and class information to each object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Our work is related to prior research on object detection and semantic segmentation in robotic vision. More specific, it is related to robotic grasp detection and scene understanding for robotic manipulations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Robotic Grasp Detection</head><p>The conventional method for grasp detection uses information about object geometry, physics models and force analytics <ref type="bibr" target="#b4">[5]</ref>. With the rise of deep learning, data-driven methods <ref type="bibr" target="#b5">[6]</ref> became more common. Methods like <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> were the first to use deep neural networks and supervised learning to predict multiple grasp candidates for a single object. Methods like <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> perform grasp detection based on two-stage object detectors. This type of detectors consists of a region proposal network and a detector. After extracting features using proposals from the first stage, objects are detected in the second stage. Approaches like <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b13">[14]</ref> are single-stage detectors which divide the input image using a grid and perform detection on each cell. This usually reduces the computation time, with decreased prediction accuracy. Other methods <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> perform grasp detection on depth images as input, where accuracy is highly dependent on the quality of the input data. Kumra et al. <ref type="bibr" target="#b16">[17]</ref> use a generative architecture to predict pixel-wise grasps in the form of three images, namely quality, angle and width. Asif et al. <ref type="bibr" target="#b18">[18]</ref>, <ref type="bibr" target="#b19">[19]</ref> use an ensemble of neural networks for grasp detection and segmentation, whereas using multiple networks is usually more expensive in terms of computational cost and memory consumption. DSGD <ref type="bibr" target="#b20">[20]</ref> uses image information on different levels of the image hierarchy (global-, region-, pixel-level) for grasp detection. The works of <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> perform the tasks of object detection with reasoning and grasp detection for complex scenes with multiple objects. Other approaches <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b22">[22]</ref> use Reinforcement Learning (RL) on a real or simulated robot to perform thousands of grasp attempts and use the feedback to improve grasp detection. RL has the advantage that no labeled data is necessary for training, but with the downside of being time and hardware consuming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Semantic Segmentation</head><p>Several works use encoder/decoder based CNN architectures <ref type="bibr" target="#b23">[23]</ref>, <ref type="bibr" target="#b19">[19]</ref> or dilated convolutions <ref type="bibr" target="#b24">[24]</ref> for semantic segmentation. He et al. <ref type="bibr" target="#b25">[25]</ref> performs the task of instancespecific semantic segmentation. In robotic vision, several methods <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b27">[27]</ref> predict semantic segmentation for unknown objects. Araki et al. <ref type="bibr" target="#b28">[28]</ref> proposed a network for semantic segmentation and grasp detection for a suction cup using multi-task learning with a single deep neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROBLEM STATEMENT</head><p>Grasp detection. As proposed by Lenz et al. <ref type="bibr" target="#b7">[8]</ref>, we use the five-dimensional rectangular representation for robotic grasps. This representation consists of the location and orientation of a parallel-plate gripper before it closes on the object. A grasp candidate g is defined as</p><formula xml:id="formula_0">g = (x, y, w, h, ?),<label>(1)</label></formula><p>whereas x and y describe the center of the grasp candidate, w and h describe the width and height, and ? describes the orientation of the rotated box representation. Semantic Segmentation. Semantic segmentation is the task of assigning a class label to each pixel in the image. Note that for grasp detection of unseen objects, we define the set of semantic classes as {graspable, non-graspable}. If additional class information about the objects is available in the dataset, the set of semantic classes can be adjusted accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PROPOSED METHOD</head><p>The proposed architecture is based on <ref type="bibr" target="#b29">[29]</ref>, a framework originally used for panoptic segmentation, which we adapted for our purpose. This section provides an overview about the specific network parts, which can be seen in <ref type="figure" target="#fig_0">Figure 2</ref>. Both outputs (grasp candidates and semantic segmentation) are used as input for the grasp refinement head, which predicts refined grasp candidates with increased accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Shared Backbone</head><p>As feature extractor, we use a slightly modified ResNet-101 <ref type="bibr" target="#b30">[30]</ref> with a Feature Pyramid Network (FPN) <ref type="bibr" target="#b31">[31]</ref> on top. The modules {conv2, conv3, con4, conv5} of the ResNet-101 architecture are linked to the FPN. All Batch Normalization + ReLU layers in the original ResNet-101 structure are replaced by synchronized Inplace Activated Batch Normalization (InPlaceABNSync) <ref type="bibr" target="#b32">[32]</ref>, using LeakyReLU as activation function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Grasp Detection Branch</head><p>The grasp detection branch is based on the state-ofthe-art Faster R-CNN object detector <ref type="bibr" target="#b33">[33]</ref>, consisting of a Region Proposal Network (RPN) and a detection stage. This subsection provides details about how we modified this object detector for grasp detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Region Proposal Network:</head><p>The RPN outputs rectangular region proposals using the features of the backbone network as input. These region proposals are defined asr = (x,?,?,?) where (x,?) denote the center of the proposal in pixel coordinates and (?,?) denote the width and height dimension, respectively. Note that the region proposals are axis-aligned, without information about a possible orientation.</p><p>2) Grasp Detection Head: The grasp detection head predicts grasp candidates, whereas one candidate g is defined as described in Equation 1. Each previously calculated region proposalr is used as input for the grasp detection head. Then, ROIAlign <ref type="bibr" target="#b25">[25]</ref> is applied to extract feature maps with 14 ? 14 spatial resolution that directly correspond to the region proposals. Afterwards, average pooling with kernel size 2 is applied to each feature map, before they are fed into two fully connected (fc) layers with 1024 neurons each. After each fc layer, an InPlaceABNSync normalization layer and Leaky ReLU activation with 0.01 is applied. The results are forwarded into two sub-networks: Grasp orientation prediction. The first sub-network consists of a fc layer with 1024 neurons followed by N classes +1 output units, whereas N classes defines the number of orientation classes. Similar to <ref type="bibr" target="#b8">[9]</ref>, we discretized the grasp orientation ? into N classes = 18 intervals with equal length, where each interval is represented by its mean value. The full set of possible orientation classes is defined as C = {(1, ..., N classes )} and the additional class ? to asses the possible invalidity of a proposal. The output units provide logits for a softmax layer that represents the probability distribution over all possible orientation classes. The probability associated to orientation class c ? C is used as score function s c . Bounding box prediction. The second sub-network consists of a fc layer with 1024 neurons followed by 4N classes output units. The output units encode class-specific correction factors (t c x , t c y , t c w , t c h ) for each input proposalr. These correction factors, and the orientation information given by the score function s c , can then directly be used to calculate a grasp candidate g.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Segmentation Branch</head><p>The segmentation head takes the output of the first four scales of the FPN as input. Each scale of FPN features are fed to an independent Mini-DeepLab module <ref type="bibr" target="#b29">[29]</ref>, which makes it possible to capture global structures of the input image with relatively few memory consumption. The output of each Mini-DeepLab module is then up-sampled to <ref type="bibr">1 4</ref> of the input image size. Afterwards, all feature maps are concatenated and fed to a 1 ? 1 convolution with S classes output channels, representing the probability distribution over all semantic classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Grasp Refinement Head</head><p>The grasp refinement head (see <ref type="figure" target="#fig_1">Figure 3</ref>) takes as input the previously predicted grasp candidates from the grasp detection branch and a semantic probability map from the segmentation branch. First, we fuse both information by cropping the area of the grasp candidate from the probability map. The cropped probability map and the original probability map are then stacked together and used as input for a Multilayer Perceptron (MLP). Note that this approach enables us to combine geometric information of grasp candidates with information about object shape. The MLP is a two-layer fc network with five output neurons. After each fc layer, an InPlaceABNSync normalization layer and Leaky ReLU activation with 0.01 is applied. The output represents refined correction factors (t g x , t g y , t g w , t g h , t g ? ) for each input grasp candidate g, which are then used to calculate the refined grasp candidateg. x , t g y , t g w , t g h , t g ? ) which can directly be used to calculate the refined grasp candidate. Note that this operation is performed simultaneously for N grasp candidates, leading to an (N ? 2 ? H ? W ) dimensional input for the MLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. TRAINING LOSSES</head><p>For simultaneously learning the tasks of grasp detection, instance segmentation and grasp candidate refinement, we define the composite loss function L as</p><formula xml:id="formula_1">L = ? grasp L grasp + ? sem L sem + ? ref ine L ref ine ,<label>(2)</label></formula><p>with the grasp detection loss L grasp , the semantic segmentation loss L sem and the grasp refinement loss L ref ine . All parts are weighted with a specific hyperparameter ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Grasp Detection Loss</head><p>The grasp detection loss L grasp is defined as</p><formula xml:id="formula_2">L grasp = L RP N + L box + L rot ,<label>(3)</label></formula><p>where L RP N defines the loss for training the RPN, L box defines the regression loss for the box coordinates and L rot defines the classification loss for the grasp orientation. We refer to <ref type="bibr" target="#b25">[25]</ref> for additional information about L RP N . The grasp orientation loss L rot is defined as</p><formula xml:id="formula_3">L rot = ? 1 |R| r?R+ log s cr r ? 1 |R| r?R? log s ? r .<label>(4)</label></formula><p>Note that R = R + ?R ? is the set of valid and invalid region proposals generated using the RPN. The score function s cr r defines the probability that the region proposalr belongs to the ground truth orientation class c r , and s ? r defines the probability that the region proposal is invalid. For bounding box regression we use the loss L box defined as</p><formula xml:id="formula_4">L box = i?{x,y,h,w} smooth L1 (t c i ? t * i ),<label>(5)</label></formula><p>with the smooth L1 norm <ref type="bibr" target="#b33">[33]</ref> defined as</p><formula xml:id="formula_5">smooth L1 (z) = 0.5z 2 , if |z| &lt; 1 |z| ? 0.5, otherwise.<label>(6)</label></formula><p>Note that the parameters t c i are the output of the bounding box prediction. The correction factors t * i <ref type="bibr" target="#b34">[34]</ref> are defined as:</p><formula xml:id="formula_6">t * x = (x * ?x)/?, t * y = (y * ??)/? t * w = log(w * /?), t * h = log(h * /?),<label>(7)</label></formula><p>whereas variables (x,?,?,?) and (x * , y * , w * , h * ) are parameters from the region proposalr and ground truth grasp candidate respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Segmentation Loss</head><p>We denote Y = {1, ..., S classes } as the set of semantic segmentation classes. The per-image segmentation loss is a weighted per-pixel loss <ref type="bibr" target="#b29">[29]</ref> defined as</p><formula xml:id="formula_7">L sem = ? j,k w j,k log P j,k (Y j,k ),<label>(8)</label></formula><p>where (j, k) correspond to the pixel position in the image. Let Y j,k ? Y be the semantic segmentation ground truth and P j,k (s) the predicted probability for the same pixel to be assigned to a semantic class s ? Y, respectively. The weights w j,k are computed to implement a pixel-wise hard negative mining, which selects the 25% of the lowest predicted probabilities P j,k for all (j, k) using w j,k = 4 W H , and w j,k = 0 otherwise. The spatial resolution of the input image is given as (W ? H).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Grasp Refinement Loss</head><p>The grasp refinement loss L ref ine is defined as</p><formula xml:id="formula_8">L ref ine = i?{x,y,h,w,?} smooth L1 (t g i ?t * i ).<label>(9)</label></formula><p>Compared to L box in Equation 5, the smooth L1 loss is additionally calculated for the orientation parameter ?. Note that the parameters t g i are the output of the grasp refinement head. The correction factorst * i were adapted based on <ref type="bibr" target="#b35">[35]</ref>, to take the orientation of the previously predicted grasp candidate g into account:</p><formula xml:id="formula_9">t * x = 1 w ((x * ? x) cos ? + (y * ? y) sin ?) t * y = 1 h ((y * ? y) cos ? ? (x * ? x) sin ?) t * w = log(w * /w),t * h = log(h * /h) t * ? = 1 ? ((? * ? ?) mod ?).<label>(10)</label></formula><p>Note that variables (x, y, w, h, ?) and (x * , y * , w * , h * , ? * ) are parameters from the grasp candidate g and the ground truth grasp candidate respectively. The mod operation is used to adjust the orientation offsett * ? to [0, ?), which corresponds to the interval of possible values for the grasp orientation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTS AND RESULTS</head><p>We assess the benefits of our proposed method on wellknown grasping datasets, namely Cornell <ref type="bibr" target="#b7">[8]</ref> and Jacquard <ref type="bibr" target="#b36">[36]</ref>. Additionally, we evaluate our approach on our novel OCID <ref type="bibr" target="#b3">[4]</ref> extension for robotic grasping. We compare our proposed architecture to state-of-the-art methods which also only use RGB data as input modality, to ensure fair comparison. Due to the fact that our grasp detection and segmentation branches are independent, we evaluate several model configurations throughout our experiments, with the following terminology: Detection (ours) refers to the model using only the grasp detection branch, Det Seg (ours) consists of both grasp detection and segmentation branch, and Det Seg Refine (ours) consists of the full architecture (see <ref type="figure" target="#fig_0">Figure 2</ref>). We initialize the backbone network with pretrained ImageNet <ref type="bibr" target="#b37">[37]</ref> weights and freeze the parameters of the first two network modules {conv1, conv2} during all training runs. All training and evaluation runs were performed using a single Nvidia GeForce RTX 2080 Ti graphics card. Unless otherwise stated, results from other methods are taken from the corresponding paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evaluation Metric</head><p>For grasp detection, we use the popular Jaccard index as accuracy measurement. A grasp candidate is reported correct if:</p><p>1) the angle difference between predicted grasp candidate g p and ground truth grasp candidate g gt is within 30 ? and 2) the Intersection over Union (IoU) of them is greater than 0.25 which means</p><formula xml:id="formula_10">IoU = |g p ? g gt | |g p ? g gt | &gt; 0.25<label>(11)</label></formula><p>For semantic segmentation, we report the IoU between predicted and ground truth segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Cornell dataset</head><p>The Cornell grasp dataset <ref type="bibr" target="#b7">[8]</ref> contains 885 RGB-D images of 240 graspable objects, whereas each image has a dimension of 640 ? 480px. Correct grasp candidates are handannotated using the rectangular grasp representation. Like previous work, we used 5-fold cross-validation and report the mean grasp detection accuracy. We performed an image-wise split into training and test set, whereas image-wise means that images are split randomly without considering which object is in the image. Due to the fact that this dataset does not provide ground truth segmentation data, we are only able to evaluate the model Detection (ours) in this experiment. Data Preprocessing and Data Augmentation. Because of the relatively small size of this dataset, we used data augmentation to enlarge the dataset during training. Each image was center cropped to 351?351px. Next, we applied random rotation between 0 ? and 360 ? , and randomly translated the image in x and y direction independently up to 50px. Training schedule. The network was trained end-to-end, using a learning rate of 0.04 with weight decay of 0.0001, a momentum factor of 0.9 with enabled nesterov momentum and SGD as optimizer. We used a batch size of 12 during training.</p><p>Quantitative Results. <ref type="table" target="#tab_0">Table I</ref> shows the results on the Cornell grasp dataset. Note that our implementation achieves state-of-the-art results for accuracy compared to other methods using RGB images as input. Additionally, our model achieves a high FPS rate during inference, making it well suited for real-time applications. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Jacquard Dataset</head><p>The Jacquard dataset <ref type="bibr" target="#b36">[36]</ref> contains 54k synthetic RGB-D images using 11k different objects. As ground truth, the dataset contains automatically generated grasp candidates, as well as ground truth semantic segmentation, which enables us to evaluate our multi-task, grasp detection and segmentation approach. We made an image-wise split of the dataset, using 95% of the data as training set, and the remaining images as test set. Data Preprocessing. Due to the relatively large image size of 1024 ? 1024px , we decided to downsample each image by the factor of 2. No data augmentation was needed due to the high number of samples in the dataset. Training schedule. For this experiments, the networks Detection (ours) and Det Seg (ours) were trained end-toend using a learning rate of 0.023 with weight decay of 0.0001, a momentum factor of 0.9 with enabled nesterov momentum and SGD as optimizer. We used a batch size of 8 during training and weighting factors of ? grasp = 1.0, ? sem = 0.8 and ? ref ine = 0.8 for loss balancing. For training Det Seg Refine (ours), we started from the pre-trained version Det Seg (ours), freezing all backbone parameters while re-training all other parts. We changed the learning rate to 0.008 and the batch size to 4. Although Det Seg Refine (ours) could also be trained from scratch, we believe that starting from a well-known baseline makes it easier to demonstrate the effect of the proposed extension and lead to results that are easier to interpret.</p><p>Quantitative Results. Our experiments on the Jacquard dataset lead to several findings: a) All of our model configurations deliver state-of-theart performances: <ref type="table" target="#tab_0">Table II</ref> shows the results on the Jacquard grasp dataset according to grasp accuracy, segmentation accuracy and computational speed. All our models achieve higher scores for grasp detection accuracy compared to other state-of-the-art methods using RGB images as input. Furthermore, the inference speed indicates that all our models are fast enough for real-time application. b) Multi-task learning is not automatically beneficial: Focusing on the results of Det Seg (ours), we see that multitask learning for grasp detection and segmentation has in our case no beneficial effect on grasp accuracy. However, given the fact that these two tasks are quite different (grasp detection focuses on very small parts of the object whereas segmentation takes the whole scene into account) it is already remarkable that both models perform nearly equally well.</p><p>c) The grasp refinement head significantly increases orientation accuracy: <ref type="table" target="#tab_0">Table II</ref> shows that using Det Seg Refine (ours) increases the grasp accuracy by a small margin compared to Det Seg (ours). Due to the fact that the Jaccard index criteria are quite weak (see Subsection VI-A), Det Seg (ours) is accurate enough to pass them in most cases. However, if we apply more strict criteria, we can see a significant benefit using our grasp refinement module. <ref type="table" target="#tab_0">Table III</ref> shows the results for decreased orientation tolerance. Using our proposed refinement head, we were able to outperform our baseline model by more than 16%, which highlights a significant increase of orientation accuracy (see <ref type="table" target="#tab_0">Table III</ref>, column 5 ? ). <ref type="table" target="#tab_0">Table IV</ref> shows the results for different IoU criteria. As there is no significant difference for our two models, this experiment indicates that multiple iterations of bounding box regression do not increase the performance (as stated in <ref type="bibr" target="#b34">[34]</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. OCID Dataset with Grasp-Extension</head><p>OCID <ref type="bibr" target="#b3">[4]</ref> contains diverse settings of objects, background, context, sensor to scene distance, viewpoint angle and lighting conditions. The original purpose of this dataset is to evaluate semantic segmentation methods in scenes with increasing amount of complexity. OCID contains RGB-D data with automatically annotated segmentation. For our proposed dataset extension (we denote it as OCID grasp), we manually annotated the ARID10 and ARID20 subsets of OCID with valid grasp candidates for each graspable object in the scene.</p><p>Additionally, we added class information to each object, and assigned each grasp candidate to the corresponding object class. This allows us to evaluate grasp candidates for each object class in the scene. Overall, OCID grasp consists of 1763 selected images with over 11.4k segmented object masks and more than 75k hand-annotated grasp candidates, and each object is classified into one of 31 classes. <ref type="figure">Figure 4</ref> (left column) shows sample images of the dataset, indicating the high complexity of the scenes. Data Preprocessing and Augmentation. We applied random rotation between 0 ? and 360 ? , and randomly translated the image in x and y direction independently up to 50px as data augmentation during training. Training schedule. The network was trained end-to-end using a learning rate of 0.03 with weight decay of 0.0001, a momentum factor of 0.9 with enabled nesterov momentum and SGD as optimizer. We used a batch size of 8 during training and weighting factors of ? grasp = 1.0 and ? sem = 0.75 for loss balancing. Evaluation metrics. Due to the fact that the scenes in this dataset contain multiple graspable objects, we decided to calculate the Jaccard index for each graspable object class in the scene, as well as the segmentation IoU for each object class. The Jaccard index at object class level has the additional criteria that the center of the grasp candidate has to be located on the predicted segmentation mask of this specific class. We used 5-fold cross-validation and report the mean grasp detection and segmentation accuracy.</p><p>Quantitative &amp; Qualitative Results. We used Det Seg (ours) as network architecture for this experiment. The network outputs possible grasp candidates for each graspable object in the scene, as well as semantic segmentation for each class. We selected the best grasp candidate for each object class according to the grasp confidence score, with the constraint that the center of the grasp candidate has to be located on the segmentation mask of this object class. <ref type="table" target="#tab_3">Table V</ref> shows the results for grasp accuracy at object class level and mean IOU for instance segmentation. Both, the high grasp accuracy and segmentation IoU, indicate that our proposed model delivers highly accurate results for complex scenes. Qualitative results are shown in <ref type="figure">Figure 4</ref>. <ref type="figure">Fig. 4</ref>. Qualitative results for OCID grasp. Explanation of images from left to right: 1) raw input image; 2) predicted semantic segmentation, whereas each color represents a specific class; 3) best possible grasp for each class in the scene (blue lines denote parallel plates of the gripper, red lines denote opening width). If multiple instances of the same class are in the scene, e.g. class orange in the first and third row, only the one grasp candidate with the highest confidence will be shown. Each row refers to a separate input example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSIONS</head><p>In this work, we have proposed a novel CNN architecture for jointly producing highly accurate grasp detection and semantic segmentation, using a shared backbone network as feature extractor. We showed that our proposed grasp refinement module can be successfully used to increase the accuracy of previously predicted grasp candidates, especially in terms of grasp orientation. Using our proposed OCID grasp dataset extension, we showed high accuracy for grasp detection in complex scenes, and demonstrated how semantic segmentation can additionally be used to assign grasp candidates to specific objects. In the future, we plan to extend our approach using multi-modal input data to leverage the full extend of information provided by RGB-D grasp datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Architecture of our proposed model. Both branches for grasp detection and segmentation share the backbone network as feature extractor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Structure of the grasp refinement head. The Multilayer Perceptron (MLP) takes as input two stacked semantic probability maps with dimension (H ? W ), whereas (H, W ) defines the height and width of the probability map. The output units of the MLP represent refined correction factors (t g</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I .</head><label>I</label><figDesc>Comparison of grasp accuracy and computational speed for the Cornell dataset. All results are produced with image-wise data split and RGB as input modality using 5-fold cross-validation.</figDesc><table><row><cell>Method</cell><cell>Input</cell><cell>Grasp Accuracy (%)</cell><cell>Speed (FPS)</cell></row><row><cell>Zhou [10], Resnet-50</cell><cell>RGB</cell><cell>97.7</cell><cell>9.9</cell></row><row><cell>Zhou [10], Resnet-101</cell><cell>RGB</cell><cell>97.7</cell><cell>8.5</cell></row><row><cell>Zhang [11], Resnet-101</cell><cell>RGB</cell><cell>93.6</cell><cell>25.2</cell></row><row><cell>Park [3], Darknet-19</cell><cell>RGB</cell><cell>97.7</cell><cell>140</cell></row><row><cell>Karaoguz [12], VGG-16</cell><cell>RGB</cell><cell>88.7</cell><cell>2</cell></row><row><cell>Chu [9], Resnet-50</cell><cell>RGB</cell><cell>94.4</cell><cell>8.3</cell></row><row><cell>Kumra [17], GR-ConvNet</cell><cell>RGB</cell><cell>96.6</cell><cell>52</cell></row><row><cell>Detection (ours)</cell><cell>RGB</cell><cell>98.2</cell><cell>63</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II .</head><label>II</label><figDesc>Comparison of grasp accuracy, segmentation IoU and computational speed for the Jacquard dataset. All results are produced with image-wise data split and RGB as input modality.</figDesc><table><row><cell>Method</cell><cell>Grasp Accuracy (%)</cell><cell>Seg. IoU (%)</cell><cell>Speed (FPS)</cell></row><row><cell>Zhang[11], ROI-GD</cell><cell>90.4</cell><cell>-</cell><cell>-</cell></row><row><cell>Song [13], Resnet-101</cell><cell>91.5</cell><cell>-</cell><cell>-</cell></row><row><cell>Kumra [17], GR-ConvNet</cell><cell>91.8</cell><cell>-</cell><cell>-</cell></row><row><cell>Depierre [38]</cell><cell>85.7</cell><cell>-</cell><cell>-</cell></row><row><cell>Detection (ours)</cell><cell>92.69</cell><cell>-</cell><cell>53</cell></row><row><cell>Det Seg (ours)</cell><cell>92.59</cell><cell>95.12</cell><cell>48</cell></row><row><cell>Det Seg Refine (ours)</cell><cell>92.95</cell><cell>94.86</cell><cell>31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III .</head><label>III</label><figDesc>Comparison of grasp accuracy (in [%]) for the Jacquard dataset using different angle thresholds and an IoU threshold of 25%.Results for the methods Zhou and Depierre are taken from<ref type="bibr" target="#b38">[38]</ref>.TABLE IV. Grasp accuracy (in [%]) for the Jacquard dataset using different IoU thresholds and an angle threshold of 30 ? . Results for the methods Zhou and Depierre are taken from<ref type="bibr" target="#b38">[38]</ref>.</figDesc><table><row><cell>Method</cell><cell>30 ?</cell><cell>25 ?</cell><cell>20 ?</cell><cell>15 ?</cell><cell>10 ?</cell><cell>5 ?</cell></row><row><cell>Zhou [10]</cell><cell>81.95</cell><cell>81.76</cell><cell>81.27</cell><cell>80.23</cell><cell>77.79</cell><cell>-</cell></row><row><cell>Depierre [38]</cell><cell>85.74</cell><cell>85.55</cell><cell>85.01</cell><cell>83.65</cell><cell>80.82</cell><cell>-</cell></row><row><cell cols="2">Detection (ours) 92.69</cell><cell>92.34</cell><cell>92.08</cell><cell>91.40</cell><cell>88.12</cell><cell>56.23</cell></row><row><cell>Det Seg Refine (ours)</cell><cell>92.95</cell><cell>92.88</cell><cell>92.42</cell><cell>91.52</cell><cell>88.92</cell><cell>72.79</cell></row><row><cell cols="2">Method</cell><cell cols="3">IoU 25% IoU 30%</cell><cell>IoU 35%</cell><cell></cell></row><row><cell cols="2">Zhou [10]</cell><cell>81.95</cell><cell cols="2">78.26</cell><cell>74.33</cell><cell></cell></row><row><cell cols="2">Depierre [38]</cell><cell>85.74</cell><cell cols="2">82.58</cell><cell>78.71</cell><cell></cell></row><row><cell cols="2">Song [13]</cell><cell>91.5</cell><cell cols="2">89.7</cell><cell>87.3</cell><cell></cell></row><row><cell cols="2">Detection (ours)</cell><cell>92.69</cell><cell cols="2">91.29</cell><cell>88.99</cell><cell></cell></row><row><cell cols="2">Det Seg Refine (ours)</cell><cell>92.95</cell><cell cols="2">91.33</cell><cell>88.96</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE V .</head><label>V</label><figDesc>Results for grasp accuracy and semantic segmentation for the OCID grasp dataset. For grasp accuracy, the Jaccard index was calculated on object class level, which corresponds to finding the best possible grasp for each class.</figDesc><table><row><cell>Method</cell><cell cols="2">Grasp Accuracy (%) Seg. IoU (%)</cell></row><row><cell>Deg Seg (ours)</cell><cell>89.02</cell><cell>94.05</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Task-oriented grasping in object stacking scenes with crf-based semantic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6427" to="6434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A multitask convolutional neural network for autonomous robotic grasping in object stacking scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6435" to="6442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A single multitask deep neural network with post-processing for object detection with reasoning and robotic grasp detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Chun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7300" to="7306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Easylabel: A semi-automatic pixel-wise object annotation tool for creating robotic rgb-d datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Patten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fischinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vincze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6678" to="6684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Robotic grasping and contact: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bicchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="348" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Data-driven grasp synthesis-a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Asfour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="page" from="289" to="309" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Real-time grasp detection using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1316" to="1322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep learning for detecting robotic grasps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4-5</biblScope>
			<biblScope unit="page" from="705" to="724" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Real-world multiobject, multigrasp detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-J</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Vela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3355" to="3362" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fully convolutional grasp detection network with oriented anchor box</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7223" to="7230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Roi-based robotic grasp detection for object overlapping scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4768" to="4775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Object detection approach for robot grasp detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Karaoguz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jensfelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4953" to="4959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A novel robotic grasp detection method based on region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Computer-Integrated Manufacturing</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page">101963</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Real-time, highly accurate robotic grasp detection using fully convolutional neural network with rotation ensemble module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Chun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9397" to="9403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Orientation attentive robot grasp synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gkanatsios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chalvatzaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Maragos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05123</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Closing the Loop for Robotic Grasping: A Real-time, Generative Grasp Synthesis Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Corke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leitner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Robotics: Science and Systems (RSS)</title>
		<meeting>of Robotics: Science and Systems (RSS)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Antipodal robotic grasping using generative residual convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sahin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ensemblenet: Improving grasp detection using an ensemble of convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Asif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harrer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Graspnet: An efficient convolutional neural network for realtime grasp detection for low-powered devices</title>
	</analytic>
	<monogr>
		<title level="m">International Joint Conferences on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4875" to="4882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Densely supervised grasp detector (dsgd)</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8085" to="8093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3406" to="3413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pastor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Quillen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4-5</biblScope>
			<biblScope unit="page" from="421" to="436" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Segmenting unknown 3d objects from real depth images using mask r-cnn trained on synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danielczuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mahler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7283" to="7290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The best of both modes: Separately leveraging rgb and depth for unseen object instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning. PMLR, 2020</title>
		<imprint>
			<biblScope unit="page" from="1369" to="1378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mtdssd: Deconvolutional single shot detector using multi task learning for object detection, segmentation, and grasping detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Araki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Onishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirakawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fujiyoshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Seamless scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Colovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8277" to="8286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">In-place activated batchnorm for memory-optimized training of dnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5639" to="5647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning roi transformer for oriented object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Jacquard: A large scale dataset for robotic grasp detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Depierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dellandr?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3511" to="3516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Optimizing correlated graspability score and grasp regression for better grasp prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Depierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dellandr?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.00872</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
