<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-Region Domain Adaptation for Class-level Alignment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijie</forename><surname>Wang</surname></persName>
							<email>zhijie@vision.is.tohoku.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Information Sciences</orgName>
								<orgName type="institution">Tohoku University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Information Sciences</orgName>
								<orgName type="institution">Tohoku University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Suganuma</surname></persName>
							<email>suganuma@vision.is.tohoku.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Information Sciences</orgName>
								<orgName type="institution">Tohoku University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">RIKEN Center for AIP</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takayuki</forename><surname>Okatani</surname></persName>
							<email>okatani@vision.is.tohoku.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Information Sciences</orgName>
								<orgName type="institution">Tohoku University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">RIKEN Center for AIP</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Cross-Region Domain Adaptation for Class-level Alignment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic segmentation requires a lot of training data, which necessitates costly annotation. There have been many studies on unsupervised domain adaptation (UDA) from one domain to another, e.g., from computer graphics to real images. However, there is still a gap in accuracy between UDA and supervised training on native domain data. It is arguably attributable to class-level misalignment between the source and target domain data. To cope with this, we propose a method that applies adversarial training to align two feature distributions in the target domain. It uses a self-training framework to split the image into two regions (i.e., trusted and untrusted), which form two distributions to align in the feature space. We term this approach cross-region adaptation (CRA) to distinguish from the previous methods of aligning different domain distributions, which we call cross-domain adaptation (CDA). CRA can be applied after any CDA method. Experimental results show that this always improves the accuracy of the combined CDA method, having updated the state-of-the-art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic image segmentation is one of the fundamental problems of computer vision <ref type="bibr" target="#b17">(Minaee et al. 2021)</ref>. Methods employing neural networks have achieved great success for the problem, which presume the availability of a large amount of labeled data. As manual annotation is costly, researchers have considered using synthetic images generated by computer graphics, for which precise pixel-level annotation is readily available <ref type="bibr" target="#b19">(Richter et al. 2016;</ref><ref type="bibr" target="#b21">Ros et al. 2016;</ref><ref type="bibr" target="#b30">Wrenninge and Unger 2018)</ref>.</p><p>However, it is generally hard to apply neural networks trained with synthetic data to real images because of the distributional difference between synthetic and real images. Many studies have been conducted on domain adaptation <ref type="bibr" target="#b25">Sun and Saenko 2016)</ref> to cope with the difference known as domain shift. Among several problem settings, the one that attracts researchers' most interest is unsupervised domain adaptation (UDA) <ref type="bibr" target="#b22">(Saito et al. 2018;</ref><ref type="bibr" target="#b1">Baktashmotlagh et al. 2013)</ref>. It is to train a model using labeled data in a domain (called the source domain) so that it will work well on data in a different domain (called a target domain) for which labels are not available.</p><p>There are currently two approaches to UDA for semantic segmentation. One is adversarial training <ref type="bibr" target="#b26">(Tsai et al. 2018</ref>; <ref type="table">Table 1</ref>: The proposed method (cross-region adaptation, or CRA) can be employed after any existing method (which we call cross-domain adaptation, or CDA), always leading to performance improvement. UDA from GTA5 ? Cityscapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Base CDA +CRA ? AdaptSegNet <ref type="bibr" target="#b26">(Tsai et al. 2018)</ref> 42.4 43.4 +1.0 ADVENT <ref type="bibr" target="#b27">(Vu et al. 2019a)</ref> 43.8 46.7 +2.9 FADA <ref type="bibr" target="#b29">(Wang et al. 2020)</ref> 50.1 52.2 +2.1 IAST  52.2 54.1 +1.9 ProDA <ref type="bibr" target="#b33">(Zhang et al. 2021)</ref> 57.5 58.6 +1.1 <ref type="bibr">Vu et al. 2019a,b;</ref><ref type="bibr" target="#b29">Wang et al. 2020)</ref>, which attempts to obtain domain-invariant features by aligning the data distributions of the source and target domains. An issue with this approach is that while it may be easy to align the two distributions as a whole, it is hard to attain class-level alignment, leading to suboptimal results. The other approach is self-training, in which a teacher model trained with the labeled data in the source domain is used to generate pseudo labels of the target domain data and use them for training a student model <ref type="bibr" target="#b37">(Zou et al. 2018</ref><ref type="bibr" target="#b38">(Zou et al. , 2019</ref><ref type="bibr" target="#b13">Li, Yuan, and Vasconcelos 2019;</ref><ref type="bibr" target="#b34">Zhang et al. 2019</ref>). An issue with this approach is that pseudo labels could be inaccurate, which will lead to unsatisfactory performance. A promising direction for further improvements is to integrate adversarial training and self-training, as is attempted by recent studies <ref type="bibr" target="#b18">(Pan et al. 2020;</ref><ref type="bibr" target="#b29">Wang et al. 2020)</ref>. This paper proposes a new approach in the same direction, which applies adversarial training to align two feature distributions in the target domain. Using a self-training framework, it splits target domain images into two regions, thereby specifying the feature distributions to align.</p><p>We term this approach cross-region adaptation (CRA) to distinguish from the conventional method of aligning different domain distributions, which we will refer to as crossdomain adaptation (CDA). Its objective is to resolve classlevel misalignment between the source and target domain data; see <ref type="figure" target="#fig_0">Fig. 1</ref>. CRA can be employed after any existing CDA method, which we will show always improves the accuracy of the baseline CDA; a summary is shown in <ref type="table">Table  1</ref>.</p><p>We show through experiments the effectiveness of the proposed approach on three benchmark tasks, GTA5 ?  Cityscapes, SYNTHIA ? Cityscapes, and Synscapes ? Cityscapes. The results show that our CRA, employed upon any existing baseline CDA, improves its performance. This is also the case with the CDA method achieving the current state-of-the-art <ref type="bibr" target="#b33">(Zhang et al. 2021</ref>), meaning we have updated it.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Semantic Segmentation</head><p>Methods based on convolutional neural networks (CNNs) have been the most successful for semantic segmentation. FCN is the first fully convolutional network for the pixellevel classification task proposed in a pioneering work <ref type="bibr" target="#b14">(Long, Shelhamer, and Darrell 2015)</ref>. Later, UNet <ref type="bibr" target="#b20">(Ronneberger, Fischer, and Brox 2015)</ref> and SegNet (Badrinarayanan, Kendall, and Cipolla 2017) were proposed, which are the networks consisting of an encoder and a decoder, leading to better performance. Architectural designs have been extensively studied since then such as DeepLab <ref type="bibr" target="#b3">(Chen et al. 2017)</ref>, PSPNet <ref type="bibr" target="#b35">(Zhao et al. 2017)</ref>, UperNet <ref type="bibr" target="#b31">(Xiao et al. 2018)</ref>, DANet <ref type="bibr" target="#b9">(Fu et al. 2019)</ref>, and EncNet <ref type="bibr" target="#b32">(Zhang et al. 2018)</ref>, to name a few.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Unsupervised Domain Adaptation</head><p>There are two approaches to unsupervised domain adaptation (UDA) for semantic segmentation, i.e., adversarial training and self-training. The former mainly attempts to decrease a domain gap by performing adversarial training in feature space <ref type="bibr" target="#b4">(Chen et al. 2019b;</ref><ref type="bibr" target="#b29">Wang et al. 2020)</ref>, in input space <ref type="bibr" target="#b10">(Gong et al. 2019;</ref><ref type="bibr" target="#b12">Lee et al. 2018)</ref>, or in output space <ref type="bibr" target="#b26">(Tsai et al. 2018</ref>). The core idea of selftraining is to generate pseudo labels for target domain samples and use them for training the model <ref type="bibr" target="#b2">(Chen et al. 2019a</ref>).</p><p>CBST <ref type="bibr" target="#b37">(Zou et al. 2018</ref>) and CRST <ref type="bibr" target="#b38">(Zou et al. 2019</ref>) conduct class-balanced self-training and confidence-regularized selftraining, respectively, to generate better pseudo labels.</p><p>Recently, several attempts have been made to combine adversarial training and self-training to improve the performance <ref type="bibr" target="#b13">(Li, Yuan, and Vasconcelos 2019;</ref><ref type="bibr" target="#b36">Zheng and Yang 2020;</ref><ref type="bibr" target="#b16">Mei et al. 2020)</ref>. BLF <ref type="bibr" target="#b13">(Li, Yuan, and Vasconcelos 2019)</ref> uses pseudo labels without any filtering, which could lead to label errors. AdaptMR (Zheng and Yang 2020) filters pseudo labels but ignores the filtered-out pixels, which could retain valuable information. IAST ) uses filtered pseudo-labels for supervised training and applied entropy minimization, a semi-supervised learning method, to the filtered-out pixels. Our method splits the targetdomain images into trusted and untrusted regions based on an entropy-based confidence map and applies adversarial training to these two regions' features, aiming to achieve finer alignment of the source-and target-domain distributions. Thus, our method is orthogonal to the above methods and experimental results show that it performs better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Use of Predictive Uncertainty for Segmentation</head><p>Many methods utilize the entropy of predicted class probabilities to measure the uncertainty of the model's prediction for better training. ADVENT <ref type="bibr" target="#b27">(Vu et al. 2019a</ref>) proposes to use the entropy for adversarial training and also for unsupervised training on the target domain data (i.e., entropy-minimization). DADA <ref type="bibr" target="#b28">(Vu et al. 2019b</ref>) estimates scene depth from the same input images at training time. It aligns the source and target distribution in the standard feature space and jointly in the depth space, aiming for more accurate alignment. ESL <ref type="bibr" target="#b23">(Saporta et al. 2020</ref>) uses the entropy to assess the confidence of the prediction better and filter pseudo labels for self-training while ignoring unselected pixels. IntraDA <ref type="bibr" target="#b18">(Pan et al. 2020</ref>) splits target domain data into easy and hard samples based on the entropy and performs intra-domain adversarial training. Although our method is similar in using adversarial training within the target domain, ours consider aligning features from different image regions; more importantly, its performance is much higher.</p><p>3 Proposed Method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Revisiting Adversarial Domain Adaptation</head><p>Before explaining our method, we revisit the standard adversarial training for cross-domain adaptation (CDA). The problem is stated as follows. We are given labeled data X s = {(x (s) , y (s) )} of the source domain and unlabeled data X t = {x (t) } of the target domain. We assume here the two domains share the same K semantic classes to predict. We wish to train a segmentation network G = C ? F , where C is a classifier and F is a feature extractor. We first train G on X s by minimizing the cross-entropy loss:</p><formula xml:id="formula_0">L cda seg = ? H?W i=1 K k=1 y (s) ik log p (s) ik ,<label>(1)</label></formula><p>where p</p><formula xml:id="formula_1">(s) ik = G(x (s) )</formula><p>is the softmax probability of pixel i belonging to class k and y (s) ik is the ground-truth one-hot label. To make G works well also with the target domain images, we consider a discriminator D that distinguishes the domain (i.e., source or target) from an input feature f = F (x). Freezing C, we then train F and D in an adversarial fashion, aiming at aligning the distributions of the two domains in the feature space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Cross Region Adaptation (CRA)</head><p>If CDA is successful, the source and target distributions should be well aligned. However, there is no guarantee that this achieves the class-level alignment between the two domains, as is well recognized in the community <ref type="bibr" target="#b29">(Wang et al. 2020)</ref>. We consider reducing this class-level misalignment after CDA.</p><p>Outline of the Method. Specifically, suppose we have conducted CDA using an existing UDA method, obtaining the segmentation network G. If class-level alignment is inaccurate, errors should occur around the class boundaries in the feature space, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. We wish to reduce the number of wrongly classified pixels, or equivalently, to move the pixels currently on the wrong side of the class boundary to the right side in the feature space.</p><p>We may be able to identify such erroneous pixels using the uncertainty of the class prediction. We employ here the entropy of the predicted class probability for this purpose. To be specific, we first classify each image pixel into two classes, trusted and untrusted, by thresholding the entropy. The details will be explained in Sec. 3.3.</p><p>We then align the feature distributions of the trusted and untrusted pixels. To do so, we conduct adversarial training to these distributions within the target domain. While CDA, the standard UDA method, aligns the feature distributions of source-and target-domain images, CRA aligns the distributions of the trusted and untrusted pixels in the target domain. In parallel to this adversarial training, we employ the standard self-training in which we train the network with the pseudo labels on the trusted pixels provided by its earlier model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Underlying Assumptions (Why Should CRA Work?)</head><p>The above procedure should achieve our goal of reducing the number of wrongly classified pixels, if the following two conditions are met:</p><p>? All the trusted pixels are correctly classified, and some of the untrusted pixels are not. ? The population of the untrusted pixels is relatively small, as shown in <ref type="figure" target="#fig_3">Fig.3</ref>.</p><p>If the first condition is met, CRA attempts to align the untrusted pixels, some of which may be wrongly classified, to the trusted (thus correctly classified) pixels in the feature space, achieving the above goal. In practice, the mere alignment of the two could lead to a worse result; for instance, it could reshape the class boundary, leading to more misclassifications. This is not the case if the second condition is met. As there are a small number of untrusted pixels, their alignment to the trusted ones will not significantly impact the class boundary, which is supported by many trusted pixels. We will experimentally show the approach's effectiveness, thereby showing that the above conditions are satisfied in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Details of the Method</head><p>Choosing Trusted and Untrusted Image Regions. We first apply an existing CDA method and obtain a segmentation network G. We then classify each image pixel into two classes, trusted and untrusted using the entropy of the predicted class probability. Specifically we first apply G to each image x (t) of the target domain, obtaining the softmax probability</p><formula xml:id="formula_2">p (t) = [p (t) i1 , . . . , p (t) iK ] at pixel i of x (t) .</formula><p>We calculate the entropy of the class probability as</p><formula xml:id="formula_3">e i = ? 1 K log K K k =1 p (t) ik log(p (t) ik ).</formula><p>(2)</p><p>We then classify each pixel (i) of the image by thresholding e i with a constant ? into two classes, trusted and untrusted. Let m i be a pixel-wise mask indicating the pixel being trusted, which is given by:</p><formula xml:id="formula_4">m i = 1 if e i &lt; ? 0 otherwise.<label>(3)</label></formula><p>The mask for untrusted pixels is obtained as m i = 1 ? m i . The choice of the hyperparameter ? is important. If we have an access to validation data, as is assumed so in previous studies, we should choose it using them. Besides, there is a simple method to calculate a good value for ?. The minimum value for the entropy is given when a single class has a probability = 1, and the theoretical maximum is given when all the class probabilities are equal, i.e., p k = 1/K (k = 1, . . . , K), where K is the number of classes. Now, suppose that the prediction is completely split among multiple classes and thus we cannot choose a particular class. The minimum entropy is given when two classes share the probability = 1/2 and the others have zero, which is log 2/(K log K). For the Cityscapes dataset, which has K = 19 classes, this yields 0.012. The threshold ? should  After training G on the source domain data, we first apply a CDA method to align the feature distributions of the two domains, yielding updated G and D (Step 1). Next, for the target domain data, we generate pseudo labels? (t) and split each image into trusted and untrusted regions based on a confidence map e (t) (</p><p>Step 2). We finally apply the proposed CRA training to align the feature distributions of the two regions within the target domain data, resulting in updated G (and D).</p><p>be lower than this value; we choose ? = 0.01 in our experiments.</p><p>There is always class imbalance in segmentation data. Classes occupying less than a few percentage in the data tend to always yield high entropy and their pixels are mostly be treated as untrusted. To avoid this to occur, it is necessary to adaptively choose the threshold ? for those classes. We found that a simple remedy works instead, which is to multiply the entropy for rare classes by 1/2. The rare classes are identified by counting the number of pseudo labels? (t) explained below and thresholding it with 1/100 of all the pixels.</p><p>We will also use pseudo label for each pixel of a target domain image x (t) . We define this a?</p><formula xml:id="formula_5">y (t) ik = 1 if k = arg max k p (t) ik 0 otherwise.<label>(4)</label></formula><p>Training of the Network. We train the segmentation network G = C ? F and the discriminator D by fine-tuning the model trained in the previous step. We first train G with the trusted pixels of the target domain images using the above pseudo label? <ref type="bibr">(t)</ref> ik . We ignore the untrusted pixels here. We use the standard cross-entropy loss:</p><formula xml:id="formula_6">L cra seg = ? H?W i=1 K k=1 m i? (t) ik log p (t) ik .<label>(5)</label></formula><p>The insertion of the mask m i ensures the loss is computed over only the trusted pixels. After this, we train G and D in an adversarial fashion, where G and D are updated alternately as follows. We first train G so that D will missclassify untrusted pixels as trusted pixels. We employ the fine-grained adversarial approach <ref type="bibr" target="#b29">(Wang et al. 2020)</ref> for the training of G and D, in which we first generate a domain encoding label containing class probabilities, i.e., [a; 0] for source domain and [0; a] for target domain, where a is a K-vector containing the class probabilities and 0 is a zero vector of size K. These are computed from the outputs of the segmentation network G. To be specific, a is defined for pixel i as follows:</p><formula xml:id="formula_7">a ik = exp( z ik T ) K j=1 exp( zij T ) ,<label>(6)</label></formula><p>where z ik is the logit for class k from G and T is a hyperparameter (temperature). We denote the label by [a (s) ; a (t) ] below. Then, we minimize the following with respect to G while freezing D.</p><formula xml:id="formula_8">L cra adv = ? H?W i=1 K k=1 a (t) ik m i log P (d = 0, c = k | f i ). (7)</formula><p>The mask m i ensures the sum is taken over the untrusted pixels. We then train D to classify the input feature f i as trusted or untrusted regions as accurately as possible. This is done by minimizing</p><formula xml:id="formula_9">L cra D = ? H?W i=1 K k=1 a (s) ik m i log P (d = 0, c = k | f i ) ? H?W i=1 K k=1 a (t) ik m i log P (d = 1, c = k | f i ). (8)</formula><p>As above, m i and m i ensure the two sums taken over trusted and untrusted pixels, repectively.</p><p>In summary, after obtaining the trusted and untrusted regions we perform the following with a single minibatch and repeat it for a certain number of iterations: 1. Update G using L cra seg of (5) with the pseudo labels (4) on the trusted regions. 2. Compute the domain encoding label with class probabilities by (6) and obtain [0; a (t) ] for the untrusted region data and [a (s) ; 0] for the trusted region data. 3. Update G using L cra adv of (7). 4. Update D using L cra D of (8). Overall Procedure. Given the labeled source domain data X s and unlabeled target domain data X t , we perform the following steps to obtain a model (G) that performs well on the target domain. There are four steps in total. We start with training G on X s in a standard supervised fashion. We then perform CDA with any UDA method, to fine-tune G and train D; see Step 1 of <ref type="figure" target="#fig_1">Fig. 2</ref>. Next, we generate pseudo labels and confidence maps for the images in X t and split each of them into trusted and untrusted regions according to the confidence maps, as in Sec. 3.3; see Step 2 of <ref type="figure" target="#fig_1">Fig. 2</ref>. Finally, we perform the CRA between the trusted and untrusted regions to fine-tune G and D, as explained above; see Step 3 of <ref type="figure" target="#fig_1">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluate our method on three scenarios of domain adaptation from a synthetic to a real dataset. For the source domain dataset, we consider either of GTA5 <ref type="bibr" target="#b19">(Richter et al. 2016)</ref>, SYNTHIA <ref type="bibr" target="#b21">(Ros et al. 2016)</ref>, and Synscapes (Wrenninge and Unger 2018). For the target domain dataset, we consider Cityscapes <ref type="bibr">(Cordts et al. 2016)</ref>.</p><p>Cityscapes <ref type="bibr">(Cordts et al. 2016</ref>) This is an urban scene dataset consisting of data collected from the real world, which contains 19 categories. It provides 2,975 images for training, 500 images for validation, and 1,525 images for testing. For the use of these data, we follow previous studies <ref type="bibr" target="#b27">(Vu et al. 2019a;</ref><ref type="bibr" target="#b26">Tsai et al. 2018;</ref><ref type="bibr" target="#b29">Wang et al. 2020)</ref>. We report the performance on the validation set below unless otherwise noted. We also report the results on the test set in Sec. 4.5. GTA5 <ref type="bibr" target="#b19">(Richter et al. 2016)</ref> This is a synthetic dataset generated from a video game. It contains 24,966 images with segmentation labels and shares the same 19 classes as Cityscapes. We use all the images as the source training data. SYNTHIA <ref type="bibr" target="#b21">(Ros et al. 2016</ref>) This is a synthetic dataset which mainly contains urban scene samples. In our experiments, we use the SYNTHIA-RAND-CITYSCAPES subset as our source domain training data, which shares 16 classes with Cityscapes and has 9,400 images with segmentation labels. Synscapes (Wrenninge and Unger 2018) This is a photorealistic synthetic dataset for street scene parsing. It shares the same 19 classes as Cityscapes and contains 25,000 images with segmentation labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Configuration</head><p>For the design of the segmentation network G and the discriminator D, we follow previous studies. To be specific, we use DeepLabv2 <ref type="bibr" target="#b3">(Chen et al. 2017)</ref> with two different backbone (i.e., feature extraction networks), VGG-16 <ref type="bibr" target="#b24">(Simonyan and Zisserman 2015)</ref> and ResNet101 <ref type="bibr" target="#b11">(He et al. 2016)</ref>, for G. Each backbone is pretrained on ImageNet <ref type="bibr" target="#b7">(Deng et al. 2009</ref>). We use a network consisting of three convolution layers for D. We train G and D as explained in Sec. 3.3. We first train G on the source domain and then apply an existing CDA method for the alignment of the feature distributions across the domains, where we follow the experimental setting of the CDA method. We then apply the proposed CRA method for 40k iterations.</p><p>For the CDA method, we select the existing state-ofthe-art CDA methods, i.e., AdaptSegNet <ref type="bibr" target="#b26">(Tsai et al. 2018)</ref>, ADVENT <ref type="bibr" target="#b27">(Vu et al. 2019a</ref>), FADA <ref type="bibr" target="#b29">(Wang et al. 2020)</ref>, IAST  and ProDA <ref type="bibr" target="#b33">(Zhang et al. 2021</ref>). The results obtained by the combination of a CDA and the proposed CRA will be referred to as '(the CDA)+CRA'; for example, ProDA+CRA indicates the combination of ProDA and the proposed CRA.</p><p>We use the SGD optimizer for the training of G with momentum = 0.9 and weight decay = 10 ?4 . The learning rate follows polynomial decay from 2.5 ? 10 ?4 with power of 0.9. We use the Adam optimizer for the training of D; the initial learning rate is set to 10 ?4 , and ? 1 = 0.9 and ? 2 = 0.99. We use the polynomial decay of learning rate.</p><p>For evaluation, we follow the standard method for semantic segmentation <ref type="bibr" target="#b3">(Chen et al. 2017)</ref>; we use the intersectionover-union (IoU) <ref type="bibr" target="#b8">(Everingham et al. 2015)</ref> as our metric. We report per-class IoU and mean IoU over all classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Main Results on Different UDA Scenarios</head><p>GTA5 ? Cityscapes. Table 2(a) shows the results. ProDA+CRA, the combination of ProDA <ref type="bibr" target="#b33">(Zhang et al. 2021)</ref> with CRA, achieves 58.6% mean IoU, which outperforms all the previous methods. As compared with the original ProDA, it brings about 1.1pp for the ResNet-101 backbone. The combinations of CRA with other CDA methods, <ref type="table" target="#tab_3">Table 2</ref>: Results of UDA from (a) GTA5, (b) SYNTHIA, and (c) Synscapes to Cityscapes. '*+CRA' in the 'method' column represents the combination of a CDA method and the proposed CRA. The column 'B' indicates backbones; V and R means VGG-16 and ResNet-101, respectively. '(?)' in the last column indicates the improvement from the base CDA. The best result is highlighted for each class. The mIoU 16 and mIoU 13 in (b) denote the mIoU scores over 16 and 13 classes, respectively. <ref type="table">Method  B  road  sidewalk  building   wall  fence  pole  light  sign  veg  terrain  sky  person  rider  car  truck  bus  train  mbike  bike</ref>    AdaptSegNet, ADVENT, FADA and IAST, also improve the baselines. <ref type="figure" target="#fig_3">Fig. 3</ref> shows a few examples of the results of FADA+CRA. It is observed that the CDA (i.e., FADA) fails in most of the image regions classified as untrusted based on the entropy. It is also seen that the application of CRA consistently leads to the improved segmentation results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a) GTA5 ? Cityscapes</head><p>SYNTHIA ? Cityscapes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with Entropy Minimization</head><p>CRA aligns the feature distributions of the untrusted (i.e., high-entropy) regions and the trusted (low-entropy) regions. As it will effectively reduce the areas of the untrusted regions, the closest is the traditional entropy minimization. We conducted an experiment to compare CRA with the entroy minimization and a few other related methods. For the base CDA, we chose FADA without its two optional steps (i.e., self-distillation and multi-scale testing) <ref type="bibr" target="#b29">(Wang et al. 2020)</ref>. We first apply it to the target domain images, splitting the image into the two regions. We then apply two methods instead of CRA. The first is to retrain the base model using the pseudo labels obtained only for the trusted regions <ref type="bibr" target="#b37">(Zou et al. 2018</ref><ref type="bibr" target="#b38">(Zou et al. , 2019</ref>. The second is the entropy minimization, specifically adding a loss minimizing the entropy of untrusted regions to the loss of training on the pseudo labels. <ref type="table" target="#tab_4">Table 3</ref> shows the results. It is seen that CRA works better than the others, validating our approach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Evaluation with the Cityscapes Test Set</head><p>The Cityscapes dataset is primarily used as the target domain dataset in existing studies. The dataset consists of training, validation, and test subsets. The ground truth labels for the test subset are unavailable, and the evaluation of results on it needs to post them to the official server <ref type="bibr" target="#b3">(Chen et al. 2017;</ref><ref type="bibr" target="#b9">Fu et al. 2019;</ref><ref type="bibr" target="#b32">Zhang et al. 2018</ref>). Thus, a common practice of the existing studies is to evaluate methods on the validation subset. However, this may make the fair comparison very hard, considering the necessity of choosing hyperparemters also on the validation subset. To cope with this, we evaluate the results for the test subset by the proposed method and the compared methods on the official server. <ref type="table" target="#tab_5">Table 4</ref> shows the results, which validates the effectiveness of the proposed approach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Summary and Conclusion</head><p>We have presented a method for unsupervised domain adaptation for semantic segmentation, named cross-region adaptation (CRA). Its basic idea is to perform adversarial training inside the target domain, aiming to reduce class-level misalignment of feature distributions between the source and the target domains. The proposed method specifies the two feature distributions in the target domain in a self-training framework. After applying an existing UDA method, which we refer to as cross-domain adaptation (CDA), to best align two feature distributions from different domains, the proposed method splits each target domain image into trusted and untrusted regions based on a confidence map. It then aligns the feature distributions from the two image regions by adversarial training. We have shown experimental results showing that the proposed CRA method combined with ProDA achieves new state-of-the-art in the standard tests of adaptation from three different CG datasets to Cityscapes. When combined with other CDA methods, the proposed approach yields better performance than using the combined method alone. Finally, we compared the major UDA methods using the Cityscapes test dataset to aim at the fairest comparison without a leakage from training to test data. The results show that the proposed approach yields the best performance. All these results support the effectiveness of the proposed approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of our method. The cross-domain adaptation aligns the two feature distributions of different domains by adversarial training, but it may not align their classes. The cross-region adaptation splits the image into trusted and untrusted regions based on the confidence map and aligns the feature distributions of the two regions by adversarial training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Overall procedure of training a model (G) using labeled source domain data {(x (s) , y (s) )} and unlabeled target domain data {x (t) }.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Examples of the results on GTA5 ? Cityscapes. From left to right, target images, results of CDA (FADA<ref type="bibr" target="#b29">(Wang et al. 2020)</ref>), trusted and untrusted regions (in purple and yellow, respectively), results of CRA, and ground truths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc>(b) shows the results. ProDA+CRA outperforms the original ProDA method and achieves the best performance.</figDesc><table><row><cell>Synscapes ? Cityscapes. Table 2(c) shows the results.</cell></row><row><cell>ProDA+CRA achieves the best mIoU score = 60.2, which</cell></row><row><cell>is 0.8pp higher than the former state-of-the-art (Zhang et al.</cell></row><row><cell>2021).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison with entropy minimization etc.</figDesc><table><row><cell>Method</cell><cell>mIoU</cell></row><row><cell>CDA (FADA w/o options)</cell><cell>46.9</cell></row><row><cell>Only training on pseudo labels</cell><cell>47.5</cell></row><row><cell>Entropy minimization</cell><cell>48.4</cell></row><row><cell>CRA</cell><cell>50.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Results on the test set of Cityscapes.</figDesc><table><row><cell>Method</cell><cell>mIoU</cell></row><row><cell>AdaptSegNet</cell><cell>43.6</cell></row><row><cell>ADVENT</cell><cell>45.7</cell></row><row><cell>IntraDA</cell><cell>47.2</cell></row><row><cell>FADA</cell><cell>52.4</cell></row><row><cell>FADA+CRA</cell><cell>53.8</cell></row><row><cell>IAST</cell><cell>54.4</cell></row><row><cell>IAST+CRA</cell><cell>55.6</cell></row><row><cell>ProDA</cell><cell>56.9</cell></row><row><cell>ProDA+CRA</cell><cell>58.9</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Seg-Net: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised Domain Adaptation by Domain Invariant Projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baktashmotlagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Lovell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="769" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Progressive Feature Alignment for Unsupervised Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="627" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning Semantic Segmentation from Synthetic Data: A Geometrically Guided Input-Output Adaptation Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1841" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The Cityscapes Dataset for Semantic Urban Scene Understanding</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ImageNet: a Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The PASCAL Visual Object Classes Challenge: A Retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dual Attention Network for Scene Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">DLOW: Domain Flow for Adaptation and Generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2477" to="2486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Diverse Image-to-Image Translation via Disentangled Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="35" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bidirectional Learning for Domain Adaptation of Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6936" to="6945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fully Convolutional Networks for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning Transferable Features with Deep Adaptation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Instance Adaptive Self-Training for Unsupervised Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="415" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image Segmentation Using Deep Learning: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Minaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kehtarnavaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised Intra-domain Adaptation for Semantic Segmentation through Self-Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rameau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3763" to="3772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Playing for Data: Ground Truth from Computer Games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="102" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Medical Image Computing and Computer Assisted Intervention</title>
		<meeting>the International Conference on Medical Image Computing and Computer Assisted Intervention</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The SYNTHIA Dataset: A Large Collection of Synthetic Images for Semantic Segmentation of Urban Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3234" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Maximum Classifier Discrepancy for Unsupervised Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3723" to="3732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ESL: Entropy-guided Self-supervised Learning for Domain Adaptation in Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saporta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep CORAL: Correlation Alignment for Deep Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to Adapt Structured Output Space for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7472" to="7481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">ADVENT: Adversarial Entropy Minimization for Domain Adaptation in Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2517" to="2526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">DADA: Depth-aware Domain Adaptation in Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7364" to="7373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Classes Matter: A Fine-grained Adversarial Approach to Cross-domain Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="642" to="659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Synscapes: A Photorealistic Synthetic Dataset for Street Scene Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wrenninge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Unger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.08705</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unified Perceptual Parsing for Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="418" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Context Encoding for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7151" to="7160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Prototypical pseudo label denoising and target structure learning for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12414" to="12424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Category Anchor-Guided Unsupervised Domain Adaptation for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Neural Information Processing Systems</title>
		<meeting>the Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="433" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pyramid Scene Parsing Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rectifying Pseudo Label Learning via Uncertainty Estimation for Domain Adaptive Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>IJCV</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="289" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Confidence Regularized Self-Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5982" to="5991" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
