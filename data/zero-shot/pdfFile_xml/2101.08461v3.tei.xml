<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Segmenting Transparent Object in the Wild with Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjia</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Sensetime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Nanjing University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Sensetime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Segmenting Transparent Object in the Wild with Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work presents a new fine-grained transparent object segmentation dataset, termed Trans10K-v2, extending Trans10K-v1, the first large-scale transparent object segmentation dataset. Unlike Trans10K-v1 that only has two limited categories, our new dataset has several appealing benefits. (1) It has 11 fine-grained categories of transparent objects, commonly occurring in the human domestic environment, making it more practical for realworld application.</p><p>(2) Trans10K-v2 brings more challenges for the current advanced segmentation methods than its former version. Furthermore, a novel transformer-based segmentation pipeline termed Trans2Seg is proposed. Firstly, the transformer encoder of Trans2Seg provides the global receptive field in contrast to CNN's local receptive field, which shows excellent advantages over pure CNN architectures. Secondly, by formulating semantic segmentation as a problem of dictionary look-up, we design a set of learnable prototypes as the query of Trans2Seg's transformer decoder, where each prototype learns the statistics of one category in the whole dataset. We benchmark more than 20 recent semantic segmentation methods, demonstrating that Trans2Seg significantly outperforms all the CNN-based methods, showing the proposed algorithm's potential ability to solve transparent object segmentation. Code is available in github.com/xieenze/Trans2Seg.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work presents a new fine-grained transparent object segmentation dataset, termed Trans10K-v2, extending Trans10K-v1, the first large-scale transparent object segmentation dataset. Unlike Trans10K-v1 that only has two limited categories, our new dataset has several appealing benefits. (1) It has 11 fine-grained categories of transparent objects, commonly occurring in the human domestic environment, making it more practical for realworld application.</p><p>(2) Trans10K-v2 brings more challenges for the current advanced segmentation methods than its former version. Furthermore, a novel transformer-based segmentation pipeline termed Trans2Seg is proposed. Firstly, the transformer encoder of Trans2Seg provides the global receptive field in contrast to CNN's local receptive field, which shows excellent advantages over pure CNN architectures. Secondly, by formulating semantic segmentation as a problem of dictionary look-up, we design a set of learnable prototypes as the query of Trans2Seg's transformer decoder, where each prototype learns the statistics of one category in the whole dataset. We benchmark more than 20 recent semantic segmentation methods, demonstrating that Trans2Seg significantly outperforms all the CNN-based methods, showing the proposed algorithm's potential ability to solve transparent object segmentation. Code is available in github.com/xieenze/Trans2Seg.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Modern robots, mainly mobile robots and mechanical manipulators, would benefit a lot from the efficient perception of the transparent objects in residential environments since the environments vary drastically. The increasing utilization of glass wall and transparent door in the building interior and the glass cups and bottles in residential rooms has resulted in the wrong detection in various range sensors. In robotic research, most systems perceive the environment by multi-data sensor fusion via sonars or lidars. The sensors are relatively consistent in detecting opaque objects but are still affected by the scan mismatching due to transparent objects. The unique (a) Selected images and corresponding high-quality masks.  feature of reflection, refraction, and light projection from the transparent objects may confuse the sensors. Thus a reliable vision-based method, which is much cheaper and more robust than high-precision sensors, would be efficient.</p><p>Although some transparent objects dataset <ref type="bibr" target="#b28">[Xu et al., 2015;</ref><ref type="bibr" target="#b3">Chen et al., 2018a;</ref><ref type="bibr" target="#b20">Mei et al., 2020]</ref> were proposed, there are some obvious problems. (1) Limited dataset scale. These datasets often have less than 1K images captured from the real-world and less than 10 unique objects. (2) Poor diversity. The scene of these datasets is monotonous. (3) Fewer classes. All these datasets have only two classes, background and transparent objects. They lack fine-grained categories, which limited their practicality. Recently,  proposed a large-scale and high-diversity dataset termed Trans10K, which divide transparent objects as 'Things' and 'Stuff'. The dataset is high diversity, but it also lacks fine-grained transparent categories.</p><p>In this paper, we proposes a fine-grained transparent object segmentation dataset termed Trans10K-v2 with more elaborately defined categories. The images are inherit from <ref type="bibr">Trans10K-v1 [Xie et al., 2020]</ref>. We annotate the 10428 images with 11 fine-grained categories: shelf, jar, freezer, window, glass door, eyeglass, cup, glass wall, glass bowl, water bottle, storage box. In Trans10K-v1, transparent things are defined to be grabbed by the manipulators and stuff are for robot navigation. Though two basic categories can partially help robots to interact with transparent objects, the provided fine-grained classes in Trans10K-v2 can provide more. We analyze these objects' functions and how robots interact with them in appendix.</p><p>Based on this challenging dataset, we design Trans2Seg, introducing Transformer into segmentation pipeline for its encoder-decoder architecture. First, the transformer encoder provides a global receptive field via self-attention. Larger receptive field is essential for segmenting transparent objects because transparent objects often share similar textures and context with its surroundings. Second, the decoder stacks successive layers to interact query embedding with transformer encoder output. To facilitate the robustness of transparent objects, we carefully design a set of learnable class prototype embeddings as the query for transformer decoder and the key is the feature map from the transformer encoder. Compared with convolutional paradigm, where the class prototypes is the fixed parameters of convolution kernel weight, our design provides a dynamic and context-aware implementation. As shown in <ref type="figure">Figure.</ref> 1b, we train and evaluate 20 existing representative segmentation methods on Trans10K-v2, and found that simply applying previous methods to this task is far from sufficient. By successfully introducing Transformer into this task, our Trans2Seg significantly surpasses the best TransLab [Xie et al., 2020] by a large margin (72.1 vs. 69.0 on mIoU).</p><p>In summary, our main contributions are three-fold:</p><p>? We propose the largest glass segmentation dataset (Trans10K-v2) with 11 fine-grained glass image categories with a diverse scenario and high resolution. All the images are elaborately annotated with fine-shaped masks and function-oriented categories.</p><p>? We introduce a new transformer-based network for transparent object segmentation with transformer encoder-decoder architecture. Our method provides a global receptive field and is more dynamic in mask prediction, which shows excellent advantages.</p><p>? We evaluate more than 20 semantic segmentation methods on Trans10K-v2, and our Trans2Seg significantly outperforms these methods. Moreover, we show this task is largely unsolved. Thus more research is needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Semantic Segmentation. In deep learning era, convolutional neural network (CNN) puts forwards the development of semantic segmentation in various datasets, such as ADE20K, CityScapes and PASCAL VOC. One of the pioneer works approaches, FCN <ref type="bibr" target="#b19">[Long et al., 2015]</ref>, transfers semantic segmentation into an end-to-end fully convolutional classification network. For improving the performance, especially around object boundaries, <ref type="bibr" target="#b2">[Chen et al., 2017;</ref><ref type="bibr" target="#b15">Lin et al., 2016;</ref><ref type="bibr">Zheng et al., 2015]</ref> propose to use structured prediction module, conditional random fields (CRFs) <ref type="bibr" target="#b1">[Chen et al., 2014]</ref>, to refine network output. Dramatic improvements in performance and inference speed have been driven by aggregating features at multiples scales, for example, PSPNet  and DeepLab <ref type="bibr" target="#b2">[Chen et al., 2017;</ref><ref type="bibr" target="#b4">Chen et al., 2018b]</ref>, and propagating structured information across intermediate CNN representations <ref type="bibr" target="#b10">[Gadde et al., 2016;</ref><ref type="bibr" target="#b18">Liu et al., 2017;</ref>. Transparent Object Datasets. <ref type="bibr" target="#b28">[Xu et al., 2015]</ref> introduces TransCut dataset which only contain 49 images of 7 unique objects. To generate the segmentation result, <ref type="bibr" target="#b28">[Xu et al., 2015]</ref> optimized an energy function based on LF-linearity which also need to utilize the light-field cameras. <ref type="bibr" target="#b3">[Chen et al., 2018a]</ref> proposed TOM-Net. It contains 876 real images and 178K synthetic images which are generated by POV-Ray. However, only 4 unique objects are used in synthesizing the training data. Recnetly,  introduce a first large-scale real-world transparent object segmentation dataset, termed Trans10K. It has 10K+ images. However, there are two categories in this dataset, which limits its practical use. In this work, our Trans10K-v2 inherited the data and annotates 11 fine-grained categories.</p><p>Transformer in Vision Tasks. Transformer <ref type="bibr" target="#b23">[Vaswani et al., 2017]</ref> has been successfully applied in both highlevel vision and low-level vision <ref type="bibr" target="#b12">[Han et al., 2020]</ref>. In ViT <ref type="bibr" target="#b9">[Dosovitskiy et al., 2020]</ref>, Transformer is directly applied to sequences of image patches to complete image classification. In object detection areas <ref type="bibr" target="#b0">[Carion et al., 2020;</ref><ref type="bibr" target="#b32">Zhu et al., 2020]</ref>, DETR reasons about the relations of the object queries and the global image context via Transformer and outputs the final set of predictions in parallel without nonmaximum suppression(NMS) procedures and anchor generation. SETR <ref type="bibr" target="#b32">[Zheng et al., 2020]</ref> views semantic segmentation from a sequence-to-sequence perspective with Transformer. IPT  applies Transformer model to low-level computer vision task, such as denoising, superresolution and deraining. In video processing, Transformer has received significantly growing attention. VisTR <ref type="bibr" target="#b27">[Wang et al., 2020]</ref> accomplishes instance sequence segmentation by Transformer. Multiple-object tracking <ref type="bibr" target="#b22">[Sun et al., 2020;</ref><ref type="bibr" target="#b21">Meinhardt et al., 2021]</ref> employs Transformers to decode object queries and feature queries of the previous frame into bounding boxes of the current frame, and merged by Hungarian Algorithm or NMS.</p><p>3 Trans10K-v2 Dataset Dataset Introduction. Our Trans10K-v2 dataset is based on Trans10K dataset . Following Trans10K, we use 5000, 1000 and 4428 images in training, validation and testing respectively. The distribution of the images is abundant in occlusion, spatial scales, perspective distortion. We further annotate the images with more fine-grained categories due to the functional usages of different objects. Trans10K-  v2 dataset contains 10,428 images, with two main categories and 11 fine-grained categories: (1) Transparent Things containing cup, bottle, jar, bowl and eyeglass.</p><p>(2) Transparent Stuff containing windows, shelf, box, freezer, glass walls and glass doors. In respect to fine-grained categories and high diversity, Trans10K-v2 is very challenging, and have promising potential in both computer vision and robotic researches. Annotation Principle. The transparent objects are manually labeled by expert annotators with professional labeling tool. The annotators were asked to provide more than 100 points when they trace the boundaries of each transparent object, which ensures the high-quality outline of the mask shapes. The way of annotation is mostly the same with semantic segmentation datasets such as ADE20K. We set the background with 0, and the 11 categories from 1 to 11. We also provide the scene environment of each image locates at. The annotators are asked to strictly following principles when they label the images: (I) Only highly transparent pixels are annotated as masks, other semi-transparent and nontransparent pixels are ignored. Highly transparent objects no matter made of glass, plastics or crystals should also be annotated. (II) When occluded by opaque objects, the pixels will be cropped from the masks. (III) The setting of all 11 finegrained categories are elaborately observed and induced from the point of function. We analyze firstly how the robots need to deal with the transparent objects as avoiding or grasping or manipulating, then categorize the objects similar in shape and function into a fine-grained category. The detailed principle of how we categorize the objects is listed in appendix.</p><p>Dataset Statistics. The statistic information of CMCC, imaga number, pixel proportion are listed in <ref type="table">Table 1</ref> in detail. From Table1, the sum of all the image numbers is larger than 10428 since some image has multiple category of objects. CMCC denotes Mean Connected Components of each category. It is caculated by dividing the connected components number of a certain category by the image number. The number of connected components are counted by the bound-ary of the masks. It represents the complexity of the transparent objects.</p><p>Evaluation Metrics. Results are reported in three metrics that are widely used in semantic segmentation to benchmark the performance of fine-grained transparent object segmentation.</p><p>(1) Pixel Accuracy indicates the proportion of correctly classified pixels.</p><p>(2) Mean IoU indicates mean intersection over union.</p><p>(3) Category IoU indicates the intersection over union of each category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overall Pipeline</head><p>The overall Trans2Seg architecture contains a CNN backbone, an encoder-decoder transformer, and a small convolutional head, as shown in <ref type="figure">Figure 3</ref>. For an input image of (H, W, 3), </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformer Encoder</head><p>Transformer Decoder <ref type="figure">Figure 3</ref> -The whole pipeline of our hybrid CNN-Transformer architecture. First, the input image is fed to CNN to extract features F . Second, for transformer encoder, the features and position embedding are flatten and fed to transformer for self-attention, and output feature(Fe) from transformer encoder. Third, for transformer decoder, we specifically define a set of learnable class prototype embeddings(E cls ) as query, Fe as key, and calculate the attention map with E cls and Fe. Each class prototype embedding corresponds to a category of final prediction. We also add a small conv head to fuse attention map and Res2 feature from CNN backbone. Details of transformer decoder and small conv head refer to <ref type="figure">Figure 4</ref>. Finally, we can get the predict results by doing pixel-wise argmax on the attention map. For example, in this figure, the segmentation mask of two categories (Bottle and Eyeglass) corresponds to two class prototypes with same colors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decoder Layer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>??</head><p>Decoder Layer Decoder Layer Decoder Layer  <ref type="figure">Figure 4</ref> -Detail of Transformer Decoder and small conv head. Input: The learnable category prototypes as query, features from transformer encoder as key and value. The inputs are fed to transformer decoder, which consists of several decoder layers. The attention map from last decoder layer and the Res2 feature from CNN backbone are combined and fed to a small conv head to get final prediction result. We also provide the Pseudo Code of small conv head for better understanding.</p><formula xml:id="formula_0">old query N x C C x ! "# x $ "# Multi-head attention ? N x M x ! "# x $ "#</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Encoder</head><p>The Transformer encoder takes a sequence as input, so the spatial dimensions of the feature map ( H 16 , W 16 , C) is flattened into one dimension( H</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>16</head><p>W 16 , C). To compensate missing spatial dimensions, positional embedding <ref type="bibr" target="#b11">[Gehring et al., 2017]</ref> is supplemented to one dimension feature to provide information about the relative or absolute position of the feature in the sequence. The positional embedding has the same di-mension ( H</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>16</head><p>W 16 , C) with the flattened feature. The encoder is composed of stacked encoder layers, each of which consists of a multi-head self-attention module and a feed forward network <ref type="bibr" target="#b23">[Vaswani et al., 2017]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Decoder</head><p>The Transformer decoder takes input a set of learnable class prototype embeddings as query, denoted by E cls , the encoded feature as key and value, denoted by F e , and output the attention map followed by Small Conv Head to obtain final segmentation result, as shown in <ref type="figure">Figure 4</ref>.</p><p>The class prototype embeddings are learned category prototypes, updated iteratively by a series of decoder layers through multi-head attention mechanisms. We denoted iterative update rule by , then the class prototype in each decoder layer is:</p><formula xml:id="formula_1">E s cls = i=0,..,s?1 softmax(E i cls F e )F e<label>(1)</label></formula><p>In the final decoder layer, the attention map is extracted out to into small conv head:</p><formula xml:id="formula_2">attention map = E s cls F e<label>(2)</label></formula><p>The pseudo code of small conv head is shown in shown in <ref type="figure">Figure 4</ref>. The attention map from Transformer decode is the shape of (N, M, H </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Discussion</head><p>The most related work with Trans2Seg is SETR and DETR <ref type="bibr" target="#b32">[Zheng et al., 2020;</ref><ref type="bibr" target="#b0">Carion et al., 2020]</ref>. In this section we discuss the relations and differences in details.</p><p>SETR. Trans2Seg and SETR are both segmentation pipelines. Their key difference is reflected in the design of the decoder. In SETR, the decoder is simple several convolutional layers, which is similar with most previous methods. However, the decoder of Trans2Seg is also transformer, which fully utilize the advantages of attention mechanism in semantic segmentation.</p><p>DETR. Trans2Seg and DETR share similar components in the pipeline, including CNN backbone, Transformer encoder and decoder. The biggest difference is the definition of query. In DETR, the decoder's queries represents N learnable objects because DETR is designed for object detection. However, in Trans2Seg, the queries represents N learnable class prototypes, where each query represents one category. We could see that the minor change on query design could generalize Transformer architecture to apply to diverse vision tasks, such as object detection and semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implementation Details.</head><p>We implement Trans2Seg with Pytorch. The <ref type="bibr">ResNet-50 [He et al., 2016]</ref> with dilation convolution at last stage. is adoped as the CNN extractor. For loss optimization, we use Adam optimizer with epsilon 1e-8 and weight decay 1e-4. Batch size is 8 per GPU. We set learning rate 1e-4 and decayed by the poly strategy <ref type="bibr" target="#b30">[Yu et al., 2018]</ref> for 50 epochs. We use 8 V100 GPUs for all experiments. For all CNN based methods, we random scale and crop the image to 480 ? 480 in training, and resize image to 513 ? 513 in inference, following common setting on PASCAL VOC <ref type="bibr" target="#b9">[Everingham and Winn, 2011]</ref>. For our Trans2Seg, we adopt transformer architecture and need to keep the shape of learned position embedding same in training/inference, so we directly resize the image to 512 ? 512. Code has been released for community to follow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Studies.</head><p>We use the FCN <ref type="bibr" target="#b19">[Long et al., 2015]</ref> as our baseline. FCN is a fully convolutional network with very simple design, and it is also a very classic semantic segmentation method. First, we demonstrate that transformer encoder can build long range attention between pixels, which has much larger receptive field than CNN filters. Second, we remove the CNN decoder in FCN and replace by our Transformer decoder, we design a set of learnable class prototypes as queries and show that this design further helps improve the accuracy. Third, we verify our method with transformer at different scales.</p><p>Self-Attention of Transformer Encoder. As shown in <ref type="figure" target="#fig_2">Figure 2</ref>, the FCN baseline without transformer encoder achieves 62.7% mIoU, when adding transformer encoder, the mIoU directly improves 6.1%, achieving 66.8% mIoU. It demonstrates that the self-attention module in transformer encoder provides global receptive filed, which is better than CNN's local receptive field in transparent object segmentation.</p><p>Category Prototypes of Transformer Decoder. In <ref type="figure" target="#fig_2">Figure 2</ref>, we verify the effectiveness of learnable category prototypes in transformer decoder. In column 2, with traditional CNN decoder, the mIoU is 68.8%. However, with our transformer decoder, the mIoU boosts up to 72.1% with 3.3% improvement. The strong performance benefits from the flexible representation that learnable category prototypes as queries to find corresponding pixels in feature map.</p><p>Scale of Transformer. The scale of transformer is mainly influenced by three hyper-parameters: (1) embedding dim of feature.</p><p>(2) number of attention layers. (3) mlp ratio in feed forward layer. We are interested in whether enlarge the model size can continuously improve performance. So we set three combinations, as shown in <ref type="figure">Figure 3</ref>. We can find that with the size of transformer increase, the mIoU first increase then decrease. We argue that if without massive data to pretrain, e.g. BERT <ref type="bibr" target="#b8">[Devlin et al., 2019]</ref> used large-scale nlp data, the transformer size is not the larger the better for our task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison to the state-of-the-art.</head><p>We select more than 20 semantic segmentation methods <ref type="bibr" target="#b6">Chen et al., 2018c;</ref><ref type="bibr" target="#b14">Li et al., 2019a;</ref><ref type="bibr" target="#b29">Yang et al., 2018;</ref><ref type="bibr" target="#b19">Long et al., 2015;</ref><ref type="bibr" target="#b22">Ronneberger et al., 2015;</ref><ref type="bibr" target="#b16">Lin et al., 2017;</ref><ref type="bibr" target="#b1">Chao et al., 2019;</ref><ref type="bibr" target="#b25">Wang et al., 2019a;</ref><ref type="bibr" target="#b21">Poudel et al., 2019;</ref><ref type="bibr" target="#b21">Poudel et al., 2018;</ref><ref type="bibr" target="#b26">Wang et al., 2019b;</ref><ref type="bibr" target="#b13">Jin et al., 2019;</ref><ref type="bibr" target="#b14">Li et al., 2019a;</ref><ref type="bibr" target="#b17">Liu and Yin, 2019;</ref><ref type="bibr" target="#b14">Li et al., 2019b;</ref><ref type="bibr" target="#b10">Fu et al., 2019;</ref><ref type="bibr" target="#b19">Mehta et al., 2019]</ref> to evaluate on our Trans10K-v2 dataset, the methods selection largely follows the benchmark of TransLab . For fair comparsion, we train all the methods with 50 epochs. <ref type="table" target="#tab_5">Table 4</ref> reports the overall quantitative comparison results on test set. Our Trans2Seg achieves state-of-the-art 72.15% mIoU and 94.14% pixel ACC, significant outperforms other pure CNN-based methods. For example, our method is 2.1% higher than TransLab, which is the previous SOTA method. We also find that our method tend to performs much better on small objects, such as 'bottle' and 'eyeglass' (10.0% and 5.0% higher than previous SOTA). We consider that the transformer's long range attention benefits the small transparent object segmentation.</p><p>In <ref type="figure">Figure 5</ref>, we visualize the mask prediction of Trans2Seg and other CNN-based methods. We can find that benefit from transformer's large receptive field and attention mechanism,   <ref type="figure">Figure 5</ref> -Visual comparison of Trans2Seg to other CNN-based semantic segmentation methods. Our Trans2Seg clearly outperforms others thanks to the transformer's global receptive field and attention mechanism, especially in dash region. Zoom in for best view. Refer to supplementary materials for more visualized results. Trans2Seg also works well on general semantic segmentation tasks. "d8" and "d16" means dilation 8 and 16, respectively. "R50" means ResNet-50 backbone.</p><p>our method can distinguish background and different categories transparent objects much better than other methods, especially when multiple objects with different categories occurs in one image. Moreover, our method can obtain high quality detail information,e.g. boundary of object, and tiny transparent objects, while other CNN-based methods fail to do so. More results are shown in supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">General Semantic Segmentation</head><p>We try to transfer Trans2Seg on general semantic segmentation and it also achieves satisfied performance. Experiment Settings. We choose ADE20K <ref type="bibr" target="#b33">[Zhou et al., 2017]</ref>, a challenging scene parsing benchmark for semantic segmentation. ADE20K contains 150 fine-grained semantic categories, where there are 20210, 2000, and 3352 images for training, validation and, testing, respectively. We set learning rate to 2.5e-5 for ADE20K experiments. We train all models with 40k iterations with 8 images/GPU and 8 GPUs, and use single-scale test in inference. The data augmentation is same as DeeplabV3+ <ref type="bibr" target="#b6">[Chen et al., 2018c]</ref>.</p><p>Results. As shown in <ref type="table">Table 5</ref>, compared with Semantic FPN and DeeplabV3+, our Trans2Seg achieves 39.7 mIoU, which is a satisfied performance. Our Trans2Seg verifies robust transfer ability on challenging general segmentation dataset. Please note that we do not carefully tuned the hyperparameters of Trans2Seg on ADE20K dataset. We are highly interested to design a better transformer-based general semantic segmentation pipeline in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we present a new fine-grained transparent object segmentation dataset with 11 common categories, termed Trans10K-v2, where the data is based on the previous Trans10K. We also discuss the challenging and practical of the proposed dataset. Moreover, we propose a transformerbased pipeline, termed Trans2Seg, to solve this challenging task. In Trans2Seg, the transformer encoder provides global receptive field, which is essential for transparent objects segmentation. In the transformer decoder, we model the segmentation as dictionary look up with a set of learnable queries, where each query represents one category. Finally, we evaluate more than 20 mainstream semantic segmentation methods and shows our Trans2Seg clearly surpass these CNN-based segmentation methods.</p><p>In the future, we are interested in exploring our Transformer encoder-decoder design on general segmentation tasks, such as Cityscapes and PASCAL VOC. We will also put more effort to solve transparent object segmentation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Detailed Dataset Information</head><p>More Visualized Demonstration of Trans10K-v2. In this section we show more visualized demonstrations to show the diversity and quality of Trans10K-v2. In <ref type="figure">Figure 6</ref> and <ref type="figure">Figure 7</ref>, we show more cropped objects to illustrate the high-diversity of the objects. We also show more images and ground-truth masks in <ref type="figure" target="#fig_5">Figure 8</ref>. All images and transparent objects in Trans10K-v2 are selected from complex real-world scenarios that have large variations such as scale, viewpoint, contrast, occlusion, categories and transparency. <ref type="figure" target="#fig_5">From Figure 8</ref>, we can also find that it is challenging for current semantic segmentation methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scene information</head><p>We also provide each image with a scene label that represents where the objects located in. As shown in the upper part of <ref type="table" target="#tab_7">Table 6</ref>, we list the statistics of the distribution in different scenarios of each category in detail. The distribution highly follows the distribution of our residential environments. For example, the cups, bowls, and bottles are mostly placed on the desk, while glass walls are often located in mega-malls or office buildings.</p><p>The visualized demonstration of our diverse scene distribution is shown in <ref type="figure">Figure 9</ref>. Trans10k-v2 contains abundant scenarios and we induce them into 13 categories: on the desk, mega-mall, store, bedroom, sitting room, kitchen, bathroom, windowsill, office, office building, outdoor, in the vehicle, study-room. This information is mainly used to demonstrate our abundant image distribution which could cover most of the common real-life scenarios. Each image is provided with a scene label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How Robots Deal with Transparent Objects</head><p>Transparent objects are widespread in human residential environments, so the human-aiding robots find ways to deal with transparent objects. Some former robotic research illustrates the substantial value of solving this problem, mainly from grasping and navigation. This research primarily focuses on modifying the algorithm to deal with optical signals reflected from the transparent objects.</p><p>For the manipulator grasping, previous work mainly focuses on grabbing water cups. <ref type="bibr">[Klank et al., 2011]</ref> propose an approach to reconstruct an approximate surface of the transparent cups and bottles by the internal sensory contradiction from two ToF (time of flight) images captured from an SR4k camera. The robot arm could grasp and manipulate the objects. <ref type="bibr" target="#b22">[Spataro et al., 2015]</ref> set up a BCI-robot platform to help patients suffering from limb muscle paralysis by grasping a glass cup for the patients. Starting from the point that the usual glass material absorbs light in specific wavelengths, <ref type="bibr" target="#b33">[Zhou et al., 2018]</ref> propose the Depth Likelihood Volume (DLV), which uses a Monte Carlo object localization algorithm to help the Michigan Progress Fetch robot localize and manipulate translucent objects.</p><p>For the mobile robot navigation, some work also finds ways to exclude the side-effect of transparent stuff in residential scenarios. <ref type="bibr">[Foster et al., 2013]</ref> modify the standard occupancy grid algorithm during the procedure of autonomousmapping robot localize transparent objects from certain angles. <ref type="bibr" target="#b13">[Kim and Chung, 2016</ref>] design a novel scan matching algorithm by comparing all candidate distances scanned by the laser range finder penetrate and reflected from the glass walls. <ref type="bibr" target="#b22">[Singh et al., 2018]</ref> use information fusion by combining a laser scanner and a sonar on an autonomous-mapping mobile robot to reduce the uncertainty caused by glass.</p><p>We analyze how robots deal with transparent objects from previous work and grade them into 4 patterns: navigation, grasping, manipulation, human-aiding. Navigation and grasping are the two fundamental interactions between robots and objects. Manipulation happens on complex objects like windows, doors, or bottles with lids. Human-aiding is the highest level of robot mission, and this kind of interaction always involve human, especially disabled patients. From these 4 patterns, we can then analyze and categorize the transparent objects in respect to functions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Categorization Principle</head><p>The 11 fine-grained categories are based on how the robots need to deal with transparent objects like avoiding or grasping or manipulating. For example, the goblet and cup are both open-mouthed and mainly used to drink water. These objects need to be grasped carefully since they do not have lids. They have different interactive actions with the robots. So they are both categorized as cup. We show the detailed demonstration of each category: (1) Shelf. Containing bookshelf, showcase, cabinet, etc. They mostly have sliding glass doors and are used to store goods. (2) Freezer. Containing vending machine, horizontal freezer, etc. They are electrical equipment and are used to storing drinks and food. (3) Door. Containing automatic glass door, standard glass door, etc. The doors are located in mega-mall, bathroom or office building. They are highly transparent and extensive. They could be used in navigation and helping disabled people pass through. (4) Wall. Glass walls look like doors. However, walls can not be opened. This clue should be perceived during mobile robots' mapping procedure. Glass walls are common in mega-mall and office buildings. (5) Window. Windows could be opened like glass doors but should not be traveled through. (6) Box. Large boxes may not need to be grasped, but the manipulator robot needs to open the box and search for specific items. (7) Cup. We category all open-mouthed cups like goblets and regular cups into this category. Cups are used for drinking water. The manipulators need to grasp a cup carefully and be able to assist disabled people to drink water. (8) Bottle. Bottles are also used to drink water. But bottles have lids, so they need careful manipulation. (9) Eyeglass. Eyeglasses need careful grasping and manipulation to help disable people wear the eyeglasses. (10) Jar. This category contains jars, kettles and other transparent containers used to hold water, flavoring and food. (11) Bowl. Bowls are usually used to contain water or food. Different from jars, they do not have lids and need careful grasping. The sample objects of these categories could be find in <ref type="figure" target="#fig_5">Figure 8</ref>. We show the most common type of different categories by cropping the objects through masks.</p><p>As shown in the lower part of <ref type="table" target="#tab_7">Table 6</ref>, we analyze and list the interactive patterns of all the 11 fine-grained categories of objects. Navigation is the basic interactive pattern of stuff and grasping is the basic interactive pattern of things. All the objects with some complex interactions need to be manipulated like the robots helping people open the shelf or window. Human-aiding is the highest level of interaction and it always involves patients. The patients need robots to help with opening the door, or feeding water by a cup or bottle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">More Visual Results Comparison.</head><p>In this section, we visualize more test examples produced by our Trans2Seg and other CNN-based methods on Trans10K-v2 dataset in <ref type="figure" target="#fig_1">Figure 11</ref>. From these results, we can easily observe that our Trans2Seg outputs very high-quality transparent object segmentation masks than other methods. Such strong results mainly benefit from the successfully introducing Transformer into transparent object segmentation, which is the lack in other CNN-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Failure Case Analysis</head><p>As shown in <ref type="figure" target="#fig_1">Figure 10</ref>, our method also has some limitations. For instance, in <ref type="figure" target="#fig_1">Figure 10 (a)</ref>, when transparent objects are occluded by different categories, our method would confuse and fail to segment part of the items. In <ref type="figure" target="#fig_1">Figure 10 (b)</ref>, when the objects are of extreme transparency, our method would also confuse and output wrong segmentation results. In such a case, even humans would also fail to distinguish these transparent objects.  <ref type="figure">Figure 9</ref> -The image number distribution and selected images of different scenes in Trans10K-v2. For better demonstration, the image number in vertical axis is listed as logarithmic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image GT Trans2Seg</head><p>(a) Occlusion and Crowd.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image GT Trans2Seg</head><p>(b) Extreme Transparency.   <ref type="figure" target="#fig_1">Figure 11</ref> -Visualized results of comparison with state-of-the-art methods. Our Trans2Seg has the best mask prediction among all methods. Zoom in for the best view.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Performance comparison on Trans10K-v2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 -</head><label>1</label><figDesc>(a) shows the high diversity of our dataset and highquality annotations. (b) is Comparisons between Trans2Seg and other CNN-based semantic segmentation methods. All methods are trained on Trans10K-v2 with same epochs. mIoU is chosen as the metric. Deeper color bar indicates methods with larger FLOPS. Our Trans2Seg significantly surpasses other methods with lower flops.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 -</head><label>2</label><figDesc>Images in Trans10K-v2 dataset are carefully annotated with high quality. The first row shows sample images and the second shows the segmentation masks. The color scheme which encodes the object categories are listed on the right of the figure. Zoom in for best view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>where N is number of categories, M is number of heads in multi-head attention. It is up-sampled to (N, M, H 4 , W 4 ), then fused with high-resolution feature map Res2 in the second dimension to (N, M +C, H 4 , W 4 ), and finally transformed into output attention map of (N, H 4 , W 4 ). The final segmentation is obtained by pixel-wise argmax operation on the output attention map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 -Figure 7 -</head><label>67</label><figDesc>Cropped objects of 5 kinds of transparent things: cup, jar, bottle, bowl, eyeglass. Zoom in for the best view. Cropped objects of 6 kinds of transparent stuff: wall, freezer, box, door, shelf, window. Zoom in for the best view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 -</head><label>8</label><figDesc>More images and corresponding high-quality masks in Trans10K-v2. Our dataset is high diversity in scale, categories, pose, contrast, occlusion, and transparency. Zoom in for the best view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 -</head><label>10</label><figDesc>Failure cases analysis. Our Trans2Seg fails to segment transparent objects in some complex scenarios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The final segmentation is obtained by pixel-wise argmax operation on the output attention map.</figDesc><table><row><cell>input image</cell><cell>C x ( 16 16</cell><cell>)</cell><cell>C x ( 16 16</cell><cell>)</cell><cell cols="2">N x M x 16 x 16</cell><cell>small</cell><cell>predicted mask</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>conv</cell></row><row><cell>CNN</cell><cell>?</cell><cell></cell><cell>?</cell><cell></cell><cell>?</cell><cell></cell><cell>head</cell></row><row><cell>3 x H x W</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>attention map</cell><cell cols="3">N: number of categories</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">M: number heads in Transformer</cell></row><row><cell></cell><cell>position embedding</cell><cell></cell><cell cols="2">N x C</cell><cell>Learned Class Prototypes ?</cell><cell cols="3">C: feature channels H: image height W: image width</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">? The CNN backbone generates image feature map of</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>( H 16 , W 16 , C).</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">? The encoder takes in the summation of flattened feature</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">of ( H 16 and outputs encoded feature of ( H W 16 , C) and positional embedding of ( H 16 16 W 16 , C).</cell><cell>W 16 , C),</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">? The decoder interacts the learned class prototypes of</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">(N, C) with encoded feature, and generates attention</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">map of (N, M, H 16 M is number of heads in multi-head attention. W 16 ), where N is number of categories,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">? The small convolutional head up-samples the attention</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">map to (N, M, H 4 , W 4 ), fuses it with high-resolution fea-ture map Res2 and outputs attention map of (N, H 4 , W 4 ).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Pseudo Code of Small Conv Head</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>Transformer</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Prototypes Features</cell><cell>Category Encoded</cell><cell>Decoder</cell><cell>A tt n . M a p R e s 2 F e a t.</cell><cell></cell><cell>Small Conv Head</cell><cell>NxHxW Result</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">CNN backbone</cell><cell></cell></row><row><cell></cell><cell></cell><cell>key &amp; value Encoded Features</cell><cell>4x up &amp; Concat</cell><cell>Conv &amp; BN &amp; Relu</cell><cell>Conv 1x1</cell><cell>Point-wise Argmax</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4"># Attn: attention map, [N,M,H/16,W/16]</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4"># Res2: backbone feature, [1,C,H/4, W/4]</cell></row><row><cell></cell><cell></cell><cell>Attention Map</cell><cell cols="4">Res2 = Res2.repeat(N,1,1,1) # [N,C,H/4,W/4] Attn = Up(Attn, (H/4, W/4)) # [N,M,H/4,W/4]</cell></row><row><cell></cell><cell></cell><cell>new query</cell><cell cols="4">X = Concat([Res2, Attn]) # [N,M+C,H/4,W/4] X = Conv_BN_ReLU(X) # [N,M+C, H/4, W/4]</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">X = Conv(X) # [N,1,H/4,W/4]</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Res = X.reshape(N, H/4, W/4) # [N,H/4,W/4]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 -</head><label>2</label><figDesc>Effectiveness of Transformer encoder and decoder. 'Trans.' indicates Transformer. 'Enc.' and 'Dec.' means encoder and decoder.</figDesc><table><row><cell>id</cell><cell>Trans. Enc.</cell><cell cols="2">Trans. Dec.</cell><cell>CNN Dec.</cell><cell>mIoU</cell></row><row><cell>0</cell><cell>?</cell><cell></cell><cell>?</cell><cell></cell><cell>62.7</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell>?</cell><cell></cell><cell>68.8</cell></row><row><cell>2</cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell>72.1</cell></row><row><cell>Scale</cell><cell cols="2">hyper-param.</cell><cell>GFlops</cell><cell cols="2">MParams mIoU</cell></row><row><cell>small</cell><cell cols="2">e128-n1-m2</cell><cell>40.9</cell><cell>30.5</cell><cell>69.2</cell></row><row><cell>medium</cell><cell cols="2">e256-n4-m3</cell><cell>49.0</cell><cell>56.2</cell><cell>72.1</cell></row><row><cell>large</cell><cell cols="2">e768-n12-m4</cell><cell>221.8</cell><cell>327.5</cell><cell>70.3</cell></row><row><cell cols="6">Table 3 -Performance of Transformer at different scales.</cell></row><row><cell cols="6">'e{a}-n{b}-m{c}' means the transformer with number of 'a' em-</cell></row><row><cell cols="4">bedding dims, 'b' layers and 'c' mlp ratio.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>23.22 34.88 32.34 44.24 42.25 50.36 65.23 60.00 43.88 53.81 20.17 FastSCNN 1.01 88.05 51.93 90.64 32.76 41.12 47.28 47.47 44.64 48.99 67.88 63.80 55.08 58.86 24.65 DFANet 1.02 85.15 42.54 88.49 26.65 27.84 28.94 46.27 39.47 33.06 58.87 59.45 43.22 44.87 13.53.43 67.82 64.20 59.64 60.56 88.52 86.67 75.99 73.98 82.43 57.17</figDesc><table><row><cell>Method</cell><cell cols="3">FLOPs ACC ? mIoU ?</cell><cell>bg</cell><cell>shelf</cell><cell>Jar</cell><cell cols="5">Category IoU ? freezer window door eyeglass cup</cell><cell>wall</cell><cell>bowl</cell><cell>bottle</cell><cell>box</cell></row><row><cell>FPENet</cell><cell>0.76</cell><cell>70.31</cell><cell>10.14</cell><cell>74.97</cell><cell>0.01</cell><cell>0.00</cell><cell>0.02</cell><cell>2.11</cell><cell>2.83</cell><cell>0.00</cell><cell cols="2">16.84 24.81</cell><cell>0.00</cell><cell>0.04</cell><cell>0.00</cell></row><row><cell>ESPNetv2</cell><cell>0.83</cell><cell>73.03</cell><cell>12.27</cell><cell>78.98</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>6.17</cell><cell>0.00</cell><cell cols="2">30.65 37.03</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>ContextNet</cell><cell>0.87</cell><cell>86.75</cell><cell>46.69</cell><cell cols="12">89.86 37</cell></row><row><cell>ENet</cell><cell>2.09</cell><cell>71.67</cell><cell>8.50</cell><cell>79.74</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>22.25</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>HRNet w18</cell><cell>4.20</cell><cell>89.58</cell><cell>54.25</cell><cell cols="12">92.47 27.66 45.08 40.53 45.66 45.00 68.05 73.24 64.86 52.85 62.52 33.02</cell></row><row><cell>HardNet</cell><cell>4.42</cell><cell>90.19</cell><cell>56.19</cell><cell cols="12">92.87 34.62 47.50 42.40 49.78 49.19 62.33 72.93 68.32 58.14 65.33 30.90</cell></row><row><cell>DABNet</cell><cell>5.18</cell><cell>77.43</cell><cell>15.27</cell><cell>81.19</cell><cell>0.00</cell><cell>0.09</cell><cell>0.00</cell><cell>4.10</cell><cell>10.49</cell><cell>0.00</cell><cell cols="2">36.18 42.83</cell><cell>0.00</cell><cell>8.30</cell><cell>0.00</cell></row><row><cell>LEDNet</cell><cell>6.23</cell><cell>86.07</cell><cell>46.40</cell><cell cols="12">88.59 28.13 36.72 32.45 43.77 38.55 41.51 64.19 60.05 42.40 53.12 27.29</cell></row><row><cell>ICNet</cell><cell>10.64</cell><cell>78.23</cell><cell>23.39</cell><cell>83.29</cell><cell>2.96</cell><cell>4.91</cell><cell>9.33</cell><cell cols="5">19.24 15.35 24.11 44.54 41.49</cell><cell>7.58</cell><cell>27.47</cell><cell>3.80</cell></row><row><cell>BiSeNet</cell><cell>19.91</cell><cell>89.13</cell><cell>58.40</cell><cell cols="12">90.12 39.54 53.71 50.90 46.95 44.68 64.32 72.86 63.57 61.38 67.88 44.85</cell></row><row><cell>DenseASPP</cell><cell>36.20</cell><cell>90.86</cell><cell>63.01</cell><cell cols="12">91.39 42.41 60.93 64.75 48.97 51.40 65.72 75.64 67.93 67.03 70.26 49.64</cell></row><row><cell>DeepLabv3+</cell><cell>37.98</cell><cell>92.75</cell><cell>68.87</cell><cell cols="12">93.82 51.29 64.65 65.71 55.26 57.19 77.06 81.89 72.64 70.81 77.44 58.63</cell></row><row><cell>FCN</cell><cell>42.23</cell><cell>91.65</cell><cell>62.75</cell><cell cols="12">93.62 38.84 56.05 58.76 46.91 50.74 82.56 78.71 68.78 57.87 73.66 46.54</cell></row><row><cell>OCNet</cell><cell>43.31</cell><cell>92.03</cell><cell>66.31</cell><cell cols="12">93.12 41.47 63.54 60.05 54.10 51.01 79.57 81.95 69.40 68.44 78.41 54.65</cell></row><row><cell>RefineNet</cell><cell>44.56</cell><cell>87.99</cell><cell>58.18</cell><cell cols="12">90.63 30.62 53.17 55.95 42.72 46.59 70.85 76.01 62.91 57.05 70.34 41.32</cell></row><row><cell>Translab</cell><cell>61.31</cell><cell>92.67</cell><cell>69.00</cell><cell cols="12">93.90 54.36 64.48 65.14 54.58 57.72 79.85 81.61 72.82 69.63 77.50 56.43</cell></row><row><cell>DUNet</cell><cell cols="2">123.69 90.67</cell><cell>59.01</cell><cell cols="12">93.07 34.20 50.95 54.96 43.19 45.05 79.80 76.07 65.29 54.33 68.57 42.64</cell></row><row><cell>UNet</cell><cell cols="2">124.55 81.90</cell><cell>29.23</cell><cell>86.34</cell><cell>8.76</cell><cell cols="9">15.18 19.02 27.13 24.73 17.26 53.40 47.36 11.97 37.79</cell><cell>1.77</cell></row><row><cell>DANet</cell><cell cols="2">198.00 92.70</cell><cell>68.81</cell><cell cols="12">93.69 47.69 66.05 70.18 53.01 56.15 77.73 82.89 72.24 72.18 77.87 56.06</cell></row><row><cell>PSPNet</cell><cell cols="2">187.03 92.47</cell><cell>68.23</cell><cell cols="12">93.62 50.33 64.24 70.19 51.51 55.27 79.27 81.93 71.95 68.91 77.13 54.43</cell></row><row><cell>Trans2Seg</cell><cell>49.03</cell><cell>94.14</cell><cell>72.15</cell><cell>95.35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 -</head><label>4</label><figDesc>Evaluated state-of-the-art semantic segmentation methods. Sorted by FLOPs. Our proposes Trans2Seg surpasses all the other methods in pixel accuracy and mean IoU, as well as most of the category IoUs (8 in 11).</figDesc><table><row><cell>Image GroundTruth</cell><cell>Trans2Seg DeepLabv3+</cell><cell>FCN</cell><cell>ICNet</cell><cell>PSPNet</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 -</head><label>6</label><figDesc>The upper part of this table: the number of the scene. The lower part of this table: the interaction pattern of each category.</figDesc><table><row><cell>Image</cell><cell>GroundTruth</cell><cell>Trans2Seg</cell><cell>BiseNet</cell><cell>DeepLab</cell><cell>FCN</cell><cell>ICNet</cell><cell>PSPNet</cell><cell>UNet</cell><cell>HRNet</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">End-to-End object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>References [carion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hardnet: A low memory traffic network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Semantic image segmentation with deep convolutional nets and fully connected crfs. arXiv</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tom-net: Learning transparent object matting from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2012.00364</idno>
	</analytic>
	<monogr>
		<title level="m">Pre-trained image processing transformer</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>ECCV</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Devlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<editor>Jill Burstein, Christy Doran, and Thamar Solorio</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06-02" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
	</analytic>
	<monogr>
		<title level="m">Pattern Analysis, Statistical Modelling and Computational Learning</title>
		<editor>Foster et al., 2013] Paul Foster, Zhenghong Sun, Jong Jin Park, and Benjamin Kuipers</editor>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2213" to="2220" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep</note>
	<note>2013 IEEE International Conference on Robotics and Automation</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Superpixel convolutional networks using bilateral inceptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
	<note>ECCV</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gehring</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03122</idno>
		<title level="m">Convolutional sequence to sequence learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12556</idno>
	</analytic>
	<monogr>
		<title level="m">Xiangyu Zhang, Shaoqing Ren, and Jian Sun</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Localization of a mobile robot using a laser range finder in a glass-walled environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics &amp; Automation</title>
		<editor>Klank et al., 2011] Ulrich Klank, Daniel Carton, and Michael Beetz</editor>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="3616" to="3627" />
		</imprint>
	</monogr>
	<note>Transparent object detection and reconstruction on a mobile platform</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dabnet: Depth-wise asymmetric bottleneck for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9522" to="9531" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
	<note>Dfanet: Deep feature aggregation for real-time semantic segmentation</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Anton Van Den Hengel, and Ian Reid. Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Feature pyramid encoding network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Yin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning affinity via spatial propagation networks</title>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Espnetv2: A light-weight, power efficient, and general purpose convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="9190" to="9200" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Don&apos;t hit me! glass detection in real-world scenes</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Contextnet: Exploring context and detail for semantic segmentation in real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Meinhardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.02702</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
	<note>Trackformer: Multi-object tracking with transformers. Poudel et al., 2019. Fast-scnn: fast semantic segmentation network. arXiv</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Reaching and grasping a glass of water by locked-in als patients through a bci-controlled humanoid robot. Frontiers in Human Neuroscience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ronneberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15460</idno>
	</analytic>
	<monogr>
		<title level="m">Transtrack: Multiple-object tracking with transformer</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">357</biblScope>
			<biblScope unit="page" from="48" to="49" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>MICCAI</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Abhinav Gupta, and Kaiming He. Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Lednet: A lightweight encoder-decoder network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">End-to-end video instance segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.14503</idno>
		<idno>arXiv:2003.13948</idno>
	</analytic>
	<monogr>
		<title level="m">Mingyu Ding, Chunhua Shen, and Ping Luo. Segmenting transparent objects in the wild</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Xie et al., 2020</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Transcut: Transparent object segmentation from a light-field image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Denseaspp for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Icnet for realtime semantic segmentation on high-resolution images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang ; Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ocnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vibhav Vineet, Zhizhong Su, Dalong Du, Chang Huang, and Philip HS Torr. Conditional random fields as recurrent neural networks. In ICCV</title>
		<editor>Zheng et al., 2015] Shuai Zheng, Sadeep Jayasumana</editor>
		<meeting><address><addrLine>Bernardino Romera-Paredes</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Object context network for scene parsing. arXiv</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Rethinking semantic segmentation from a sequence-tosequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15840</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Plenoptic monte carlo object localization for robot grasping under layered translucency</title>
		<idno type="arXiv">arXiv:2010.04159</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<editor>Zhu et al., 2020] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai</editor>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Deformable detr: Deformable transformers for end-to-end object detection</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
