<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust Scene Text Recognition with Automatic Rectification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
							<email>shibaoguang@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Information and Communications</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Information and Communications</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyuan</forename><surname>Lyu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Information and Communications</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Information and Communications</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
							<email>xbai@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Information and Communications</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Robust Scene Text Recognition with Automatic Rectification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recognizing text in natural images is a challenging task with many unsolved problems. Different from those in documents, words in natural images often possess irregular shapes, which are caused by perspective distortion, curved character placement, etc. We propose RARE (Robust text recognizer with Automatic REctification), a recognition model that is robust to irregular text. RARE is a speciallydesigned deep neural network, which consists of a Spatial Transformer Network (STN) and a Sequence Recognition Network (SRN). In testing, an image is firstly rectified via a predicted Thin-Plate-Spline (TPS) transformation, into a more "readable" image for the following SRN, which recognizes text through a sequence recognition approach. We show that the model is able to recognize several types of irregular text, including perspective text and curved text. RARE is end-to-end trainable, requiring only images and associated text labels, making it convenient to train and deploy the model in practical systems. State-of-the-art or highly-competitive performance achieved on several benchmarks well demonstrates the effectiveness of the proposed model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In natural scenes, text appears on various kinds of objects, e.g. road signs, billboards, and product packaging. It carries rich and high-level semantic information that is important for image understanding. Recognizing text in images facilitates many real-world applications, such as geolocation, driverless car, and image-based machine translation. For these reasons, scene text recognition has attracted great interest from the community <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b14">15]</ref>. Despite the maturity of the research on Optical Character Recognition (OCR) <ref type="bibr" target="#b25">[26]</ref>, recognizing text in natural images, rather than scanned documents, is still challenging. Scene text images exhibit large variations in the aspects of illumination, mo- <ref type="bibr">Figure 1</ref>. Schematic overview of RARE, which consists a spatial transformer network (STN) and a sequence recognition network (SRN). The STN transforms an input image to a rectified image, while the SRN recognizes text. The two networks are jointly trained by the back-propagation algorithm <ref type="bibr" target="#b21">[22]</ref>. The dashed lines represent the flows of the back-propagated gradients. tion blur, text font, color, etc. Moreover, text in the wild may have irregular shape. For example, some scene text is perspective text <ref type="bibr" target="#b28">[29]</ref>, which is caused by side-view camera angles; some has curved shapes, meaning that its characters are placed along curves rather than straight lines. We call such text irregular text, in contrast to regular text which is horizontal and frontal.</p><p>Usually, a text recognizer works best when its input images contain tightly-bounded regular text. This motivates us to apply a spatial transformation prior to recognition, in order to rectify input images into ones that are more "readable" by recognizers. In this paper, we propose a recognition method that is robust to irregular text. Specifically, we construct a deep neural network that combines a Spatial Transformer Network <ref type="bibr" target="#b17">[18]</ref> (STN) and a Sequence Recognition Network (SRN). An overview of the model is given in <ref type="figure">Fig. 1</ref>.</p><p>In the STN, an input image is spatially transformed into a rectified image. Ideally, the STN produces an image that contains regular text, which is a more appropriate input for the SRN than the original one. The transformation is a thinplate-spline <ref type="bibr" target="#b5">[6]</ref> (TPS) transformation, whose nonlinearity allows us to rectify various types of irregular text, including perspective and curved text. The TPS transformation is configured by a set of fiducial points, whose coordinates are regressed by a convolutional neural network.</p><p>In an image that contains regular text, characters are arranged along a horizontal line. It bares some resemblance to a sequential signal. Motivated by this, for the SRN we construct an attention-based model <ref type="bibr" target="#b3">[4]</ref> that recognizes text in a sequence recognition approach. The SRN consists of an encoder and a decoder. Given an input image, the encoder generates a sequential feature representation, which is a sequence of feature vectors. The decoder recurrently generates a character sequence conditioning on the input sequence, by decoding the relevant contents which are determined by its attention mechanism at each step.</p><p>We show that, with proper initialization, the whole model can be trained end-to-end. Consequently, for the STN, we do not need to label any geometric ground truth, i.e. the positions of the TPS fiducial points, but let its training be supervised by the error differentials back-propagated by the SRN. In practice, the training eventually makes the STN tend to produce images that contain regular text, which are desirable inputs for the SRN.</p><p>The contributions of this paper are three-fold: First, we propose a novel scene text recognition method that is robust to irregular text. Second, our model extends the STN framework <ref type="bibr" target="#b17">[18]</ref> with an attention-based model. The original STN is only tested on plain convolutional neural networks. Third, our model adopts a convolutional-recurrent structure in the encoder of the SRN, thus is a novel variant of the attention-based model <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In recent years, a rich body of literature concerning scene text recognition has been published. Comprehensive surveys have been given in <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b43">44]</ref>. Among the traditional methods, many adopt bottom-up approaches, where individual characters are firstly detected using sliding window <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b34">35]</ref>, connected components <ref type="bibr" target="#b27">[28]</ref>, or Hough voting <ref type="bibr" target="#b38">[39]</ref>. Following that, the detected characters are integrated into words by means of dynamic programming, lexicon search <ref type="bibr" target="#b34">[35]</ref>, etc.. Other work adopts top-down approaches, where text is directly recognized from entire input images, rather than detecting and recognizing individual characters. For example, Alm?zan et al. <ref type="bibr" target="#b1">[2]</ref> propose to predict label embedding vectors from input images. Jaderberg et al. <ref type="bibr" target="#b16">[17]</ref> address text recognition with a 90k-class convolutional neural network, where each class corresponds to an English word. In <ref type="bibr" target="#b15">[16]</ref>, a CNN with a structured output layer is constructed for unconstrained text recognition. Some recent work models the problem as a sequence recognition problem, where text is represented by character sequence. Su and Lu <ref type="bibr" target="#b33">[34]</ref> extract sequential image representation, which is a sequence of HOG <ref type="bibr" target="#b9">[10]</ref> descriptors, and predict the corresponding character sequence with a recurrent neural network (RNN). Shi et al. <ref type="bibr" target="#b31">[32]</ref> propose an endto-end sequence recognition network which combines CNN and RNN. Our method also adopts the sequence prediction scheme, but we further take the problem of irregular text into account.</p><p>Although being common in the tasks of scene text detection and recognition, the issue of irregular text is relatively less addressed in explicit ways. Yao et al. <ref type="bibr" target="#b37">[38]</ref> firstly propose the multi-oriented text detection problem, and deal with it by carefully designing rotation-invariant region descriptors. Zhang et al. <ref type="bibr" target="#b41">[42]</ref> propose a character rectification method that leverages the low-rank structures of text. Phan et al. propose to explicitly rectify perspective distortions via SIFT <ref type="bibr" target="#b22">[23]</ref> descriptor matching. The above-mentioned work brings insightful ideas into this issue. However, most methods deal with only one type of irregular text with specifically designed schemes. Our method rectifies several types of irregular text in a unified way. Moreover, it does not require extra annotations for the rectification process, since the STN is supervised by the SRN during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Model</head><p>In this section we formulate our model. Overall, the model takes an input image I and outputs a sequence l = (l 1 , . . . , l T ), where l t is the t-th character, T is the variable string length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Spatial Transformer Network</head><p>The STN transforms an input image I to a rectified image I with a predicted TPS transformation. It follows the framework proposed in <ref type="bibr" target="#b17">[18]</ref>. As illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>, it first predicts a set of fiducial points via its localization network. Then, inside the grid generator, it calculates the TPS transformation parameters from the fiducial points, and generates a sampling grid on I. The sampler takes both the grid and the input image, it produces a rectified image I by sampling on the grid points.</p><p>A distinctive property of STN is that its sampler is differentiable. Therefore, once we have a differentiable localization network and a differentiable grid generator, the STN can back-propagate error differentials and gets trained.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Localization Network</head><p>The localization network localizes K fiducial points by directly regressing their x, y-coordinates. Here, constant K is an even number. The coordinates are denoted by C = [c 1 , . . . , c K ] ? 2?K , whose k-th column c k = [x k , y k ] contains the coordinates of the k-th fiducial point. We use a normalized coordinate system whose origin is the image center, so that x k , y k are within the range of [?1, 1].</p><p>We use a convolutional neural network (CNN) for the regression. Similar to the conventional structures <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b20">21]</ref>, the CNN contains convolutional layers, pooling layers and fully-connected layers. However, we use it for regression instead of classification. For the output layer, which is the last fully-connected layer, we set the number of output nodes to 2K and the activation function to tanh(?), so that its output vectors have values that are within the range of (?1, 1). Last, the output vector is reshaped into C.</p><p>The network localizes fiducial points based on global image contexts. It is expected to capture the overall text shape of an input image, and localizes fiducial points accordingly. It should be emphasized that we do not annotate coordinates of fiducial points for any sample. Instead, the training of the localization network is completely supervised by the gradients propagated by the other parts of the STN, following the back-propagation algorithm <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Grid Generator</head><p>The grid generator estimates the TPS transformation parameters, and generates a sampling grid. We first define another set of fiducial points, called the base fiducial points, denoted by C = [c 1 , . . . , c K ] ? 2?K . As illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>, the base fiducial points are evenly distributed along the top and bottom edge of a rectified image I . Since K is a constant and the coordinate system is normalized, C is always a constant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Image</head><p>Rectified Image The parameters of the TPS transformation is represented by a matrix T ? 2?(K+3) , which is computed by</p><formula xml:id="formula_0">T = ? ?1 C C 0 3?2 ,<label>(1)</label></formula><p>where ? C ? (K+3)?(K+3) is a matrix determined only by C , thus also a constant:</p><formula xml:id="formula_1">? C = ? ? 1 K?1 C R 0 0 1 1?K 0 0 C ? ? ,</formula><p>where the element on the i-th row and j-th column of R is</p><formula xml:id="formula_2">r i,j = d 2 i,j ln d 2 i,j , d i,j is the euclidean distance between c i and c j .</formula><p>The grid of pixels on a rectified image I is denoted by</p><formula xml:id="formula_3">P = {p i } i=1,...,N , where p i = [x i , y i ]</formula><p>is the x,ycoordinates of the i-th pixel, N is the number of pixels. As illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>, for every point p i on I , we find the corresponding point p i = [x i , y i ] on I, by applying the transformation:</p><formula xml:id="formula_4">r i,k = d 2 i,k ln d 2 i,k<label>(2)</label></formula><formula xml:id="formula_5">p i = 1, x i , y i , r i,1 , . . . , r i,K<label>(3)</label></formula><formula xml:id="formula_6">p i = Tp i ,<label>(4)</label></formula><p>where d i,k is the euclidean distance between p i and the k-th base fiducial point c k . By iterating over all points in P , we generate a grid P = {p i } i=1,...,N on the input image I. The grid generator can back-propagate gradients, since its two matrix multiplications, Eq. 1 and Eq. 4, are both differentiable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Sampler</head><p>Lastly, in the sampler, the pixel value of p i is bilinearly interpolated from the pixels near p i on the input image. By setting all pixel values, we get the rectified image I :</p><formula xml:id="formula_7">I = V (P, I),<label>(5)</label></formula><p>where V represents the bilinear sampler <ref type="bibr" target="#b17">[18]</ref>, which is also a differentiable module. The flexibility of the TPS transformation allows us to transform irregular text images into rectified images that contain regular text. In <ref type="figure" target="#fig_4">Fig. 4</ref>, we show some common types of irregular text, including a) loosely-bounded text, which resulted by imperfect text detection; b) multi-oriented text, caused by non-horizontal camera views; c) perspective text, caused by side-view camera angles; d) curved text, a commonly seen artistic style. The STN is able to rectify images that contain these types of irregular text, making them more readable for the following recognizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Sequence Recognition Network</head><p>Since target words are inherently sequences of characters, we model the recognition problem as a sequence recognition problem, and address it with a sequence recognition network. The input to the SRN is a rectified image I , which  ideally contains a word that is written horizontally from left to right. We extract a sequential representation from I , and recognize a word from it.</p><p>In our model, the SRN is an attention-based model <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8]</ref>, which directly recognizes a sequence from an input image. The SRN consists of an encoder and a decoder. The encoder extracts a sequential representation from the input image I . The decoder recurrently generates a sequence conditioned on the sequential representation, by decoding the relevant contents it attends to at each step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Encoder: Convolutional-Recurrent Network</head><p>A na?ve approach for extracting a sequential representation for I is to take local image patches from left to right, and describe each of them with a CNN. However, this approach does not share the computation among overlapping patches, thus inefficient. Besides, the spatial dependencies between the patches are not exploited and leveraged. Instead, following <ref type="bibr" target="#b31">[32]</ref>, we build a network that combines convolutional layers and recurrent networks. The network extracts a sequence of feature vectors, given an input image of arbitrary size.</p><p>As illustrated in <ref type="figure" target="#fig_5">Fig. 5</ref>, at the bottom of the encoder is several convolutional layers. They produce feature maps that are robust and high-level descriptions of an input image. Suppose the feature maps have the size D conv ? H conv ? W conv , where D conv is the depth, and H conv , W conv are the height and width respectively. The next operation is to convert the maps into a sequence of W conv vectors, each has D conv W conv dimensions. Specifically, the "map-to-sequence" operation takes out the columns of the maps in the left-to-right order, and flattens them into vectors. According to the translation invariance property of CNN, each vector corresponds to a local image region, i.e. receptive field, and is a descriptor for that region.</p><p>Restricted by the sizes of the receptive fields, the feature sequence leverages limited image contexts. We further apply a two-layer Bidirectional Long-Short Term Memory (BLSTM) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b12">13]</ref> network to the sequence, in order to model the long-term dependencies within the sequence. The BLSTM is a recurrent network that can analyze the dependencies within a sequence in both directions, it outputs another sequence which has the same length as the input one. The output sequence is h = (h 1 , . . . , h L ), where L = W conv .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Decoder: Recurrent Character Generator</head><p>The decoder recurrently generates a sequence of characters, conditioned on the sequence produced by the encoder. It is a recurrent neural network with the attention structure proposed in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8]</ref>. In the recurrency part, we adopt the Gated Recurrent Unit (GRU) <ref type="bibr" target="#b6">[7]</ref> as the cell.</p><p>The generation is a T -step process, at step t, the decoder computes a vector of attention weights ? t ? L via the attention process described in <ref type="bibr" target="#b7">[8]</ref>:</p><formula xml:id="formula_8">? t = Attend(s t?1 , ? t?1 , h),<label>(6)</label></formula><p>where s t?1 is the state variable of the GRU cell at the last step. For t = 1, both s 0 and ? 0 are zero vectors. Then, a glimpse g t is computed by linearly combining the vectors in h: g t = L i=1 ? ti h i . Since ? t has non-negative values that sum to one, it effectively controls where the decoder focuses on.</p><p>The state s t?1 is updated via the recurrent process of GRU <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>:</p><formula xml:id="formula_9">s t = GRU(l t?1 , g t , s t?1 ),<label>(7)</label></formula><p>where l t?1 is the (t ? 1)-th ground-truth label in training, while in testing, it is the label predicted in the previous step, i.e.l t?1 .</p><p>The probability distribution over the label space is estimated by:? t = softmax(W s t ).</p><p>Following that, a characterl t is predicted by taking the class with the highest probability. The label space includes all English alphanumeric characters, plus a special "end-ofsequence" (EOS) token, which ends the generation process.</p><p>The SRN directly maps a input sequence to another sequence. Both input and output sequences may have arbitrary lengths. It can be trained with only word images and associated text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Model Training</head><p>We denote the training set by X = {(I (i) , l (i) )} i=1...N . To train the model, we minimize the negative log-likelihood over X :</p><formula xml:id="formula_11">L = N i=1 log |l (i) | t=1 p(l (i) t |I (i) ; ?),<label>(9)</label></formula><p>where the probability p(?) is computed by Eq. 8, ? is the parameters of both STN and SRN. The optimization algorithm is the ADADELTA <ref type="bibr" target="#b40">[41]</ref>, which we find fast in convergence speed.</p><p>(a) (b) (c) <ref type="figure" target="#fig_6">Figure 6</ref>. Some initialization patterns for the fiducial points.</p><p>The model parameters are randomly initialized, except the localization network, whose output fully-connected layer is initialized by setting weights to zero. The initial biases are set to such values that yield the fiducial points pattern displayed in <ref type="figure" target="#fig_6">Fig. 6</ref>.a. Empirically, we also find that the patterns displayed   <ref type="figure">Figure 7</ref>. A prefix tree of three words: "ten", "tea", and "to". and ? are the tree root and the EOS token respectively. The recognition starts from the tree root. At each step the posterior probabilities of all child nodes are computed. The child node with the highest probability is selected as the next node. The process iterates until a leaf node is reached. Numbers on the edges are the posterior probabilities. Blue nodes are the selected nodes. In this case, the predicted word is "tea".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Recognizing With a Lexicon</head><p>When a test image is associated with a lexicon, i.e. a set of words for selection, the recognition process is to pick the word with the highest posterior conditional probability:</p><formula xml:id="formula_12">l * = arg max l log |l| t=1 p(l t |I; ?).<label>(10)</label></formula><p>However, on very large lexicons, e.g. the Hunspell <ref type="bibr" target="#b0">[1]</ref> which contains more than 50k words, computing Eq. 10 is time consuming, as it requires iterating over all lexicon words. We adopt an efficient approximate search scheme on large lexicons. The motivation is that computation can be shared among words that share the same prefix.</p><p>We first construct a prefix tree over a given lexicon. As illustrated in <ref type="figure">Fig. 7</ref>, each node of the tree is a character label. Nodes on a path from the root to a leaf forms a word (including the EOS). In testing, we start from the root node, every time the model outputs a distribution? t , the child node with the highest posterior probability is selected as the next node to move to. The process repeats until a leaf node is reached, and a word is found on the path from the root to that leaf. Since the tree depth is at most the length of the longest word in the lexicon, this search process takes much less computation than the precise search.</p><p>Recognition performance could be further improved by incorporating beam search. A list of nodes is maintained, and the above search process is repeated on each of them. After each step, the list is updated to store the nodes with top-B accumulated log-likelihoods, where B is the beam width. Larger beam width usually results in better performance, but lower search speed. <ref type="table">Table 1</ref>. Recognition accuracies on general recognition benchmarks. The titles "50", "1k" and "50k" are lexicon sizes. The "Full" lexicon contains all per-image lexicon words. "None" means recognition without a lexicon. <ref type="table" target="#tab_2">IIIT5K  SVT  IC03  IC13  50  1k  None  50  None  50  Full  50k  None  None  ABBYY [35]</ref> 24. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section we evaluate our model on a number of standard scene text recognition benchmarks, paying special attention to recognition performance on irregular text. First we evaluate our model on some general recognition benchmarks, which mainly consist of regular text, but irregular text also exists. Next, we perform evaluations on benchmarks that are specially designed for irregular text recognition. For all benchmarks, performance is measured by word accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Spatial Transformer Network The localization network of STN has 4 convolution layers, each followed by a 2 ? 2 max-pooling layer. The filter size, padding size and stride are 3, 1, 1 respectively, for all convolutional layers. The number of filters are respectively 64, 128, 256 and 512. Following the convolutional and the max-pooling layers is two fully-connected layers with 1024 hidden units. We set the number of fiducial points to K = 20, meaning that the localization network outputs a 40-dimensional vector. Activation functions for all layers are the ReLU <ref type="bibr" target="#b26">[27]</ref>, except the output layer which uses tanh(?). Sequence Recognition Network In the SRN, the encoder has 7 convolutional layers, whose {filter size, number of filters, stride, padding size} are respec- <ref type="figure" target="#fig_1">tively {3,64,1,1}, {3,128,1,1}, {3,256,1,1}, {3,256,1,1,},  {3,512,1,1}, {3,512,1,1}, and {2,512,1,0}</ref>. The 1st, 2nd, 4th, 6th convolutional layers are each followed by a 2 ? 2 max-pooling layer. On the top of the convolutional layers is a two-layer BLSTM network, each LSTM has 256 hidden units. For the decoder, we use a GRU cell that has 256 memory blocks and 37 output units (26 letters, 10 digits, and 1 EOS token). Model Training Our model is trained on the 8-million synthetic samples released by Jaderberg et al. <ref type="bibr" target="#b14">[15]</ref>. No extra data is used. The batch size is set to 64 in training. Following <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b15">16]</ref>, images are resized to 100 ? 32 in both training and testing. The output size of the STN is also 100?32. Our model processes ?160 samples per second during training, and converges in 2 days after ?3 epochs over the training dataset. Implementation We implement our model under the Torch7 framework <ref type="bibr" target="#b8">[9]</ref>. Most parts of the model are GPUaccelerated. All our experiments are carried out on a workstation which has one Intel Xeon(R) E5-2620 2.40GHz CPU, an NVIDIA GTX-Titan GPU, and 64GB RAM.</p><p>Without a lexicon, the model takes less than 2ms recognizing an image. With a lexicon, recognition speed depends on the lexicon size. We adopt the precise search (Sec. 3.4) when lexicon size ? 1k. On larger lexicons, we adopt the approximate beam search (Sec. 3.4) with a beam width of 7. With a 50k-word lexicon, the search takes ?200ms per image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results on General Benchmarks</head><p>Our model is firstly evaluated on benchmarks that are designed for general scene text recognition tasks. Samples in these benchmarks mostly contain regular text, but irregular text also exists. The benchmark datasets are:</p><p>? IIIT 5K-Words <ref type="bibr" target="#b24">[25]</ref> (IIIT5K) contains 3000 cropped word images for testing. The images are collected from the Internet. For each image, there is a 50-word lexicon and a 1000-word lexicon. All lexicons consist of a ground truth word and some randomly picked words.</p><p>? Street View Text <ref type="bibr" target="#b34">[35]</ref> (SVT) is collected from Google Street View. Its test dataset consists of 647 word images. Many images in SVT are severely corrupted by noise and blur, or have very low resolutions. Each sample is associated with a 50-word lexicon.</p><p>? ICDAR 2003 <ref type="bibr" target="#b23">[24]</ref> (IC03) contains 860 cropped word images, each associated with a 50-word lexicon defined by Wang et al. <ref type="bibr" target="#b34">[35]</ref>. Following <ref type="bibr" target="#b34">[35]</ref>, we discard images that contain non-alphanumeric characters or have less than three characters. Besides, there is a "full lexicon" which contains all lexicon words, and the Hunspell <ref type="bibr" target="#b0">[1]</ref> lexicon which has 50k words.</p><p>? ICDAR 2013 <ref type="bibr" target="#b19">[20]</ref> (IC13) inherits most of its samples from IC03. After filtering samples as done in IC03, the dataset contains 857 samples.</p><p>In Tab. 1 we report our results, and compare them with other methods. On unconstrained recognition tasks (recognizing without a lexicon), our model outperforms all the other methods in comparison. On IIIT5K, RARE outperforms prior art CRNN <ref type="bibr" target="#b31">[32]</ref> by nearly 4 percentages, indicating a clear improvement in performance. We observe that IIIT5K contains a lot of irregular text, especially curved text, while RARE has an advantage in dealing with irregular text. Note that, although our model falls behind <ref type="bibr" target="#b16">[17]</ref> on some datasets, our model differs from <ref type="bibr" target="#b16">[17]</ref> in that it is able recognize random strings such as telephone numbers, while <ref type="bibr" target="#b16">[17]</ref> only recognizes words that are in its 90k-dictionary. On constrained recognition tasks (recognizing with a lexicon), RARE achieves state-of-the-art or highly competitive accuracies. On IIIT5K, SVT and IC03, constrained recognition accuracies are on par with <ref type="bibr" target="#b16">[17]</ref>, and slightly lower than <ref type="bibr" target="#b31">[32]</ref>.</p><p>We also train and test a model that contains only the SRN. As reported in the last row of Tab. 1, we see that the SRN-only model is also a very competitive recognizer, achieving higher or competitive performance on most of the benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Recognizing Perspective Text</head><p>To validate the effectiveness of the rectification scheme, we evaluate RARE on the task of perspective text recognition. SVT-Perspective <ref type="bibr" target="#b28">[29]</ref> is specifically designed for evaluating performance of perspective text recognition algorithms. Text samples in SVT-Perspective are picked from side view angles in Google Street View, thus most of them are heavily deformed by perspective distortion. Some examples are shown in <ref type="figure" target="#fig_8">Fig. 8</ref>.a. SVT-Perspective consists of 639 cropped images for testing. Each image is associated with a 50-word lexicon, which is inherited from the SVT <ref type="bibr" target="#b34">[35]</ref> dataset. In addition, there is a "Full" lexicon which contains all the per-image lexicon words. We use the same model trained on the synthetic dataset without fine-tuning. For comparison, we test the CRNN model <ref type="bibr" target="#b31">[32]</ref> on SVT-Perspective. We also compare RARE with <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b28">29]</ref>, whose recognition accuracies are reported in <ref type="bibr" target="#b28">[29]</ref>. <ref type="table">Table 2</ref>. Recognition accuracies on SVT-Perspective <ref type="bibr" target="#b28">[29]</ref>. "50" and "Full" represent recognition with 50-word lexicons and the full lexicon respectively. "None" represents recognition without a lexicon. Tab. 2 summarizes the results. In the second and third columns, we compare the accuracies of recognition with the 50-word lexicon and the full lexicon. Our method outperforms <ref type="bibr" target="#b28">[29]</ref>, which is a perspective text recognition method, by a large margin on both lexicons. However, this gap is partially due to that we use a much larger training set than <ref type="bibr" target="#b28">[29]</ref>. In the comparisons with <ref type="bibr" target="#b31">[32]</ref>, which uses the same training set as RARE, we still observe significant improvements in both the Full lexicon and the lexicon-free settings. Furthermore, recall the results in Tab. 1, on SVT-Perspective RARE outperforms <ref type="bibr" target="#b31">[32]</ref> by a even larger margin. The reason is that the SVT-perspective dataset mainly consists of perspective text, which is inappropriate for direct recognition. Our rectification scheme can significantly alleviate this problem.</p><p>In <ref type="figure" target="#fig_9">Fig. 9</ref> we present some qualitative analysis. Fiducial points predicted by the STN are plotted on input images in green crosses. We see that the STN tends to place fiducial points along upper and lower edges of scene text, and where green crosses are the predicted fiducial points. The middle column is the rectified images (we use gray-scale images for recognition). The right column is the recognized text and the ground truth text. Green and red characters are correctly and mistakenly recognized characters, respectively. The first five rows are taken from SVT-Perspective <ref type="bibr" target="#b28">[29]</ref>, the rest rows are taken from CUTE80 <ref type="bibr" target="#b29">[30]</ref>.</p><p>hence produces rectified images that are more readable for the SRN. However, the STN fails sometimes in the case of heavy perspective distortion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Recognizing Curved Text</head><p>Curved text is a commonly seen artistic-style text in natural scenes. Due to its irregular character placement, recognizing curved text is very challenging. CUTE80 <ref type="bibr" target="#b29">[30]</ref> focuses on the recognition of curved text. The dataset contains 80 high-resolution images taken in natural scenes. Originally, the dataset is proposed for detection tasks. We crop the words, resulting in 288 word images for testing. For comparisons, we evaluate the trained models of <ref type="bibr" target="#b16">[17]</ref> and <ref type="bibr" target="#b31">[32]</ref>. All models are evaluated without a lexicon.</p><p>From the results summarized in Tab. 3, we see that RARE outperforms the other two methods by a large margin. <ref type="bibr" target="#b16">[17]</ref> is a constrained recognition model, it cannot recognize words that are not in its dictionary. <ref type="bibr" target="#b31">[32]</ref> is able to recognize arbitrary words, but it does not have a specific mechanism for handling curved text. Our model rectifies images that contain curved text before recognizing them. Therefore, it is advantageous on this task. In <ref type="figure" target="#fig_9">Fig. 9</ref>, we demonstrate the effect of rectification through some examples. Generally, the rectification made by the STN is not perfect, but it alleviates the recognition difficulty to some extent. RARE tends to fail when curve angles are too large, as shown in the last two rows of <ref type="figure" target="#fig_9">Fig. 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We study a common but difficult problem in scene text recognition, called the irregular text problem. Traditional solutions typically use a separate text rectification component. We address this problem in a more feasible and elegant way by adopting a differentiable spatial transformer network module. In addition, the spatial transformer network is connected to an attention-based sequence recognizer, allowing us to train the whole model end-to-end. The extensive experimental results show that 1) without geometric supervision, the learned model can automatically generate more "readable" images for both human and the sequence recognition network; 2) the proposed text rectification method can significantly improve recognition accuracies on irregular scene text; 3) the proposed scene text recognition system is competitive compared with the stateof-the-arts. In the future, we plan to address the end-toend scene text reading problem through the combination of RARE with a scene text detection method, e.g. <ref type="bibr" target="#b42">[43]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Structure of the STN. The localization network localizes a set of fiducial points C, with which the grid generator generates a sampling grid P. The sampler produces a rectified image I , given I and P.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Fiducial points and the TPS transformation. Green markers on the left image are the fiducial points C. Cyan markers on the right image are the base fiducial points C . The transformation T is represented by the pink arrow. For a point (x i , y i ) on I , the transformation T finds the corresponding point (xi, yi) on I.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>The STN rectifies images that contain several types of irregular text. Green markers are the predicted fiducial points on the input images. The STN can deal with several types of irregular text, including (a) loosely-bounded text; (b) multi-oriented text; (c) perspective text; (d) curved text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Structure of the SRN, which consists of an encoder and a decoder. The encoder uses several convolution layers (ConvNet) and a two-layer BLSTM network to extract a sequential representation (h) for the input image. The decoder generates a character sequence (including the EOS token) conditioned on h.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>b and Fig. 6.c yield relatively poorer performance. Randomly initializing the localization network results in failure of convergence during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Examples of irregular text. a) Perspective text. Samples are taken from the SVT-Perspective [29] dataset; b) Curved text. Samples are taken from the CUTE80 [30] dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 .</head><label>9</label><figDesc>Examples showing the rectifications our model makes and the recognition results. The left column is the input images,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Recognition accuracies on CUTE80<ref type="bibr" target="#b28">[29]</ref>.</figDesc><table><row><cell>Method</cell><cell>Accuracy</cell></row><row><cell>Jaderberg et al. [17]</cell><cell>42.7</cell></row><row><cell>Shi et al. [32]</cell><cell>54.9</cell></row><row><cell>RARE</cell><cell>59.2</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="http://hunspell.sourceforge.net/" />
		<title level="m">Hunspell</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Word spotting and recognition with embedded attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Almaz?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Forn?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Valveny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2552" to="2566" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">End-to-end text recognition with hybrid hmm maxout models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Alsharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Photoocr: Reading text in uncontrolled conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Principal warps: Thin-plate splines and the decomposition of deformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">L</forename><surname>Bookstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="567" to="585" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation: Encoderdecoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.1259</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1506.07503</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Torch7: A matlab-like environment for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BigLearn, NIPS Workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Whole is greater than sum of parts: Recognizing scene text words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Supervised mid-level features for word image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICASSP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Synthetic data and artificial neural networks for natural scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep structured output learning for unconstrained text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Reading text in the wild with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<title level="m">Spatial transformer networks. CoRR, abs/1506.02025</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep features for text spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Mestre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Mota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Almaz?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heras</surname></persName>
		</author>
		<title level="m">ICDAR 2013 robust reading competition</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>ICDAR</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Panaretos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nagai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jolion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Todoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR 2003 robust reading competitions: entries, results, and future directions</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="105" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Scene text recognition using higher order language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Twenty years of document image analysis in PAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nagy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="38" to="62" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Real-time scene text localization and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Recognizing text with perspective distortion in natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shivakumara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A robust arbitrary text detection system for natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Risnumawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shivakumara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="8027" to="8048" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Label embedding: A frugal baseline for text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Rodr?guez-Serrano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="193" to="207" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<idno>abs/1507.05717</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Accurate scene text recognition based on recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">End-to-end scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Word spotting in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">End-to-end text recognition with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Detecting texts of arbitrary orientations in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Strokelets: A learned multi-scale representation for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Text detection and recognition in imagery: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1480" to="1500" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">ADADELTA: an adaptive learning rate method. CoRR, abs/1212</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5701</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">TILT: transform invariant low-rank textures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multi-oriented text detection with fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Scene text detection and recognition: recent advances and future trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers of Computer Science</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="36" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
