<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Billion-scale semi-supervised learning for image classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Zeki</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalniz</forename><surname>Herv?</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?gou</forename><surname>Kan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Facebook</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ai</forename><forename type="middle">Manohar</forename><surname>Paluri</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
						</author>
						<title level="a" type="main">Billion-scale semi-supervised learning for image classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a study of semi-supervised learning with large convolutional networks. We propose a pipeline, based on a teacher/student paradigm, that leverages a large collection of unlabelled images (up to 1 billion). Our main goal is to improve the performance for a given target architecture, like ResNet-50 or ResNext. We provide an extensive analysis of the success factors of our approach, which leads us to formulate some recommendations to produce high-accuracy models for image classification with semisupervised learning. As a result, our approach brings important gains to standard architectures for image, video and fine-grained classification. For instance, by leveraging one billion unlabelled images, our learned vanilla ResNet-50 achieves 81.2% top-1 accuracy on Imagenet benchmark.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently, image and video classification techniques leveraging web-scale weakly supervised datasets have achieved state-of-the-art performance on variety of problems, including image classification, fine-grain recognition, etc. However, weak supervision in the form of tags has a number of drawbacks. There is a significant amount of inherent noise in the labels due to non-visual, missing and irrelevant tags which can significantly hamper the learning of models. Moreover, weakly supervised web-scale datasets typically follow a Zipfian or long-tail label distribution that favors good performance only for the most prominent labels. Lastly, these approaches assumes the availability of large weakly supervised datasets for the target task, which is not the case in many applications.</p><p>This paper explores web-scale semi-supervised deep learning. We leverage billions of unlabeled images along with a relatively smaller set of task-specific labeled data. To the best of our knowledge, semi-supervised learning with neural networks has not been explored before at this scale. In order to scale our approach, we propose the simple semisupervised strategy depicted in <ref type="figure">Figure 1</ref>: <ref type="bibr" target="#b0">(1)</ref> We train on the labeled data to get an initial teacher model; <ref type="bibr" target="#b1">(2)</ref> For each class/label, we use the predictions of this teacher model to rank the unlabeled images and pick top-K images to con-  <ref type="figure">Figure 1</ref>: Illustration of our approach: with a strong teacher model, we extract from a very large unlabelled image collection (100M-1 billion images) a new (large) training set. The student model is first trained with this noisy supervision, and fine-tuned with the original dataset. struct a new training data; <ref type="bibr" target="#b2">(3)</ref> We use this data to train a student model, which typically differs from the teacher model: hence we can target to reduce the complexity at test time; (4) finally, pre-trained student model is fine-tuned on the initial labeled data to circumvent potential labeling errors.</p><p>Compared to most weakly-supervised approaches, we circumvent the issue of long-tail distribution by selecting same number of images per label. This is possible since for a tail class like "African Dwarf Kingfisher" bird, there may not be sufficient number of weakly-supervised/tagged examples, but a lot of un-tagged images of this bird is likely to exist in the unlabelled dataset. A teacher model trained with labels is able to identify enough images from the unlabeled data and hence to improve the recall for tail classes.</p><p>The choice of the semi-supervised algorithm described above is motivated by several approaches, such as selftraining, distillation, or boosting. In fact, under some specific settings of our teacher and student models, and of the  <ref type="bibr" target="#b0">1</ref>. Train with a teacher/student paradigm: It produces a better model for a fixed complexity, even if the student and teacher have the same architecture. 2. Fine-tune the model with true labels only. 3. Large-scale unlabelled dataset is key to performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>Use a large number of pre-training iterations (as opposed to a vanilla supervised setting, where a number of epochs as used in common practice is sufficient). 5. Build a balanced distribution for inferred labels. 6. Pre-training the (high-capacity) teacher model by weak supervision (tags) further improves the results.</p><p>type of data, our work boils down to these approaches. Nevertheless, our paper analyzes several variants, such as considering a pure self-training learning procedure by removing the teacher/student model, or fine-tuning on the inferred labels instead of only those from the labelled set.</p><p>Our reference pipeline trains a teacher model that is more powerful than the student model. Even in the case of selftraining -when teacher and student models are the same-, the sheer scale of unlabeled data allows us to achieve significant gains. Our analysis shows the performance is sensitive to several factors like strength of initial (teacher) model for ranking, scale and nature of unlabeled data, relationship between teacher and final model, etc. In particular, we analyze the performance as a function of relative strength of teacher and student models. We also show that leveraging hashtags or queries in search as a weak-supervision signal to collect the unlabeled dataset significantly boosts the performance.</p><p>Overall, this paper makes the following contributions:</p><p>? We explore the semi-supervised deep-learning at a large scale of billions of unlabeled examples and show that a simple strategy of ranking unlabeled images with a teacher model trained on labeled data works is effective for learning a powerful (student) network.</p><p>? We analyze the conditions under which such a strategy gives a substantial benefit. Inspired by a prior work <ref type="bibr" target="#b32">[32]</ref> on large-scale supervised learning, we summarize these recommendations in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>? More specifically, we present a detailed analysis and ablations of various design choices, like strength of student and teacher CNN models, nature of unlabeled dataset, number of examples selected per label, etc.</p><p>? We demonstrate the performance of our method on popular classification benchmarks for both images and videos and significantly outperforms the state of the art.</p><p>? Our proposal is also effective in other tasks, namely video classification and fine-grain recognition.</p><p>This paper is organized as follows. Section 2 introduces related works and terminology. Section 3 introduces our approach. Section 4 provides an extensive analysis and ablation studies for the classification task. Other tasks are evaluated in Section 5. Section 6 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section we review several topics and specific works related to our approach. Since the terminology is not always consistent across papers, we clarify the different concepts employed through our paper.</p><p>Image classification. Our work mainly targets at improving image classification, whose performance is routinely evaluated on benchmarks like Imagenet <ref type="bibr" target="#b7">[8]</ref>. In the recent years, a lot of effort has been devoted to improving the neural networks architectures. For instance the introduction of residual connections <ref type="bibr" target="#b16">[16]</ref> has led to several competitive convolutional neural networks <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b43">43]</ref> that are widely adopted by the community. More recently, other directions have been investigated to further push the state of the art <ref type="bibr" target="#b0">[1]</ref> One proposal is to learn a gigantic convolutional neural network taking high-resolution images on input <ref type="bibr" target="#b21">[21]</ref>, as resolution is a key factor to classification performance <ref type="bibr" target="#b17">[17]</ref>. Another line of research <ref type="bibr" target="#b27">[27]</ref> learns a high-capacity convnet with a large weakly labeled dataset, in which annotation is provided in the form of tags. Transfer learning is a task in which one re-uses a network trained on a large labelled corpus to solve other tasks for which less data is available. This is a historical line of research in machine learning, see the survey <ref type="bibr" target="#b30">[30]</ref> by Pan and Yang for an extensive classification of the different problems and approaches related to this topic. This approach is typically required for tasks for which no enough labels are available <ref type="bibr" target="#b29">[29]</ref>, such a fine-grain classification or detection (although a recent work <ref type="bibr" target="#b15">[15]</ref> shows that learning from scratch is possible in the latter case).</p><p>On a side note, transfer learning is employed to evaluate the quality of unsupervised learning <ref type="bibr" target="#b9">[10]</ref>: a network is trained with unlabelled data on a proxy task, and the quality of the trunk is evaluated by fine-tuning the last layer on a target task. A particular case of transfer is the task of low-shot learning, where one has to learn with very few examples of a new class. Typical works on this line of research propose parameter prediction <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b33">33]</ref> and data augmentation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">14]</ref>.</p><p>Semi-supervised learning. Recent approaches have investigated the interest of using additional unlabelled data to improve the supervision. To the best of our knowledge, most results from the literature obtained mitigated results <ref type="bibr" target="#b27">[27]</ref>, except those employing weak labels for the additional data, which is a form a weak supervision.</p><p>Recent approaches <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b35">35]</ref> revisited traditional label propagation algorithms (See <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9]</ref> for a review) to leverage the large YFCC unlabelled collection for low-shot learning. Douze et al. <ref type="bibr" target="#b10">[11]</ref> improve classification when the number of labelled data for the new classes is very low (1-5), but the gain disappears when more labelled data is available. Additionally this strategy has an important limitation: it requires to compute a large graph connecting neighboring images, which is constructed with an adhoc descriptor. This class of approach is often referred to as transductive methods: a few labels are provided and the goal is to extend the labeling to the unlabeled data. A classifier must be learned to provide an out-of-sample extension <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">23]</ref>.</p><p>Distillation was originally introduced to compress a large model <ref type="bibr" target="#b18">[18]</ref>, called the teacher, into a smaller one. The distillation procedure amounts to training the small model (the student) such that it reproduces at best the output of the teacher. The student model is typically smaller and faster than the teacher model. Distillation can be seen as a particular case of self-training, in that the teacher model makes prediction on unlabelled data, and the inferred labels are used to train the student in a supervised fashion. This strategy was shown successful for several tasks, like detection <ref type="bibr" target="#b34">[34]</ref>. On a related note, back-translation <ref type="bibr" target="#b37">[37]</ref> is key to performance in translation <ref type="bibr" target="#b26">[26]</ref>, offering large gains by leveraging (unannotated) monolingual data.</p><p>To the best of our knowledge, our paper is the first to demonstrate that and how it can be effective for image classification by leveraging a large amount of unannotated data.</p><p>Data augmentation is a key ingredient for performance in image classification, as demonstrated by Krizhevsky et al. <ref type="bibr" target="#b25">[25]</ref> in their seminal paper. Several recent strategies have been successful at improving classification accuracy by learning the transformation learned for augmentation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b31">31]</ref> or by synthesizing mixtures of images <ref type="bibr" target="#b45">[45]</ref>. To some extent, our semi-supervised approach is a special form of dataaugmentation, in that we add new actual images from a supplemental unlabelled dataset to improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our semi-supervised training pipeline</head><p>This section introduces our reference large-scale semisupervised approach. This algorithm builds upon several concepts from the literature. We complement them with good practices and new insights gathered from our extensive experimental study to achieve a successful learning. We then discuss several key considerations regarding our approach and provides some elements of intuition and motivation supporting our choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem statement: semi-supervised learning</head><p>We consider a multi-class image classification task, where the goal is to assign to a given input image a label.</p><p>We assume that we are given a labeled set D comprising N examples with corresponding labels. In particular we will consider the Imagenet ILSVRC classification benchmark, which provides a labeled training set of N = 1.2 million images covering 1000 classes.</p><p>In addition, we consider a large collection U of M N unlabeled images. Note that the class distribution of examples in U may be different from those in D. Our semisupervised learning task extends the original problem by allowing to train the models on D ? U, i.e., to exploit unlabelled data for the learning. The inference, and therefore the performance evaluation, is unchanged, and follow the protocol of the target task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Our learning pipeline</head><p>Our semi-supervised approach consists of four stages:</p><p>(1) Train a teacher model on labeled data D;</p><p>(2) Run the trained model on unlabeled data U and select relevant examples for each label to construct a new labeled datasetD;</p><p>(3) Train a new student model onD;</p><p>(4) Fine-tune the trained student on the labeled set D.</p><p>This pipeline is akin to existing distillation works. We depart from the literature in (i) how we jointly exploit unlabelled and labelled data; (ii) how we constructD; (iii) the scale we consider and targeting improvement on Imagenet. We now discuss these stages into more details.</p><p>Teacher model training. We train a teacher model on the labeled dataset D to label the examples in U. The main advantage of this approach is that inference is highly parallelizable. Therefore we perform it on billions of examples in a short amount of time, either on CPUs or GPUs. A good teacher model is required for this stage, as it is responsible to remove unreliable examples from U in order to label enough relevant examples correctly without introducing substantial labelling noise.</p><p>Data selection and labeling. Our algorithm leverage the desirable property that learning is tolerant to a certain degree of noise <ref type="bibr" target="#b28">[28]</ref>. Our goal here is to collect a large number of images while limiting the labelling noise. Due to large scale of unlabeled data, a large proportion of data may not even contain any of the classes in I. We propose to select top-K examples from U for each target label. First, the teacher model is run on each example in U to obtain the softmax prediction vector. For each image, we retain only the classes associated with the P highest scores, P being a parameter accounting for the fact that we expect only a few number of relevant classes to occur in each image. The reason for choosing P &gt; 1 is that it is difficult to identify accurately under-represented concepts, or some may be oc-  <ref type="figure">Figure 2</ref>: Examples of images from YFCC100M collected by our procedure for the classes "Tiger shark", "Leaf beetle" and "American black bear" for a few ranks.</p><p>culted by more prominent co-occurring concepts. The role of this parameter is analyzed into more details in Section 4. Then, for each class l, we rank the images based on the corresponding classification scores. The top-K images define the set ofD l of images that are henceforth considered as new positive training examples for that class. We also experiment with using raw predictions before applying the softmax for ranking, but found that they perform worse (see supplementary material). The new image training set collected for the multi-class classification problem is therefore defined asD = L l=1D l . <ref type="figure">Figure 2</ref> shows the qualitative results of ranking YFCC100M dataset <ref type="bibr" target="#b39">[39]</ref> with ResNet-50 <ref type="bibr" target="#b16">[16]</ref> teacher model trained on ImageNet-val <ref type="bibr" target="#b36">[36]</ref> for 5 classes. As expected, the images at the top of ranking are simple and clean without much labelling noise. They become progressively less obvious positives as we go down in the ranking. Introducing such hard examples is important to get a good classifier, but then the probability of introducing false positive becomes higher as K increases. Therefore, there is an important trade-off on K that strongly depends on the ratio K/M . It is analyzed in Section 4. There is also a subtle interplay between P , K and M . As mentioned above, setting P &gt; 1 is a way to collect enough reliable examples for the tail classes when the collection U is not large enough.</p><p>Student model training and fine-tuning. We train the student model using the supervision provided byD. While our main interest is to learn a more simple model than the teacher, it is actually possible to use the same architecture for the teacher and the model, and as we will see later in the paper there is a practical interest. Note that, although our assignment procedure makes it possible to assign an image to multiple classes inD, we still treat the problem as multi-class classification by simply allowing the image to be replicated in the dataset. The trained student model is then fine-tuned on D and evaluated on the test data. Softmax loss is used for both pre-training and fine-tuning. Remark: It is possible to use a mixture of data in D andD for training like in previous approaches <ref type="bibr" target="#b34">[34]</ref>. However, this requires for searching for optimal mixing parameters, which depend on other parameters. This is resource-intensive in the case of our large-scale training. Additionally, as shown later in our analysis, taking full advantage of large-scale unlabelled data requires adopting long pre-training schedules, which adds some complexity when mixing is involved.</p><p>Our fine-tuning strategy on D does not have these problems: instead it requires a parameter sweep over the initial learning rate for fine-tuning, which is more efficient due to smaller data size and training schedules. It also ensures that the final model is fine-tuned with clean labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Bonus: weakly-supervised pre-training</head><p>We consider a further special case where in addition to U, D also includes weakly-supervised data (W) plus a relatively small task-specific data (V). Prior work <ref type="bibr" target="#b27">[27]</ref> has shown that the teacher model obtained by pre-training on W followed by fine-tuning on V is a compelling choice. The student model obtained by training on the data selected by the teacher model is significantly better than the one obtained by training directly on the weakly-supervised data. As shown later in the experiments, this particular setting allows us to achieve state-of-the-art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Image classification: experiments &amp; analysis</head><p>The effectiveness of our approach is demonstrated for the image classification task by performing a series of experiments on the ImageNet1K dataset. These experiments support the main recommendations proposed in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Datasets: The following web-scale datasets are used for semi-supervised learning experiments involving an unlabeled dataset U.</p><p>? YFCC-100M <ref type="bibr" target="#b38">[38]</ref> is a publicly available dataset of about 90 million images from Flickr website with associated tags. After removing duplicates, we use this data for most experiments and ablations.</p><p>? IG-1B-Targeted: Following <ref type="bibr" target="#b27">[27]</ref>, we collected a dataset of 1B public images with associated hashtags from a social media website. We consider images tagged with at least one of the 1500 hashtags associated with one of the 1000 ImageNet-1k classes. Unless specified otherwise, we use the standard ImageNet with 1000 classes as the labeled set D.</p><p>Models: For student and teacher models, we use residual networks <ref type="bibr" target="#b16">[16]</ref>, ResNet-d with d = {18, 50} and residual networks with group convolutions <ref type="bibr" target="#b43">[43]</ref>, ResNeXt-101 32XCd with 101 layers and group widths C = {4, 8, 16, 48}. In addition, we consider a ResNeXt-50 32x4 network. The number of model parameters is shown in <ref type="table" target="#tab_4">Table 3</ref>.</p><p>Training Details: We train our models using synchronous stochastic gradient descent (SGD) on 64 GPUs across 8 machines. Each GPU processes 24 images at a time and apply batch normalization <ref type="bibr" target="#b22">[22]</ref> to all convolutional layers on each GPU. The weight decay parameter is set to 0.0001 in all the experiments. We set the learning rate following the linear scaling procedure proposed in <ref type="bibr" target="#b12">[13]</ref> with a warm-up and overall minibatch size of 64 ? 24 = 1536.</p><p>For pre-training, we use a warm-up from 0.1 to 0.1/256 ? 1536, where 0.1 and 256 are the standard learning rate and minibatch size used for ImageNet training. We decrease the learning rate by a factor of 2 at equally spaced steps such that the total number of reductions is 13 over the course of training. For fine-tuning on ImageNet, we set the learning rate to 0.00025/256 ? 1536 and use the learning rate schedule involving three equally spaced reductions by a factor of 0.1 over 30 epochs.</p><p>Default parameters. In rest of this section, the number of "iteration" refers to the total number of times we process an image (forward/backward). Unless stated otherwise, we adopt the following parameters by default: We process #iter = 1B images for training. We limit to P = 10 the number of concepts selected in an image to collect a short-list of K = 16k images per class forD. Our default teacher model is a vanilla ResNext-101 architecture (32x48) as defined by Xie et al. <ref type="bibr" target="#b43">[43]</ref>. Since our main goal is to maximize the accuracy of our student model, we adopt a vanilla ResNet-50 student model <ref type="bibr" target="#b16">[16]</ref> in most experiments to allow a direct comparison with many works from the literature. Finally, the unlabeled dataset U is YFCC100M by default.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Analysis of the different steps</head><p>Our approach vs supervised learning. <ref type="table" target="#tab_3">Table 2</ref> compares our approach with the fully-supervised setting of training only on ImageNet for different model capacities. Our teacher model brings a significant improvement over the supervised baseline for various capacity target models (1.6-2.6%), which supports our recommendation in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Importance of fine-tuning. Since labels of pre-training datasetD and labeled datasetD are same, we also compare the performance of model trained onD before and after the last full fine-tuning step of our approach. From <ref type="table" target="#tab_3">Table 2</ref> we observe that fine-tuning the model on clean labeled data is crucial to achieve good performance (Point 2 in <ref type="table" target="#tab_0">Table 1</ref>).</p><p>Effect of the student and teacher capacities. In Table 2, we observe that the accuracy increases significantly for lower capacity student models, before saturating around the ResNeXt-101 32x8 model. <ref type="table" target="#tab_4">Table 3</ref> shows accuracy as a function of teacher model capacity. The accuracy of ResNet-50 student model improves as we increase the strength of teacher model until ResNeXt-101 32x16. Beyond that the classification accuracy of the teacher saturates due to the relatively small size of ImageNet. Consequently, increasing the teacher model capacity further has no effect on the performance of the student model.  Together, the above experiments suggest that the improvement with our approach depends on the relative performance of student and teacher models on the original supervised task (i.e., training only on the labeled data D). Interestingly, even in a case of where both teacher and student models are ResNet-50, we get an improvement of around 1% over the supervised baseline.</p><p>Self-training: Ablation of the teacher/student. A special case of our proposed semi-supervised training approach is when the teacher and student models have the same architecture and capacity. In other words, the sampling is performed by the target model itself trained only on the available labeled dataset. In this case, our approach reduces to be a self-training method. <ref type="table" target="#tab_6">Table 4</ref> shows the accuracy, with our default setting, for different models as well as the gain achieved over simply training on ImageNet for reference. All models benefit from self-training. Higher capacity models have relatively higher accuracy gains. For example, the ResNeXt-101 32x16 model is improved by 2% compared to 1% with ResNet-50. We also run a second round of self-training for ResNet-50 and ResNeXt-101 32x16 models and observed subsequent gains of 0.3% and 0.7%, respectively.</p><p>As discussed earlier, self-training is a challenging task since errors can get amplified during training. Our experiments show that, with a fine-tuning stage to correct possible labeling noise, the model can improve itself by benefiting from a larger set of examples. However, by comparing these results to those provided in <ref type="table" target="#tab_3">Table 2</ref>, we observe that our teacher/student choice is more effective, as it generally offers a better accuracy for a specific target architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Parameter study</head><p>Size of the unlabeled set U.</p><p>A number of varying size datasets are created by randomly sampling from YFCC100M image distribution. The hyper-parameter value K is adjusted for each random subset proportionally in order to marginalize the effect of the dataset scale. For example, values of K = 16k, 8k and 4k are set for dataset sizes 100M , 50M and 25M , respectively. The semi-supervised model accuracy is plotted as a function of unlabeled dataset size in <ref type="figure">Figure 3</ref>. A fixed accuracy improvement is achieved every time the dataset size is doubled until reaching the dataset size of 25M. However, this log-linear behavior disappears as the dataset size grows further. The target model is possibly reaching to its saturation point where adding additional training samples from the same YFCC image distribution does not help improve the accuracy.</p><p>Overall, the above experiments indicate that leveraging the large-scale of unlabeled data is important for good performance (Point 3 in our recommendations of <ref type="table" target="#tab_0">Table 1</ref>).</p><p>Number of pre-training iterations. <ref type="figure">Figure 4</ref> shows the performance as a function of total number of images processed during pre-training. To make sure that the learning rate drops by the same factor in each setting, we divide the overall training in to 13 equally spaced steps and drop the learning rate by a factor of 2 after every step. We observe that performance keeps on improving as we increase the number of processed images. This indicates that our approach needs a longer training stage to be fully beneficial, as summarized by our Point 4 in <ref type="table" target="#tab_0">Table 1</ref>). As a reference, we also show results of training a ResNet-50 model on Im-ageNet with the same number of iterations. Increasing the number of training iterations did not help much and in fact reduced accuracy by a small margin. Note, we also tried the common choice of dropping the learning rate 3 times by a factor of 0.1, with similar conclusion. Overall, we found that 1 billion iterations offers a satisfactory trade-off between attaining a good accuracy and resource usage.</p><p>Parameters K and P . <ref type="figure" target="#fig_3">Figure 5</ref> shows the effect of varying the number K of images selected per class for P = 10, for ResNet-50 and ResNext-101-32x16 student models. For ResNet50, we observe that the performance first improves as we increase the value of K to 8k due to increase in diversity as well as hardness of examples. It is stable in a broad 4k-32k regime indicating that a coarse sweep to find the optimal value is sufficient. Increasing K further introduces a lot of labeling noise inD and the accuracy drops. Similar observations hold for ResNext-101-32x16 except that the higher capacity model can leverage more data. It achieves its best performance between 16k and 32k per class.</p><p>The performance does not vary much with P for a nearoptimal value of K like 16k. A high value of P produce a more balanced class distribution for pre-training by allowing a training example to appear in more than one ranked list, see the supplemental material for an analysis. We fix P = 10 in all of our experiments since it allows us to produces an almost balanced pre-training dataset (Point 5 in <ref type="table" target="#tab_0">Table 1</ref>) when the collection size U is not large enough.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Semi-weakly supervised experiments</head><p>Prior work <ref type="bibr" target="#b27">[27]</ref> shows that images associated with metainformation semantically relevant to the task can improve performance. One of the ways to achieve this is to leverage task relevant search queries or hashtag to create a noisy     <ref type="table">Table 5</ref>: Analysis of the selection step on IG-1B-Targeted. All methods selects a subset of 8 million images for training, our balanced-ranked method works the best.</p><p>weakly-supervised dataset and use it as U. Our ranking based selection could be viewed as a preliminary noise removal and class balancing process.</p><p>Study: variants forD selection procedure. In order to investigate this further, we create three different 8 million subsets of IG-1B-Targeted data to train a ResNet-50 student model: (1) balanced-ranked is created using our ranking approach with same number of examples (top-8k) per class, (2) unbalanced-ranked also uses our ranking approach but the number of examples per label follows a Zipfian distribution, and, (3) balanced-with-tags randomly selects 8k images per class using relevant hashtags. Please see supplementary material for details. <ref type="table">Table 5</ref> shows the results of training on these datasets followed by fine-tuning on Ima-geNet. We observe that our classification-based selection via ranking is key to achieve a good performance. Note, leveraging a large amount of unlabeled data is necessary to obtain similar number of images per class.  <ref type="table">Table 6</ref>: State of the art on ImageNet with standard architectures (ResNet, ResNext). The lower part of the table reports methods that leverage unlabeled data. Our teacher model is either learned on ImageNet (semi-supervised) or pre-trained on weakly-supervised IG-1B-Targeted (semiweakly sup., ? trained with #iter=2B). The weakly supervised baseline <ref type="bibr" target="#b27">[27]</ref> is learned on IG-1B-Targeted data followed by fine-tuning on ImageNet.</p><p>weakly-supervised approach <ref type="bibr" target="#b27">[27]</ref> that pre-trains models on IG-1B-Targeted with hashtags as labels. This is a strong baseline with respect to the state of the art on most models. We observe a consistent improvement of 0.6%-2.2% with lower capacity models benefiting the most. All our models achieve the state-of-the-art performance with ResNet-50 achieving a solid 80.9% accuracy. Hence, leveraging weakly-supervised data to pre-train the teacher model significantly improves the results (Point 6 in <ref type="table" target="#tab_0">Table 1</ref>). <ref type="table">Table 6</ref> compares our approach against the state of the art results for existing architectures, in particular the popular ResNet-50, for which we have several points of comparison. As one can see, our approach provides a large improvement (&gt;3%) over all methods that only use labelled data. As discussed above, it also significantly outperforms (+0.6-2.5%) a state-of-the-art weakly supervised method <ref type="bibr" target="#b27">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison with the state of the art</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Other applications</head><p>In this section, motivated by our findings on the Ima-geNet dataset, we apply our method to two different tasks: (1) video action classification, and, (2) transfer learning for improved classification using our semi-supervised image models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Video Classification Experiments</head><p>We run experiments with the popular multi-class Kinetics video benchmark which has ? 246k training videos and 400 human action labels. The models are evaluated on the 20k validation videos. Similar to IG-1B-Targeted, we construct an IG-Kinetics of 65 million videos by leveraging 359 hashtags that are semantically relevant to Kinetics label space. Please see supplementary material for details. We use R(2+1)D-d <ref type="bibr" target="#b40">[40]</ref> clip-based models with model depths 18 and 34 and use RGB video frames.</p><p>The teacher is a weakly-supervised R(2+1)D-34 model with clip length 32. It is pre-trained with IG-Kinetics dataset and fine-tuned with labeled Kinetics videos. We uniformly sample 10 clips from each video and average the softmax predictions to produce video level predictions. For our approach, we use K = 4k and P = 4 and IG-Kinetics as unlabeled data U to train student models. Please see the supplemental for training details. <ref type="table" target="#tab_10">Table 7</ref> reports the results for 3 different student models along with number of parameters and multiply-add FLOPS. For comparison, we also show results with: (1) training with full-supervision on Kinetics, and, (2) weaklysupervised pre-training on IG-Kinetics followed by finetuning on Kinetics. Our approach gives significant improvements over fully-supervised training. We also observe further gains over the competitive weakly-supervised pretraining approach with models having lower FLOPS benefiting the most. We also compare with other state-of-theart approaches and perform competitively. We note that we process at most 32 RGB frames of the input video (no optical flow), at a much lower resolution (112 ? 112) compared to the state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Transfer Learning Experiments</head><p>ImageNet has been used as the de facto pre-training dataset for transfer learning. A natural question that arises is how effective models trained using our approach are for transfer learning tasks. We consider CUB2011 <ref type="bibr" target="#b41">[41]</ref> dataset with 5994 training images and 5794 test images associated with 200 bird species (labels). Two transfer learning settings are investigated: (1) full-ft involves fine-tuning the full network, and, (2) fc-only involves extracting features from the final fc layer and training a logistic regressor. <ref type="table" target="#tab_11">Table 8</ref> reports the accuracy of ResNet-50 models trained with our semi-supervised (Section 4.2) and semiweakly supervised (Section 2) methods to the performance   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have leveraged very large scale unlabeled image collections via semi-supervised learning to improve the quality of vanilla CNN models. The scale of the unlabelled dataset allows us to infer a training set much larger than the original one, allowing us to learn stronger convolutional neural networks. Unlabelled and labelled images are exploited in a simple yet practical and effective way, in separate stages. An extensive study of parameters and variants leads us to formulate recommendations for large-scale semi-supervised deep learning. As a byproduct, our ablation study shows that a model self-trained with our method also achieves a compelling performance. Overall, we report state-of-the-art results for several architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SUPPLEMENTAL MATERIAL</head><p>This supplemental material provides details that complement our paper. First we provide statistics on our parameter P in Section 1. Then Section 2 provides details about the variants discussed in Section 4.4. Section 3 provides additional results that show the effectiveness of our method to train a teacher with our own technique. Section 4 describes how we collected our IG-Kinectics video dataset. Section 5 explains our de-duplication procedure that ensures that no validation or test data belongs to our unlabelled dataset. Finally, we give in Section 6 the parameters associated with our transfer learning experiments on CUB2011. <ref type="table" target="#tab_13">Table 9</ref> shows the effect of varying P on the ResNet-50 student model trained with the vanilla ResNext-101 32x48 teacher model for K = 16k. We consider YFCC-100M and construct two datasets of 25M and 100M examples by randomly sampling the data. We notice that increasing P balances the ranked list sizes across all the classes (column |D|). We also observe that it does not have a significant impact on the performance. Since our main focus of our work is large-scale training with an interest to handle underrepresented classes, we prefer to guarantee a fixed number of samples per class and therefore choose P = 10 in all the experiments.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Effect of parameter P</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Semi+weakly supervised: variants forD</head><p>This section gives more details about the variants for collecting a training set, as discussed in Section 4.4. We create three different 8 million subsets of IG-1B-Targeted data.</p><p>? balanced-ranked is created using our ranking approach with same number of examples (top-8k) per class.</p><p>? unbalanced-ranked also uses our ranking approach but the number of examples per label follows a Zipfian distribution. In order to make sure that our Zipfian distribution matches the real-world data, we assign the   1.5k hashtags from IG-1B-Targeted to one of ImageNet classes to determine the approximate number of images per class. We then use this distribution as basis to determine K to select top ? K images for each class so that total number of images across the classes is 8M .</p><p>? balanced-with-tags randomly selects 8k images per class using relevant hashtags after mapping them to Im-ageNet classes and does not use any ranked lists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Self-trained teacher models</head><p>In the paper, we observe that in the self-training setting where teacher and student models have the same architecture and capacity, we still get 1% ? 2% gains in accuracy over the fully-supervised setting. Motivated by this, we consider another scenario where we take a high capacity ResNext-101-32x16 model trained on ImageNet and selftrain it using YFCC-100M as unlabeled data. We then use this self-trained model as a teacher and train a lower capacity student model using our approach on YFCC-100M. Standard training settings mentioned in the main paper are used. <ref type="table" target="#tab_0">Table 10</ref> shows the accuracy of different student models trained in the above setting. For reference, we also compare against semi-supervised result from <ref type="table" target="#tab_3">Table 2</ref> in the main paper using an ImageNet trained ResNext-101 32x48 teacher model. We observe a consistent 0.2%?0.7% improvement even by using a lower capacity self-trained ResNeXt-101 32x16 teacher. We intend to release these models publicly.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Weakly-supervised video experiments 4.1. Construction of IG-Kinetics dataset</head><p>We consider millions of public videos from a social media and the associated hashtags. For each label in Kinetics dataset, we take the original and stemmed version of every word in the label and concatenate them in different permutations. For example, for the label "playing the guitar", different permutations are -"playingtheguitar", "playguitar", "playingguitar", "guitarplaying", etc. We then find all the hashtags and associated videos that match any of these permutations and assign them the label. Finally, we end up with 65 million videos with 369 labels. There are no corresponding hashtags for 31 labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training details</head><p>We first down-sample the video frames to a resolution of 128 ? 171 and generate each video clip by cropping a random 112 ? 112 patch from a frame. We also apply temporal jittering to the input. We train our models using synchronous stochastic gradient descent (SGD) on 128 GPUs across 16 machines. For the video clip with 32 frames, each GPU processes 8 images (due to memory constraints) at a time and apply batch normalization <ref type="bibr" target="#b22">[22]</ref> to all convolutional layers on each GPU. For the 8 frames video clip, each GPU processes 8 images at a time. The weight decay parameter is set to 0.0001 in all the experiments. We set the learning  <ref type="table" target="#tab_0">Table 13</ref>: Hyper-parameters for full fine-tuning of models on CUB2011 dataset. Same settings are used for different kinds of supervision ( <ref type="table" target="#tab_11">Table 8</ref> in main paper) and models.</p><p>rate following the linear scaling procedure proposed in <ref type="bibr" target="#b12">[13]</ref> with a warm-up.</p><p>Pre-training: Both weakly-supervised and semi-weakly supervised settings require pre-training the models on IG-Kinetics. We process 490M videos in total. <ref type="table" target="#tab_0">Table 11</ref> shows the minibatch size and learning rate settings for different models. We decrease the learning rate by a factor of 2 at equally spaced steps such that the total number of reductions is 13 over the course of training.</p><p>Fine-tuning and fully-supervised setting: <ref type="table" target="#tab_0">Table 12</ref> shows the training settings for fine-tuning and fullysupervised runs. For fine-tuning experiments, we do a gridsearch on the initial learning rate and LR schedule using a separate held-out set. Learning rate decay is set to 0.1 for all the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Deduplication of images</head><p>In order to ensure a fair evaluation in our large-scale experiments, it is crucial that we remove images in the largescale dataset that are also present in the labeled test or validation set. We leverage ImageNet trained ResNet-18 model and use its 512 dimensional pool5 features to compute the Euclidean distance between the images from ImageNet validation set and YFCC-100M after L2 normalization. We use the Faiss <ref type="bibr" target="#b24">[24]</ref> nearest neighbor library to implement the searchable image index of 100 million images. We sorted all pairs of images globally according to their Euclidean distances in ascending order and manually reviewed the top pairs. The top ranking 5000 YFCC images are removed from the dataset. The same procedure is applied to CUB2011 to remove 218 YFCC-100M images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Transfer learning on CUB2011: parameters</head><p>For the full fine-tuning scenario, we fine-tune the models on 32 GPUs across 4 machines. <ref type="table" target="#tab_0">Table 13</ref> provides the parameter settings for different runs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>samples from unlabeled dataset for each category based on teacher model's prediction to form the aggregated dataset "</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>ResNet-50 student model accuracy as a function of the size of the unlabeled dataset U. Effect of number of training iterations on the accuracy of fully-supervised and semi-supervised ResNet-50 student models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Student model accuracies as a function of the sampling hyperparameter K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>OUR RECOMMENDATIONS FOR LARGE-SCALE SEMI-SUPERVISED LEARNING</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>ImageNet1k-val top-1 accuracy for students models of varying capacity before and after fine-tuning compared to corresponding fully-supervised baseline models.</figDesc><table><row><cell>Model</cell><cell cols="3">Teacher # Params top-1</cell><cell cols="2">Student Gain (%) top-1</cell></row><row><cell>ResNet-18</cell><cell></cell><cell>8.6M</cell><cell>70.6</cell><cell>75.7</cell><cell>-0.7</cell></row><row><cell>ResNet-50</cell><cell></cell><cell>25M</cell><cell>76.4</cell><cell>77.6</cell><cell>+1.2</cell></row><row><cell cols="2">ResNext-50-32x4</cell><cell>25M</cell><cell>77.6</cell><cell>78.2</cell><cell>+1.8</cell></row><row><cell cols="2">ResNext-101-32x4</cell><cell>43M</cell><cell>78.5</cell><cell>78.7</cell><cell>+2.3</cell></row><row><cell cols="2">ResNext-101-32x8</cell><cell>88M</cell><cell>79.1</cell><cell>78.7</cell><cell>+2.3</cell></row><row><cell cols="3">ResNext-101-32x16 193M</cell><cell>79.6</cell><cell>79.1</cell><cell>+2.7</cell></row><row><cell cols="3">ResNext-101-32x48 829M</cell><cell>79.8</cell><cell>79.1</cell><cell>+2.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Varying the teacher capacity for training a ResNet- 50 student model with our approach. The gain is the abso- lute accuracy improvement over the supervised baseline.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Self-training: top-1 accuracy of ResNet and ResNeXt models self-trained on the YFCC dataset. Gains refer to improvement over the fully supervised baseline.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Pre-training with hash tags. Pre-training on weaklysupervised data has recently achieved impressive performances. Motivated by these results, we follow Mahajan et.</figDesc><table><row><cell></cell><cell>ResNet-50</cell><cell cols="3">ResNeXt-101-* 32x4 32x8 32x16</cell></row><row><cell>Xie et al. [43]</cell><cell>76.1</cell><cell>78.8</cell><cell>-</cell><cell>-</cell></row><row><cell>Mixup [45]</cell><cell>76.7</cell><cell>79.9</cell><cell>-</cell><cell>-</cell></row><row><cell>LabelRefinery [2]</cell><cell>76.5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Autoaugment [7]</cell><cell>77.6</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Weakly supervised [27]</cell><cell>78.2</cell><cell cols="3">81.2 82.7 84.2</cell></row><row><cell>ours (semi-supervised)</cell><cell>79.1</cell><cell cols="3">80.8 81.2 81.2</cell></row><row><cell cols="5">ours (semi-weakly sup.) 80.9 (81.2  ? ) 83.4 84.3 84.8</cell></row></table><note>al. et al. [27] to train a ResNext-101 32x48 teacher model (85.4% top-1 accuracy) by pre-training on IG-1B-Targeted and fine-tuning on ImageNet. Due to the larger size dataset, we use K = 64k. Table 6 compares our method with a</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">: Accuracy on Kinetics video dataset for differ-</cell></row><row><cell cols="5">ent approaches using R(2+1)D models. Weakly-supervised</cell></row><row><cell cols="5">refers to weakly-supervised training on IG-Kinetics fol-</cell></row><row><cell cols="5">lowed by fine-tuning on Kinetics. Our approach uses</cell></row><row><cell cols="5">R(2+1)D-34 teacher model with clip length 32.</cell></row><row><cell cols="3">Pre-trained ImageNet weakly</cell><cell>semi</cell><cell>semi-weakly</cell></row><row><cell>Model</cell><cell>sup.</cell><cell>sup.</cell><cell>sup. (ours)</cell><cell>sup. (ours)</cell></row><row><cell>full-ft</cell><cell>82.1</cell><cell>83.2</cell><cell>83.6</cell><cell>84.8</cell></row><row><cell>fc-only</cell><cell>73.3</cell><cell>74.0</cell><cell>80.4</cell><cell>80.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell>: CUB2011: Transfer learning accuracy (ResNet50).</cell></row><row><cell>of fully-supervised and weakly-supervised models. See the</cell></row><row><cell>supplemental for the training settings. The models trained</cell></row><row><cell>with our approach perform significantly better. Results are</cell></row><row><cell>particularly impressive for fc-only setting, where our semi-</cell></row><row><cell>weakly supervised model outperforms highly competitive</cell></row><row><cell>weakly-supervised model by 6.7%.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>The effect of the training hyper-parameter P on the accuracy of the ResNet-50 student model.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10 :</head><label>10</label><figDesc></figDesc><table><row><cell cols="4">The top-1 accuracy gain (%) of varying stu-</cell></row><row><cell cols="4">dent models pre-trained with a "self-trained" ResNext-101-</cell></row><row><cell cols="4">32x16d teacher model on YFCC dataset. The gain is the ab-</cell></row><row><cell cols="4">solute accuracy improvement over the semi-supervised re-</cell></row><row><cell cols="3">sult in Table 2 in the main paper.</cell><cell></cell></row><row><cell>Model</cell><cell cols="2">Clip Minibatch len. size</cell><cell>Initial LR</cell></row><row><cell>R(2+1)D-18</cell><cell>8</cell><cell cols="2">128 ? 16 = 2048 0.064/256 ? 2048</cell></row><row><cell>R(2+1)D-18</cell><cell>32</cell><cell>128 ? 8 = 1024</cell><cell>0.064/256 ? 1024</cell></row><row><cell>R(2+1)D-34</cell><cell>8</cell><cell cols="2">128 ? 16 = 2048 0.064/256 ? 2048</cell></row><row><cell>R(2+1)D-34</cell><cell>32</cell><cell>128 ? 8 = 1024</cell><cell>0.064/256 ? 1024</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 11 :</head><label>11</label><figDesc>Hyper-parameters for pre-training video models.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 12 :</head><label>12</label><figDesc>Hyper-parameters for fully-supervised training and fine-tuning video models.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leaderboard</surname></persName>
		</author>
		<ptr target="https://tinyurl.com/yygsyy8q.2" />
	</analytic>
	<monogr>
		<title level="j">Image classification on imagenet</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Label refinery: Improving imagenet classification through label progression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bagherinezhad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Horton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Label propagation and quadratic criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Delalleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Semi-Supervised Learning</title>
		<editor>O. Chapelle, B. Sch?lkopf, and A. Zien</editor>
		<meeting><address><addrLine>Boston</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="195" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning feed-forward one-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Quo vadis, action recognition? a new model and the kinetics dataset. CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Extrapolating learned manifolds for human activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Suter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Autoaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<title level="m">Learning augmentation policies from data</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Diffusion processes for retrieval revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Donoser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Low-shot learning with large-scale diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Low-shot learning via covariance-preserving adversarial augmentation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Accurate, large minibatch sgd: training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Low-shot visual recognition by shrinking and hallucinating features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08883</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Rethinking imagenet pretraining</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gpipe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.06965</idno>
		<title level="m">Efficient training of giant neural networks using pipeline parallelism</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Scalable out-of-sample extension of graph embeddings using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lyzinski</surname></persName>
		</author>
		<idno>abs/1508.04422</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08734</idno>
		<title level="m">Billion-scale similarity search with gpus</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks. In nips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Phrasebased &amp; neural unsupervised machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Denoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In EMNLP</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bharambe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">A survey on transfer learning. T-PAMI</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Transformation pursuit for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">cvpr</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Towards good practice in large-scale learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Low-shot learning with imprinted weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Data distillation: Towards omni-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Transfer learning in a transductive setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<idno>2015. 4</idno>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<title level="m">The new data and new challenges in multimedia research. CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Yfcc100m: the new data in multimedia research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<idno>2016. 4</idno>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<idno>abs/1711.11248</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>- port CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Re</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<title level="m">Non-local neural networks. CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Revisiting the effectiveness of off-the-shelf temporal modeling approaches for large-scale video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03805</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
