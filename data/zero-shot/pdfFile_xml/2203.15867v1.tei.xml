<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image Retrieval from Contextual Descriptions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benno</forename><surname>Krojer</surname></persName>
							<email>benno.krojer@mila.quebecsiva.reddy@mila.quebec</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
								<address>
									<settlement>Mila</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaibhav</forename><surname>Adlakha</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
								<address>
									<settlement>Mila</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Vineet</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Goyal</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Samsung -SAIT AI Lab</orgName>
								<address>
									<settlement>Montreal</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edoardo</forename><surname>Ponti</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
								<address>
									<settlement>Mila</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
								<address>
									<settlement>Mila</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Facebook CIFAR AI Chair</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Image Retrieval from Contextual Descriptions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The ability to integrate context, including perceptual and temporal cues, plays a pivotal role in grounding the meaning of a linguistic utterance. In order to measure to what extent current vision-and-language models master this ability, we propose a new multimodal challenge, Image Retrieval from Contextual Descriptions (IMAGECODE). In particular, models are tasked with retrieving the correct image from a set of 10 minimally contrastive candidates based on a contextual description. As such, each description contains only the details that help distinguish between images. Because of this, descriptions tend to be complex in terms of syntax and discourse and require drawing pragmatic inferences. Images are sourced from both static pictures and video frames. We benchmark several state-of-the-art models, including both cross-encoders such as ViLBERT and bi-encoders such as CLIP, on IMAGECODE. Our results reveal that these models dramatically lag behind human performance: the best variant achieves an accuracy of 20.9 on video frames and 59.4 on static pictures, compared with 90.8 in humans. Furthermore, we experiment with new model variants that are better equipped to incorporate visual and temporal context into their representations, which achieve modest gains. Our hope is that IMAGECODE will foster progress in grounded language understanding by encouraging models to focus on fine-grained visual differences. We make code and dataset publicly available. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Natural languages are highly contextual <ref type="bibr" target="#b9">(Fodor, 2001)</ref>: for a listener, recovering the speaker's intended meaning requires integrating information from different streams, such as grounding in perception <ref type="bibr" target="#b33">(Pecher and Zwaan, 2005)</ref>, shared world knowledge, and temporal reasoning <ref type="bibr" target="#b40">(Wilson and Sperber, 1998)</ref>. These processes, more generally, fall under the umbrella term of pragmatics <ref type="bibr" target="#b14">(Grice, 1957)</ref>. Despite recent progress in multimodal systems, it remains unclear to which extent they can handle settings where context plays a major role, such as in real-world communication.</p><p>To this end, we present a new challenge that requires multimodal models to leverage context to retrieve images from text. In particular, given a contextual description and a set of minimally contrastive candidate images, i.e. differing only in some details, the model has to retrieve the target image. In order to discriminate between similar images, human annotators naturally produce highly nuanced and grammatically complex descriptions. An example of our new challenging dataset, Image Retrieval from Contextual Descriptions (IMAGECODE), is shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>During the data collection process, sets of similar images are selected among static pictures from Open Images <ref type="bibr" target="#b25">(Kuznetsova et al., 2020)</ref> and (a larger portion) among video frames from diverse domains.</p><p>Including both types of images allows for diversifying the dataset while representing different degrees of visual similarity within each set. Next, we crowdsource a contextual description of a target image (presented together with the rest of the set) that contains only differences relevant for retrieval. After a filtering phase involving human retrievers, we obtain a large-scale dataset with 94,020 images and 21,202 descriptions associated with image sets of size 10.</p><p>As a result of this annotation protocol, successfully completing the task requires models to integrate several kinds of context: i) the image set, as the descriptions often only make sense in the context of several other images and are not suitable as stand-alone captions. In fact, aspects of the image that are very salient and that therefore would normally be emphasized are not useful in our proposed task. Instead, the focus of our descriptions are finegrained details that help discriminate between images (see <ref type="figure" target="#fig_0">Figure 1)</ref>; ii) the speaker's intention. Due to their high degree of image similarity, contextual descriptions may be literally true for multiple images; however, once the speaker's intention is taken into account, the correct image can be determined by virtue of pragmatics, i.e. Grice's maxim of quality 2 (see <ref type="figure" target="#fig_1">Figure 2, Figure 7)</ref>; iii) temporal sequences: for video frames temporal reasoning is also required to compare different moments of an unfolding event.</p><p>On our new dataset IMAGECODE, we benchmark a series of vision-and-language models that achieve state-of-the-art performance on other multimodal tasks, specifically ViLBERT <ref type="bibr" target="#b28">(Lu et al., 2019)</ref> and UNITER <ref type="bibr" target="#b4">(Chen et al., 2020)</ref> as two cross-encoder variants and CLIP as a strong biencoder <ref type="bibr" target="#b34">(Radford et al., 2021)</ref>. We report several findings. First, accuracy on static images is vastly superior than on video frames. Therefore, the degree of similarity among the candidate images has an overwhelming impact on retrieval performance. Second, all state-of-the-art models generally struggle with image retrieval from contextual descriptions, whereas humans consistently achieve high accuracy.</p><p>Hence, we propose model variants capable of better taking context into account: i) once an imagedescription pair is encoded, we refine this representation by attending to the other images in the set; 2 Note: While we do not model pragmatics explicitly in our baselines, we find that the IMAGECODE contains many examples suitable for pragmatic modeling ii) we augment image encodings with temporal embeddings. Based on our results, models take advantage of this additional information fruitfully but only to a limited degree.</p><p>Because of its challenging nature, due to the minimally contrastive images and complex descriptions, we believe that IMAGECODE will help make visio-linguistic models more context-aware and sensitive to fine-grained details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There is a long tradition of grounding language understanding on single images, in the form of visual question answering <ref type="bibr" target="#b13">(Goyal et al., 2017;</ref><ref type="bibr" target="#b20">Hudson and Manning, 2019)</ref>, visual dialogue <ref type="bibr" target="#b8">(de Vries et al., 2017;</ref><ref type="bibr" target="#b6">Das et al., 2017)</ref>, or visual entailment <ref type="bibr" target="#b41">(Xie et al., 2019)</ref>. Recently, more and more focus has been directed to settings where the visual context consists of multiple images, either conventional static pictures <ref type="bibr" target="#b39">(Vedantam et al., 2017;</ref><ref type="bibr" target="#b19">Hu et al., 2019;</ref><ref type="bibr" target="#b36">Suhr et al., 2019;</ref><ref type="bibr" target="#b10">Forbes et al., 2019;</ref><ref type="bibr" target="#b15">Hendricks and Nematzadeh, 2021;</ref><ref type="bibr" target="#b43">Yan et al., 2021;</ref><ref type="bibr" target="#b18">Hosseinzadeh and Wang, 2021;</ref><ref type="bibr" target="#b2">Bogin et al., 2021;</ref><ref type="bibr" target="#b27">Liu et al., 2021)</ref>, or video frames <ref type="bibr" target="#b21">(Jhamtani and Berg-Kirkpatrick, 2018a;</ref><ref type="bibr" target="#b1">Bansal et al., 2020)</ref>. While many of these benchmarks involve just two images, COVR <ref type="bibr" target="#b2">(Bogin et al., 2021)</ref> and ISVQA <ref type="bibr" target="#b1">(Bansal et al., 2020)</ref> provide more images, similar to our sets of 10 images.</p><p>ISVQA and Spot-the-diff <ref type="bibr" target="#b21">(Jhamtani and Berg-Kirkpatrick, 2018a)</ref> are most similar to our dataset, IMAGECODE. ISVQA is based on several video frames that are synthetic and cover a restricted domain, with short questions for Visual Question Answering. Spot-the-diff provides two frames from surveillance video cameras and descriptions of all their differences. IMAGECODE is unique as a) we cover a wider range of domains; b) we construct image sets that are maximally similar while being distinguishable through natural language (Section 3) and c) we limit descriptions to relevant differences. This results in (a) diverse, (b) complex and (c) pragmatically informative descriptions.</p><p>We do not claim to explicitly model pragmatics in this paper, i.e. with Rational Speech Acts <ref type="bibr" target="#b12">(Goodman and Frank, 2016)</ref>. Instead we present a dataset that is naturally suitable for pragmatic reasoning <ref type="bibr" target="#b0">(Andreas and Klein, 2016;</ref><ref type="bibr" target="#b5">Cohn-Gordon et al., 2018)</ref> as a listener has to consider the context, assume a Gricean speaker and resolve ambiguities resulting from nuanced differences. The reasoning in our task and data collection is therefore also similar to ReferItGame and subsequent work <ref type="bibr" target="#b24">(Kazemzadeh et al., 2014;</ref><ref type="bibr" target="#b30">Mao et al., 2016)</ref> where one crowdworker generates a referring expressing for an object in a single image and another worker picks an object based on the expression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data Collection</head><p>Our data collection involves two steps with a human describer and retriever. The describer is given a set of 10 highly similar images S = [I 1 , I 2 , ..., I 10 ], one of them marked as the target image I t , and has to write a description D that clearly distinguishes I t from the other distractor images. In the second step, the retriever is given the same 10 images and the description from the first step and has to identify the target image based on the description. S and D are only added to our dataset if the retrieval is successful.</p><p>Below, we outline the main stages of data collection: first, the collection of similar, contrastive images in Section 3.1. Then, the crowdsourcing of contextual descriptions in Section 3.2 and validation of the examples via image retrieval (Section 3.3). The final IMAGECODE dataset consists of 94,020 images (partitioned into 9,402 sets) and 21,202 contextual descriptions (16,594 in the train split, 2,302 and 2,306 in the validation and test split respectively).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Collecting Similar Images</head><p>In the first stage, we collect sets of images that are highly similar but still distinguishable from each other by a human. To quantitatively measure the pairwise similarity of two images, we compute the Euclidean distance between their encodings extracted from a pre-trained CLIP model <ref type="bibr" target="#b34">(Radford et al., 2021)</ref>. <ref type="bibr">3</ref> To study the effect of different degrees of similarity, further variegate our dataset, and enable temporal reasoning, we source our candidate images from collections of static pictures as well as videos, as detailed below. Static Pictures. We obtain image sets from one of the largest repositories of static pictures, the Open Images Dataset V6 <ref type="bibr" target="#b25">(Kuznetsova et al., 2020)</ref>, containing 1.74M images. For each image, we retrieve the 9 closest images from the training set based on their CLIP encodings. We then randomly sample 4,845 of these image sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>After ?3.  Video Frames. As sources for our video frames, we use i) Video-Storytelling <ref type="bibr" target="#b26">(Li et al., 2019)</ref>, covering social events (wedding, birthday, Christmas, camping); ii) general-domain MSR-VTT <ref type="bibr" target="#b42">(Xu et al., 2016)</ref>; and iii) YouCook <ref type="bibr" target="#b7">(Das et al., 2013)</ref>, covering cooking events. We choose these datasets as they contain publicly available and general-purpose videos (not specific to downstream tasks). We retain the original splits for train, validation, and test.</p><p>To obtain disjoint sets of 10 similar frames, we first segment the videos into smaller scenes (also known as shots) via the scene detection functionality of ffmpeg <ref type="bibr" target="#b38">(Tomar, 2006)</ref>. Then, for each scene, we add its first frame to the set of selected images. We then iterate over every following frame and add it to the set if its pairwise Euclidean distance with each of the previously selected frames is larger than a threshold. 4 Once the set contains 10 images, we reiterate the procedure for a new set. If the scene ends and the current set contains less than 10 images, the set is discarded.</p><p>During this process, we additionally remove frames that i) are too blurry, i.e. their BRISQUE score <ref type="bibr" target="#b32">(Mittal et al., 2012)</ref> is larger than 0.65; or ii) contain too much text, which is detected with the OCR tool Tesseract <ref type="bibr" target="#b35">(Smith, 2007)</ref>. <ref type="bibr">5</ref> We use all of YouCook's image sets and (due to cost constraints) randomly sample image sets from Video-Storytelling and MSR-VTT for crowdsourcing (cf. <ref type="table" target="#tab_1">Table 1</ref>). We remark that image sets are further filtered at the final stage of annotation (Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Crowdsourcing Contextual Descriptions</head><p>After creating sets of highly-similar images in Section 3.1, we request annotators from Amazon Mechanical Turk (AMT) to write contextual descriptions for each target image in a set. Each round, a set of images is presented in random order for static pictures and respecting temporal order for 4 The distance threshold was manually chosen as 0.35 based on qualitative results. <ref type="bibr">5</ref> The rationale of the second criterion is to prevent workers from focusing on the overlaid text rather than image content.  video frames. This encourages annotators to take the dynamics of the event into account. We then (randomly) select 3 target images per set, and ask annotators to produce a description that discriminates them from the other images in the set. To encourage pragmatic reasoning, we do not ask for all the differences (just those sufficient for retrieval) and do not allow explicit mentions of other images (see <ref type="figure" target="#fig_1">Figure 2</ref>). We select high-quality annotators according to criteria in Appendix B and assign partly disjoint sets of annotators to train and test in order to avoid annotator bias <ref type="bibr" target="#b11">(Geva et al., 2019)</ref>. 6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Human Validation via Image Retrieval</head><p>Finally, we validate the annotation crowdsourced in Section 3.2 by asking AMT workers to retrieve the correct target image from a set given its contextual description. For the final dataset, we retained only the examples that i) were retrieved successfully in the training set by a single worker or ii) were retrieved successfully by at least 2 out of 3 workers in the validation and test sets. As a consequence, we filtered out 26.5% of the contextual descriptions generated in Section 3.2.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Human Accuracy and Agreement</head><p>To quantify the reliability of the process outlined in Section 3, we report the inter-annotator agreement on our final dataset in <ref type="table" target="#tab_5">Table 3</ref>. We use Krippendorff's ? as a metric (the higher the better), which accounts for incomplete data, since the number of annotators per example is not fixed. We treat the index of the target image either as a nominal variable for static images or as an ordinal variable for video frames. In both cases, we find a high degree of agreement. Moreover, in <ref type="table" target="#tab_5">Table 3</ref>, we also report human accuracy-the percentage of times an annotator retrieved the correct target image from a contextual description (as described in Section 3.3). This provides an upper ceiling for the model performances (see Section 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Language Statistics</head><p>In <ref type="table" target="#tab_7">Table 4</ref>, we measure a series of statistics of the descriptions collected for IMAGECODE and compare them with other vision-and-language datasets   with multiple naturalistic images (cf. Section 2), such as NLVR2 <ref type="bibr" target="#b36">(Suhr et al., 2019)</ref> and Spot-thediff <ref type="bibr" target="#b22">(Jhamtani and Berg-Kirkpatrick, 2018b)</ref>. <ref type="bibr">8</ref> In particular, we count the average description length, the number of distinct word types, the average dependency tree depth of each sentence, 9 and the average number of sentences per description. Based on these metrics, we find evidence that IMAGE-CODE's descriptions are longer and more syntactically complex than in the other datasets. Moreover, they include multiple sentences (11.8% of examples have 3 or more).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Vision Statistics</head><p>By calculating the average pairwise Euclidean distance between CLIP-based encodings of images in the same set, we find that video frames are more similar than static pictures -as expected -by a factor of 1.13. Moreover, we find that descriptions of video frames mention human body parts (72.1%) more often than static pictures (30.2%). On the other hand, names of colors appear in descriptions of static pictures (61.4%) more frequently than video frames (33.6%). 10 Thus, annotators resort to different strategies to discriminate between different types of image sets, focusing on the aspects that vary the most.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Challenging Phenomena</head><p>Finally, we identify 9 interesting and challenging phenomena in IMAGECODE and annotate whether they are present in 200 examples from the validation set. We provide the definition of each phenomenon, its frequency, and an illustrative example in <ref type="table" target="#tab_3">Table 2</ref>. An example for each phenomena is given in Appendix G. For 4 of these phenomena unique to IMAGECODE, we further annotated 800 examples for the purpose of error analysis in Section 6. Inspecting these examples, we find a high number of cases where the visual context (47.0%) is required to complete the task. For instance, consider <ref type="figure" target="#fig_1">Figure 2</ref>: the description "No bridesmaid visible at all." requires a retriever to resolve the co-references of the entities in 5 frames. In particular, the body parts of the bridesmaids (red boxes) visible in frames 2 and 4 would not be identifiable as such without frame 1 and 5, respectively (where they appear with matching dresses and flowers in their hands). A common example we find in the data are "gradable" scenarios, i.e. "The person is looking down" might be semantically true for more than one image but it fits best to the image where the person is looking down the most. Another group of phenomena characteristic for IMAGECODE originates from its minimally contrastive setup: annotators might focus on how an event unfolds over time (temporal context), on what is missing in a specific frame but visible in the others (negation), on what moved out of frame (visibility / occlusion), or on small regions and patches of pixels (nuances). Importantly, these phenomena are less prominent in static pictures than in video frames (cf. <ref type="table" target="#tab_3">Table 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Baselines</head><p>In order to assess whether vision-and-language models can retrieve the correct image from a contextual description on a par with humans, we benchmark three state-of-the-art models that represent three main families of multimodal architectures <ref type="bibr" target="#b31">Miech et al., 2021)</ref>: i) ViL-BERT, a cross-encoder where language and vision streams can interact via cross-attention at intermediate layers <ref type="bibr" target="#b28">(Lu et al., 2019)</ref>; ii UNITER, a singlestream encoder where language and vision tokens are concatenated as inputs and processed with a single Transformer <ref type="bibr" target="#b4">(Chen et al., 2020)</ref>; iii) CLIP, a bi-encoder where language and vision streams are independent <ref type="bibr" target="#b34">(Radford et al., 2021)</ref>. It is worth noting that ViLBERT and UNITER are more expressive due to their architecture, whereas CLIP boasts a higher parameter count, is pre-trained on a larger dataset and uses a contrastive objective.</p><p>We evaluate these models under two different regimes: i) zero-shot inference, where pre-trained models are deployed on the IMAGECODE test set directly; and ii) fine-tuning, where the models are refined on the full training set before evaluation. We cast the training objective as binary classification for ViLBERT and as 10-class classification for CLIP. 11 Crucially, in both cases, positive and negative examples during training are sampled at random independently from the image set they belong to (see the first column of <ref type="figure">Figure 3)</ref>. Thus, the visual context of the other images in a set is only indirectly accessible at inference time, where the image with the highest probability is predicted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Integrating Context into Vision-and-Language Models</head><p>For the fine-tuning regime, we further investigate some modifications in the training setup and model architecture that facilitate the integration of visual and temporal context into the model. First, we use an alternative objective where all three models are trained on 10-class classification, but the 1 positive and 9 negatives are sourced from the same image set. The consequence of including positive and negative examples from the same image set in the same mini-batch is providing a wider visual context. We refer to this variant as +CONTEXTBATCH (second column of <ref type="figure">Figure 3</ref>). This setup only conveys the visual context as a weak signal, since the model has no chance to directly compare the images in the same set. Hence, we experiment with enhancing the architecture of vision-and-language models with a mechanism inspired by <ref type="bibr" target="#b2">Bogin et al. (2021)</ref>. In particular, given an encoder (CLIP, ViLBERT or UNITER), we obtain the representations of a contextual description x L ? R e (where e is the model hidden size) and of the images in a set (x</p><formula xml:id="formula_0">(1) V , . . . , x (10) V ), x (i) V ? R e</formula><p>from their final layer. 12 Then, we create a series of multimodal embeddings via element-wise multiplication:</p><formula xml:id="formula_1">m = (x L ? x (1) V , . . . , x L ? x (10) V ).</formula><p>Finally, we feed these to a l-layer Transformer Tf ? R 10?e ? R 10?e to obtain context-aware multimodal embeddings (Tf(m) 1 , . . . , Tf(m) 10 ).</p><p>Since each description-image pair can now attend on the others in a set, the model can fully exploit the visual context. We obtain the score for the i-th pair through a linear classifier head W ? R 1?e . The target image is predicted as</p><formula xml:id="formula_2">arg max i softmax W Tf(m) i + m (i)<label>(1)</label></formula><p>Note that we add a highway layer from the input to the output of the Transformer. We label this model variant +CONTEXTMODULE.</p><p>Finally, in addition to visual context, we make models aware of the temporal context too, as shown in the fourth column of <ref type="figure">Figure 3</ref>. For videobased examples only, the multimodal embeddings of each description-image pair are summed with a learnable positional embedding t ? R e that reflects the temporal order of the frames. <ref type="bibr">13</ref> Thus, m = (x L ?x (1) V ?t (1) , . . . , x L ?x (10) V ?t (10) ). Multimodal embeddings are then fed to a Transformer as above. We label this variant encapsulating both visual and temporal context +TEMPORALEMBED-DINGS. <ref type="bibr">12</ref> We use the CLS tokens for UNITER/ViLBERT. <ref type="bibr">13</ref> In the examples with static pictures, no temporal embedding is added.  <ref type="figure">Figure 3</ref>: Models with increasing levels of context integration: see Section 5 for more details. In the figure, we colour visual embeddings in red, text embeddings in blue, and positional embeddings in grey. POS is the score for the target image and NEG for the other candidates. ? represents dot product for CLIP and element-wise multiplication followed by a linear layer for ViLBERT/UNITER. ? represents element-wise multiplication. For ease of exposition, we show 3 images instead of 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experimental Setup</head><p>For all CLIP experiments, we use a pre-trained model with the vision backbone VIT-B/16. 14 We train the full models with a batch size of 360 examples (i.e., 36 image sets) for CLIP and 150 examples for ViLBERT/UNITER. We perform early stopping based on the validation accuracy with a maximum of 30 epochs. In the variants that adopt the base version of a model, we select a learning rate of 4 ? 10 ?6 for CLIP, 5 ? 10 ?6 for ViLBERT, 4 ? 10 ?5 for ViL-BERT+CONTEXTBATCH, 8 ? 10 ?6 for UNITER, and 7 ? 10 ?6 for UNITER++CONTEXTBATCH. We find these values via hyper-parameter search on the range [10 ?4 , 10 ?7 ]. For CLIP variants that modify the model architecture, we adopt the following setup: first, we fine-tune the full model in the +CONTEXTBATCH regime as detailed above. Afterwards, we freeze the encoder parameters and train the components responsible for processing the multimodal embeddings, described in Equation (1). More details are provided in Appendix F. For ViLBERT and UNITER we finetune the whole architecture at the same time.</p><p>All descriptions in IMAGECODE exceeding the maximum length of the three models are truncated. Due to their negligible amount, this does not affect 14 https://github.com/openai/CLIP performance significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>In <ref type="table" target="#tab_10">Table 5</ref>, we report the performance of the models from Section 5 for all the test examples in IMAGE-CODE as well as for the subsets containing only video frames or static pictures (see Appendix E for validation scores). Note that the random chance baseline has an accuracy of 10%. In what follows, we compare the results across several dimensions. Zero-shot vs. fine-tuning. In the zero-shot setting, we observe that CLIP representations are surprisingly superior to UNITER/ViLBERT even though CLIP has separate streams to encode an image and its description. In the simplest fine-tuning setting (i.e., if negatives are randomly sampled independent of the image set), we find that overall there is only a small increase in performance compared to zero-shot inference. This demonstrates that in the regime where images in the same set do not appear in the same batch during training, models cannot extrapolate how to leverage the visual context at inference time. Adding context. For the fine-tuning regime, we observe instead a different trend once the visual context of the other images in a set is provided during training (+CONTEXTBATCH): CLIP and UNITER receive a significant boost in performance (i.e. +14.4% for CLIP), which is  Finally, all three models achieve the highest performance when fine-tuned with both visual and temporal context. Adding temporal positional embeddings on top of the contextual module (+TEMPORALEMBEDDINGS) yields an accuracy of 29.9 for CLIP, 25.7 for UNITER and 24.5 for ViL-BERT. Crucially, even the best-performing models lag significantly behind the (micro-averaged) human accuracy of 90.8 (cf <ref type="table" target="#tab_5">. Table 3</ref>). Hence, despite some limited ability to integrate context, models are currently incapable of the fine-grained reasoning and pragmatic inferences needed to solve IM-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AGECODE.</head><p>Pre-trained model. Across all model variants and training regimes, CLIP consistently achieves higher accuracy than ViLBERT or UNITER. This implies that a larger amount of parameters, pretraining examples or the contrastive objective are more beneficial than ViLBERT's or UNITER's more expressive model architecture. Thus, these results violate the expectations that attention between vision and language would be more suitable to jointly encode highly nuanced visual details and descriptions <ref type="bibr" target="#b31">(Miech et al., 2021)</ref>. Additionally UNITER slightly outperforms ViLBERT as its single-stream architecture might enable richer cross-modal interactions.</p><p>Video frames vs. static pictures. The highest accuracy on the subset of the data with video frames (20.9) is far lower than that for static pictures (59.4). This confirms that videos represent the main challenge in IMAGECODE, both because of the higher similarity of images in a set and of the particular factors of variation that help differentiate among them (cf. Section 4.3 and examples in Appendix G). Additionally, model performance on video frames seems to increase more consistently as more context (both visual and temporal) is provided, whereas there is no clear trend in the case of static pictures.</p><p>Error Analysis. On a broad level, we have seen that video frames are much more challenging for models. Next, to identify more fine-grained causes for the overall low performance of the vision-andlanguage models on IMAGECODE, we compute the Pearson's correlation between accuracy and a series of possible explanatory variables. In particular, we find a weak negative correlation with the number of tokens in the description (r = ?0.11) and a weak positive correlation with the average pair-wise Euclidean distance between CLIP encodings of the images in a set (r = 0.22), which represents visual similarity.</p><p>By focusing on the 1000 annotated examples in <ref type="table" target="#tab_3">Table 2</ref> we observe a stark drop from overall performance on the subset of examples containing nuances, visibility/occlusion, and negation <ref type="figure" target="#fig_2">(Figure 4)</ref>. This confirms insights from <ref type="bibr" target="#b23">Kassner and Sch?tze (2020)</ref> and <ref type="bibr" target="#b17">Hosseini et al. (2021)</ref> on the difficulty of modeling negation in text-only models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and Future Work</head><p>We created a new challenge, Image Retrieval from Contextual Descriptions (IMAGECODE), which is designed to evaluate the ability of vision-andlanguage models to integrate visual, pragmatic, and temporal context into their predictions. In particular, given a complex and nuanced contextual description, a model is required to retrieve the corresponding image from a set of highly similar candidates. We benchmarked state-of-the-art biencoder and cross-encoder models, such as CLIP and ViLBERT. Moreover, we proposed new variants of these models that are more suitable to solve this task, by augmenting them with a module to attend on the other images in a set and temporal embeddings. We found that IMAGECODE is highly challenging for all variants: even the best model (28.9) lags behind human performance (90.8) dramatically. Images sourced from video frames display the largest gap in performance. The most challenging phenomena in IMAGECODE include pragmatics, negation, fine-grained distinctions between images, and occlusion among others.</p><p>8 Acknowledgements IMAGECODE wouldn't have been possible without the herculean effort of the Amazon Mechanical Turkers and their feedback on the interface. We also thank Emanuelle Bugliarello for his help with VOLTA, an excellent codebase for several vision and language models. We thank the members of SR's research group for their feedback on the ideas presented here. IMAGECODE is funded by the Mila-Samsung grant program. We thank Microsoft for providing us Azure credits. SR acknowledges the support of the NSERC Discovery Grant program and the Facebook CIFAR AI Chair program.</p><p>A Length Distribution of the Image Descriptions <ref type="figure">Figure 5</ref>: Distribution of the number of tokens across contextual descriptions in IMAGECODE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Criteria for Selecting Annotators</head><p>We keep data quality high through entry requirements (English speaking country, over 98% approval rate, etc.), qualification test, whitelisting workers and manually inspecting data. Most importantly our two-stage setup also allowed us to automate monitoring data quality as we could measure the description and retrieval accuracy of workers and only whitelisted those with high accuracy. We paid 0.25$ per description and 0.1$ per retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Annotator Bias</head><p>The majority of descriptions in our test and validation split come from workers who did not work on the training set in order to avoid annotation bias. Our validation set contains 502 descriptions from workers "seen" from the training set and 1,800 description from "unseen" workers. In <ref type="table" target="#tab_12">Table 6</ref> we can see that models perform slightly better on seen workers across our CLIP model variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Crowdsourcing Interface</head><p>Our AMT interface for the description task can be seen in <ref type="figure" target="#fig_3">Figure 6</ref>. The retriever interface looks conceptually similar, with a select-button for each image. Note that workers see images almost in almost half of full-screen (opposed to the shown examples in this PDF) and can quickly go back and forth between consecutive frames with arrow-keys, making it significantly easier to spot and compare nuanced changes.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Additional Hyper-parameters</head><p>The Transformer consists of 2 layers in CLIP variants and 4/5 layers in the ViLBERT/UNITER variants, both employing gelu activation. The learn- ing rate for the fine-tuning of the Transformer and linear heads is 2 ? 10 ?6 for the CLIP +CON-TEXTMODULE, 10 ?4 for CLIP +TEMPORALEM-BEDDINGS, 2 ? 10 ?5 for both ViLBERT variants, and 6 ? 10 ?6 for both UNITER variants. We use the Volta-framework  for the standardized ViLBERT and UNITER model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Examples from IMAGECODE for all phenomena</head><p>For each phenomenon we provide 1 example and a definition we used for annotation purposes.</p><p>Since most examples contain more than one phenomenon, some phenomena will be effectively showcased several times. Note that we picked examples that are relatively easy to understand and spot differences in. Figure 7: Example of Context: "Both hands are on the piece of bread closest to the person." Note: This is contextual since since without any context of other images, the description is also literally true for Frame 9. A model might even score it higher since the direct visual appearance is closer to typical bread. Definition: To understand the description, a listener has to consider other images and/or the speakers intention of describing only one of the images. In line with Grice's maxim of quality, a description is contextual if it is literally true for several images but we know it was intended for only one image. A description is also contextual if an objects cannot clearly be identified in the target image directly but only through cross-referencing other images. Figure 8: Example of negation: "The knife is most centrally placed to insert into the onion without having fully cut deeply into it yet." Definition: Explicit linguistic negation ("not", "unseen", "non-") or negation quantifiers ("no person"). Figure 9: Example of quantifiers/quantities: "A yellow 3 way traffic light with a green arrow on the side facing closest to the camera" Definition: We annotate for quantifiers (most, every, no, several,...) and absolute quantities ("five") as well as relative quantities (ratios like " a third of his hand"). Figure 11: Example of temporality: " A smiling boy just begins to look towards the dog." Definition: While most examples based on video frames implicitly require some temporal knowledge, we focus on explicit textual mentions of 1) temporal markers ("after", "during", "about to", etc) and 2) temporal verbs ("beginning to", "end to"). Figure 13: Example of nuances (we marked small details with red/green rectangles): " The person's palm is towards us and touching the left bottom corner of the cake. There is a small amount of dark space between the right bottom corner of the photo and the edge of the cake. " Definition: Minor details, that are either a) not salient at all and would usually be left unmentioned and/or b) language reference is grounded on a small patch of pixels. Note that this phenomena is often linked with very minimally contrastive images.  <ref type="figure" target="#fig_0">Figure 15</ref>: Example of meta properties: " The cucumber is just to be cut into, you can see a transparent image covering the image." Definition: Descriptions that mention aspects that stem from the way the photo/video was taken: two overlayed images (when a video transitions), black-and-white, blurriness, brightness.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(Figure 1 :</head><label>1</label><figDesc>1 https://github.com/McGill-NLP/imagecode (a) Frame 1 (b) Frame 2 An example of the new challenge, Image Retrieval from Contextual Descriptions (IMAGECODE):"The girl in blue is to the left of the girl in the middle with the purple shoes. The girl in blue is not obscured in any way." Frames 5-10 are left out for simplicity's sake. The target image, frame 3, is in green, whereas the incorrect frames are in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>An example with description: "No bridesmaid visible at all.". Visual context is necessary to identify the correct target image, by cross-referencing the portions of images with bridesmaids (red boxes).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Performance of different CLIP variants (rows) on subsets of examples containing phenomena of interest (columns) in 1000 annotated validation examples. The hue of each cell indicates accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>AMT interface for the describer task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Example of spatial relations/reasoning: "The small girl in front is looking directly to the right with her right hand on the side of her face." Definition: Any relations or adjectives regarding space. Examples: "in the top left corner", "left to the chair", but also camera perspective, or body orientation ("turned towards...")</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12 :</head><label>12</label><figDesc>Example of visibility/occlusion: " The tire is directly on top of the person's right shoe and you can just barely see fingers at the top. " Definition: A description that mentions objects/people being occluded, (partially) out of frame, or in the process of leaving the frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 14 :</head><label>14</label><figDesc>Example of coreference: " A woman with a white background smiles at the camera. Most of her body is visible. She is wearing a black outfit. " Definition: Linguistic coreference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Number of descriptions from each source of images at different stages of the annotation process.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>A smiling boy just begins to look towards the dog.markers (e.g., after)   and verbs (e.g., starts) There is an equal amount of yellow and white between both hands. The cloud on top left side of box only has half of it showing. The spoon is at the top right corner, it is not moving any of the food. The flowers the woman in the teal strapless dress is carrying are completely obscured by the man in the black shirt's head. There is the slightest of openings to see the end of the bridge through the obstruction.</figDesc><table><row><cell>Phenomenon</cell><cell cols="4">all videos static Example from IMAGECODE</cell><cell>Definition</cell></row><row><cell></cell><cell>%</cell><cell>%</cell><cell>%</cell><cell></cell><cell></cell></row><row><cell>Context</cell><cell>47.3</cell><cell>57.3</cell><cell cols="2">6.6 Figure 2</cell><cell>Visual context or pragmatic in-ference required.</cell></row><row><cell cols="6">Temporal Temporal Quantities 15.0 18.5 4.1 48.5 47.7 51.0 -</cell></row><row><cell cols="2">Spatial Relations 70.5</cell><cell>72.2</cell><cell>65.3</cell><cell></cell><cell>-</cell></row><row><cell>Negation</cell><cell>17.9</cell><cell>20.7</cell><cell>6.1</cell><cell></cell><cell>-</cell></row><row><cell>Visibility / Occlusion</cell><cell>45.5</cell><cell>54.5</cell><cell>8.6</cell><cell></cell><cell>An entity is covered or partially outside of the image.</cell></row><row><cell>Nuances</cell><cell>26.3</cell><cell>31.6</cell><cell>5.1</cell><cell></cell><cell>Description grounded on small patch of pixels or very non-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>salient aspects.</cell></row><row><cell>Co-reference</cell><cell>41.5</cell><cell>42.4</cell><cell>38.8</cell><cell>The cloud on top left side of box only has half of it showing.</cell><cell>-</cell></row><row><cell>Meta Properties</cell><cell>12.0</cell><cell>13.9</cell><cell>6.1</cell><cell>Bright shot of a girl and boy standing up straight. Her eyes are closed.</cell><cell></cell></row></table><note>Blurriness, brightness, overlays, and transitions of frames.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Distribution of challenging phenomena in IMAGECODE based on 200 (or 1000 if underlined) manually annotated examples.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>com-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Human performance (accuracy) and interannotator agreement (Krippendorff's ?) on the validation and test splits of IMAGECODE.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Comparison of the text statistics of IMAGE-CODE with other vision-and-language datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Performance</figDesc><table><row><cell>(test accuracy) on IMAGECODE</cell></row><row><cell>across two training regimes (zero-shot and fine-tuning),</cell></row><row><cell>three models (CLIP, UNITER, ViLBERT) and 4 model</cell></row><row><cell>variants. We report separate figures for all the exam-</cell></row><row><cell>ples and two disjoint subsets: video frames and static</cell></row><row><cell>pictures.</cell></row><row><cell>particularly accentuated for static pictures. On</cell></row><row><cell>the other hand, ViLBERT's performance remains</cell></row><row><cell>the same. Stacking a special module for con-</cell></row><row><cell>textualizing multimodal representations on top</cell></row><row><cell>of the encoders (+CONTEXTMODULE), instead,</cell></row><row><cell>yields gains for ViLBERT compared to +CON-</cell></row><row><cell>TEXTBATCH, whereas CLIP and UNITER are un-</cell></row><row><cell>affected (slight drop). This shows that all models</cell></row><row><cell>can exploit visual context, but different strategies</cell></row><row><cell>(contrastive training or dedicated modules) may be</cell></row><row><cell>necessary.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>Performance (accuracy) on two subsets of the distinct validation split: seen workers (workers who also produced description on the train split) and unseen workers (who only worked on the test and validation data).</figDesc><table><row><cell cols="2">E Validation performance</cell></row><row><cell></cell><cell>all</cell><cell>video static</cell></row><row><cell cols="2">ZERO-SHOT</cell></row><row><cell>CLIP</cell><cell cols="2">21.8 14.9 51.6</cell></row><row><cell cols="2">FINE-TUNING</cell></row><row><cell>CLIP</cell><cell cols="2">23.4 17.3 50.2</cell></row><row><cell>+CONTEXTBATCH</cell><cell cols="2">29.7 21.1 67.2</cell></row><row><cell>+CONTEXTMODULE</cell><cell cols="2">29.9 21.4 67.2</cell></row><row><cell>+TEMPORALEMBEDDINGS</cell><cell cols="2">30.6 22.3 67.0</cell></row><row><cell cols="2">ZERO-SHOT</cell></row><row><cell>UNITER</cell><cell cols="2">19.8 13.6 42.9</cell></row><row><cell cols="2">FINE-TUNING</cell></row><row><cell>UNITER</cell><cell cols="2">23.8 17.5 51.2</cell></row><row><cell>+CONTEXTBATCH</cell><cell cols="2">25.5 19.3 52.3</cell></row><row><cell>+CONTEXTMODULE</cell><cell cols="2">24.8 18.9 50.7</cell></row><row><cell>+TEMPORALEMBEDDINGS</cell><cell cols="2">26.0 19.9 52.8</cell></row><row><cell cols="2">ZERO-SHOT</cell></row><row><cell>ViLBERT</cell><cell cols="2">18.5 14.0 37.9</cell></row><row><cell cols="2">FINE-TUNING</cell></row><row><cell>ViLBERT</cell><cell cols="2">21.9 16.1 46.7</cell></row><row><cell>+CONTEXTBATCH</cell><cell cols="2">22.9 18.1 43.5</cell></row><row><cell>+CONTEXTMODULE</cell><cell cols="2">23.5 18.9 43.5</cell></row><row><cell>+TEMPORALEMBEDDINGS</cell><cell cols="2">25.1 19.4 49.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Performance</figDesc><table><row><cell>(validation accuracy) on IMAGE-</cell></row><row><cell>CODE across two training regimes (zero-shot and fine-</cell></row><row><cell>tuning), three models (CLIP, UNITER, ViLBERT) and</cell></row><row><cell>4 model variants. We report separate figures for all the</cell></row><row><cell>examples and two disjoint subsets: video frames and</cell></row><row><cell>static pictures.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We also experimented with ResNet-50 features, but we found CLIP results to be more similar to that of humans in preliminary experiments.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">For further details on crowdsourcing instructions, analysis of annotator bias and the AMT interface, please refer to Appendix C and Appendix D.7  Again, the set of workers validating train and test sets were partly disjoint to avoid annotator bias.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">For comparability, we measured the statistics for all the datasets with the same tools.9  We use spaCy(Honnibal and Montani, 2017)  as a parser.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">We calculated these percentages based on a list of 171 body parts in English collected by<ref type="bibr" target="#b37">Tjuka (2021)</ref> and a list of colors in English from games4esl.com.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">We found this solution to work better for each model in practice, which is justified by their different pre-training objectives.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Ethics and Limitations</head><p>We distribute the descriptions in IMAGECODE under MIT and adopt the licenses of the video and image sources on which our image sets build on top. We report details about crowdsourcing such as payment and selection criteria in Section 3.2 and Appendix B. For the tested model variants, we only train a single run for each hyperparameter setting due to long run times.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Reasoning about Pragmatics with Neural Listeners and Speakers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1125</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1173" to="1182" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Visual question answering on image sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankan</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">COVR: A test-bed for visually grounded compositional generalization with real images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Bogin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivanshu</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9824" to="9846" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
	<note>Dominican Republic</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multimodal Pretraining Unmasked: A Meta-Analysis and a Unified Framework of Vision-and-Language BERTs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Bugliarello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00408</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="978" to="994" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">UNITER: UNiversal Image-TExt Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58577-8_7</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<editor>Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12375</biblScope>
			<biblScope unit="page" from="104" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pragmatically informative image captioning with character-level inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reuben</forename><surname>Cohn-Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2070</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="439" to="443" />
		</imprint>
	</monogr>
	<note>Short Papers. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khushi</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deshraj</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Jos?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="326" to="335" />
		</imprint>
	</monogr>
	<note>Visual dialog</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradipto</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Doell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2634" to="2641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Guesswhat?! visual object discovery through multi-modal dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Language, thought and compositionality. Royal Institute of Philosophy Supplements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><surname>Fodor</surname></persName>
		</author>
		<idno type="DOI">https:/onlinelibrary.wiley.com/doi/abs/10.1111/1468-0017.00153</idno>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="227" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural Naturalist: Generating Fine-Grained Image Comparisons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Kaeser-Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1065</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="708" to="717" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Are we modeling the task or the annotator? an investigation of annotator bias in natural language understanding datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mor</forename><surname>Geva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1107</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1161" to="1166" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pragmatic language interpretation as probabilistic inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael C</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="818" to="829" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Making the v in vqa matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6904" to="6913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Utterer&apos;s Meaning and Intentions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>H Paul Grice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Philosophical Review</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="147" to="77" />
			<date type="published" when="1957" />
		</imprint>
	</monogr>
	<note>the philosophical review</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Lisa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aida</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nematzadeh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09141[cs].ArXiv:2106.09141</idno>
		<title level="m">Probing Image-Language Transformers for Verb Understanding</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">2017. spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ines</forename><surname>Montani</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Understanding by understanding not: Modeling negation in language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arian</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.102</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1301" to="1312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image change captioning by learning from an auxiliary task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrdad</forename><surname>Hosseinzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2725" to="2734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.06595</idno>
		<title level="m">Binary Image Selection (BISON): Interpretable Evaluation of Visual Grounding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gqa: A new dataset for real-world visual reasoning and compositional question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6700" to="6709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsh</forename><surname>Jhamtani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.10584[cs].ArXiv:1808.10584</idno>
		<title level="m">Learning to Describe Differences Between Pairs of Similar Images</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to describe differences between pairs of similar images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsh</forename><surname>Jhamtani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1436</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4024" to="4034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nora</forename><surname>Kassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.698</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7811" to="7818" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Referitgame: Referring to objects in photographs of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahar</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="787" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The open images dataset v4: Unified image classification, object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<idno type="DOI">https:/link.springer.com/article/10.1007/s11263-020-01316-z</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>and visual relationship detection at scale. IJCV</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Video storytelling: Textual summaries for events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongkang</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="554" to="565" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Visually grounded reasoning across languages and cultures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Bugliarello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Edoardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Ponti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Collier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Elliott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Online and Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10467" to="10485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<title level="m">ViLBERT: Pretraining Task</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Agnostic Visiolinguistic Representations for Visionand-Language Tasks</title>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Generation and comprehension of unambiguous object descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oana</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.9</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.16553[cs].ArXiv:2103.16553</idno>
		<title level="m">Thinking Fast and Slow: Efficient Text-to-Visual Retrieval with Transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">No-reference image quality assessment in the spatial domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Anush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">Conrad</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4695" to="4708" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Grounding cognition: The role of perception and action in memory, language, and thinking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Pecher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rolf</forename><forename type="middle">A</forename><surname>Zwaan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An overview of the Tesseract OCR engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ray</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ninth international conference on document analysis and recognition (ICDAR 2007)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="629" to="633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A Corpus for Reasoning about Natural Language Grounded in Photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alane</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ally</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iris</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1644</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6418" to="6428" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A list of color, emotion, and human body part concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annika</forename><surname>Tjuka</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.5572303</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Converting video formats with ffmpeg</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suramya</forename><surname>Tomar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linux Journal</title>
		<imprint>
			<biblScope unit="issue">146</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Context-Aware Captions from Context-Agnostic Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.120</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Honolulu, HI</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1070" to="1079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deirdre</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Sperber</surname></persName>
		</author>
		<title level="m">Pragmatics and time. Pragmatics and Beyond New Series</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farley</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Doran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asim</forename><surname>Kadav</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.06706</idno>
		<title level="m">Visual entailment: A novel task for fine-grained image understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Msrvtt: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5288" to="5296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">L2C: Describing visual differences needs semantic understanding of individuals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsu-Jui</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.eacl-main.196</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2315" to="2320" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
