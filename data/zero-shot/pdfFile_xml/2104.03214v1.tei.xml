<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised Learning for Semi-Supervised Temporal Action Proposal</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Image Processing and Intelligent Control</orgName>
								<orgName type="department" key="dep2">School of Artificial Intelligence and Automation</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">DAMO Academy</orgName>
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Qing</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Image Processing and Intelligent Control</orgName>
								<orgName type="department" key="dep2">School of Artificial Intelligence and Automation</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjie</forename><surname>Shao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Image Processing and Intelligent Control</orgName>
								<orgName type="department" key="dep2">School of Artificial Intelligence and Automation</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Image Processing and Intelligent Control</orgName>
								<orgName type="department" key="dep2">School of Artificial Intelligence and Automation</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
							<email>nsang@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Image Processing and Intelligent Control</orgName>
								<orgName type="department" key="dep2">School of Artificial Intelligence and Automation</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Supervised Learning for Semi-Supervised Temporal Action Proposal</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-supervised learning presents a remarkable performance to utilize unlabeled data for various video tasks. In this paper, we focus on applying the power of selfsupervised methods to improve semi-supervised action proposal generation. Particularly, we design an effective Selfsupervised Semi-supervised Temporal Action Proposal (SSTAP) framework. The SSTAP contains two crucial branches, i.e., temporal-aware semi-supervised branch and relation-aware self-supervised branch. The semisupervised branch improves the proposal model by introducing two temporal perturbations, i.e., temporal feature shift and temporal feature flip, in the mean teacher framework. The self-supervised branch defines two pretext tasks, including masked feature reconstruction and clip-order prediction, to learn the relation of temporal clues. By this means, SSTAP can better explore unlabeled videos, and improve the discriminative abilities of learned action features. We extensively evaluate the proposed SSTAP on THUMOS14 and ActivityNet v1.3 datasets. The experimental results demonstrate that SSTAP significantly outperforms state-of-the-art semi-supervised methods and even matches fully-supervised methods. Code is available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Temporal action proposal aims to localize action instances in untrimmed videos by predicting both action-ness probabilities and temporal boundaries. Recently, various approaches <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b26">27]</ref> for the task have been proposed and achieve significant progress with the quick development of spatio-temporal feature learning <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14]</ref>. Almost all the methods rely on dense temporal annotations for the training videos. However, the annotating task is tedious and * Corresponding Author  <ref type="figure">Figure 1</ref>: (a) Feature similarity matrix visualization. We use cosine similarity to measure the degree of similarity between arbitrary two snippet-level feature vectors within the same video. Note that, snippet-level features of the action as similar as possible while separating actions from backgrounds. Compared to Ji et al. <ref type="bibr" target="#b19">[20]</ref> (top), better representations of the features can be learned by adding our relationaware self-supervised branch (bottom). (b) Our SSTAP consistently exceeds the state-of-the-art semi-supervised method (Ji et al. <ref type="bibr" target="#b19">[20]</ref>) in terms of Average Recall when trained with different percentages of labels on the THU-MOS14 dataset.</p><p>requires large amounts of human labor. Thus these methods may have limited abilities to meet practical demands.</p><p>To alleviate the dependence of labeled videos, Ji et al. <ref type="bibr" target="#b19">[20]</ref> first apply the semi-supervised method, i.e., Mean Teacher <ref type="bibr" target="#b40">[41]</ref>, to temporal action proposal. In this method, Ji et al. only use a small portion of labeled videos and reach high performances. Due to perturbation is an essential component of semi-supervised methods, the method proposes two sequential perturbations, i.e., time warping and time masking, to improve robustness and generalization. However, the perturbations ignore the temporal interactions, which is critical to learn robust action representations. Another line of paradigm to utilize unlabeled videos is about self-supervised methods. These methods explore undergoing video structure by predefining pretext tasks, e.g., learning temporal order <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b49">50]</ref>, pace prediction <ref type="bibr" target="#b43">[44]</ref>, and learning playback rate <ref type="bibr" target="#b52">[53]</ref>. They have reached an impressive performance in several video-related tasks <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b11">12]</ref>, and thus self-supervised learning is proved to be a promising methodology. However, the methodology has never been explored to generate temporal action proposals. We believe that it can contribute to improving the performance by fully utilizing unlabeled videos.</p><p>Based on the above observations, we propose to apply self-supervised methods to improve the semi-supervised temporal action proposal by designing the SSTAP framework. The proposed SSTAP contains two main branches, i.e., temporal-aware semi-supervised branch and relationaware self-supervised branch. The temporal-aware semisupervised branch targets to improve the method in <ref type="bibr" target="#b19">[20]</ref> by designing two simple but effective perturbations, i.e., temporal feature shift and temporal feature flip. The first perturbation bidirectionally moves some randomly selected channels of feature maps, which is inspired by <ref type="bibr" target="#b27">[28]</ref>, and the second perturbation flips the total features, both of them along the temporal dimension. By this means, the proposal model can be more robust and generalized. In the relation-aware self-supervised branch, we define two pretext tasks, i.e., masked feature reconstruction and clip-order prediction. The pretext tasks respectively reconstruct the randomly masked features and predict the correct order of the randomly shuffled clip features. Therefore, SSTAP can better explore the unlabeled videos and learns discriminative features. In <ref type="figure">Figure 1</ref>(a), the covariance-like similarity matrixes show that the self-supervised branch can help to decrease intra-class distance and increase inter-class distance simultaneously. Hence SSTAP can improve proposal performance <ref type="figure">(Figure 1(b)</ref>). We evaluate the proposed SSTAP on the challenging THUMOS14 <ref type="bibr" target="#b20">[21]</ref> and Activi-tyNet v1.3 <ref type="bibr" target="#b6">[7]</ref> datasets and achieve a remarkable improvement on both datasets.</p><p>In summary, our main contributions are as follows:</p><p>? To the best of our knowledge, we are the first to incorporate self-supervised learning in semi-supervised temporal action proposal by designing a unified SSTAP framework;</p><p>? We have designed two simple but effective types of temporal sequential perturbations and defined two types of self-supervised pretext tasks for SSTAP;</p><p>? We extensively test the proposed SSTAP on two public datasets and achieve state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Fully-Supervised Temporal Action Proposal. There are two mainstream approaches: anchor-based methods and boundary-based methods. Anchor-based methods generate proposals by designing multi-scale anchors or sliding windows. The works in <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b12">13]</ref> adopt the C3D network <ref type="bibr" target="#b41">[42]</ref> as the binary classifier for sliding window proposal evaluation. The works in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b3">4]</ref> use LSTM networks to evaluate the pre-defined anchors. <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b8">9]</ref> propose to apply temporal regression to adjust the action boundaries. <ref type="bibr" target="#b15">[16]</ref> proposes to use the complementarity of multi-scale anchors and sliding windows to improve performance. Instead, boundary-based methods evaluate each temporal location in the video. TAG <ref type="bibr" target="#b48">[49]</ref> generates proposals by a temporal watershed algorithm to group continuous high-score regions. BSN <ref type="bibr" target="#b30">[31]</ref> generates proposals via locally locating temporal boundaries and globally evaluating confidence scores. MGG <ref type="bibr" target="#b31">[32]</ref> combines anchor-based methods and boundarybased methods to generate proposals. The works in <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b0">1]</ref> propose to use graph convolutional networks <ref type="bibr" target="#b21">[22]</ref> to model temporal relationships in the input video. BMN <ref type="bibr" target="#b28">[29]</ref> proposes a boundary-matching mechanism for the confidence evaluation of densely distributed proposals in an end-to-end pipeline. BMN has become the champion method on Ac-tivityNet Challenge 2019 <ref type="bibr" target="#b6">[7]</ref> and the mainstream solution on ActivityNet Challenge 2020 <ref type="bibr" target="#b6">[7]</ref>. In this work, we focus on evaluating our SSTAP with the BMN due to its superior performance. Semi-Supervised Learning. Semi-supervised learning describes a class of algorithms that seek to learn from both unlabeled and labeled data, typically assumed to be sampled from the same or similar distributions. Approaches differ on what information to gain from the structure of the unlabeled data. In the image classification task, there are two important approaches for semi-supervised learning: pseudo-labeling and consistency regularization. Pseudolabel <ref type="bibr" target="#b24">[25]</ref> imputes approximate classes on unlabeled data by making predictions from a model trained only on labeled data. Consistency regularization methods measure the discrepancy between predictions made perturbed data points. Approaches of this kind include ?-Model <ref type="bibr" target="#b22">[23]</ref>, Temporal ensembling <ref type="bibr" target="#b22">[23]</ref>, Mean Teacher <ref type="bibr" target="#b40">[41]</ref>, and Virtual Adversarial Training <ref type="bibr" target="#b33">[34]</ref>. In the semi-supervised temporal action proposal task, <ref type="bibr" target="#b19">[20]</ref> adopts the Mean Teacher framework and proposes two perturbations. Self-Supervised Learning. Self-supervised learning is a general learning framework that relies on surrogate tasks that can be formulated using only unlabeled data. For im- Relation-aware Self-Supervised Branch <ref type="figure">Figure 2</ref>: Overview of our SSTAP. We first encode a sampled untrimmed input video into a feature sequence f 1 . In the temporal-aware semi-supervised branch (top right), there are two sequential perturbation operations: temporal feature shift and temporal feature flip. And the Base Module takes the perturbed sequences f pert and the unobstructed f 1 as inputs. Next, the student model and the teacher model of the same network structure generate outputs. In the relation-aware self-supervised branch (bottom right), there are two self-supervised pretext tasks: masked feature reconstruction and clip-order prediction.</p><p>In the end, a unified multi-task framework is exploited for optimization. Color-coded arrows denote the associations between the features in the framework and the respective modules.</p><p>age data, there exist self-supervised tasks such as predicting relative positions of image patches <ref type="bibr" target="#b10">[11]</ref>, jigsaw puzzles <ref type="bibr" target="#b34">[35]</ref>, image inpainting <ref type="bibr" target="#b35">[36]</ref> and image color channel prediction <ref type="bibr" target="#b23">[24]</ref>. Since the particular property of the video is temporal information, recent works also attempt to leverage the temporal relations among frames, such as order verification <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b14">15]</ref>, order prediction of frames <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b49">50]</ref>, and perceive multiple temporal resolutions <ref type="bibr" target="#b52">[53]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SSTAP</head><p>Following the previous work <ref type="bibr" target="#b19">[20]</ref>, we build our SSTAP on top of a state-of-the-art fully-supervised proposal generation network, Boundary-Matching Network (BMN) <ref type="bibr" target="#b28">[29]</ref>. Note that, compared with the multi-stage BSN <ref type="bibr" target="#b30">[31]</ref> framework employed by <ref type="bibr" target="#b19">[20]</ref>, the BMN with end-to-end training can eliminate the mutual influence between multiple stages. At the same time, we also have conducted a fair comparison with <ref type="bibr" target="#b19">[20]</ref> using the same BMN as our SSTAP. We extend the Mean Teacher <ref type="bibr" target="#b40">[41]</ref> framework with two types of sequential perturbations, i.e., temporal feature shift and temporal feature flip in the temporal-aware semi-supervised branch. And in the relation-aware self-supervised branch, two types of self-supervised auxiliary tasks, i.e., masked feature reconstruction and clip-order prediction, are utilized to assist in training the proposal model. <ref type="figure">Figure 2</ref> shows an overview of our method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Description</head><p>Given an untrimmed video sequence S = {s n } ls n=1 with its length as l s , our method aims at detecting action instances ? p = {? n = [t s,n , t e,n ]} Ms n=1 with a relatively small amount of training labels, where M s is the total number of action instances, and [t s,n , t e,n ] denotes the starting and ending points of an action instance ? n , respectively. Note that, classes of these action instances are not considered in the semi-supervised temporal action proposal task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature Encoding</head><p>Following recent proposal generation methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b51">52]</ref>, we construct SSTAP framework upon visual feature sequence extracted from the raw video. Given an untrimmed video sequence S = {s n } ls n=1 with length l s , we first divide it into non-overlapping short snippets that contain ? frames each. Then the two-stream network <ref type="bibr" target="#b45">[46]</ref> is adopted to extract a visual feature sequence ? = {? tn } T n=1 ? R T ?C , where C is the dimension of feature and T = l s /?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Temporal-aware Semi-Supervised Branch</head><p>In this section, we present our temporal-aware semisupervised branch in SSTAP. We first provide a brief description of the proposal generation network and mean teacher framework. Afterward, we introduce two types of sequential perturbations proposed by us, i.e., temporal feature shift and temporal feature flip. Proposal Generation Network. To validate our semisupervised framework and better illustrate our approach, we build our method on top of the Boundary-Matching Network (BMN) <ref type="bibr" target="#b28">[29]</ref> , an effective and end-to-end proposal generation method.</p><p>The same feature encoding is performed as the first step. The BMN comprises three modules: "Base Module", "Temporal Evaluation Module" (TEM), "Proposal Evaluation Module" (PEM). "Base Module" handles the input feature sequence ? and outputs feature sequence ? shared by the following TEM and PEM. TEM evaluates the starting and ending probabilities of each location in the video to generate boundary probability sequences. PEM contains a Boundary-Matching layer (like the ROI Pooling in Faster-RCNN <ref type="bibr" target="#b36">[37]</ref>) to transfer the feature sequence ? to a boundary-matching feature map and contains a series of 3D and 2D convolutional layers to generate boundary-matching confidence maps. The three modules are trained in a unified framework. Therefore, given an untrimmed video, BMN can simultaneously generate (1) boundary probability sequences to construct proposals and (2) boundary-matching confidence maps to evaluate the confidences of all proposals densely. Please refer to <ref type="bibr" target="#b28">[29]</ref> for more details of BMN. Mean Teacher Framework. In the Mean Teacher framework, there are two models: a student proposal model f ? and a teacher proposal model f ? . The student proposal model learns as in fully-supervised learning, with its weights ? optimized by the supervised losses applied on labeled videos. The teacher proposal model has the identical neural network architecture as the student, while its weights ? are updated with an exponential moving average (EMA) of the weights from a sequence of student models of different training iterations:</p><formula xml:id="formula_0">? ? = ?? ? ?1 + (1 ? ?) ? ? ,<label>(1)</label></formula><p>where ? denotes the training iteration, and ? is a smoothing coefficient, which is always set to 0.999. Sequential Perturbations. In the temporal-aware semisupervised branch, the Mean Teacher framework is adopted on BMN to form our semi-supervised learning framework.</p><p>Meanwhile, in the literature, stochastic perturbations have been found crucial for learning robust models by many semi-supervised learning works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b47">48]</ref>. And a typical way of perturbation is adding noise to feature maps. The work in <ref type="bibr" target="#b40">[41]</ref> adds gaussian noise to intermediate feature maps of both student and teacher models. Ji et al. <ref type="bibr" target="#b19">[20]</ref> add two perturbations to the input sequence. However, those perturbations ignore the temporal interactions, which is critical to temporal action proposal task. In our work, we further explore what other specific perturbations are necessary for sequential learning and propose two essential sequential perturbations: temporal feature shift and temporal feature flip.</p><p>The temporal feature shift perturbation is bi-directional moving some randomly selected channels on the feature map of input video along the temporal dimension <ref type="figure">(Figure 2)</ref>. Therefore, temporal feature shift can significantly increase the diversity of the input features. Note that, this perturbation is inspired by <ref type="bibr" target="#b27">[28]</ref>. The differences between <ref type="bibr" target="#b27">[28]</ref> and temporal feature shift include: (1) that <ref type="bibr" target="#b27">[28]</ref> chooses fixed channels (select the first 1/4 of the feature channels, with half moving forward and the other half moving backward). While we randomly choose ? of feature channels (? is a hyper-parameter, ?/2 of feature channels move forward, and the other ?/2 of channels move backward). Hence ours will add more feature diversity. And in the experiments, we observe that the method <ref type="bibr" target="#b27">[28]</ref> can lead to a sharp decline in performance since the perturbed training features are completely misaligned compared to the testing features without perturbations. (2) the purpose of <ref type="bibr" target="#b27">[28]</ref> is to achieve the effect of 3D convolution (i.e., to capture the spatio-temporal interactive information between adjacent time points) by inserting this 2D disturbance in residual blocks for action recognition task. Our temporal feature shift serves as a way of data augmentation, providing more data for training.</p><p>Besides temporal feature shift, we propose temporal feature flip as another source of sequential perturbation. Since sequential video features with different perturbations may have different numbers of proposals with various locations and sizes, it is challenging to match the given video features. Therefore, the horizontally flipped video features are adopted so that one-to-one correspondence between the proposals in the original and the flipped video features can be easily aligned <ref type="figure">(Figure 2</ref>). During the training, the student models at each iteration are encouraged to generate the symmetric outputs with the teacher models.</p><p>During the training process, each mini-batch includes both labeled and unlabeled data, and we also adopt a dropout strategy to prevent overfitting. The labeled samples are trained using supervised loss. However, without ground truth labels, the supervised loss is undefined upon unlabeled videos. Consistency regularization in mean teacher frame-work utilizes unlabeled data based on the assumption that the model should output similar predictions when fed perturbed versions of the same input. In our temporal-aware semi-supervised branch, the consistency loss is applied to both the labeled and unlabeled data. Note that, we add consistency loss (L2-loss) to both boundary probability sequences and boundary-matching confidence maps output by BMN. Therefore, in the temporal-aware semi-supervised branch, the total loss formula is:</p><formula xml:id="formula_1">L semi = L supervised + ? 1 L pert shif t + ? 2 L pert f lip ,<label>(2)</label></formula><p>where weight terms ? 1 and ? 2 are set to 1 and 0.1 separately, L pert shif t and L pert f lip are consistency losses for temporal feature shift perturbation and temporal feature flip perturbation separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Relation-aware Self-Supervised Branch</head><p>Inspired by recent progress in self-supervised learning in video analysis <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b52">53]</ref>, we hypothesize that the semi-supervised temporal action proposal method could dramatically benefit from self-supervised learning techniques. And based on this insight, in the relation-aware self-supervised branch, we propose two auxiliary tasks. The two auxiliary tasks, i.e., masked feature reconstruction and clip-order prediction, can assist the network in learning temporal relations and discriminative representations. Masked feature reconstruction. As shown in <ref type="figure">Figure 2</ref>, the key idea of this self-supervised auxiliary task is to generate the feature f 2 by randomly masking the video feature f 1 at some time points along the time dimension. The Base Module then utilizes f 2 to reconstruct f 1 . The details of the Base Module are shown in <ref type="figure" target="#fig_1">Figure 3</ref>. Masked feature reconstruction produces self-supervised signals from the original feature f 1 , which can learn discriminative representations in a simple-yet-effective way.</p><p>In the masked feature reconstruction auxiliary task, the Base Module will be driven to perceive and aggregate information from the context to predict the dropped snippets. In this way, the learned temporal semantic relations and discriminative features are conducive to semi-supervised temporal action proposal naturally. We use ? to represent the degree of the random mask, and we measure the effect of ? later in Section 4.3. Clip-order prediction. This auxiliary task needs to predict clip feature sequences' correct order in a randomly scrambled feature map. Specifically, the reordering of three randomly shuffled feature sequences is shown in <ref type="figure">Figure 2</ref>. Actually, the clip-order prediction is formulated as a classification task. The input is a tuple of clip feature sequences, and the output is a probability distribution over different orders. In the experiment, we empirically designed a reordering of two randomly shuffled feature sequences. The module used for clip-order prediction is shown in <ref type="figure" target="#fig_1">Figure 3</ref>.</p><p>Clip-order prediction can leverage the chronological order of feature f 1 to learn discriminative temporal representations. And clip-order prediction is at the clip sequence level, which can reduce the uncertainty of orders and is more appropriate to learn video feature representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Overall Loss</head><p>The total training loss is composed of the losses from section 3.3 and section 3.4, as follows:</p><formula xml:id="formula_2">L total = L semi + ? 3 L aux recons + ? 4 L aux order ,<label>(3)</label></formula><p>where loss functions L aux recons and L aux order are designed for masked feature reconstruction and clip-order prediction mentioned above separately. Among them, L aux recons is L2-loss and L aux order is typical crossentropy loss for both labeled and unlabeled data. Eventually, the final loss function L total is composed of the L semi in the temporal-aware semi-supervised branch and the losses in the relation-aware self-supervised branch. Hyper-parameters ? 3 and ? 4 are set to 0.0001 and 0.001 separately. To jointly learn the semi-supervised pattern and the self-supervised pattern, a unified multi-task framework is exploited for optimization in an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and Setup</head><p>THUMOS14. This dataset has 1010 validation videos and 1574 testing videos with 20 classes. There are 200 validation videos and 213 testing videos labeled with temporal annotations for the action proposal or detection task. We train our model on the validation set and evaluate on the test set. To make a fair comparison with the previous works <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b19">20]</ref>, we employ the same two-stream features <ref type="bibr" target="#b45">[46]</ref>. ActivityNet v1.3. This dataset is a large-scale dataset containing 19994 videos with 200 activity classes for action recognition, temporal action proposal generation and detection. The quantity ratio of training, validation, and testing sets satisfy 2:1:1. Two-stream features are employed to make a fair comparison with the previous works <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b19">20]</ref>. Meanwhile, in order to show that our method is featureagnostic, we also adopt I3D features <ref type="bibr" target="#b7">[8]</ref> pre-trained on Kinetics <ref type="bibr" target="#b7">[8]</ref> and without fine-tuned on ActivityNet v1.3.</p><p>We follow the same pre-processing and post-processing steps as the BMN <ref type="bibr" target="#b28">[29]</ref>, including parameters adopted in Soft-NMS <ref type="bibr" target="#b2">[3]</ref> and network structure parameters for a fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Temporal Action Propsal Generation</head><p>The proposal generation task's goal is to generate highquality proposals to cover action instances with high recall and high temporal overlap.   <ref type="table">Table 2</ref>: Comparisons between our method and fullysupervised proposal generation methods on THUMOS14 in terms of AR@AN.</p><p>methods on the validation set of ActivityNet v1.3. <ref type="table" target="#tab_2">Table 1</ref> lists a set of proposal generation methods, including TCN <ref type="bibr" target="#b9">[10]</ref>, Prop-SSAD <ref type="bibr" target="#b29">[30]</ref>, CTAP <ref type="bibr" target="#b15">[16]</ref>, BSN <ref type="bibr" target="#b30">[31]</ref>, MGG <ref type="bibr" target="#b31">[32]</ref>, and BMN <ref type="bibr" target="#b28">[29]</ref>. Specifically, with only 60% of the videos labeled, our SSTAP surpasses the fullysupervised BMN trained with all labels (100%) and other fully-supervised methods ( <ref type="figure">Figure 4</ref> and <ref type="table" target="#tab_2">Table 1</ref>). Meanwhile, <ref type="table" target="#tab_2">Table 1</ref> also shows that the performance of our SSTAP can be further improved when more labels are available (i.e., 90% and 100%). Our approach also performs well with I3D feature inputs, which proves that our SSTAP is feature-agnostic. Similarly, <ref type="table">Table 2</ref> and <ref type="figure">Figure 5</ref> show the proposal generation performance comparisons on the testing set of THUMOS14.</p><p>Comparisons with semi-supervised baselines. <ref type="table" target="#tab_4">Table 3</ref> compares semi-supervised proposal generation methods on the testing set of the THUMOS14 dataset. To ensure a fair comparison, we adopt the same video feature and post-processing steps. <ref type="table" target="#tab_4">Table 3</ref> shows that our method using two-stream video features outperforms other semisupervised methods significantly when the proposal number is set within [50, 100, 200, 500, 1000]. Especially, <ref type="figure">Figure 5</ref> demonstrates that our SSTAP outperforms the strong semisupervised method in Ji et al. <ref type="bibr" target="#b19">[20]</ref> consistently under the different ratios of labeled/(labeled + unlabeled) training Method Label @50 @100 @200 @500 @1000   <ref type="table">Table 4</ref>: Ablation study of the effectiveness of components in our SSTAP on THUMOS14. Abbreviations: F for temporal feature flip, R for masked feature reconstruction, C for clip-order prediction, and S for temporal feature shift.</p><p>videos. Unless otherwise stated, the results of Ji et al. <ref type="bibr" target="#b19">[20]</ref> are all based on BMN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>In this section, we present ablation studies of several components of our algorithm. We use different values of hyper-parameters that give the best result for each architectural change. The THUMOS14 dataset is employed in all studies performed in this section. Complementarity between components. We further conduct detailed ablation studies to evaluate different components of the proposed framework, including temporal feature shift (S), temporal feature flip (F), clip-order prediction (C), and masked feature reconstruction (R). Ablation studies include the following: Vanilla BMN : All of the above four components are discarded. SSTAP -F : Only temporal feature flip perturbation is discarded. SSTAP -F -R : The temporal feature flip perturbation and masked feature reconstruction auxiliary task are discarded. SSTAP -F -R -C : The temporal feature flip perturbation and the two self-supervised auxiliary tasks in the relation-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Label @50 @100 @200 @500 @1000   <ref type="figure">Figure 6</ref>: Ablation comparisons. The effects of temporal feature shift perturbation and masked feature reconstruction auxiliary task under different hyper-parameter choices on the THUMOS14 dataset.</p><p>aware self-supervised branch are discarded. SSTAP -R -C : The two self-supervised auxiliary tasks in the relation-aware self-supervised branch are discarded. SSTAP -S -R -C : The temporal feature shift perturbation and the two self-supervised auxiliary tasks in the relationaware self-supervised branch are discarded. <ref type="table">Table 4</ref> demonstrates that the four components are complementary in terms of improving performance. In particular, when combined with the four components (i.e., SSTAP (ALL)), the best performance is achieved. And the results for SSTAP -F -R -C and SSTAP -S -R -C show that our single perturbation also performs very well. Effectiveness of self-supervised branch. As illustrated in <ref type="table" target="#tab_6">Table 5</ref>, we compare the results of applying clip-order prediction (C) and masked feature reconstruction (R) directly to the original BMN. That shows the effectiveness of the two self-supervised auxiliary tasks for performance improvement. Note that, both labeled and unlabeled data are used for training the auxiliary tasks. Selection of hyper-parameters. <ref type="figure">Figure 6</ref> illustrates the comparison of the selection of hyper-parameters. It can be observed that the adjustment of parameters has a certain effect on the performance of AR@50 on THUMOS14, meanwhile, ? = 2 ?4 and ? = 0.3 appear to be the optimal operating points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Action Detection with Our Proposals</head><p>To further examine the quality of the proposals generated by SSTAP, we put the proposals in a temporal action detection framework. The evaluation metric of temporal action detection is mAP, which calculates the Average Precision under multiple IoU thresholds for each action cate-  We adopt the two-stage "detection by classifying proposals" temporal action detection framework to combine our proposals with action classifiers. For fair comparisons, following <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b0">1]</ref>, on ActivityNet v1.3, we adopt top-1 video-level classification results generated by method <ref type="bibr" target="#b54">[55]</ref> and use confidence scores of BMN proposals for detection results retrieving. On THUMOS14, following BMN <ref type="bibr" target="#b28">[29]</ref>, we also use both top-2 video-level classification results generated by UntrimmedNet <ref type="bibr" target="#b44">[45]</ref>. And the same classifiers are also used for other proposal generation methods, including SST <ref type="bibr" target="#b4">[5]</ref>, TURN <ref type="bibr" target="#b16">[17]</ref>, BSN <ref type="bibr" target="#b30">[31]</ref>, MGG <ref type="bibr" target="#b31">[32]</ref>, DBG <ref type="bibr" target="#b26">[27]</ref>, G-TAD <ref type="bibr" target="#b51">[52]</ref>, and BC-GNN <ref type="bibr" target="#b0">[1]</ref>. <ref type="table" target="#tab_8">Table 6</ref> illustrates the performance comparisons, which are evaluated on the testing set of THUMOS14. With only 60% of the videos labeled, our SSTAP achieves better performance than fully-supervised BMN trained with all labels in metrics of average mAP. Especially, with 100% of the videos labeled, our SSTAP outperforms the fully-supervised proposal methods, namely BMN <ref type="bibr" target="#b28">[29]</ref>, G-TAD <ref type="bibr" target="#b51">[52]</ref>, BC-GNN <ref type="bibr" target="#b0">[1]</ref>, and Ji et al. <ref type="bibr" target="#b19">[20]</ref>. Similar results on THUMOS14 are shown in <ref type="table" target="#tab_10">Table 7</ref>, thus demonstrating the effectiveness of our proposed SSTAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Generalization Experiments</head><p>To prove the SSTAP method is valid for other network architectures and frameworks, we introduce SSTAP to G-TAD <ref type="bibr" target="#b51">[52]</ref>. G-TAD proposes to use graph convolutional networks <ref type="bibr" target="#b21">[22]</ref> to model temporal relationships between each time point in the input video. As illustrated in <ref type="table">Table 8</ref>, introducing SSTAP to G-TAD also improves performance. In particular, our SSTAP outperforms the strong   <ref type="table">Table 8</ref>: Generalizing our SSTAP to G-TAD <ref type="bibr" target="#b51">[52]</ref> in terms of mAP@tIoU on THUMOS14. The comparison experiments all use the same two-stream feature <ref type="bibr" target="#b45">[46]</ref> as in G-TAD <ref type="bibr" target="#b51">[52]</ref>.</p><p>semi-supervised baseline <ref type="bibr" target="#b19">[20]</ref> by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we incorporate self-supervised learning in the semi-supervised temporal action proposal task and propose a unified SSTAP framework. Specially, we have designed two simple but effective types of temporal sequential perturbations and defined two types of self-supervised pretext tasks for SSTAP. We show empirically that SSTAP consistently outperforms the state-of-the-art semi-supervised methods and even matches the fully-supervised methods. Furthermore, we indicate that our SSTAP is agnostic to specific proposal methods and can be effectively applied to other temporal action proposal approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The details of our Base Module. Our Base Module is an extension of the "Base Module" in BMN<ref type="bibr" target="#b28">[29]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Comparisons between our SSTAP and fullysupervised temporal action proposal generation methods on the validation set of ActivityNet v1.3 dataset in terms of AR@AN and AUC.</figDesc><table><row><cell>Feature</cell><cell>Method</cell><cell cols="2">@50 @100 @200 @1000</cell></row><row><cell cols="2">2-Stream TAG [49]</cell><cell>18.55 29.00 39.61</cell><cell>-</cell></row><row><cell>Flow</cell><cell>TURN [17]</cell><cell>21.86 31.89 43.02</cell><cell>64.17</cell></row><row><cell cols="2">2-Stream CTAP [16]</cell><cell>32.49 42.61 51.97</cell><cell>-</cell></row><row><cell cols="2">2-Stream BSN [31]</cell><cell>37.46 46.06 53.21</cell><cell>64.52</cell></row><row><cell cols="2">2-Stream MGG [32]</cell><cell>39.93 47.75 54.65</cell><cell>64.06</cell></row><row><cell cols="2">2-Stream DBG [27]</cell><cell>37.32 46.67 54.50</cell><cell>66.40</cell></row><row><cell cols="2">2-Stream BC-GNN [1]</cell><cell>40.50 49.60 56.33</cell><cell>66.57</cell></row><row><cell cols="2">2-Stream BMN@60%</cell><cell>34.88 42.11 49.76</cell><cell>61.15</cell></row><row><cell cols="2">2-Stream BMN@90%</cell><cell>38.45 46.31 53.36</cell><cell>65.29</cell></row><row><cell cols="2">2-Stream BMN@100%</cell><cell>39.36 47.72 54.70</cell><cell>65.49</cell></row><row><cell cols="2">2-Stream SSTAP@60%</cell><cell>39.42 48.02 55.03</cell><cell>67.07</cell></row><row><cell cols="2">2-Stream SSTAP@90%</cell><cell>40.12 49.22 55.86</cell><cell>68.21</cell></row><row><cell cols="3">2-Stream SSTAP@100% 41.01 50.12 56.69</cell><cell>68.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Vanilla BMN 10% 23.71 31.11 37.98 46.35 52.25 Mean Teacher [41] 10% 27.95 36.27 43.42 51.68 57.28 Pseudo-label [25] 10% 26.89 35.48 42.11 50.89 55.56 Ji et al. [20] 10% 29.10 37.43 45.07 52.67 57.96 SSTAP 10% 32.33 40.92 48.27 54.99 59.38</figDesc><table><row><cell>Vanilla BMN</cell><cell>60% 34.88 42.11 49.76 56.76</cell><cell>61.15</cell></row><row><cell cols="2">Mean Teacher [41] 60% 36.77 45.23 52.26 59.50</cell><cell>64.04</cell></row><row><cell>Pseudo-label [25]</cell><cell>60% 36.46 45.43 53.08 59.94</cell><cell>63.93</cell></row><row><cell>Ji et al. [20]</cell><cell>60% 37.42 46.71 53.96 61.01</cell><cell>65.10</cell></row><row><cell>SSTAP</cell><cell>60% 39.42 48.02 55.03 62.64</cell><cell>67.07</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparisons between semi-supervised baselines trained with 10% and 60% of the labels. For fair comparisons, semi-supervised baselines are all based on BMN. We report AR at various AN on THUMOS14.</figDesc><table><row><cell>Method</cell><cell cols="2">Label @50 @100 @200 @500 @1000</cell></row><row><cell>Vanilla BMN</cell><cell>10% 23.71 31.11 37.98 46.35</cell><cell>52.25</cell></row><row><cell>SSTAP -F</cell><cell>10% 32.07 40.52 47.88 54.59</cell><cell>58.77</cell></row><row><cell>SSTAP -F -R</cell><cell>10% 30.82 39.24 46.85 54.31</cell><cell>58.71</cell></row><row><cell cols="2">SSTAP -F -R -C 10% 30.23 38.75 46.12 53.96</cell><cell>58.16</cell></row><row><cell>SSTAP -R -C</cell><cell>10% 30.80 38.96 46.31 54.28</cell><cell>58.23</cell></row><row><cell cols="2">SSTAP -S -R -C 10% 29.21 37.57 45.10 52.92</cell><cell>57.99</cell></row><row><cell>SSTAP (ALL)</cell><cell>10% 32.33 40.92 48.27 54.99</cell><cell>59.38</cell></row><row><cell>Vanilla BMN</cell><cell>60% 34.88 42.11 49.76 56.76</cell><cell>61.15</cell></row><row><cell>SSTAP -F</cell><cell>60% 39.26 48.00 54.95 62.07</cell><cell>66.65</cell></row><row><cell>SSTAP -F -R</cell><cell>60% 38.52 47.24 54.69 61.89</cell><cell>66.72</cell></row><row><cell cols="2">SSTAP -F -R -C 60% 38.04 46.71 54.35 62.17</cell><cell>66.51</cell></row><row><cell>SSTAP -R -C</cell><cell>60% 38.57 46.89 54.48 62.35</cell><cell>66.83</cell></row><row><cell cols="2">SSTAP -S -R -C 60% 37.44 46.86 54.07 61.23</cell><cell>65.21</cell></row><row><cell>SSTAP (ALL)</cell><cell>60% 39.42 48.02 55.03 62.64</cell><cell>67.07</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Ablation study of the effectiveness of selfsupervised branch. Abbreviations: R for masked feature reconstruction, and C for clip-order prediction.</figDesc><table><row><cell>40.00</cell><cell cols="2">AR@50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">40.00 AR@50</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>39.00</cell><cell></cell><cell></cell><cell></cell></row><row><cell>39.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>38.00</cell><cell></cell><cell></cell><cell></cell></row><row><cell>38.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>37.00</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>36.00</cell><cell></cell><cell></cell><cell></cell></row><row><cell>37.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>35.00</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2 -8</cell><cell>2 -7</cell><cell>2 -6</cell><cell>2 -5 coefficient ?</cell><cell>2 -4</cell><cell>2 -3</cell><cell>2 -2</cell><cell>0.0</cell><cell>0.1</cell><cell>0.2 coefficient ? 0.3</cell><cell>0.4</cell><cell>0.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Action detection results on the validation set of ActivityNet v1.3, where our proposals are combined with video-level classification results generated by [55]. gory. On ActivityNet v1.3, the IoU thresholds for mAP are set to {0.5, 0.75, 0.95}, and the IoU thresholds for average mAP are set to [0.5 : 0.05 : 0.95]. On THUMOS14, the IoU thresholds for mAP are set to {0.3, 0.4, 0.5, 0.6, 0.7}.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>10.9 20.0 31.5 41.2 TURN [17]+UNet ICCV'17 6.3 14.1 24.5 35.3 46.3 BSN [31]+UNet ECCV'18 20.0 28.4 36.9 45.0 53.5 MGG [32]+UNet CVPR'19 21.3 29.5 37.4 46.8 53.9 DBG [27]+UNet AAAI'20 21.7 30.2 39.8 49.4 57.8 G-TAD [52]+UNet CVPR'20 23.4 30.8 40.2 47.6 54.5 BC-GNN [1]+UNet ECCV'20 23.1 31.2 40.4 49.1 57.1 BMN@60%+UNet ICCV'19 17.0 25.5 34.0 44.7 53.4 BMN@90%+UNet ICCV'19 19.7 28.9 38.2 46.8 55.5 BMN@100%+UNet ICCV'19 20.5 29.7 38.8 47.4 56.0 Ji et al. @60%+UNet ICCV'19 19.2 28.1 37.1 47.2 55.4 Ji et al. @90%+UNet ICCV'19 21.5 31.6 41.2 50.6 57.2 Ji et al. @100%+UNet ICCV'19 21.9 32.2 41.7 51.2 57.9</figDesc><table><row><cell>Method</cell><cell cols="2">Reference 0.7</cell><cell>0.6</cell><cell>0.5</cell><cell>0.4</cell><cell>0.3</cell></row><row><cell cols="7">SST [5]+UNet 4.7 SSTAP@60%+UNet CVPR'17 -20.7 30.5 39.4 48.8 56.5</cell></row><row><cell>SSTAP@90%+UNet</cell><cell>-</cell><cell cols="5">22.1 32.3 41.9 51.2 57.8</cell></row><row><cell>SSTAP@100%+UNet</cell><cell>-</cell><cell cols="5">22.8 32.8 42.3 51.5 58.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Action detection results on the testing set of THU-MOS14 in terms of mAP@tIoU. We compare with "proposal + classification" methods, where classification results are generated by UntrimmedNet<ref type="bibr" target="#b44">[45]</ref>.Ji  et al. [20]+G-TAD 60% 20.1 29.4 39.6 47.5 53.8 SSTAP+G-TAD 60% 21.8 31.1 41.4 50.2 56.3 Vanilla G-TAD 100% 23.4 30.8 40.2 47.6 54.5 Ji et al. [20]+G-TAD 100% 21.3 31.3 41.2 49.6 55.3 SSTAP+G-TAD 100% 22.6 32.4 42.7 51.3 57.0</figDesc><table><row><cell>Method</cell><cell>Label</cell><cell>0.7</cell><cell>0.6</cell><cell>0.5</cell><cell>0.4</cell><cell>0.3</cell></row><row><cell>Vanilla G-TAD</cell><cell>10%</cell><cell cols="5">6.8 12.6 20.4 28.5 37.3</cell></row><row><cell>Ji et al. [20]+G-TAD</cell><cell>10%</cell><cell cols="5">9.5 17.4 25.8 34.4 43.4</cell></row><row><cell>SSTAP+G-TAD</cell><cell cols="6">10% 11.1 18.4 27.6 35.9 45.5</cell></row><row><cell>Vanilla G-TAD</cell><cell>60%</cell><cell cols="5">16.5 25.3 35.4 44.8 50.9</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by the National Natural Science Foundation of China under grant (61871435, 61901184), and the Fundamental Research Funds for the Central Universities no.2019kfyXKJC024.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Boundary content graph neural network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueran</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhai</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiyue</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5049" to="5059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Soft-nms-improving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navaneeth</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5561" to="5569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">End-to-end, single-stream temporal action detection in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyamal</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sst: Single-stream temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyamal</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2911" to="2920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast temporal activity proposals for efficient detection of human actions in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1914" to="1923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rethinking the faster r-cnn architecture for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1130" to="1139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Temporal context network for activity localization in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Larry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan Qiu</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5793" to="5802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Temporal cycleconsistency learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debidatta</forename><surname>Dwibedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1801" to="1810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Daps: Deep action proposals for action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">Caba</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="768" to="784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6201" to="6210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Self-supervised video representation learning with odd-one-out networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ctap: Complementary temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="68" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Turn tap: Temporal unit regression network for temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3628" to="3636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Cascaded boundary regression for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.01180</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scc: Semantic context cascade for efficient action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayner</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Barrios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3175" to="3184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning temporal action proposals with fewer labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="7073" to="7082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Thumos challenge: Action recognition with a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roshan</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Temporal ensembling for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02242</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Colorization as a proxy task for visual understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6874" to="6883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMLW</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by sorting sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fast learning of temporal action proposal via dense boundary generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="11499" to="11506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bmn: Boundary-matching network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3889" to="3898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Single shot temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="988" to="996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bsn: Boundary sensitive network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haisheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-granularity generator for temporal action proposal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3604" to="3613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Shuffle and learn: unsupervised learning using temporal order verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Shin-Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cdc: Convolutional-deconvolutional networks for precise temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5734" to="5743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Self-supervised learning of video-induced visual invariances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josip</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravindh</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13806" to="13815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Selfsupervised video representation learning by pace prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangliu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Hui</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.05861</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Untrimmednets for weakly supervised action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4325" to="4334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multi-level temporal pyramid network for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Shiwei Zhang, and Nong Sang</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="41" to="54" />
		</imprint>
	</monogr>
	<note>PRCV</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Enaet: Self-trained ensemble autoencoding transformations for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09265</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.02716</idno>
		<title level="m">A pursuit of temporal accuracy in general activity detection</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Self-supervised spatiotemporal learning via video clip order prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">R-c3d: Region convolutional 3d network for temporal activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abir</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="5783" to="5792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">G-tad: Sub-graph localization for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="10156" to="10165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Video playback rate perception for self-supervised spatio-temporal representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dezhao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7094" to="7103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Cuhk &amp; ethz &amp; siat submission to activitynet challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.08011</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
