<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Escaping the Big Data Paradigm with Compact Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Hassani</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SHI Lab</orgName>
								<orgName type="institution">University of Oregon</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Walton</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SHI Lab</orgName>
								<orgName type="institution">University of Oregon</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Shah</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SHI Lab</orgName>
								<orgName type="institution">University of Oregon</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abulikemu</forename><surname>Abuduweili</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SHI Lab</orgName>
								<orgName type="institution">University of Oregon</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SHI Lab</orgName>
								<orgName type="institution">University of Oregon</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SHI Lab</orgName>
								<orgName type="institution">University of Oregon</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Picsart AI Research (PAIR)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Escaping the Big Data Paradigm with Compact Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the rise of Transformers as the standard for language processing, and their advancements in computer vision, there has been a corresponding growth in parameter size and amounts of training data. Many have come to believe that because of this, transformers are not suitable for small sets of data. This trend leads to concerns such as: limited availability of data in certain scientific domains and the exclusion of those with limited resource from research in the field. In this paper, we aim to present an approach for small-scale learning by introducing Compact Transformers. We show for the first time that with the right size, convolutional tokenization, transformers can avoid overfitting and outperform state-of-the-art CNNs on small datasets. Our models are flexible in terms of model size, and can have as little as 0.28M parameters while achieving competitive results. Our best model can reach 98% accuracy when training from scratch on CIFAR-10 with only 3.7M parameters, which is a significant improvement in data-efficiency over previous Transformer based models being over 10x smaller than other transformers and is 15% the size of ResNet50 while achieving similar performance. CCT also outperforms many modern CNN based approaches, and even some recent NAS-based approaches. Additionally, we obtain a new SOTA result on Flowers-102 with 99.76% top-1 accuracy, and improve upon the existing baseline on ImageNet (82.71% accuracy with 29% as many parameters as ViT), as well as NLP tasks. Our simple and compact design for transformers makes them more feasible to study for those with limited computing resources and/or dealing with small datasets, while extending existing research efforts in data efficient transformers. Convolution</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the rise of Transformers as the standard for language processing, and their advancements in computer vision, there has been a corresponding growth in parameter size and amounts of training data. Many have come to believe that because of this, transformers are not suitable for small sets of data. This trend leads to concerns such as: limited availability of data in certain scientific domains and the exclusion of those with limited resource from research in the field. In this paper, we aim to present an approach for small-scale learning by introducing Compact Transformers. We show for the first time that with the right size, convolutional tokenization, transformers can avoid overfitting and outperform state-of-the-art CNNs on small datasets. Our models are flexible in terms of model size, and can have as little as 0.28M parameters while achieving competitive results. Our best model can reach 98% accuracy when training from scratch on CIFAR-10 with only 3.7M parameters, which is a significant improvement in data-efficiency over previous Transformer based models being over 10x smaller than other transformers and is 15% the size of ResNet50 while achieving similar performance. CCT also outperforms many modern CNN based approaches, and even some recent NAS-based approaches. Additionally, we obtain a new SOTA result on Flowers-102 with 99.76% top-1 accuracy, and improve upon the existing baseline on ImageNet (82.71% accuracy with 29% as many parameters as ViT), as well as NLP tasks. Our simple and compact design for transformers makes them more feasible to study for those with limited computing resources and/or dealing with small datasets, while extending existing research efforts in data efficient transformers. , the basic compact transformer, and CCT (left), the convolutional variant of our compact transformer models. CCT can be quickly trained from scratch on small datasets, while achieving high accuracy (in under 30 minutes one can get 90% on an NVIDIA 2080Ti GPU or 80% on an AMD 5900X CPU on CIFAR-10 dataset).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Convolutional neural networks (CNNs) <ref type="bibr">[23]</ref> have been the standard for computer vision, since the success of AlexNet <ref type="bibr">[22]</ref>. Krizhevsky et al. showed that convolutions are adept at vision based problems due to their invariance to spatial translations as well as having low relational inductive bias. He et al. <ref type="bibr">[16]</ref> extended this work by introducing residual connections, allowing for significantly deeper models to perform efficiently. Convolutions leverage three important concepts that lead to their efficiency: sparse interaction, weight sharing, and equivariant representations <ref type="bibr">[14]</ref>. Translational equivariance and invariance are properties of the convolutions and pooling layers, respectively <ref type="bibr">[14,</ref><ref type="bibr" target="#b12">36]</ref>. They allow CNNs to leverage natural image statistics and subsequently allow models to have higher sampling efficiency <ref type="bibr">[34,</ref><ref type="bibr">34]</ref>.</p><p>On the other end of the spectrum, Transformers have become increasingly popular and a major focus of modern machine learning research. Since the advent of Attention is All You Need <ref type="bibr" target="#b17">[41]</ref>, the research community saw a spike in transformer-based and attention-based research. While this work originated in natural language processing, these models have been applied to other fields, such as computer vision. Vision Transformer (ViT) <ref type="bibr">[12]</ref> was the first major demonstration of a pure transformer backbone being applied to computer vision tasks. ViT highlights not only the power of such models, but also that large-scale training can trump inductive biases. The authors argued that "Transformers lack some of the inductive biases inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well when trained on insufficient amounts of data." Over the past few years, an explosion in model sizes and datasets has also become noticeable which has led to a "data hungry" paradigm, making training transformers from scratch seem intractable for many types of pressing problems, where there are typically several orders of magnitude less data. It also limits major contributions in the research to those with vast computational resources.</p><p>As a result, CNNs are still the go-to models for smaller datasets because they are more efficient, both computationally and in terms of memory, when compared to transformers. Additionally, local inductive bias shows to be more important in smaller images. They require less time and data to train while also requiring a lower number of parameters to accurately fit data. However, they do not enjoy the long range interdependence that attention mechanisms in transformers provide. Reducing machine learning's dependence on large sums of data is important, as many domains, such as science and medicine, would hardly have datasets the size of ImageNet <ref type="bibr" target="#b9">[10]</ref>. This is because events are far more rare and it would be more difficult to properly assign labels, let alone create a set of data which has low bias and is appropriate for conventional neural networks. In medical research, for instance, it may be difficult to compile positive samples of images for a rare disease without other correlating factors, such as medical equipment being attached to patients who are actively being treated. Additionally, for a sufficiently rare disease there may only be a few thousand images for positive samples, which is typically not enough to train a network with good statistical prediction unless it can sufficiently be pre-trained on data with similar attributes. This inability to handle smaller datasets has impacted the scientific community where they are much more limited in the models and tools that they are able to explore. Frequently, problems in scientific domains have little in common with domains of pre-trained models and when domains are sufficiently distinct pre-training can have little to no effect on the performance within a new domain <ref type="bibr" target="#b30">[54]</ref>. In addition, it has been shown that strong performance on ImageNet does not necessarily result in equally strong performance in other domains, such as medicine <ref type="bibr">[20]</ref>. Furthermore, the requisite of large data results in a requisite of large computational resources and this prevents many researchers from being able to provide insight. This not only limits the ability to apply models in different domains, but also limits reproducibility. Verification of state of the art machine learning algorithms should not be limited to those with large infrastructures and computational resources.</p><p>The above concerns motivated our efforts to build more efficient models that can be effective in less data intensive domains and allow for training on datasets that are orders of magnitude smaller than those conventionally seen in computer vision and natural language processing (NLP) problems. Both Transformers and CNNs have highly desirable qualities for statistical inference and prediction, but each comes with their own costs. In this work, we try to bridge the gap between these two architectures and develop an architecture that can both attend to important features within images, while also being spatially invariant, where we have sparse interactions and weight sharing. This allows for a Transformer based model to be trained from scratch on small datasets like CIFAR-10 and CIFAR-100, providing competitive results with fewer parameters and low computational requirements.</p><p>In this paper we introduce ViT-Lite, a smaller and more compact version of ViT, which can obtain over 90% accuracy on CIFAR-10. We expand on ViT-Lite by introducing a sequence pooling and forming the Compact Vision Transformer (CVT). We further iterate by adding convolutional blocks to the tokenization step and thus creating the Compact Convolutional Transformer (CCT). Both of these simple additions add to significant increases in performance, leading to a top-1%accuracy of 98% on CIFAR-10. This makes our work the only transformer based model in the top 25 best performing models on CIFAR-10, without pretraining, and significantly smaller than the vast majority. Our model also outperforms most comparable CNN-based models within this domain, with the exception of certain Neural Architectural Search techniques <ref type="bibr" target="#b4">[5]</ref>. Additionally, we show that our model can be lightweight, only needing 0.28 million parameters and still reach close to 90% top-1% accuracy on CIFAR-10. On ImageNet, CCT achieves 80.67% accuracy while still maintaining a small number of parameters and reduced computation. CCT outperforms ViT, while containing less than a third of the number of parameters with about a third of the computational complexity (MACs). Additionally, CCT outperform similarly sized and more recent models, such as <ref type="bibr">DeiT [19]</ref>. This demonstrates the scalability of our model while maintaining compactness and computational efficiency.</p><p>The main contributions of this paper are:</p><p>? Extending transformer-based research to small data regimes, by introducing ViT-Lite, which can be trained from scratch and achieve high accuracy on datasets such as CIFAR-10.</p><p>? Introducing Compact Vision Transformer (CVT) with a new sequence pooling strategy, which pools over output tokens and improves performance.</p><p>? Introducing Compact Convolutional Transformer (CCT) to increase performance and provide flexibility for input image sizes while also demonstrating that these variants do not depend as much on Positional Embedding compared to the rest.</p><p>In addition, we demonstrate that our CCT model is fast, obtaining 90% accuracy on CIFAR-10 using a single NVIDIA 2080Ti GPU and 80% when trained on a CPU (AMD 5900X), both in under 30 minutes. Additionally, since our model has a relatively small number of parameters, it can be trained on the majority of GPUs, even if researchers do not have access to top of the line hardware. Through these efforts, we aim to help enable and extend research around Transformers to cases with limited data and/or researchers with limited resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>In NLP research, attention mechanisms [15, 2, 28] gained popularity for their ability to weigh different features within sequential data. Transformers <ref type="bibr" target="#b17">[41]</ref> were introduced as a fully attention-based model, primarily for machine translation and NLP in general. Following this, attentionbased models, specifically transformers have been applied to a wide variety of tasks beyond machine translation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr">25,</ref><ref type="bibr" target="#b22">46]</ref>, including: visual question answering <ref type="bibr">[27,</ref><ref type="bibr" target="#b14">38]</ref>, action recognition <ref type="bibr" target="#b3">[4,</ref><ref type="bibr">13]</ref>, and the like. Many researchers also leveraged a combination of attention and convolutions in neural networks for visual tasks <ref type="bibr" target="#b18">[42,</ref><ref type="bibr">18,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b27">51]</ref>. Ramachandran et al.</p><p>[33] introduced one of the first vision models that rely primarily on attention. Dosovitskiy et al. <ref type="bibr">[12]</ref> introduced the first stand-alone transformer based model for image classification (ViT). In the following subsections, we briefly revisit ViT and several other related works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Vision Transformer</head><p>Dosovitskiy et al. <ref type="bibr">[12]</ref> introduced ViT primarily to show that reliance on CNNs or their structure is unnecessary, as prior to it, most attention-based models for vision were used either with convolutions <ref type="bibr" target="#b18">[42,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b27">51,</ref><ref type="bibr" target="#b5">6]</ref>, or kept some of their properties <ref type="bibr">[33]</ref>. The motivation, beyond self-attention's many desirable properties for a network, specifically its ability to make long range connections, was scalability. It was shown that ViT can successfully keep scaling, while CNNs start saturating in performance as the number of training samples grew. Through this, they concluded that large-scale training triumphs over the advantage of inductive bias that CNNs have, allowing their model to be competitive with CNN based architectures given sufficiently large amount of training data. ViT is composed of several parts: Image Tokenization, Positional Embedding, Classification Token, the Transformer Encoder, and a Classification Head. These subjects are discussed in more detail below.</p><p>Image Tokenization: A standard transformer takes as input a sequence of vectors, called tokens. For traditional NLP based transformers, word ordering provides a natural order to sequence the data, but this is not so obvious for images. To tokenize an image, ViT subdivides an image into non-overlapping square patches in raster-scan order. The sequence of patches, x p ? R H?(P 2 C) with patch size P , are flattened into 1D vectors and transformed into latent vectors of dimension d. This is equivalent to a convolutional layer with d filters, and P ? P kernel size and stride. This simple patching and embedding method has a few limitations, in particular: loss of information along the boundary regions.</p><p>Positional Embedding: Positional embedding adds spatial information into the sequence. Since the model does not actually know anything about the spatial relationship between tokens, adding extra information to reflect that can be useful. Typically, this is either a learned embedding or tokens are given weights from two sine waves with high frequencies, which is sufficient for the model to learn that there exists a positional relationship between these tokens.</p><p>Transformer Encoder: A transformer encoder consists of a series of stacked encoding layers. Each encoder layer is comprised of two sub-layers: Multi-Headed Self-Attention (MHSA) and a Multi-Layer Perceptron (MLP) head. Each sub-layer is preceded by a layer normalization (LN), and followed by a residual connection to the next sub-layer.</p><p>Classification: Vision transformers typically add an extra learnable [class] token to the sequence of the embedded patches, representing the class parameter of an entire image and its state after transformer encoder can be used for classification.</p><p>[class] token contains latent information, and through self-attention accumulates more information about the sequence, which is later used for classification. ViT [12] also explored averaging output tokens instead, but found no significant difference in performance. They did however find that the learning rates have to be adjusted between the two variants: [class] token vs. average pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Data-Efficient Transformers</head><p>In an effort to reduce dependence on data, Touvron et al. <ref type="bibr" target="#b16">[40]</ref> proposed Data-Efficient Image Transform-ers (DeiT). Using more advanced training techniques, and a novel knowledge transfer method, DeiT improves the classification performance of ViT on ImageNet-1k without large-scale pre-training on datasets such as JFT-300M <ref type="bibr" target="#b15">[39]</ref> or ImageNet-21k <ref type="bibr" target="#b9">[10]</ref>. By relying only on more augmentations <ref type="bibr" target="#b7">[8]</ref> and training techniques <ref type="bibr" target="#b26">[50,</ref><ref type="bibr" target="#b25">49]</ref>, it is shown that much smaller ViT variants that were unexplored by Dosovitskiy et al. can outperform the larger ones on ImageNet-1k without pre-training. Furthermore, DeiT variants were pushed even further through their novel knowledge transfer technique, specifically when using a convolutional model as the teacher. This work pushes forward accessibility of transformers in medium-sized datasets, and we aim to follow by extending the study to even smaller sets of data and smaller models. However, we base our work on the notion that if a small dataset happens to be sufficiently novel, pre-trained models will not help train on that domain and the model will not be appropriate for that dataset. While knowledge transfer is a strong technique, it requires a pre-trained model for any given dataset, adding to training time and complexity, with an additional forward pass, and as pointed out by Touvron et al. is usually only significant when there's a convolutional teacher available to transfer the inductive biases. As a result, it can be argued that if a network utilized just the bare minimum of convolutions, while keeping the pure transformer structure, it may need to rely less on large-scale training and transfer of inductive biases through knowledge transfer.</p><p>Yuan et al. <ref type="bibr" target="#b24">[48]</ref> proposed Tokens-to-token ViT (T2T-ViT), which adopts a window-and attention-based tokenization strategy. Their tokenizer extracts patches of the input feature map, similar to a convolution, applies three sets of kernel weights, and produces three sets of feature maps, which are fed to self-attention as query and keyvalue pairs. This process is equivalent to convolutions producing the QKV projections in a self-attention module. Finally, this strategy is repeated twice, followed by a final patching and embedding. The entire process replaces patch and embedding in ViT. This strategy, along with their small-strided patch extraction, allows their network to model local structures, including along the boundaries between patches. This attention-based patch interaction leads to finer-grained tokens which allow T2T-ViT to outperform previous Transformer-based models on ImageNet. T2T-ViT differs from our work, in that it focuses on medium-sized datasets like ImageNet, which are not only far too large for many research problems in science and medicine but also resource demanding. T2T tokenizer also has more parameters and complexity compared to a convolutional one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Convolution-inspired Transformers</head><p>Many works have been motivated to improve vision transformers and eliminate the need for large-scale pre-training. ConViT <ref type="bibr" target="#b8">[9]</ref> introduces a gated positional selfattention (GPSA) that allows for a "soft" convolutional inductive bias within their model. GPSA allows their network to have more flexibility with respect to positional information. Since GPSA is able to be initialized as a convolutional layer, this allows their network to sometimes have the properties of convolutions or alternatively having the properties of attention. Its gating parameter can be adjusted by the network, allowing it to become more expressive and adapt to the needs of the dataset. Convolutionenhanced image Transformers (Ceit) <ref type="bibr" target="#b23">[47]</ref> utilize convolutions throughout their model. They propose a convolutionbased Image-to-Token module for tokenization. They also re-design the encoder with layers of multi-headed selfattention and their novel Locally Enhanced Feedforward Layer, which processes the spatial information form the extracted token. This allows creates a network that is competitive with other works such as DeiT on ImageNet. Convolutional vision Transformer (CvT) <ref type="bibr" target="#b21">[45]</ref> introduces convolutional transformer encoder layers, which use convolutions instead of linear projections for the QKV in selfattention. They also introduce convolutions into their tokenization step, and report competitive results compared to other vision transformers on ImageNet-1k. All of these works report results when trained from scratch on ImageNet (or larger) datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Comparison</head><p>Our work differs from the aforementioned in several ways, in that it focuses on answering the following question: Can vision transformers be trained from scratch on small datasets? Focusing on a small datasets, we seek to create a model that can be trained, from scratch, on datasets that are orders of magnitude smaller than ImageNet. Having a model that is compact, small in size, and efficient allows greater accessibility, as training on ImageNet is still a difficult and data intensive task for many researchers. Thus our focus is on an accessible model, with few parameters, that can quickly and efficiently be trained on smaller platforms while still maintaining SOTA results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In order to provide empirical evidence that vision transformers are trainable from scratch when dealing with small sets of data, we propose three different models:  on CVT and utilizes a convolutional tokenizer, generating richer tokens and preserving local information. The convolutional tokenizer is better at encoding relationships between patches compared to the original ViT [12]. A detailed modular-level comparison of these models can be viewed in <ref type="figure" target="#fig_1">Figure 2</ref>. The components of our compact transformers are further discussed in the following subsections: Transformer-based Backbone, Small and Compact Models, SeqPool, and Convolutional Tokenizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Transformer-based Backbone</head><p>In terms of model design, we follow the original Vision Transformer <ref type="bibr">[12]</ref>, and original Transformer <ref type="bibr" target="#b17">[41]</ref>. As mentioned in Section 2.1, the encoder consists of transformer blocks, each including an MHSA layer and an MLP block. The encoder also applies Layer Normalization, GELU activation, and dropout. Positional embeddings can be learnable or sinusoidal, both of which are effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Small and Compact Models</head><p>We propose smaller and more compact vision transformers. The smallest ViT variant, ViT-Base, includes a 12 layer transformer encoder with 12 attention heads, 64 dimensions per head, and 2048-dimensional hidden layers in the MLP blocks. This, along with the classifier and 16x16 patch and embedder results in over 85M parameters. We propose vari-ants with as few as 2 layers, 2 heads, and 128-dimensional hidden layers. In Appendix A, we summarized the details of the variants we propose, the smallest of which can have as little as 0.22M parameters, while the largest (for small-scale learning) only have 3.8M parameters. We also adjust the tokenizer (patch size) according to the dataset we're training on, based on its image resolution. These variants, which are mostly similar in architecture to ViT, but different in size, are referred to as ViT-Lite. In our notation, we use the number of layers to specify size, as well as tokenization details: for instance, ViT-Lite-12/16 has 12 transformer encoder layers, and a 16?16 patch size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">SeqPool</head><p>In order to map the sequential outputs to a singular class index, ViT [12] and most other common transformer-based classifiers follow BERT <ref type="bibr" target="#b10">[11]</ref>, in forwarding a learnable class or query token through the network and later feeding it to the classifier. Other common practices include global average pooling (averaging over tokens), which have been shown to be preferable in some scenarios. We introduce Se-qPool, an attention-based method which pools over the output sequence of tokens. Our motivation is that the output sequence contains relevant information across different parts of the input image, therefore preserving this information can improve performance, and at no additional parameters compared to the learnable token. Additionally, this change slightly decreases computation, due one less token being forwarded. This operation consists of mapping the output sequence using the transformation T : R b?n?d ? R b?d . Given:</p><p>x</p><formula xml:id="formula_0">L = f(x 0 ) ? R b?n?d</formula><p>where x L is the output of an L layer transformer encoder f , b is batch size, n is sequence length, and d is the total embedding dimension. x L is fed to a linear layer g(x L ) ? R d?1 , and softmax activation is applied to the output:</p><formula xml:id="formula_1">x L = softmax g(x L ) T ? R b?1?n</formula><p>This generates an importance weighting for each input token, which is applied as follows:</p><formula xml:id="formula_2">z = x L x L = softmax g(x L ) T ? x L ? R b?1?d (1)</formula><p>By flattening, the output z ? R b?d is produced. This output can then be sent through a classifier. SeqPool allows our network to weigh the sequential embeddings of the latent space produced by the transformer encoder and correlate data across the input data. This can be thought of this as attending to the sequential data, where we are assigning importance weights across the sequence of data, only after they have been processed by the encoder. We tested several variations of this pooling method, including learnable and static methods, and found that the learnable pooling performs the best. Static methods, such as global average pooling have already been explored by ViT as well, as pointed out in section 2.1. We believe that the learnable weighting is more efficient because each embedded patch does not contain the same amount of entropy. This allows the model to apply weights to tokens with respect to the relevance of their information. Additionally, sequence pooling allows our model to better utilize information across spatially sparse data. We will further study the effects of this pooling in the ablation study (Sec 4.4). By replacing the conventional class token in ViT-Lite with SeqPool, Compact Vision Transformer is created. We use the same notations for this model: for instance, CVT-7/4 has 7 transformer encoder layers, and a 4?4 patch size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Convolutional Tokenizer</head><p>In order to introduce an inductive bias into the model, we replace patch and embedding in ViT-Lite and CVT, with a simple convolutional block. This block follows conventional design, which consists of a single convolution, ReLU activation, and a max pool. Given an image or feature map x ? R H?W ?C :</p><formula xml:id="formula_3">x 0 = MaxPool(ReLU(Conv2d(x)))<label>(2)</label></formula><p>where the Conv2d operation has d filters, same number as the embedding dimension of the transformer backbone. Additionally, the convolution and max pool operations can be overlapping, which could increase performance by injecting inductive biases. This allows our model to maintain locally spatial information. Additionally, by using this convolutional block, the models enjoy an added flexibility over models like ViT, by no longer being tied to the input resolution strictly divisible by the pre-set patch size. We seek to use convolutions to embed the image into a latent representation, because we believe that it will be more efficient and produce richer tokens for the transformer. These blocks can be adjusted in terms of downsampling ratio (kernel size, stride and padding), and are repeatable for even further downsampling. Since self-attention has a quadratic time and space complexity with respect to the number of tokens, and number of tokens is equal to the resolution of the input feature map, more downsampling results in fewer tokens which noticeably decreases computation (at the expense of performance). We found that on top of the added performance gains, this choice in tokenization also gives more flexibility toward removing the positional embedding in the model, as it manages to maintain a very good performance. This is further discussed in Appendix C.1. This convolutional tokenizer, along with SeqPool and the transformer encoder create Compact Convolutional Transformers. We use a similar notation for CCT variants, with the exception of also denoting the number of convolutional layers: for instance, CCT-7/3x2 has 7 transformer encoder layers, and a 2-layer convolutional tokenizer with 3?3 kernel size. CCT was trained longer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We conducted image classification experiments using our method on the following datasets: CIFAR-10, CIFAR-100 (MIT License) [21], MNIST, Fashion-MNIST, Oxford Flowers-102 [30], and ImageNet-1k <ref type="bibr" target="#b9">[10]</ref>. The first four datasets not only have a small number of training samples, but they are also small in resolution. Additionally, MNIST and Fashion-MNIST only contain a single channel, greatly reducing the information density. Flowers-102 has a relatively small number of samples, while having relatively higher resolution images and 102 classes. We divided these datasets into three categories: small-scale small resolution datasets (CIFAR-10/100, MNIST, and Fashion-MNIST), small-scale larger resolution (Flowers-102), and medium-scale (ImageNet-1k) datasets. We also include a study on NLP classification, presented in appendix G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Hyperparameters</head><p>We used the timm package <ref type="bibr" target="#b19">[43]</ref> to train the models (see Appendix E for details), except for cited works which are reported directly. For all experiments, we conducted a hyperparameter sweep for every different method and report the best results we were able to achieve. We will release all checkpoints corresponding to the reported numbers, and detailed training settings in the form of YAML files, with our  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Performance Comparison</head><p>Small-scale small resolution training: In order to demonstrate that vision transformers can be as effective as convolutional neural networks, even in settings with small sets of data, we compare our compact transformers to ResNets <ref type="bibr">[16]</ref>, which are still very useful CNNs for small to medium amounts of data, as well as to MobileNetV2 <ref type="bibr" target="#b11">[35]</ref>, which are very compact and small-sized CNNs. We also compare with results from <ref type="bibr">[17]</ref> where He et al. designed very deep (up to 1001 layers) CNNs specifically for CI-FAR. The results are presented in <ref type="table" target="#tab_1">Table 1</ref>, all of which are of models trained from scratch. We highlight the top performers. CCT-7/3x2 achieves on par results with the CNN models, while having significantly fewer parameters in some cases. We also compare our method to the original ViT [12] in order to express the effectiveness of smaller sized backbones, convolutional layers, as well our pooling technique. As these datasets were not trained from scratch in the original paper, we attempted to train the smallest variant: ViT-B/16 (ViT-12/16). We trained our best per-forming model, CCT-7/3x1, for longer than the 300 epochs to see how far it can go. Surprisingly, this model can get as high as 98% accuracy on CIFAR-10, and 82.87% accuracy on CIFAR-100 when trained for 5000 epochs, which is still fewer iterations an ImageNet pre-training would have. We present results from training on CIFAR-10/100 for 300, 1500 and 5000 epochs in <ref type="table">Table 2</ref>. We observed that sinusoidal positional embedding had a small but noticeable edge over learnable when training longer. This represents the only transformer based model in the top 25 results on PapersWithCode for CIFAR-10 where models have no extra data or pre-training <ref type="bibr" target="#b0">1</ref> . In addition to this, it is also one of the smallest models, being 15% the size of ResNet50 while maintaining similar performance. We present a plot of different models in <ref type="table" target="#tab_1">Table 1</ref> in <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>Medium-scale training: ImageNet training results are presented in <ref type="table" target="#tab_2">Table 3</ref>, and compared to ResNet50 [16], ViT, and DeiT. We report ResNet50 from the original paper [16], as well as from Wightman et al. <ref type="bibr" target="#b20">[44]</ref> which uses a similar training schedule to ours, and is therefore a fairer comparison. We also report a smaller ViT variant as proposed by Touvron et al. <ref type="bibr" target="#b16">[40]</ref>. We also report CCT's performance with knowledge distillation, in order to compare it to DeiT <ref type="bibr" target="#b16">[40]</ref>. Similar to DeiT, we trained our CCT-14/7x2 with a convolutional teacher and hard distillation loss. We used a RegNetY-16GF [32] (84M parameters), the same model DeiT selected as the teacher. It is noticeable that distillation does not have as significant of an effect on CCT it does on DeiT. This can be attributed to the already existing inductive biases from the convolutional tokenizer. DeiT authors argued that a convolutional teacher would be able to transfer inductive biases to the student model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Small-scale higher-resolution training:</head><p>We also present our results on Flowers-102, in which we successfully reach reasonable performance without any pretraining, and with the same model size as our ImageNet model. We also claim state of the art with 99.76% topaccuracy with ImageNet pretraining, which exceeds even far larger models pre-trained on JFT-300M. In addition to this we note that our model is at least a quarter the size of the next best model and almost 30? smaller than ViT-H/14. It can also be seen that CCT is 3 ? 27? more computationally efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>We extend our previous comparisons by doing an ablation study on our methods. In this study, we progressively transform the original ViT into ViT-Lite, CVT, and CCT, and compare their top-1 accuracy scores. In this particular study, we report the results on CIFAR-10 and CIFAR-100 in <ref type="table" target="#tab_8">Table 8</ref> in Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Transformers have commonly been perceived to be only applicable to larger-scale or medium-scale training. While their scalability is undeniable, we have shown within this paper that with proper configuration, a transformer can be successfully used in small data regimes as well, and outperform convolutional models of equivalent, and even larger, sizes. Our method is simple, flexible in size, and the smallest of our variants can be easily loaded on even a minimal GPU, or even a CPU. While part of research has been focused on large-scale models and datasets, we focus on smaller scales in which there is still much research to be done in data efficiency. We show that CCT can outperform other transformer based models on small datasets while also having a significant reduction in computational costs and memory constraints. This work demonstrates that transformers do not require vast computational resources and can allow for their applications in even the most modest of settings. This type of research is important to many scientific domains where data is far more limited that the conventional machine learning datasets which are used in general research. Continuing research in this direction will help open research up to more people and domains, extending machine learning research. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Variants</head><p>Within this appendix, we present architectural details of our variants in <ref type="table" target="#tab_5">Tables 5 and 6</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Computational Resources</head><p>For most experiments, we used a machine with an Intel(R) Core(TM) i9-9960X CPU @ 3.10GHz and 4 NVIDIA(R) RTX(TM) 2080Tis (11GB). The exception was the CPU test which was performed with an AMD Ryzen 9 5900X. Each ImageNet experiment was performed on a single machine either with 2 AMD EPYC(TM) 7662s and 8 NVIDIA(R) RTX(TM) A6000s (48GB), or 2 AMD EPYC(TM) 7713s and 8 NVIDIA(R) A100s (80GB).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional analyses</head><p>Within this appendix we present some additional performance analyses which were conducted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Positional Embedding</head><p>To determine the effects of our small &amp; compact design, sequence pooling, and convolutional tokenizer, we perform an ablation study focused on positional embedding, seen in <ref type="table" target="#tab_7">Table 7</ref>. In this study, we experiment with ViT (original sizing), ViT-Lite, CVT, and CCT, and investigate the effects of: a learnable positional embedding, a standard sinusoidal embedding, as well as no positional embedding. We finish the table with our best model, which also has augmented training and an optimal tuning (refer to Appendix E). In these experiments, we find that positional encoding matters in all variants, but to varying degrees. In particular, CCT relies less on positional encoding, and it can be safely removed much impact in accuracy. We also tested our CCT model without SeqPool, using the standard [class] token instead, and found that there was little to no effect from having a positional encoder or not, depending on model size. This suggests that convolutions are what helps provide spatially sparse information to the transformer, while also helping the model overcome some of the previous limitations, allowing for more efficient use of data. We do find that SeqPool helps slightly in this respect, but overall has a larger effect on increasing total accuracy. Lastly, we find that with proper data augmentation and tuning, the overall performance can be increased, and a low dependence on positional information can be maintained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Performance vs Dataset Size</head><p>In this experiment, we evaluated model performance on smaller subsets of CIFAR-10 to determine the relationship between performance and the number of samples within a dataset. Samples were removed uniformly from each class in CIFAR-10. For this experiment, we compared ViT-Lite and CCT. In <ref type="figure">Figure 4</ref>, we see the comparison of each model's accuracy vs the number of samples per class. We show how each model performs when given only 500, 1000, 2000, 3000, 4000, or 5000 (original) samples per class, meaning the total training set ranges from one tenth the size to full. It can be ovserved that CCT is more robust since it is able to obtain higher accuracy with a lower number of samples per class, especially in the low sample regime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Performance vs Dimensionality</head><p>In order to determine whether transformers are dependant on high dimensional data, as opposed to the number of samples (explored in Appendix C.2), we experimented with downsampled and upsampled versions of CIFAR-10. In <ref type="figure">Figure 5</ref>, we present the image dimensionality vs the performance of CCT vs. ViT-Lite. Both models were trained with images of sizes ranging from 16?16 to 64?64. It can be observed that CCT performs better on all image sizes, with a widening difference as the number of pixels increases. From this, it can be inferred that CCT is able to better utilize the information density of an image, while ViT does not see continued performance increases after the standard 32x32 size. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Dimensionality Experiments</head><p>Within this appendix, we extend the analysis from Appendix C.3, showing the difference in performance when using different types of positional embedding. <ref type="figure" target="#fig_4">Figure 6</ref> shows the difference of the accuracy when models are being trained from scratch. On the other hand, <ref type="figure" target="#fig_5">Figure 7</ref> shows the performance difference when models are only used in inference and pre-trained on the 32?32 sized images. We note that in <ref type="figure" target="#fig_5">Figure 7</ref>(a) that we do not provide inference for image sizes greater than the pre-trained image because the learnable positional embeddings do not allow us to extend in this direction. We draw the reader's attention to <ref type="figure" target="#fig_4">Figure 6</ref>(c) and <ref type="figure" target="#fig_5">Figure 7</ref>(c) to denote the large difference between the models when positional embedding is not used. We can see that in training CCT has very little difference when positional embeddings are used. Additionally, it should be noted that when performing inference our non-positional embedding CCT model has much higher generalizability than its ViT-Lite counterpart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Hyperparameter tuning</head><p>We used the timm package <ref type="bibr" target="#b19">[43]</ref> for our experiments (excluding NLP experiments). We also sued CutMix <ref type="bibr" target="#b25">[49]</ref>, Mixup <ref type="bibr" target="#b26">[50]</ref>, Randaugment <ref type="bibr" target="#b7">[8]</ref>, and Random Erasing <ref type="bibr" target="#b29">[53]</ref>. For our small-scale small-resolution experiments, we conducted a hyperparameter sweep for each model on each dataset separately. However, all experiments that trained models from scratch, were trained for 300 epochs, unless mentioned otherwise. ViT, CVT and CCT all used the weighted Adam optimizer (? 1 = 0.9 and ? 2 = 0.999). For CNNs, we observed that some models and datasets achieved their best results using AdamW, while most others performed best with SGD with momentum (0.9). We will release model checkpoints (PyTorch pickle files), as well as a full list of hyperparameters and training settings (in the form of YAML files readable by timm) along with our code for reproduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Ablation Study</head><p>Here in <ref type="table" target="#tab_8">Table 8</ref> we present the results from section 4.4. We provide a full list of ablated terms showing which factors give the largest boost in performances. "Model" column refers to variant (see <ref type="table" target="#tab_5">Table 5</ref> for details), "Conv" specifies the number of convolutional blocks (if an), and "Conv Size" specifies the kernel size. "Aug" denotes the use of AutoAugment <ref type="bibr" target="#b6">[7]</ref>. "Tuning" specifies a minor change in dropout, attention dropout, and/or stochastic depth (see <ref type="table">Table 9</ref>). The first row in <ref type="table" target="#tab_8">Table 8</ref> is essentially ViT. The next three rows are modified variants of ViT, which are not proposed in the original paper. These variants are more compact and use smaller patch sizes. It should be noted that the numbers reported in this table are best out of 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. NLP experiments</head><p>To demonstrate the general purpose nature of our model we extended it to the domain of Natural Language Processing, focusing on classification tasks. This shows that our model is a general purpose classifier and is not restricted to the domain of image classification. Within this section, we present our text classification results on 5 datasets: AG-News <ref type="bibr" target="#b28">[52]</ref>, TREC [24], SST <ref type="bibr" target="#b13">[37]</ref>, IMDb [29], DBpedia <ref type="bibr" target="#b0">[1]</ref>. The results are summarized in <ref type="table" target="#tab_1">Table 10</ref>. As can be seen, our model outperforms the vanilla transformer, demonstrating that the techniques we use here also help with NLP tasks. The network is slightly modified from the vision CCT. We use GloVe (Apache License 2.0) [31] to provide the word embedding for the model, and do not train these parameters. Note that model sizes do not reflect the number of parameters for GloVe, which is around 20M. We treat text as single channel data and the embedding dimension as size 300. Additionally, the convolution kernels have size 1. Finally, we include masking in the typical manner. By doing so, CCT can get upwards of a 3% improvement on some datasets while using less parameters than vanilla transformers. Sim-ilar to our vision results, we find that CCT performs well on small NLP datasets. We note that the CCT models that perform best all have less than 1M parameters, which are significantly smaller than there vanilla counterparts, while out performing them.  <ref type="table">Table 9</ref>: Difference between tuned and not tuned runs in <ref type="table" target="#tab_8">Table 8</ref>. We present the extended version of <ref type="table" target="#tab_1">Table 1</ref> here with additional models in <ref type="table" target="#tab_1">Table 11</ref>. <ref type="table" target="#tab_1">Table 10</ref>: Top-1 validation accuracy on text classification datasets. The number of parameters does not include the word embedding layer, because we use pretrained word-embeddings and freeze those layers while training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>AGNews TREC SST IMDb DBpedia # Params   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vanilla Transformer Encoders</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of CVT (right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Comparing ViT (top) to CVT (middle) and CCT (bottom). CVT can be thought of as an ablated version of CCT, only utilizing sequence pooling and not a convolutional tokenizer. CVT may be preferable with more limited compute, as the patch-based tokenization is faster.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>CIFAR-10 accuracy vs. model size (sizes &lt; 12M).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Reduced # samples / class (CIFAR-10) Image Size vs Accuracy (CIFAR-10)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>CIFAR-10 resolution vs top-1% validation accuracy (training from scratch). Images are square.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>CIFAR-10 resolution vs top-1% validation accuracy (inference only). Images are square.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Compact Vision Transformers (CVT), and Compact Convolutional Transformers (CCT). ViT-Lite is nearly identical to the original ViT in terms of architecture, but with a more suitable size and patch size for small-scale learning. CVT builds on this by using our Sequence Pooling method (SeqPool), that pools the entire sequence of tokens produced by the transformer encoder. SeqPool replaces the conventional [class] token. CCT builds</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Vision Transformer (ViT)</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Patch-Based Tokenization</cell><cell></cell><cell cols="3">Transformer with Class Tokenization</cell><cell></cell></row><row><cell>Inputs</cell><cell>Embed to Patches</cell><cell>Linear Projection</cell><cell>Reshape</cell><cell></cell><cell>Transformer Encoder</cell><cell>Slice</cell><cell>Linear Layer</cell><cell>Output</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Class Token</cell><cell cols="2">Positional Embedding</cell><cell></cell><cell>Class Token</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Compact Vision Transformer (CVT)</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Patch-Based Tokenization</cell><cell></cell><cell cols="3">Transformer with Sequence Pooling</cell><cell></cell></row><row><cell>Inputs</cell><cell>Embed to Patches</cell><cell>Linear Projection</cell><cell>Reshape</cell><cell></cell><cell>Transformer Encoder</cell><cell>Sequence Pooling</cell><cell>Linear Layer</cell><cell>Output</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Positional Embedding</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="5">Compact Convolutional Transformer (CCT)</cell><cell></cell></row><row><cell></cell><cell cols="2">Convolutional Tokenization</cell><cell></cell><cell cols="3">Transformer with Sequence Pooling</cell><cell></cell></row><row><cell cols="5">ConvLayer Lite, Inputs Pooling Reshape Optional Positional Embedding</cell><cell>Transformer Encoder</cell><cell>Sequence Pooling</cell><cell>Linear Layer</cell><cell>Output</cell><cell>ViT-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Top-1 validation accuracy comparisons. variants were trained longer (seeTable 2 )</figDesc><table><row><cell></cell><cell>Model</cell><cell></cell><cell>C-10</cell><cell>C-100</cell><cell cols="2">Fashion MNIST # Params</cell><cell>MACs</cell></row><row><cell></cell><cell cols="5">Convolutional Networks (Designed for ImageNet)</cell></row><row><cell></cell><cell>ResNet18</cell><cell></cell><cell cols="4">90.27% 66.46% 94.78% 99.80%</cell><cell>11.18 M</cell><cell>0.04 G</cell></row><row><cell></cell><cell>ResNet34</cell><cell></cell><cell cols="4">90.51% 66.84% 94.78% 99.77%</cell><cell>21.29 M</cell><cell>0.08 G</cell></row><row><cell></cell><cell cols="2">MobileNetV2/0.5</cell><cell cols="4">84.78% 56.32% 93.93% 99.70%</cell><cell>0.70 M</cell><cell>&lt; 0.01 G</cell></row><row><cell></cell><cell cols="2">MobileNetV2/2.0</cell><cell cols="4">91.02% 67.44% 95.26% 99.75%</cell><cell>8.72 M</cell><cell>0.02 G</cell></row><row><cell></cell><cell cols="5">Convolutional Networks (Designed for CIFAR)</cell></row><row><cell></cell><cell cols="2">ResNet56[16]</cell><cell cols="4">94.63% 74.81% 95.25% 99.27%</cell><cell>0.85 M</cell><cell>0.13 G</cell></row><row><cell></cell><cell cols="2">ResNet110[16]</cell><cell cols="4">95.08% 76.63% 95.32% 99.28%</cell><cell>1.73 M</cell><cell>0.26 G</cell></row><row><cell></cell><cell cols="3">ResNet1k-v2 [17] 95.38% Proxyless-G[5] 97.92%</cell><cell>? ?</cell><cell>? ?</cell><cell>? ?</cell><cell>10.33 M 5.7 M</cell><cell>1.55 G ?</cell></row><row><cell></cell><cell cols="2">Vision Transformers</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>ViT-12/16</cell><cell></cell><cell cols="4">83.04% 57.97% 93.61% 99.63%</cell><cell>85.63 M</cell><cell>0.43 G</cell></row><row><cell></cell><cell cols="2">ViT-Lite-7/16</cell><cell cols="4">78.45% 52.87% 93.24% 99.68%</cell><cell>3.89 M</cell><cell>0.02 G</cell></row><row><cell></cell><cell>ViT-Lite-7/8</cell><cell></cell><cell cols="4">89.10% 67.27% 94.49% 99.69%</cell><cell>3.74 M</cell><cell>0.06 G</cell></row><row><cell></cell><cell>ViT-Lite-7/4</cell><cell></cell><cell cols="4">93.57% 73.94% 95.16% 99.77%</cell><cell>3.72 M</cell><cell>0.26 G</cell></row><row><cell></cell><cell cols="3">Compact Vision Transformers</cell><cell></cell><cell></cell></row><row><cell></cell><cell>CVT-7/8</cell><cell></cell><cell cols="4">89.79% 70.11% 94.50% 99.70%</cell><cell>3.74 M</cell><cell>0.06 G</cell></row><row><cell></cell><cell>CVT-7/4</cell><cell></cell><cell cols="4">94.01% 76.49% 95.32% 99.76%</cell><cell>3.72 M</cell><cell>0.25 G</cell></row><row><cell></cell><cell cols="4">Compact Convolutional Transformers</cell><cell></cell></row><row><cell></cell><cell>CCT-2/3?2</cell><cell></cell><cell cols="4">89.75% 66.93% 94.08% 99.70%</cell><cell>0.28 M</cell><cell>0.04 G</cell></row><row><cell></cell><cell>CCT-7/3?2</cell><cell></cell><cell cols="4">95.04% 77.72% 95.16% 99.76%</cell><cell>3.85 M</cell><cell>0.29 G</cell></row><row><cell></cell><cell>CCT-7/3?1</cell><cell></cell><cell cols="4">96.53% 80.92% 95.56% 99.82%</cell><cell>3.76 M</cell><cell>1.19 G</cell></row><row><cell></cell><cell>CCT-7/3?1</cell><cell></cell><cell cols="2">98.00% 82.72%</cell><cell>?</cell><cell>?</cell><cell>3.76 M</cell><cell>1.19 G</cell></row><row><cell cols="5">Table 2: CCT-7/3?1 top-1 accuracy on CIFAR-10/100</cell><cell></cell></row><row><cell cols="2">when trained longer</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5"># Epochs Pos. Emb. CIFAR-10 CIFAR-100</cell><cell></cell></row><row><cell>300</cell><cell>Learnable</cell><cell>96.53%</cell><cell cols="2">80.92%</cell><cell></cell></row><row><cell>1500</cell><cell cols="2">Sinusoidal 97.48%</cell><cell cols="2">82.72%</cell><cell></cell></row><row><cell>5000</cell><cell cols="2">Sinusoidal 98.00%</cell><cell cols="2">82.87%</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>ImageNet Top-1 validation accuracy comparison (no extra data or pretraining). This shows that larger variants of CCT could also be applicable to medium-sized datasets</figDesc><table><row><cell>Model</cell><cell>Top-1</cell><cell cols="3"># Params MACs Training Epochs</cell></row><row><cell>ResNet50 [16]</cell><cell>77.15%</cell><cell>25.55 M</cell><cell>4.15 G</cell><cell>120</cell></row><row><cell cols="2">ResNet50 (2021) [44] 79.80%</cell><cell>25.55 M</cell><cell>4.15 G</cell><cell>300</cell></row><row><cell>ViT-S [19]</cell><cell>79.85%</cell><cell>22.05 M</cell><cell>4.61 G</cell><cell>300</cell></row><row><cell>CCT-14/7?2</cell><cell>80.67%</cell><cell>22.36 M</cell><cell>5.53 G</cell><cell>300</cell></row><row><cell>DeiT-S [19]</cell><cell>81.16%</cell><cell>22.44M</cell><cell>4.63 G</cell><cell>300</cell></row><row><cell cols="2">CCT-14/7?2 Distilled 81.34%</cell><cell>22.36 M</cell><cell>5.53 G</cell><cell>300</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Flowers-102 Top-1 validation accuracy comparison. CCT outperforms other competitive models, having significantly fewer parameters and GMACs. This demonstrates the compactness on small datasets even with large images</figDesc><table><row><cell>Model</cell><cell cols="2">Resolution Pretraining</cell><cell>Top-1</cell><cell># Params</cell><cell>MACs</cell></row><row><cell>CCT-14/7?2</cell><cell>224</cell><cell>-</cell><cell>97.19%</cell><cell>22.17 M</cell><cell>18.63 G</cell></row><row><cell>DeiT-B</cell><cell>384</cell><cell cols="2">ImageNet-1k 98.80%</cell><cell>86.25 M</cell><cell>55.68 G</cell></row><row><cell>ViT-L/16</cell><cell>384</cell><cell>JFT-300M</cell><cell cols="3">99.74% 304.71 M 191.30 G</cell></row><row><cell>ViT-H/14</cell><cell>384</cell><cell>JFT-300M</cell><cell cols="3">99.68% 661.00 M 504.00 G</cell></row><row><cell>CCT-14/7?2</cell><cell>384</cell><cell cols="2">ImageNet-1k 99.76%</cell><cell>22.17 M</cell><cell>18.63 G</cell></row></table><note>code. We also provide a report on hyperparamter settings in Appendix E. Unless stated otherwise, all tests were run for 300 epochs, and the learning rate is reduced per epoch based on cosine annealing [26]. All transformer based models (ViT-Lite, CVT, and CCT) were trained using the AdamW optimizer.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186,</figDesc><table><row><cell></cell><cell>[23] Yann LeCun, Bernhard Boser, John S Denker, Don-</cell></row><row><cell></cell><cell>nie Henderson, Richard E Howard, Wayne Hubbard,</cell></row><row><cell></cell><cell>and Lawrence D Jackel. Backpropagation applied to</cell></row><row><cell>2019. 3, 5</cell><cell>handwritten zip code recognition. Neural computa-</cell></row><row><cell>[12] Alexey Dosovitskiy, Lucas Beyer, Alexander</cell><cell>tion, 1(4):541-551, 1989. 1</cell></row><row><cell>Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,</cell><cell>[24] Xin Li and Dan Roth. Learning question classifiers.</cell></row><row><cell>Thomas Unterthiner, Mostafa Dehghani, Matthias</cell><cell>In COLING 2002: The 19th International Conference</cell></row><row><cell>Minderer, Georg Heigold, Sylvain Gelly, et al.</cell><cell>on Computational Linguistics, 2002. 13</cell></row><row><cell>An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 2, 3, 5, 8</cell><cell>[25] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A</cell></row><row><cell>[13] Rohit Girdhar, Joao Carreira, Carl Doersch, and An-</cell><cell>robustly optimized bert pretraining approach. arXiv</cell></row><row><cell>drew Zisserman. Video action transformer network.</cell><cell>preprint arXiv:1907.11692, 2019. 3</cell></row><row><cell>In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 244-253, 2019. 3</cell><cell>[26] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. ICLR, 2017. 8</cell></row><row><cell>[14] Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning. MIT press Cam-bridge, 2016. 1 [15] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines, 2014. 3 [16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.</cell><cell>[27] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. arXiv preprint arXiv:1908.02265, 2019. 3 [28] Minh-Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attention-based neural machine translation, 2015. 3</cell></row><row><cell>In Proceedings of the IEEE conference on computer</cell><cell>[29] Andrew Maas, Raymond E Daly, Peter T Pham, Dan</cell></row><row><cell>vision and pattern recognition, pages 770-778, 2016.</cell><cell>Huang, Andrew Y Ng, and Christopher Potts. Learn-</cell></row><row><cell>1, 7, 8, 17</cell><cell>ing word vectors for sentiment analysis. In Proceed-</cell></row><row><cell>[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European conference on computer vision, pages 630-</cell><cell>ings of the 49th annual meeting of the association for computational linguistics: Human language technolo-gies, pages 142-150, 2011. 13</cell></row><row><cell>645. Springer, 2016. 7, 8, 17</cell><cell>[30] Maria-Elena Nilsback and Andrew Zisserman. Au-</cell></row><row><cell>[18] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE con-ference on computer vision and pattern recognition, pages 7132-7141, 2018. 3</cell><cell>tomated flower classification over a large number of classes. In 2008 Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing, pages 722-729. IEEE, 2008. 7</cell></row><row><cell>[19] Xiao Shi Huang, Felipe Perez, Jimmy Ba, and Mak-</cell><cell>[31] Jeffrey Pennington, Richard Socher, and Christo-</cell></row><row><cell>sims Volkovs. Improving transformer optimization through better initialization. In International Confer-ence on Machine Learning, pages 4475-4483. PMLR,</cell><cell>pher D. Manning. Glove: Global vectors for word representation. In Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 1532-1543, 2014.</cell></row><row><cell>2020. 2, 8</cell><cell>14</cell></row><row><cell>[20] Alexander Ke, William Ellsworth, Oishi Banerjee,</cell><cell>[32] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Gir-</cell></row><row><cell>Andrew Y. Ng, and Pranav Rajpurkar. Chextrans-</cell><cell>shick, Kaiming He, and Piotr Doll?r. Designing net-</cell></row><row><cell>fer: performance and parameter efficiency of imagenet</cell><cell>work design spaces. In Proceedings of the IEEE/CVF</cell></row><row><cell>models for chest x-ray interpretation. Proceedings of</cell><cell>Conference on Computer Vision and Pattern Recogni-</cell></row><row><cell>the Conference on Health, Inference, and Learning,</cell><cell>tion, pages 10428-10436, 2020. 8</cell></row><row><cell>Apr. 2021. 2</cell><cell>[33] Prajit Ramachandran, Niki Parmar, Ashish Vaswani,</cell></row><row><cell>[21] Alex Krizhevsky, Geoffrey Hinton, et al. Learning</cell><cell>Irwan Bello, Anselm Levskaya, and Jonathon Shlens.</cell></row><row><cell>multiple layers of features from tiny images, 2009. 7</cell><cell>Stand-alone self-attention in vision models. arXiv</cell></row><row><cell>[22] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-</cell><cell>preprint arXiv:1906.05909, 2019. 3</cell></row><row><cell>ton. Imagenet classification with deep convolutional</cell><cell>[34] Daniel L Ruderman and William Bialek. Statistics of</cell></row><row><cell>neural networks. Advances in neural information pro-</cell><cell>natural images: Scaling in the woods. Physical review</cell></row><row><cell>cessing systems, 25:1097-1105, 2012. 1</cell><cell>letters, 73(6):814, 1994. 1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Transformer backbones in each variant.</figDesc><table><row><cell>Model</cell><cell cols="4"># Layers # Heads Ratio Dim</cell></row><row><cell>ViT-Lite-6</cell><cell>6</cell><cell>4</cell><cell>2</cell><cell>256</cell></row><row><cell>ViT-Lite-7</cell><cell>7</cell><cell>4</cell><cell>2</cell><cell>256</cell></row><row><cell>CVT-6</cell><cell>6</cell><cell>4</cell><cell>2</cell><cell>256</cell></row><row><cell>CVT-7</cell><cell>7</cell><cell>4</cell><cell>2</cell><cell>256</cell></row><row><cell>CCT-2</cell><cell>2</cell><cell>2</cell><cell>1</cell><cell>128</cell></row><row><cell>CCT-4</cell><cell>4</cell><cell>2</cell><cell>1</cell><cell>128</cell></row><row><cell>CCT-6</cell><cell>6</cell><cell>4</cell><cell>2</cell><cell>256</cell></row><row><cell>CCT-7</cell><cell>7</cell><cell>4</cell><cell>2</cell><cell>256</cell></row><row><cell>CCT-14</cell><cell>14</cell><cell>6</cell><cell>3</cell><cell>384</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Tokenizers in each variant.</figDesc><table><row><cell>Model</cell><cell cols="4"># Layers # Convs Kernel Stride</cell></row><row><cell>ViT-Lite-7/8</cell><cell>7</cell><cell>1</cell><cell>8?8</cell><cell>8?8</cell></row><row><cell>ViT-Lite-7/4</cell><cell>7</cell><cell>1</cell><cell>4?4</cell><cell>4?4</cell></row><row><cell>CVT-7/8</cell><cell>7</cell><cell>1</cell><cell>8?8</cell><cell>8?8</cell></row><row><cell>CVT-7/4</cell><cell>7</cell><cell>1</cell><cell>4?4</cell><cell>4?4</cell></row><row><cell>CCT-2/3x2</cell><cell>2</cell><cell>2</cell><cell>3?3</cell><cell>1?1</cell></row><row><cell>CCT-7/3x1</cell><cell>7</cell><cell>1</cell><cell>3?3</cell><cell>1?1</cell></row><row><cell>CCT-7/7x2</cell><cell>7</cell><cell>2</cell><cell>7?7</cell><cell>2?2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Top-1 validation accuracy comparison when changing the positional embedding method. Augmentations and training techniques such as Mixup and CutMix were turned off for these experiments to highlight differences better. The numbers reported are best out of 4 runs with random initializations. ? denotes model trained with extra augmentation and hyperparameter tuning.</figDesc><table><row><cell>Model</cell><cell>PE</cell><cell>CIFAR-10</cell><cell>CIFAR-100</cell></row><row><cell cols="4">Conventional Vision Transformers are more dependent on Positional Embedding</cell></row><row><cell></cell><cell>Learnable</cell><cell>69.82% (+3.11%)</cell><cell>40.57% (+1.01%)</cell></row><row><cell>ViT-12/16</cell><cell>Sinusoidal</cell><cell>69.03% (+2.32%)</cell><cell>39.48% (?0.08%)</cell></row><row><cell></cell><cell>None</cell><cell>66.71% (baseline)</cell><cell>39.56% (baseline)</cell></row><row><cell></cell><cell>Learnable</cell><cell>83.38% (+7.25%)</cell><cell>55.69% (+7.15%)</cell></row><row><cell>ViT-Lite-7/8</cell><cell>Sinusoidal</cell><cell>80.86% (+4.73%)</cell><cell>53.50% (+4.96%)</cell></row><row><cell></cell><cell>None</cell><cell>76.13% (baseline)</cell><cell>48.54% (baseline)</cell></row><row><cell></cell><cell>Learnable</cell><cell>84.24% (+6.52%)</cell><cell>55.49% (+7.23%)</cell></row><row><cell>CVT-7/8</cell><cell>Sinusoidal</cell><cell>80.84% (+3.12%)</cell><cell>50.82% (+2.56%)</cell></row><row><cell></cell><cell>None</cell><cell>77.72% (baseline)</cell><cell>48.26% (baseline)</cell></row><row><cell cols="4">Compact Convolutional Transformers are less dependent on Positional Embedding</cell></row><row><cell></cell><cell>Learnable</cell><cell>82.03% (+0.21%)</cell><cell>63.01% (+3.24%)</cell></row><row><cell>CCT-7/7</cell><cell>Sinusoidal</cell><cell>81.15% (?0.67%)</cell><cell>60.40% (+0.63%)</cell></row><row><cell></cell><cell>None</cell><cell>81.82% (baseline)</cell><cell>59.77% (baseline)</cell></row><row><cell></cell><cell>Learnable</cell><cell>90.69% (+1.67%)</cell><cell>65.88% (+2.82%)</cell></row><row><cell>CCT-7/3?2</cell><cell>Sinusoidal</cell><cell>89.93% (+0.91%)</cell><cell>64.12% (+1.06%)</cell></row><row><cell></cell><cell>None</cell><cell>89.02% (baseline)</cell><cell>63.06% (baseline)</cell></row><row><cell></cell><cell>Learnable</cell><cell>95.04% (+0.64%)</cell><cell>77.72% (+0.20%)</cell></row><row><cell>CCT-7/3?2  ?</cell><cell>Sinusoidal</cell><cell>94.80% (+0.40%)</cell><cell>77.82% (+0.30%)</cell></row><row><cell></cell><cell>None</cell><cell>94.40% (baseline)</cell><cell>77.52% (baseline)</cell></row><row><cell></cell><cell>Learnable</cell><cell>96.53% (+0.29%)</cell><cell>80.92% (+0.65%)</cell></row><row><cell>CCT-7/3?1  ?</cell><cell>Sinusoidal</cell><cell>96.27% (+0.03%)</cell><cell>80.12% (?0.15%)</cell></row><row><cell></cell><cell>None</cell><cell>96.24% (baseline)</cell><cell>80.27% (baseline)</cell></row><row><cell></cell><cell>Learnable</cell><cell>82.41% (+0.12%)</cell><cell>62.61% (+3.31%)</cell></row><row><cell>CCT-7/7?1-noSeqPool</cell><cell>Sinusoidal</cell><cell>81.94% (?0.35%)</cell><cell>61.04% (+1.74%)</cell></row><row><cell></cell><cell>None</cell><cell>82.29% (baseline)</cell><cell>59.30% (baseline)</cell></row><row><cell></cell><cell>Learnable</cell><cell>90.41% (+1.49%)</cell><cell>66.57% (+1.40%)</cell></row><row><cell>CCT-7/3?2-noSeqPool</cell><cell>Sinusoidal</cell><cell>89.84% (+0.92%)</cell><cell>64.71% (?0.46%)</cell></row><row><cell></cell><cell>None</cell><cell>88.92% (baseline)</cell><cell>65.17% (baseline)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>CIFAR Top-1 validation accuracy when transforming ViT into CCT step by step. We disabled advanced training techniques and augmentations for these runs.</figDesc><table><row><cell>Model</cell><cell cols="3">CLS # Conv Conv Size Aug Tuning</cell><cell>C-10</cell><cell>C-100</cell><cell cols="2"># Params MACs</cell></row><row><cell>ViT-12/16</cell><cell>CT</cell><cell></cell><cell></cell><cell cols="4">69.82% 40.57% 85.63 M 0.43 G</cell></row><row><cell cols="2">ViT-Lite-7/16 CT</cell><cell></cell><cell></cell><cell cols="2">71.78% 41.59%</cell><cell>3.89 M</cell><cell>0.02 G</cell></row><row><cell>ViT-Lite-7/8</cell><cell>CT</cell><cell></cell><cell></cell><cell cols="2">83.38% 55.69%</cell><cell>3.74 M</cell><cell>0.06 G</cell></row><row><cell>ViT-Lite-7/4</cell><cell>CT</cell><cell></cell><cell></cell><cell cols="2">83.59% 58.43%</cell><cell>3.72 M</cell><cell>0.26 G</cell></row><row><cell>CVT-7/16</cell><cell>SP</cell><cell></cell><cell></cell><cell cols="2">72.26% 42.37%</cell><cell>3.89 M</cell><cell>0.02 G</cell></row><row><cell>CVT-7/8</cell><cell>SP</cell><cell></cell><cell></cell><cell cols="2">84.24% 55.49%</cell><cell>3.74 M</cell><cell>0.06 G</cell></row><row><cell>CVT-7/8</cell><cell>SP</cell><cell></cell><cell></cell><cell cols="2">87.15% 63.14%</cell><cell>3.74 M</cell><cell>0.06 G</cell></row><row><cell>CVT-7/4</cell><cell>SP</cell><cell></cell><cell></cell><cell cols="2">88.06% 62.06%</cell><cell>3.72 M</cell><cell>0.25 G</cell></row><row><cell>CVT-7/4</cell><cell>SP</cell><cell></cell><cell></cell><cell cols="2">91.72% 69.59%</cell><cell>3.72 M</cell><cell>0.25 G</cell></row><row><cell>CVT-7/4</cell><cell>SP</cell><cell></cell><cell></cell><cell cols="2">92.43% 73.01%</cell><cell>3.72 M</cell><cell>0.25 G</cell></row><row><cell>CVT-7/2</cell><cell>SP</cell><cell></cell><cell></cell><cell cols="2">84.80% 57.98%</cell><cell>3.76 M</cell><cell>1.18 G</cell></row><row><cell>CCT-7/7?1 CCT-7/7?1 CCT-7/7?1</cell><cell>SP SP SP</cell><cell>1 1 1</cell><cell>7 ? 7 7 ? 7 7 ? 7</cell><cell cols="2">87.81% 62.83% 91.85% 69.43% 92.29% 72.46%</cell><cell>3.74 M 3.74 M 3.74 M</cell><cell>0.26 G 0.26 G 0.26 G</cell></row><row><cell>CCT-7/3?2</cell><cell>SP</cell><cell>2</cell><cell>3 ? 3</cell><cell cols="2">93.65% 74.77%</cell><cell>3.85 M</cell><cell>0.29 G</cell></row><row><cell>CCT-7/3?1</cell><cell>SP</cell><cell>1</cell><cell>3 ? 3</cell><cell cols="2">94.47% 75.59%</cell><cell>3.76 M</cell><cell>1.19 G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 :</head><label>11</label><figDesc>Top-1 comparisons. were trained longer (seeTab 2).</figDesc><table><row><cell>Model</cell><cell>C-10</cell><cell>C-100</cell><cell cols="3">Fashion MNIST # Params</cell><cell>MACs</cell></row><row><cell cols="4">Convolutional Networks (Designed for ImageNet)</cell><cell></cell><cell></cell></row><row><cell>ResNet18</cell><cell cols="4">90.27% 66.46% 94.78% 99.80%</cell><cell>11.18 M</cell><cell>0.04 G</cell></row><row><cell>ResNet34</cell><cell cols="4">90.51% 66.84% 94.78% 99.77%</cell><cell>21.29 M</cell><cell>0.08 G</cell></row><row><cell>ResNet50</cell><cell cols="4">91.63% 68.27% 94.99% 99.79%</cell><cell>23.53 M</cell><cell>0.08 G</cell></row><row><cell>MobileNetV2/0.5</cell><cell cols="4">84.78% 56.32% 93.93% 99.70%</cell><cell>0.70 M</cell><cell>&lt; 0.01 G</cell></row><row><cell>MobileNetV2/1.0</cell><cell cols="4">89.07% 63.69% 94.85% 99.75%</cell><cell>2.24 M</cell><cell>0.01 G</cell></row><row><cell cols="5">MobileNetV2/1.25 90.60% 65.24% 95.05% 99.77%</cell><cell>3.47 M</cell><cell>0.01 G</cell></row><row><cell>MobileNetV2/2.0</cell><cell cols="4">91.02% 67.44% 95.26% 99.75%</cell><cell>8.72 M</cell><cell>0.02 G</cell></row><row><cell cols="4">Convolutional Networks (Designed for CIFAR)</cell><cell></cell><cell></cell></row><row><cell>ResNet56[16]</cell><cell cols="4">94.63% 74.81% 95.25% 99.27%</cell><cell>0.85 M</cell><cell>0.13 G</cell></row><row><cell>ResNet110[16]</cell><cell cols="4">95.08% 76.63% 95.32% 99.28%</cell><cell>1.73 M</cell><cell>0.26 G</cell></row><row><cell cols="3">ResNet164-v1[17] 94.07% 74.84% ResNet164-v2[17] 94.54% 75.67% ResNet1k-v1[17] 92.39% 72.18% ResNet1k-v2[17] 95.08% 77.29% ResNet1k-v2 [17] 95.38% ? Proxyless-G[5] 97.92% ?</cell><cell>? ? ? ? ? ?</cell><cell>? ? ? ? ? ?</cell><cell>1.70 M 1.70 M 10.33 M 10.33 M 10.33 M 5.7 M</cell><cell>0.26 G 0.26 G 1.55 G 1.55 G 1.55 G ?</cell></row><row><cell>Vision Transformers</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ViT-12/16</cell><cell cols="4">83.04% 57.97% 93.61% 99.63%</cell><cell>85.63 M</cell><cell>0.43 G</cell></row><row><cell>ViT-Lite-7/16</cell><cell cols="4">78.45% 52.87% 93.24% 99.68%</cell><cell>3.89 M</cell><cell>0.02 G</cell></row><row><cell>ViT-Lite-6/16</cell><cell cols="4">78.12% 52.68% 93.09% 99.66%</cell><cell>3.36 M</cell><cell>0.02 G</cell></row><row><cell>ViT-Lite-7/8</cell><cell cols="4">89.10% 67.27% 94.49% 99.69%</cell><cell>3.74 M</cell><cell>0.06 G</cell></row><row><cell>ViT-Lite-6/8</cell><cell cols="4">88.29% 66.40% 94.36% 99.73%</cell><cell>3.22 M</cell><cell>0.06 G</cell></row><row><cell>ViT-Lite-7/4</cell><cell cols="4">93.57% 73.94% 95.16% 99.77%</cell><cell>3.72 M</cell><cell>0.26 G</cell></row><row><cell>ViT-Lite-6/4</cell><cell cols="4">93.08% 73.33% 95.14% 99.74%</cell><cell>3.19 M</cell><cell>0.22 G</cell></row><row><cell cols="2">Compact Vision Transformers</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CVT-7/8</cell><cell cols="4">89.79% 70.11% 94.50% 99.70%</cell><cell>3.74 M</cell><cell>0.06 G</cell></row><row><cell>CVT-6/8</cell><cell cols="4">89.50% 68.80% 94.53% 99.74%</cell><cell>3.21 M</cell><cell>0.05 G</cell></row><row><cell>CVT-7/4</cell><cell cols="4">94.01% 76.49% 95.32% 99.76%</cell><cell>3.72 M</cell><cell>0.25 G</cell></row><row><cell>CVT-6/4</cell><cell cols="4">93.60% 74.23% 95.00% 99.75%</cell><cell>3.19 M</cell><cell>0.22 G</cell></row><row><cell cols="3">Compact Convolutional Transformers</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CCT-2/3?2</cell><cell cols="4">89.75% 66.93% 94.08% 99.70%</cell><cell>0.28 M</cell><cell>0.04 G</cell></row><row><cell>CCT-4/3?2</cell><cell cols="4">91.97% 71.51% 94.74% 99.73%</cell><cell>0.48 M</cell><cell>0.05 G</cell></row><row><cell>CCT-6/3?2</cell><cell cols="4">94.43% 77.14% 95.34% 99.75%</cell><cell>3.33 M</cell><cell>0.25 G</cell></row><row><cell>CCT-7/3?2</cell><cell cols="4">95.04% 77.72% 95.16% 99.76%</cell><cell>3.85 M</cell><cell>0.29 G</cell></row><row><cell>CCT-6/3?1</cell><cell cols="4">95.70% 79.40% 95.41% 99.79%</cell><cell>3.23 M</cell><cell>1.02 G</cell></row><row><cell>CCT-7/3?1</cell><cell cols="4">96.53% 80.92% 95.56% 99.82%</cell><cell>3.76 M</cell><cell>1.19 G</cell></row><row><cell>CCT-7/3?1</cell><cell cols="2">98.00% 82.72%</cell><cell>?</cell><cell>?</cell><cell>3.76 M</cell><cell>1.19 G</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">SHI Lab @ University of Oregon, 2 University of Illinois at Urbana-Champaign,<ref type="bibr" target="#b2">3</ref> Picsart AI Research (PAIR) Abstract</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://paperswithcode.com/sota/image-classification-on-cifar-10</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dbpedia: A nucleus for a web of open data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?ren</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgi</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Ives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The semantic web</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="722" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Attention augmented convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3286" to="3295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Is space-time attention all you need for video understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05095</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="113" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>St?phane D&amp;apos;ascoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Leavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulio</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Biroli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sagun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10697,2021.4</idno>
		<title level="m">Convit: Improving vision transformers with soft convolutional inductive biases</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North</title>
		<meeting>the 2019 Conference of the North</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mo-bilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 conference on empirical methods in natural language processing</title>
		<meeting>the 2013 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08530</idno>
		<title level="m">Vl-bert: Pre-training of generic visual-linguistic representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<title level="m">Alexandre Sablayrolles, and Herv? J?gou. Training data-efficient image transformers &amp; distillation through attention</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Pytorch image models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Resnet strikes back: An improved training procedure in timm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.00476</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15808,2021.4</idno>
		<title level="m">Cvt: Introducing convolutions to vision transformers</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xlnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<title level="m">Generalized autoregressive pretraining for language understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Incorporating convolution designs into visual transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaopeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aojun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.11816,2021.4</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<idno>PMLR, 2019. 3</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<biblScope unit="page" from="7354" to="7363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.01626</idno>
		<title level="m">Character-level convolutional networks for text classification</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13001" to="13008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A comprehensive survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzhen</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbo</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="76" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
