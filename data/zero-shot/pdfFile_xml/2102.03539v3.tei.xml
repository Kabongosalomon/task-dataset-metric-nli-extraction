<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 Sill-Net: Feature Augmentation with Separated Illumination Representation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Haipeng</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Zhong</forename><surname>Cao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziang</forename><surname>Yan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 Sill-Net: Feature Augmentation with Separated Illumination Representation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Feature augmentation</term>
					<term>illumination representation</term>
					<term>one/few-shot learning</term>
					<term>feature disentanglement</term>
					<term>image reconstruction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For visual object recognition tasks, the illumination variations can cause distinct changes in object appearance and thus confuse the deep neural network-based recognition models. Especially for some rare illumination conditions, collecting sufficient training samples could be time-consuming and expensive. To solve this problem, in this paper we propose a novel neural network architecture called Separating-Illumination Network (Sill-Net). Sill-Net learns to separate illumination features from images, and then we augment training samples with these separated illumination features in the feature space. The model is further trained on the augmented samples to be robust to illumination variations. We provide a fresh perspective to focus on removing the semantic part of images and storing the illumination to the repository for augmentations instead of augmenting the semantic part of features. Extensive experimental results demonstrate that our approach outperforms current state-of-the-art methods on common object classification benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>A LTHOUGH deep learning models have achieved remarkable successes in various computer vision tasks <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, vast amounts of annotated training data are usually required for superior performance in most scenarios. For object classification tasks, the requirement for a large training set could be partially explained by the fact that many latent variables (e.g., positions/postures of the objects, the brightness/contrast of the image, and the illumination conditions) can cause significant changes in the appearance of objects. Although collecting a large training set to cover all possible values of these latent variables could improve the recognition performance, for rare latent values such as extreme illumination conditions it could be prohibitively time-consuming and expensive to collect enough training images <ref type="bibr" target="#b4">[5]</ref>.</p><p>In this paper, we restrict our attention to illumination conditions. For many real-world computer vision applications (e.g., autonomous driving and video surveillance) it is essential to recognize the objects under extreme illumination conditions such as backlighting, overexposure, and other complicated cast shadows. Thus, we reckon it is desirable to improve recognition models' generalization ability under different illumination conditions in order to deploy robust models in real-world applications.</p><p>We propose a novel neural network architecture named Separating-Illumination Network (Sill-Net) to deal with such problems. The key idea of our approach is to separate the illumination features from the semantic features in images, and then augment the separated illumination features onto other training samples (hereinafter we name these samples as "support samples") ? H. <ref type="bibr">Zhang</ref>  to construct a more extensive feature set for subsequent training (see <ref type="figure" target="#fig_0">Figure 1</ref>). Specifically, our approach consists of three steps. In the first step, we separate the illumination and semantic features for all images in the existing dataset via a disentanglement method, and use the separated illumination features to build an illumination repository. Then, we transplant the illumination repository to the support samples to construct an augmented training set and use it to train a recognition model. Finally, test images are fed into the trained model for classification. Our proposed approach could improve the robustness to illumination conditions since the support samples used for training are blended with many different illumination features. Thus after training, the obtained model would naturally generalize better under various illumination conditions. To separate better illumination features, we train the model on datasets with simple symbolic objects and diverse illumination in the separation step. The separated illumination features can be used for augmentation on extensive datasets including symbolic objects and general image datasets. Our method is evaluated in both traditional and one/few-shot classification tasks.</p><p>Our contributions are summarized as follows:  <ref type="figure">Fig. 2</ref>. Illustration of the architecture of Sill-Net. Sill-Net consists of three main modules: the separation module, the matching and reconstruction module, and the augmentation module. The semantic and illumination features are separated by the exchange mechanism in the first module. The semantic features are constrained to be informative by the matching and reconstruction module. The illumination features are stored into a repository. In the augmentation module, we use the illumination features in the repository to augment the support samples (e.g., template images) for training a generalizable model.</p><p>features construct an illumination feature repository, which could effortlessly enhance the illumination variety of training data and thus improving the robustness to illumination conditions of the trained deep model. The codes and repository are released 1 .</p><p>2) We provide a fresh perspective to focus on removing the semantic part of images and storing the remaining (illumination) to the repository for augmentations while recent works concentrate on how to augment the semantic part of features or learn the policies.</p><p>3) We evaluate Sill-Net on several object classification benchmarks, i.e., traffic datasets (GTSRB, TT100K, CTSD, etc.), logo datasets (Belgalogos, FlickrLogos32, and TopLogo-10) and generalized one/few-shot benchmarks (miniImageNet, CUB, CIFAR-FS). Sill-Net outperforms the state-of-the-art (SOTA) methods by a large margin in most cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PROPOSED METHOD</head><p>In this section, we introduce our Separating-Illumination Network (Sill-Net). Sill-Net first learns to separate the semantic and illumination features of training images. Then the illumination features are blended with the semantic feature of each support sample to construct an augmented feature set. Finally, we train again on the illumination-augmented feature set for classification.</p><p>The architecture of Sill-Net is illustrated in <ref type="figure">Figure 2</ref>. It mainly consists of the following modules: the separation module, the matching and reconstruction module, and the augmentation module. In detail, we implement Sill-Net in three phases (see Algorithm 1):</p><p>1) In the separation phase, the separation module is trained to separate the features into semantic parts and illumination parts for all training images. The matching and reconstruction module promotes the learning of better semantic feature representation. The learned illumination features are stored into an illumination repository. The details are illustrated in Section 2.1.</p><p>2) In the augmentation phase, the semantic feature of each support image is combined with all illumination features in the repository to build an augmented feature set to train the classifier. The augmentation module is illustrated in Section 2.2.</p><p>1. The codes are released at https://github.com/lanfenghuanyu/Sill-Net.</p><p>3) During inference, test images are input into the well-trained model to be predicted in an end-to-end manner (see Section 2.3).</p><p>This approach assumes that the illumination distribution learned from training data is able to characterize that of test data. Thus the illumination features can be used as feature augmentation for sufficient training.</p><p>We choose different support samples in different visual tasks. For instance, in traditional classification tasks, we use the training images themselves as support samples; in one-shot classification tasks, we construct the support set with template images (i.e., graphic symbols visually and abstractly representing semantic information) available from official websites or wikis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Separate semantic and illumination features</head><formula xml:id="formula_0">Let X = {(x i , y i , t i )} N i=1</formula><p>represent the labeled dataset of training classes with N images, where x i denotes the i-th training image, y i is the one-hot label, and t i denotes the corresponding template image (or any other image of the object without much deformation).</p><p>Separation module A feature extractor denoted by z = E(x) learns the separated features z from images x, where z is artifically split along channels: z ? [z sem , z illu ]. We expect z sem as the semantic feature to learn the consistent information of the same category during training, while z illu as the illumination feature to learn various illumination information.</p><p>We first specify what illumination represents in our paper. Illumination is one of the environmental impacts causing appearance changes but no label changes. It should be noted that illumination is always related to or influenced by objects both in the foreground and background. Here we call the features most relevant to general illumination impacts such as light and shadows, but not categoryspecific as illumination features. Technically, we divide the object feature into different channels, one half learning to determine the category label defined as the semantic feature, and the other half reflecting illumination variations but unrelated to the category label defined as the illumination feature. To ensure that the learned features contain minimal irrelevant information beyond labeled objects and illumination, we choose training datasets with various illumination conditions and few confounding objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross combination Classifier</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>Extractor Label Image Feature <ref type="figure">Fig. 3</ref>. Illustration of the exchange mechanism. The semantic and illumination features are exchanged between random paired images with labels y i and y j . Then we obtain cross combined features labeled the same as the images corresponding to the semantic features. These features are then classified as the specified labels.</p><p>To achieve the separation, we design our model according to the following three basic requirements: 1) Semantic features can predict labels while illumination features can not.</p><p>2) Semantic features are informative to reconstruct the corresponding template images.</p><p>3) Illumination features contain minimal semantic information. Exchange mechanism. To fulfill the first requirement that semantic features can predict labels while illumination features can not, we utilize a feature exchange mechanism enlightened by <ref type="bibr" target="#b5">[6]</ref> to separate the features. The semantic feature z sem(i) of one image x i is blended with the illumination feature z illu(j) of another image x j to form a new one through feature mixup <ref type="bibr" target="#b6">[7]</ref> (see <ref type="figure">Figure 3</ref>):</p><formula xml:id="formula_1">z = rz sem(i) + (1 ? r)z illu(j) ,<label>(1)</label></formula><p>where the proportion r ? (0, 1). Generally, we set r = 0.5.</p><p>As required, the blended feature z retains the same label y i as the semantic feature. It means that z should contain more information representing y i and less information representing y j . Hence through subsequent training, the semantic information of x i would be retained in the semantic feature z sem(i) while the semantic information of x j would be reduced in the illumination feature z illu(j) .</p><p>We implement the exchange process for random pairs of images, building a new exchanged feature set:</p><formula xml:id="formula_2">Z exc = rz sem(i) + (1 ? r)z illu(j) , y i i, j = 1, ? ? ? , N .</formula><p>(2) The mixed features are then input into a classifier P for label prediction. We denote the distribution of the predicted label y given the mixed feature z by P (y|z). Then we minimize the cross-entropy loss:</p><formula xml:id="formula_3">L exc = ? 1 N exc Nexc i=1 M c=1 y ic log P (y ic |z i ? Z exc ) ,<label>(3)</label></formula><p>where N exc = |Z exc | denotes the number of recombined features in the augmented feature set, M represents the class number of all images for training and test, and y ic is the c-th element of the one-hot label y i . After training by the exchange mechanism, the semantic information would be retained in semantic features but reduced in illumination features for all images.</p><p>Matching and reconstruction module. To fulfill the second requirement, we first construct a matching module (as shown in <ref type="figure">Figure 2</ref>) to make the semantic feature informative. Since we design the extractor without downsampling operations to maintain the spatial information of the object, the semantic feature of each training image should be similar to that of its corresponding template image. However, due to different camera distances and angles, real-world images are usually deformed in size and position compared to regular template images. Therefore, we adopt a spatial transformer T <ref type="bibr" target="#b7">[8]</ref> to learn the parameters of affine transformation on the deformed objects and then rectify their semantic features to the regular positions (aligned with the templates). We constrain the transformed semantic feature T z sem(i) | xi to be consistent with the template feature z sem(i) | ti by the mean square error (MSE):</p><formula xml:id="formula_4">L match = 1 N N i=1 T z sem(i) | xi ? z sem(i) | ti 2 . (4)</formula><p>Then we design a reconstructort = R(T (z sem )) (see <ref type="figure">Figure  2</ref>) to retrieve the template image t from the semantic feature z sem to ensure that it is informative enough. We constrain the reconstructed templatet i by binary cross-entropy (BCE) loss:</p><formula xml:id="formula_5">L recon = 1 N N i=1 j ?t ij logt ij ?(1 ? t ij ) log 1 ?t ij ,<label>(5)</label></formula><p>where t ij represents the j-th pixel of the i-th template image t i . Since the template images are composed of primary colors within the range of [0, 1], binary cross-entropy (BCE) loss is sufficiently efficient for the retrieval <ref type="bibr" target="#b8">[9]</ref>. So far, the semantic feature is constrained to be consistent with the template feature and informative enough to be reconstructed to its template image.</p><p>Constraints on illumination features. According to the third requirement, it is essential to impose additional constraints on illumination features to reduce the semantic information. However, it is difficult to find suitable restrictions since the generally used datasets have no illumination labels.</p><p>Enlightened by the disentanglement metric proposed in <ref type="bibr" target="#b9">[10]</ref>, we design a constraint on illumination features by negative Post Interventional Disagreement (PIDA). Given a subset X c = {(x ci , y c )} Nc i=1 including N c images of the same label y c , we write the loss as follows:</p><formula xml:id="formula_6">L illu = ?P IDA = ? M c=1 Nc i=1 D (E (z illu | xc,yc ) , z illu | xci,yc ) ,<label>(6)</label></formula><p>here, D is a proper distance function (e.g., 2 -norm), z illu | xci,yc is the illumination feature of image x ci with the same label y c , E is the expectation, and N c is the number of images in class c. According to Eq. (6), PIDA quantifies the distances between the illumination feature of each same-labeled image z illu | xci,yc and their expectation E (z illu | xc,yc ) when the illumination conditions are changed. In the subset X c , the semantic information of each image is similar while the illumination information is different. Suppose an undesirable situation that the illumination features capture much semantic information rather than illumination information. The expectation would strengthen the common semantic component and weaken the distinct illumination components, and thus PIDA would be small. It means that the smaller PIDA is, the more semantic information the illumination feature captures compared to illumination information. By maximizing PIDA (i.e., minimizing L illu ), we can effectively reduce the common semantic information remaining in the illumination features. Besides, the constraint also avoids a trivial solution (all zeros) for the illumination features, since otherwise PIDA would be zero.</p><p>In summary, the overall loss function in the separation phase can be written as:</p><formula xml:id="formula_7">L = L exc + L match + L recon + L illu .<label>(7)</label></formula><p>Through the above training, the model learns to separate the features into semantic and illumination features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Augment samples by the illumination repository</head><p>After the separation phase, the illumination features are separated from training images. These features are collected to construct an illumination repository, expressed as follows:</p><formula xml:id="formula_8">Z illu = z illu(i) N i=1 .<label>(8)</label></formula><p>In the augmentation phase, we then use the illumination features to augment the support samples by a multiple of the repository size N .</p><formula xml:id="formula_9">Consider X tst = {(x tst i , y tst i , t tst i )} N tst i=1</formula><p>with N tst images of label y tst , here we assume that the template images t tst i constitute the support set. We combine all illumination features in the repository with the semantic feature of each template z sup sem(i) = E(t tst i ) by feature mixup, building an augmented feature set as follows:</p><formula xml:id="formula_10">Zaug = rz sup sem(i) + (1 ? r)z illu(j) , y tst i i = 1, ? ? ? , N tst ,<label>(9)</label></formula><p>where z illu(j) ? Z illu and r = 0.5 by default. We train the model again on the feature set Z aug . Despite the limited support samples provided, the model can be trained on the augmented feature set blended with various illumination features, making it generalizable to test data. The classification loss of augmented training is expressed as follows:</p><formula xml:id="formula_11">L aug = ? 1 N aug N i=1 M c=1 y ic log P (y ic |z i ? Z aug ) ,<label>(10)</label></formula><p>where N aug = |Z aug | denotes the number of all recombined features in the augmented feature set. Now, the model has been trained to be generalizable for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Inference</head><p>The feature extractor and classifier have been fully trained after the first two phases. Given the i-th test image x tst i , the separated feature is extracted as z i = E(x tst i ). Then the classifier outputs the category label?, formulated as:</p><formula xml:id="formula_12">c i = arg max c P (y ic |z i ) .<label>(11)</label></formula><p>It should be noted that we use the mixed features instead of the semantic ones for inference, which is consistent with the training of the classifier during the augmentation phase. The inference is achieved in an end-to-end manner. Overall, the main pipeline of Sill-Net is shown in Algorithm 1. Due to the page limit, detailed architectures and parameter settings of the network are described in Appendix A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Manipulations on the illumination repository</head><p>We further manipulate the illumination repository to improve the performance and interpretability of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Increase the diversity and reduce the size</head><p>Two factors affect the quality of the illumination repository: the diversity of illumination characteristics and the size.</p><p>We previously assumed that our separated illumination repository is able to characterize the illumination distribution of test data. To better fulfill this assumption, we have to further increase the diversity of the basic repository. Assumed that all the illumination features are from a feature space, so we linearly combine these illumination features to generate new ones. For simplicity, we linearly interpolate random pairs of illumination features to form an expanded repository:</p><formula xml:id="formula_13">Z exp illu = {?z illu(i) + (1 ? ?)z illu(j) | z illu(i) , z illu(j) ? Z illu , ? ? [0, 1] ,<label>(12)</label></formula><p>where ? is randomly sampled from a uniform distribution. Theoretically, we can expand the repository to any size N exp . Furthermore, the size of the illumination repository directly affects the time cost of the application. When the illumination repository is very large, the calculation in the augmentation phase is time-consuming. To increase the training speed, we try to compress the illumination repository without decreasing much of the accuracy of image recognition. As observed, some training images are obtained under similar illumination conditions, so that their illumination features will be very similar. We can choose the representative illumination features to replace the similar ones, so that the illumination repository can be compressed. Clustering is a simple and useful method to find representative repository features avoiding much information loss caused by data selection. Now we have constructed an illumination repository with representative and diverse illumination features via selection and expansion. Our illumination repository is pre-stored and agnostic to the applied datasets of new tasks. Assume that the manipulated repository basically characterizes the illumination distribution, feature augmentation with these illumination features can facilitate not only symbolic objects recognition but also general few-shot classification. See Section 3.3.1 and 3.4 for experimental details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Illumination reconstruction and transplantation</head><p>So far, all our manipulations have been performed in feature space. To further interpret the separated illumination features, we reconstruct them back to visual images.</p><p>Previously, we have separated the semantic and illumination features of training images x. Then we train another reconstructor to retrieve real-world images from the separated features, expressed asx = R real (z sem , z illu ). We adopt the binary cross-entropy (BCE) as the reconstruction loss the same as before:</p><formula xml:id="formula_14">L real recon = 1 N N i=1 j ?x ij logx ij ? (1 ? x ij ) log (1 ?x ij ) ,<label>(13)</label></formula><p>where x ij represents the j-th pixel of the i-th training image x i .</p><p>After training, we can transplant certain illumination features to any support samples and generate new images with realistic illumination by the well-trained reconstructor:</p><formula xml:id="formula_15">x new = R real (z sup sem , z illu ),<label>(14)</label></formula><p>where z sup sem = E(t tst ) represent the extracted semantic features of support templates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets and experimental settings</head><p>Datasets. We first validate the effectiveness of Sill-Net on symbolic object datasets, i.e., four traffic sign datasets with various (especially extreme) illumination: German Traffic Sign Recognition Benchmark (GTSRB) <ref type="bibr" target="#b10">[11]</ref>, Tsinghua-Tencent 100K (TT100K) <ref type="bibr" target="#b11">[12]</ref>, Belgian Traffic Sign Classification (BTSC) <ref type="bibr" target="#b12">[13]</ref> and Chinese Traffic Sign Database (CTSD) 2 ; three logo datasets, BelgaLogos <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, FlirckrLogos-32 <ref type="bibr" target="#b15">[16]</ref> and TopLogo-10 <ref type="bibr" target="#b16">[17]</ref>, since these datasets 2. Publicly available on http://www.nlpr.ia.ac.cn/pal/trafficdata/index.html.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Sill-Net: illumination separation and augmentation</head><p>Training data:</p><formula xml:id="formula_16">X = {(x i , y i , t i )} N i=1</formula><p>Test data:</p><formula xml:id="formula_17">X tst = {(x tst i , y tst i , t tst i )} N tst i=1</formula><p>Models with learnable parameters: feature extractor E, classifier P (y|z).</p><formula xml:id="formula_18">Phase 1 -Separation for each training image pair (x i , x j ) do Extract features: z i = E(x i ), z j = E(x j ) Split features: z i ? [z sem(i) , z illu(i) ], z j ? [z sem(j) , z illu(j) ]</formula><p>Exchange mechanism for separation:</p><formula xml:id="formula_19">Z exc = rz sem(i) + (1 ? r)z illu(j) , y i L exc = ? 1 Nexc Nexc i=1 M c=1 y ic log P (y ic |z i ? Z exc )</formula><p>Matching and reconstruction to the templates:</p><formula xml:id="formula_20">L match = 1 N N i=1 T z sem(i) | xi ? z sem(i) | ti 2 L recon = 1 N N i=1 j ?t ij logt ij ? (1 ? t ij ) log 1 ?t ij Illumination constraint: L illu = ?P IDA = ? M c=1 Nc i=1 D (E (z illu | xc,yc ) , z illu | xci,yc ) Total loss: L = L exc + L match + L recon + L illu end for Construct illumination repository: Z illu = z illu(i) N i=1</formula><p>Phase 2 -Augmentation for each support image t tst i do Extract semantic features: z sup sem(i) = E(t tst i ) for each illumination feature z illu(j) do feature augmentation by mixup:</p><formula xml:id="formula_21">Z aug = rz sup sem(i) + (1 ? r)z illu(j) , y tst i</formula><p>Train on the augmented feature set:</p><formula xml:id="formula_22">L aug = ? 1 Naug N i=1 M c=1 y ic log P (y ic |z i ? Z aug ) end for end for Phase 3 -Inference for each test image x tst i do Extract features: z i = E(x tst i ) Predict labels:? i = arg max c P (y ic |z i )</formula><p>end for contain various illumination. <ref type="table" target="#tab_2">Table 1</ref> shows the size and number of classes of each dataset. We also evaluate the effectiveness of illumination feature augmentation on standardized one/few-shot image classification datasets: miniImageNet <ref type="bibr" target="#b17">[18]</ref>, CUB <ref type="bibr" target="#b18">[19]</ref> and CIFAR-FS <ref type="bibr" target="#b19">[20]</ref>. Due to the page limit, we introduce the details of all datasets in Appendix B.</p><p>Evaluation tasks. Generally, we evaluate our model by the following steps. 1) Utilize the training dataset (or subset) to separate out the illumination features. 2) The support samples are augmented with the illumination features to form an augmented feature set. 3) Train a classifier on the augmented feature set. 4) Prediction on the test dataset.</p><p>For symbolic object datasets, we validate our method on the following increasingly difficult classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Traditional classification</head><p>We implement traditional classification tasks on symbolic object datasets under the regular splits of training and test (or validation) sets. We separate the semantic and illumination features for all training images. The illumination features are randomly augmented to the other semantic features, because the training images are also used as support samples here. We train the classifier on the augmented feature sets and then infer on the test sets.</p><p>2) One-shot classification In this type of task, the training procedure requires no realworld images from test classes but one template image for each category. This task is similar to the one-shot classification.</p><p>For intra-dataset classification, we split the traffic sign datasets into training and test subsets with no class overlap. GTSRB is divided into 22 classes for training and another 21 classes for testing. For convenience, we denote this scenario by GTSRB?GTSRB, where the training set is on the left side of the arrow and the test set is on the right. Similarly, we split BTSC and CTSD for one-shot classifications, denoted by BTSC?BTSC and CTSD?CTSD.</p><p>For cross-dataset evaluation, we train on large datasets and test on small datasets. We use GTSRB for training and TT100K for testing, denoted by GTSRB?TT100K. And for logo classification, we use the largest BelgaLogos for training and the remaining two for testing respectively, denoted by Belga?Flickr32 and Belga?Toplogos.</p><p>3) Cross-domain one-shot classification To further validate the generalization ability of our method, we perform a cross-domain one-shot evaluation by another two experiments, where the model is trained on traffic sign datasets and tested on logo datasets. Specifically, we train the model on GTSRB and test on FlickrLogos-32 and Toplogo-10. We denote these two scenarios as GTSRB?Flickr32 and GTSRB?Toplogos. The setup is more challenging compared to the previous scenarios, since we train the model in the domain of traffic sign datasets while testing in an entirely different domain of logo datasets.</p><p>For natural image datasets without regular templates such as ImageNet, the model can not be directly trained since our method requires template images for the separation of features. However, the repository with diverse illumination features separated from GTSRB can facilitate the classification of natural images via the proposed illumination feature augmentation. To verify the versatility of the illumination repository, we perform illumination feature augmentation on standardized few-shot classification datasets: miniImageNet <ref type="bibr" target="#b17">[18]</ref>, CUB <ref type="bibr" target="#b18">[19]</ref>, and CIFAR-FS <ref type="bibr" target="#b19">[20]</ref>. The datasets are split into base classes, validation classes, and novel classes, following the settings in <ref type="bibr" target="#b20">[21]</ref>. During training on the base classes, we augment the input images with the illumination features through mixup. The mixup proportion r is set to 0.9. Technical details are described in Appendix A.2 and C.3 due to the page limit.</p><p>Here are two settings of few-shot classification:</p><p>1) Transductive setting We implement illumination feature augmentation during training the backbone of Wide residual networks (WRN) <ref type="bibr" target="#b21">[22]</ref> with the pre-trained model provided by <ref type="bibr" target="#b20">[21]</ref>. Once the backbone is trained, we extract the features of novel classes and then implement Power Transform and Maximum A Posteriori (PT+MAP) proposed by <ref type="bibr" target="#b20">[21]</ref>. The results are presented in Section 3.4.1</p><p>2) Inductive setting In the inductive setting, we evaluate the effectiveness of feature illumination augmentation on common baselines in few-shot classification including Baseline, Baseline++ <ref type="bibr" target="#b22">[23]</ref>, MAML <ref type="bibr" target="#b23">[24]</ref>, ProtoNet <ref type="bibr" target="#b24">[25]</ref> and MatchingNet <ref type="bibr" target="#b17">[18]</ref>. The results are presented in Section 3.4.2.</p><p>Template image processing. Previous studies <ref type="bibr" target="#b25">[26]</ref> have shown that basic image processing on template images (as support samples) helps the network's generalization. In our experiment, we diversify the template images themselves using the following methods: geometric transformations (including translation, rotation and scaling), image enhancement (including brightness, color, contrast and sharpness adjustment), and blur. The template images are diversified and thus allow the model to learn more generalizable features. We observe that basic processing on template images improves model performance. The classification results on symbolic object datasets are shown in <ref type="table" target="#tab_2">Table 1</ref>. We compare our approach Sill-Net with the SOTA method, i.e., CNN with spatial transformers (CNN3ST) <ref type="bibr" target="#b26">[27]</ref>. Our method outperforms CNN3ST on most traffic sign and logo datasets, especially the Chinese Traffic Sign Dataset (CTSD). The results also indicate that Sill-Net performs better when training samples are insufficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experiments on symbolic object datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Traditional classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">One-shot classification</head><p>We compare our method with Siamese networks <ref type="bibr" target="#b27">[28]</ref> (SiamNet), Quadruplet networks <ref type="bibr" target="#b28">[29]</ref> (QuadNet), Matching networks <ref type="bibr" target="#b17">[18]</ref> (MatchNet) and Variational Prototyping-Encoder <ref type="bibr" target="#b8">[9]</ref> (VPE) for one-shot classification, reported in <ref type="table" target="#tab_3">Table 2</ref> and 3. We quote or reproduce the results of the compared methods under their optimal settings, that is, VPE is implemented with augmentation and spatial transformer (VPE+aug+stn version), SiamNet and MatchNet is implemented with augmentation, while QuadNet is without augmentation. As shown in the tables, our method outperforms comparative methods in all scenarios.</p><p>In traffic sign classification, Sill-Net outperforms the second best method VPE by a large margin of 13.81% (from 83.79% to 97.60%) in GTSRB?GTSRB, 23.79% (from 71.80% to 95.59%) in GTSRB?TT100K, 21.45% (from 72.08% to 94.53%) in CTSD?CTSD, and 14.79% (from 78.46% to 93.25%) in BTSC?BTSC (see <ref type="table" target="#tab_3">Table 2</ref>). It indicates that training on the features augmented by illumination information does help the real-world classification, even though only one template image is provided. It is notable that in the cross-dataset scenario GTSRB?TT100K, Sill-Net achieves a comparable performance to the intra-dataset scenario GTSRB?GTSRB, while VPE performs much worse in the cross-dataset scenario. We surmise it is because VPE learns latent embeddings generalizable to test classes in the same domain (GTSRB), but the generalization might be discounted when the target domain is slightly shifted (from GTSRB to TT100K). It is observed that the illumination conditions in GTSRB are quite similar to that in TT100K, therefore Sill-Net shows better generalization performance by making full use of the illumination information in GTSRB. In logo classification, Sill-Net improves the performance by 11.68% (from 53.53% to 65.21%) and 26.68% (from 57.75% to 84.43%) compared to VPE, respectively in two scenarios (see <ref type="table" target="#tab_4">Table 3</ref>). The improvement of accuracies in logo classification is not comparable to that in traffic classification, which is due to the undesirable quality of the training logo dataset. The GTSRB is the largest dataset with various illumination conditions. And the traffic signs are always complete and well localized in the images so that illumination features can be separated more easily. In contrast, the separation is harder for logo dataset due to incomplete logos, color changes, and non-rigid deformation (e.g., logos on the bottles).</p><p>As introduced before, a parametric classifier is trained on the augmented feature set in our method. For comparison, we also implement Sill-Net with the nearest-neighbor (NN) classifier, a nonparametric approach commonly used in few-shot classification. The semantic features of test images are extracted by the same separation network and then classified by NN. The results (see Sill-Net w/ NN in <ref type="table" target="#tab_3">Tables 2 and 3)</ref> show that training a parametric classifier with feature augmentation performs better than classifying the semantic features by NN, especially when the separation of semantic features is not good enough due to domain shift (GTSRB?TT100K) or insufficient training samples (CTSD?CTSD and BTSC?BTSC).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Cross-domain one-shot classification</head><p>Sill-Net achieves the best results among all methods in crossdomain one-shot classification tasks, as shown in <ref type="table" target="#tab_5">Table 4</ref>. It outperforms VPE by a large margin of 23.63% (69.75% compared to 46.12%) in GTSRB?Flickr32 and 39.86% (69.46% compared to 29.60%) in GTSRB?Toplogos. The results illustrate that our method is still generalizable when the domain is transferred from traffic signs to logos. The unsatisfactory results of VPE are predictable. VPE learns a generalizable similarity embedding space of the semantic information among the same or similar domain (i.e., from traffic signs to traffic signs or from logos to logos). However, the embeddings learned from traffic signs are difficult to generalize to logos. In contrast, our method learns well-separated semantic and illumination representation and the illumination features are generalizable for augmentation to the template images from novel domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Ablation study</head><p>In this section, we delve into the contribution of each component of our method. The components under evaluation include the exchange mechanism, the matching and reconstruction module, the illumination constraint, and template image processing, as shown in <ref type="table" target="#tab_6">Table 5</ref>. We disable one component at a time and then record the performance to assess its importance. The experiments are implemented in the one-shot classification scenario GTSRB?GTSRB.</p><p>The results demonstrate that the exchange mechanism and matching module are the core components of our method. The accuracy of the model drops to 48.10% without the exchange mechanism. It is because the semantic and illumination features cannot be well separated without the exchange mechanism. The remaining semantic information in the illumination features is useless, or even would interfere with the recognition when they are combined with the semantic features of other objects during feature augmentation, hurting the performance of the model.</p><p>Meanwhile, the matching module cooperating with the separation module further separates the semantic and illumination features. The matching module corrects the deformation of the object features. It retains the concrete semantic information (e.g., the outline of objects and semantic details of object contents) with the supervision of template images. Without the matching module, the semantic features would be less informative, so the separation module would have difficulty separating the illumination features from the semantic features. Therefore, the accuracy of the model drops to 54.27% when the matching module is removed.</p><p>The accuracy decreases by 16.86% without the reconstruction module, which also strives to make semantic features more informative. The matching module helps the model capture some level of the concrete semantic information, while the reconstruction module prompts to retain delicate details of the object. The illumination constraint increases the accuracy by 6.87%. Intuitively, it reduces the semantic information in illumination features and thus enhances their quality. Higher-quality illumination representation improves the effectiveness of our feature augmentation method, which is consistent with the results.</p><p>Furthermore, template image processing improves model performance as expected. The processing methods (i.e., geometric transformations, image enhancement, and blur as introduced before) diversify the template images so that the trained model is more generalizable. Under the combined effect of the proposed illumination augmentation in the feature space and the variation of template images, the full model achieves the best results among the existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5">Comparison with standard augmentations</head><p>In this section, we thoroughly compare our method with standard augmentation techniques. We maintain the extraction network in the separation phase, while during the augmentation phase, we apply standard augmentation to the semantic features instead of illumination feature augmentation. The results of GTSRB?GTSRB are shown in <ref type="table" target="#tab_7">Table 6</ref>. For geometric transformations, we apply random warping, cropping, rotation, and flipping. Image enhancement (random brightness, color, contrast and sharpness adjustments) and blur are also implemented for comparison. We also apply all above standard augmentation techniques together (All-Aug) to achieve better results. To be fair, the template processing (including standard augmentations) of our method (Illu-Aug) is removed.</p><p>As we can see, the results of one single standard augmentation are not satisfactory. It is because a single augmentation is always difficult to cover all the various deformations. The results are improved when applying all standard augmentations together. However, our illumination feature augmentation still outperforms the combination of these augmentations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.6">Visualization</head><p>Feature visualization. <ref type="figure">Figure 4</ref> shows the separated semantic and illumination features of the images from training and test classes in GTSRB, visualized in the third and fourth lines. Note that the training and test datasets share no common classes. As shown in the figure, the semantic features delicately retain information consistent with the template images for both training and test classes. It is due to three aspects. First, the extractor maintains the size and spatial information of the features. Second, although objects in the input images vary in size and position, the features are corrected to the normal situation corresponding to the template images via the spatial transformer in the matching module. Third, the reconstruction module promotes the semantic feature to retain the details of the objects.</p><p>In contrast, the semantic information is effectively reduced in illumination features. They reflect the illumination conditions in the original images to a certain extent. Intuitively, the pink parts in the features represent the bright illumination while the green parts represent the dark illumination. Such well-separated representation lays the foundation for the good performance of our model.</p><p>Template reconstruction. While the reconstructor serves to obtain informative semantic features during training, it can also retrieve the template images in the inference phase. As shown in the last row of <ref type="figure">Figure 4</ref>, the reconstructor robustly generates the template images of both the training and test samples, regardless of illumination variance, object deformation, blur, and low resolution of the images. Not only outlines of the symbol contents but also fine details are well restored in the generated template images, which improves the reconstruction results by VPE. Our results further demonstrate that the proposed model has learned good representation of semantic information for both classification and reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experiments on the illumination repository</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Illumination repository expansion performance</head><p>We have constructed a repository containing 51,839 illumination features separated from the training subset of GTSRB. As we observed that the repository includes similar illumination features that might be redundant and less useful for training, we select the representative samples to reduce the number while keeping the important structure of illumination features. Subsequently, we expand the illumination feature space through linear interpolation to further diversify the augmentation features (see Section 2.4.1).</p><p>Feature selection We first select the illumination features by k-means clustering. The features in the repository are clustered into k partitions. We gather the cluster center of each partition to form a compact repository retaining representative illumination features. In the experiment, we tried both k = 100 and k = 1000.</p><p>Feature interpolation We then expand the feature space using the selected features in the compact repository. We generate new illumination features through linear interpolation of random pairs of the selected features. The number of the generated features are set as N exp = 40000. We evaluate Sill-Net by augmenting the support samples with the generated illumination features in the scenario GTSRB?GTSRB.</p><p>The results are shown in <ref type="table" target="#tab_8">Table 7</ref>. As we can see, feature selection can greatly reduce the size of the illumination repository (from 51,839 to k = 100 or k = 1000) while only causing a small decrease in accuracy, indicating that the raw illumination features are redundant to a certain extent.</p><p>Meanwhile, certain expansion of the feature space can generate useful novel illumination features, which can be used as feature augmentation to improve model performance. In our experiment, the linear interpolation method can enhance the accuracy from 85.36% to 91.12% in the case that k = 100. When k = 1000, the accuracy increases from 90.40% to 97.62%, even outperforming the best result (97.60%) achieved by the raw illumination features. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Illumination reconstruction and transplantation</head><p>To further interpret the separated illumination features intuitively, we reconstructed them back to visual images as described in Section 3.3.2. <ref type="figure">Figure 5</ref> shows the results of transplanting the illumination features separated from training images to the support template images. As we can see, the reconstructed illumination condition on new objects is similar to that of original images, while the semantic information is completely replaced in the synthesized images. Fine details of illumination such as delicate light and shadows can be well retrieved from the features. The results indicate that the illumination features indeed contain retrievable information related to illumination conditions on both foreground and background. To verify the versatility of the illumination features extracted from GTSRB, we conduct feature augmentation with our illumination repository on standardized few-shot classification benchmarks (i.e., miniImageNet, CUB, and CIFAR-FS) and compare the performance with other state-of-the-art approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Generalized one/few-shot classification on natural image datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Transductive setting</head><p>We augment the training images with the illumination features extracted from GTSRB when training the backbone and implement Power Transformation and Maximum A Posteriori (PT+MAP) proposed by <ref type="bibr" target="#b20">[21]</ref>. The results are shown in <ref type="table" target="#tab_9">Table 8</ref> (our method is denoted by Illu-Aug).</p><p>As we can see, our method achieves state-of-the-art performance on all benchmarks for both 1-shot and 5-shot classification. Generally, our method performs better when the objects are not deformed much compared to the representative template images, so that it achieves the most improvements on the CUB dataset. Also, providing more representative template images (i.e., increase the diversity of template images) to be augmented improves the performance. So our method achieves more improvements in 5-shot classification compared to 1-shot classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Inductive setting</head><p>In the inductive setting, we apply feature illumination augmentation to common few-shot baselines and compare them with standard implementations. We evaluate on CUB since our method improves the most on this dataset in the transductive setting. The results show that generally our method performs better in 5-shot classification. It does not always perform well on 1-shot tasks because illumination feature augmentation can not address the variation of objects themselves, so more support samples (e.g., 5-shot) are needed. For Baseline++, MAML and ProtoNet, our method helps improve the performance in both 1-shot and 5-shot classifications.</p><p>Overall, the results demonstrate that the repository with diverse illumination features separated from GTSRB can facilitate the generalized one/few-shot classification on natural images in both the transductive and inductive settings via our proposed augmentation method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DISCUSSIONS</head><p>Unlike most previous works concentrating on how to augment the semantic part of features <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref> or learn the augmentation policies <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, we provide a novel perspective to remove the semantic part of images and store the remaining to the repository for augmentations in feature space. Our method can be widely applied to a series of training scenarios. In the case that the training samples with certain illumination conditions are limited in the dataset, we can augment these samples with that type of illumination features separated from other images (or simply use the illumination features in our repository). Furthermore, we can utilize the method to expand a few support samples or even only one (e.g., template images) to form a large training dataset, solving the problem of lacking annotated real-world data. Overall, the imbalance both in size and illumination conditions of the dataset could be alleviated since we can transplant illumination information to specific training samples limited in number or insufficient in illumination diversity.</p><p>However, some issues and limitations need to be clarified. First, why do we blend illumination features during inference? Since we mix semantic and illumination features during augmentation to train the classifier, we have to maintain the same input pattern for classification during inference. The accuracy will decrease if we classify semantic features without blending during inference because the distribution of input features to the classifier is shifted from that of mixed features training the classifier <ref type="bibr" target="#b39">[40]</ref>. So we must maintain the same input pattern for the classifier during training and inference to obtain optimal results. Further investigation of input patterns is shown in Appendix C.2.</p><p>Second, there is no sufficient supervision or constraints to ensure that the separated illumination features contain only illumination information. One image contains various information, including object semantics referring to labels, light and shadows, irrelevant background, etc. And the illumination information is sometimes related to background objects, such as the sky, lamps and trees. It is difficult to disentangle all these factors. Theoretically, existing constraints only guarantee that the separated illumination features are non-semantic. So we have to carefully choose the training datasets with various illumination and few irrelevant objects. Experiments in Section 3.2.6 ( <ref type="figure">Figure 4)</ref> show that the visualized illumination features mainly reflect the illumination intensity. And the illumination reconstruction results in Section 3.3.2 ( <ref type="figure">Figure 5)</ref> show that these features are influenced by general illumination such as the light from the sky or shadows on trees. Since we conduct experiments to verify the effectiveness of features mostly reflecting illumination, we discreetly call them "illumination features" rather than the broader concept "non-semantic features".</p><p>Third, the training of illumination separation is limited to simple object datasets with various illumination. It is not suitable for natural images with complex structures because during separation, object features have to be spatially transformed before being matched and reconstructed to regular templates. More sophisticated methods such as semantic transformations <ref type="bibr" target="#b36">[37]</ref> should be investigated to solve non-spatial transformations for future research.</p><p>Finally, another limitation is the assumption that the illumination distribution learned from training data should characterize that of test data, which is not always fully satisfied. To better fulfill this assumption, we have made the illumination repository representative and diverse through feature selection and interpolation, achieving satisfactory results for both symbolic objects (Section 3.3.1) and natural images (Section 3.4). Future research on the repository is needed to make our method more generalizable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORKS</head><p>Data augmentation. Deep learning is always heavily reliant on large amounts of data to avoid the overfitting of the networks. Data augmentation is an effective data-space solution to the problem of limited data <ref type="bibr" target="#b40">[41]</ref>. It includes a series of methods that artificially inflate the size of the training dataset through either data warping or oversampling. These augmentations can be divided into basic image manipulations such as geometric transformations <ref type="bibr" target="#b41">[42]</ref>, color transformations <ref type="bibr" target="#b42">[43]</ref>, random erasing <ref type="bibr" target="#b43">[44]</ref>, noise injection <ref type="bibr" target="#b44">[45]</ref>, image mixing <ref type="bibr" target="#b45">[46]</ref>, kernel filters <ref type="bibr" target="#b46">[47]</ref>, etc., and deep learning approaches including feature space augmentations <ref type="bibr" target="#b47">[48]</ref>, adversarial training <ref type="bibr" target="#b48">[49]</ref>, neural style transfer <ref type="bibr" target="#b49">[50]</ref> and GAN-based data augmentations <ref type="bibr" target="#b50">[51]</ref>. Recently, many learnable augmentation methods <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref> have been proposed and applied to image classification <ref type="bibr" target="#b51">[52]</ref>, text recognition <ref type="bibr" target="#b52">[53]</ref>, and objection detection <ref type="bibr" target="#b53">[54]</ref>. AutoAugment <ref type="bibr" target="#b37">[38]</ref> designs a search space to find the best policy consisting of different random augmentation sub-policies to yield the highest validation accuracy on a target dataset, and RandAugment <ref type="bibr" target="#b38">[39]</ref> further reduces the complexity of augmentation strategy search and improve efficiency. Our Sill-Net uses a similar augmentation technique to image mixing <ref type="bibr" target="#b45">[46]</ref>. It is also a learnable augmentation method but it directly learns the augmented feature space itself instead of the parameters of augmentation policies.</p><p>Feature space augmentation implements the transformation in a learned feature space rather than the input space. DeVries and Taylor <ref type="bibr" target="#b47">[48]</ref> first performs the transformation in the feature space by adding noise, interpolating, or extrapolating. Recently, augmentation methods in the semantic feature space are proposed to regularize deep networks <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b55">[56]</ref>. We usually copy and paste the learned semantic features into new scarce data <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b57">[58]</ref>, or the statistics such as mean and variance of semantic features to increase diversity <ref type="bibr" target="#b58">[59]</ref>. Prediction on mixed labels of mixed data is also proven to be an effective way of augmentation <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b60">[61]</ref>. Unlike these methods, we augment the samples in an easy-to-use way without constraints on data distribution and in a more interpretable and intuitive perspective that the augmented features represent different illumination. We are the first to focus on removing the semantic part and storing the remaining illumination to the repository for augmentations while other works concentrate on how to augment the semantic part of features.</p><p>Feature disentanglement. One important issue of computer vision is to learn the disentangled representation comprising a number of latent factors, with each factor controlling an interpretable aspect of the generated data <ref type="bibr" target="#b61">[62]</ref>. Some works focus on disentanglement learning with respect to specified labels <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b63">[64]</ref>. They are proposed to extract features composed of two parts: one summarizes the specified factors of variation associated with the labels (content), while the other summarizes the remaining unspecified variability (e.g., style). One basic idea is to pair the content factor of a data sample with the style factor of another sample with the same label <ref type="bibr" target="#b64">[65]</ref>. The resulting representation should be able to recover the latter sample using a decoder. Such an idea is also applied to a weakly-supervised setting that only members of a given group sharing the same label are available, whereas different groups are not required to represent different classes <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b66">[67]</ref>. Our study disentangles representation into semantic parts associated with labels as specified content and illumination parts as unspecified variability.</p><p>Few-shot learning. Compared to the common machine learning paradigm that involves large-scale labeled training data, the development of Few-Shot Learning (FSL) is relatively tardy due to its intrinsic difficulty. Early efforts for FSL were based on generative models that sought to build the Bayesian probabilistic framework <ref type="bibr" target="#b67">[68]</ref>, <ref type="bibr" target="#b68">[69]</ref>. Recently, more attention was paid on meta-learning, which can be generally summarized into five subcategories: learn-to-measure (e.g., MatchNets <ref type="bibr" target="#b17">[18]</ref>, ProtoNets <ref type="bibr" target="#b24">[25]</ref>, RelationNets <ref type="bibr" target="#b69">[70]</ref>), learn-to-finetune (e.g., Meta-Learner LSTM <ref type="bibr" target="#b70">[71]</ref>, MAML <ref type="bibr" target="#b23">[24]</ref>), learn-to-remember (e.g., MANN <ref type="bibr" target="#b71">[72]</ref>, SNAIL <ref type="bibr" target="#b72">[73]</ref>), learn-to-adjust (e.g., MetaNets <ref type="bibr" target="#b73">[74]</ref>, CSN <ref type="bibr" target="#b74">[75]</ref>) and learnto-parameterize (e.g., DynamicNets <ref type="bibr" target="#b75">[76]</ref>, Acts2Params <ref type="bibr" target="#b76">[77]</ref>). In this work, we use tasks similar to one-shot learning to evaluate our method and compare the results to these FSL methods. Our method is partly a data augmentation strategy to solve the data scarcity problems, thus compatible with almost all FSL methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we develop a novel neural network architecture named Separating-Illumination Network (Sill-Net) to separate illumination features from images and then augment them to support samples. Extensive experiments validate the feasibility and effectiveness of illumination-based feature augmentation in different classification tasks. Our method not only outperforms the state-of-the-art (SOTA) methods by a large margin on symbolic object datasets but also achieves improvements on natural image benchmarks. It is a novel perspective to remove the semantic part of images and use the remaining for feature augmentations. Currently, the separation of semantic and illumination features is limited to simple-structured symbolic objects. In our future work, we shall adopt the separation phase of our method to natural images with complex structures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Illustration of the key idea of our approach. The semantic and illumination representation are separated from the training image (mandatory straight). The illumination representation is used to augment the support sample (deer crossing).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Visualization of the separated features and the reconstructed template images from training and test classes. The first two rows show the input images and their corresponding template images. The third and fourth rows show the semantic and illumination features of the input images separated by our model. The last row shows the template images reconstructed from the semantic features. Note that the training and test datasets share no common classes. Illumination transplantation. The synthesized images show objects of support samples under illumination separated from training images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, Z. Cao, Z. Yan and C. Zhang are with the Institute for Artificial Intelligence, Tsinghua University (THUAI), Beijing 100084, China, and also with the State Key Laboratory of Intelligence Technologies and Systems, Beijing National Research Center for Information Science and Technologies (BNRist) , Department of Automation, Tsinghua University, Beijing 100084, China (e-mail: zhanghp16@mails.tsinghua.edu.cn; caozhong14@mails.tsinghua.edu.cn; yza18@mails.tsinghua.edu.cn; zcs@mail.tsinghua.edu.cn).</figDesc><table /><note>? Corresponding Author: H. Zhang and C. Zhang.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Mechanism Separation Module Matching and Reconstruction Module Illumination Repository Augmentation Module</head><label></label><figDesc>1) We develop a novel algorithm to separate the semantic and illumination features from images. The separated illumination arXiv:2102.03539v3 [cs.CV] 6 Oct 2022</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Templates</cell><cell>Matching</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Reconstructor</cell><cell>Templates</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Spatial</cell></row><row><cell>Training images</cell><cell>Extractor</cell><cell>Features</cell><cell>Separator</cell><cell>Transformer</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Exchange</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Augmented</cell><cell>Classifier</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>features</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>?</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Support samples</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1</head><label>1</label><figDesc>Symbolic object datasets with their traditional classification results (%)</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Traffic sign</cell><cell></cell><cell></cell><cell>Logo</cell><cell></cell></row><row><cell>Dataset</cell><cell cols="7">GTSRB TT100K BTSC CTSD Belga Flickr32 TopLogo</cell></row><row><cell>Size</cell><cell>51839</cell><cell>11988</cell><cell>7095</cell><cell>6164</cell><cell>9585</cell><cell>3404</cell><cell>848</cell></row><row><cell>Classes</cell><cell>43</cell><cell>36</cell><cell>62</cell><cell>58</cell><cell>37</cell><cell>32</cell><cell>11</cell></row><row><cell>CNN3ST</cell><cell>99.71</cell><cell>99.06</cell><cell>98.87</cell><cell>83.55</cell><cell>87.68</cell><cell>94.78</cell><cell>85.06</cell></row><row><cell>Sill-Net</cell><cell>99.68</cell><cell>99.53</cell><cell>98.97</cell><cell>97.19</cell><cell>89.48</cell><cell>95.80</cell><cell>89.66</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2</head><label>2</label><figDesc>One-shot classification accuracies (%) on traffic sign datasets. The best results are marked in bold.</figDesc><table><row><cell></cell><cell>GTSRB?GTSRB</cell><cell>GTSRB?TT100K</cell></row><row><cell>No. support set</cell><cell>(22+21)-way</cell><cell>36-way</cell></row><row><cell>SiamNet [28]</cell><cell>33.62</cell><cell>22.74</cell></row><row><cell>QuadNet [29]</cell><cell>45.2</cell><cell>N/A</cell></row><row><cell>MatchNet [18]</cell><cell>53.30</cell><cell>58.75</cell></row><row><cell>VPE [9]</cell><cell>83.79</cell><cell>71.80</cell></row><row><cell>Sill-Net</cell><cell>97.60</cell><cell>95.59</cell></row><row><cell>Sill-Net w/ NN</cell><cell>94.53</cell><cell>56.09</cell></row><row><cell></cell><cell>BTSC?BTSC</cell><cell>CTSD?CTSD</cell></row><row><cell>No. support set</cell><cell>(21+41)-way</cell><cell>(23+35)-way</cell></row><row><cell>QuadNet [29]</cell><cell>51.92</cell><cell>46.47</cell></row><row><cell>VPE [9]</cell><cell>78.46</cell><cell>72.08</cell></row><row><cell>Sill-Net</cell><cell>93.25</cell><cell>93.53</cell></row><row><cell>Sill-Net w/ NN</cell><cell>89.67</cell><cell>69.43</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3</head><label>3</label><figDesc>One-shot classification accuracy (%) on logo datasets. The best results are marked in bold.</figDesc><table><row><cell></cell><cell>Belga?Flickr32</cell><cell>Belga?Toplogos</cell></row><row><cell>No. support set</cell><cell>32-way</cell><cell>11-way</cell></row><row><cell>SiamNet [28]</cell><cell>22.82</cell><cell>30.46</cell></row><row><cell>QuadNet [29]</cell><cell>37.72</cell><cell>36.62</cell></row><row><cell>MatchNet [18]</cell><cell>40.95</cell><cell>35.24</cell></row><row><cell>VPE [9]</cell><cell>53.53</cell><cell>57.75</cell></row><row><cell>Sill-Net</cell><cell>65.21</cell><cell>84.43</cell></row><row><cell>Sill-Net w/ NN</cell><cell>45.62</cell><cell>39.75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4</head><label>4</label><figDesc>Cross-domain one-shot classification accuracy (%). The models are trained on the traffic sign dataset (GTSRB) and tested on the logo datasets. The best results are marked in bold.</figDesc><table><row><cell></cell><cell>GTSRB?Flickr32</cell><cell>GTSRB?Toplogos</cell></row><row><cell>No. support set</cell><cell>32-way</cell><cell>11-way</cell></row><row><cell>QuadNet [29]</cell><cell>28.41</cell><cell>25.38</cell></row><row><cell>VPE [9]</cell><cell>46.12</cell><cell>29.60</cell></row><row><cell>Sill-Net</cell><cell>69.75</cell><cell>69.46</cell></row><row><cell>Sill-Net w/ NN</cell><cell>38.94</cell><cell>25.47</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 5</head><label>5</label><figDesc>Ablation study results (%) in the one-shot classification scenario GTSRB?GTSRB. We disable one component at a time and record the performance of Sill-Net.</figDesc><table><row><cell>Factor</cell><cell>Accuracy (decrement)</cell></row><row><cell>w/o exchange mechanism</cell><cell>48.10 (-49.50)</cell></row><row><cell>w/o matching module</cell><cell>54.27 (-43.33)</cell></row><row><cell>w/o reconstruction module</cell><cell>80.74 (-16.86)</cell></row><row><cell>w/o illumination constraint</cell><cell>90.73 (-6.87)</cell></row><row><cell>w/o template processing</cell><cell>80.19 (-17.41)</cell></row><row><cell>full method</cell><cell>97.60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 6 The</head><label>6</label><figDesc></figDesc><table><row><cell cols="5">results (%) of comparison with standard augmentations in</cell></row><row><cell></cell><cell cols="2">GTSRB?GTSRB</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Warping</cell><cell cols="2">Cropping Rotation</cell><cell>Flipping</cell></row><row><cell>Accuracy</cell><cell>41.11</cell><cell>31.14</cell><cell>26.60</cell><cell>21.61</cell></row><row><cell>Method</cell><cell>Enhancement</cell><cell>Blur</cell><cell>All-Aug</cell><cell>Illu-Aug</cell></row><row><cell>Accuracy</cell><cell>34.10</cell><cell>34.84</cell><cell>53.44</cell><cell>80.19</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 7</head><label>7</label><figDesc>One-shot classification accuracy (%) when augmenting the support samples with generated illumination features from k selected samples in the scenario GTSRB?GTSRB. The best result is marked in bold.</figDesc><table><row><cell>k</cell><cell>Selection</cell><cell>Interpolation</cell></row><row><cell>100</cell><cell>85.36</cell><cell>91.12</cell></row><row><cell>1000</cell><cell>90.40</cell><cell>97.62</cell></row><row><cell>51839</cell><cell>97.60</cell><cell>N/A</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 8</head><label>8</label><figDesc>Generalized 1-shot and 5-shot classification accuracy (%) on miniImageNet, CUB and CIFAR-FS in the transductive setting. Our method is aligned with the pipeline of PT+MAP with the backbone of WRN, trained with the images augmented by the illumination repository.The best results are marked in bold.</figDesc><table><row><cell>miniImageNet</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 9</head><label>9</label><figDesc>Generalized 1-shot and 5-shot classification accuracy (%) on CUB in the inductive setting. We augment training images with the illumination repository during the training of baselines. The best results are marked in bold. ? 0.74 46.84 ? 0.79 64.16 ? 0.71 66.58 ? 0.69 Baseline++ Conv4 60.53 ? 0.83 62.83 ? 0.87 79.34 ? 0.61 79.74 ? 0.60 MAML ResNet10 70.32 ? 0.99 72.45 ? 0.91 80.93 ? 0.71 82.58 ? 0.64 MatchingNet ResNet18 73.49 ? 0.89 73.26 ? 0.90 84.45 ? 0.58 85.18 ? 0.60 ProtoNet ResNet18 72.99 ? 0.88 74.35 ? 0.91 86.64 ? 0.51 87.80 ? 0.48</figDesc><table><row><cell></cell><cell></cell><cell>1-shot</cell><cell></cell><cell>5-shot</cell><cell></cell></row><row><cell>Method</cell><cell>Backbone</cell><cell>Standard</cell><cell>Illu-Aug</cell><cell>Standard</cell><cell>Illu-Aug</cell></row><row><cell>Baseline</cell><cell>Conv4</cell><cell>47.12</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving shadow suppression for illumination robust face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Morvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="611" to="624" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ELEGANT: Exchanging latent encodings with GAN for transferring multiple face attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="168" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Variational prototypingencoder: One-shot learning with prototypical images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9462" to="9470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robustly disentangled causal mechanisms: Validating deep representations for interventional robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Suter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Miladinovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6056" to="6065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlipsing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="323" to="332" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Traffic-sign detection and classification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2110" to="2118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Traffic sign recognition-how far are we from the solution?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">in International Joint Conference on Neural Networks</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Logo retrieval with a contrario visual query expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Buisson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM International Conference on Multimedia</title>
		<meeting>the 17th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="581" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scalable mining of small visual objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Letessier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Buisson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM International Conference on Multimedia</title>
		<meeting>the 20th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="599" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scalable logo recognition in real-world images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Pueyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lienhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Zwol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st ACM International Conference on Multimedia Retrieval</title>
		<meeting>the 1st ACM International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep learning logo detection with data expansion by synthesising context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="530" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<title level="m">The Caltech-UCSD birds-200-2011 dataset</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Meta-learning with differentiable closed-form solvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08136</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Leveraging the feature distribution in transfer-based few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gripon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pateux</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03806</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04232</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Deep traffic sign detection and recognition without target domain real images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tabelini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Berriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Paix?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Badue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Oliveira-Santos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.00962</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep neural network for traffic sign recognition systems: An analysis of spatial transformers and stochastic optimisation methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arcos-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Alvarez-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Soria-Morillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="158" to="165" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Deep Learning Workshop</title>
		<meeting><address><addrLine>Lille</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Co-domain embedding using deep quadruplet networks for unseen traffic sign recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.01907</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Prototype rectification for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.10713</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Exploiting unsupervised inputs for accurate few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gripon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pateux</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.09849</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">TAFSSL: Task-adaptive feature sub-space learning for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lichtenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="522" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Transductive few-shot learning with meta-learned confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Kye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12017</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adaptive subspaces for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4136" to="4145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Decaug: Out-of-distribution generalization via decomposed feature representation and semantic augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><forename type="middle">G</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09382</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Semantic-based data augmentation for math word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.02489</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Regularizing deep networks with semantic data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="113" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">On mixup regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carratino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ciss?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Vert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06049</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A survey on image data augmentation for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shorten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">60</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improving deep learning with generic data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nitschke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium Series on Computational Intelligence</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1542" to="1547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Data augmentation for improving deep learning in image classification problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miko?ajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grochowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Interdisciplinary PhD Workshop</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="117" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Forward noise adjustment scheme for data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Moreno-Barea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strazzera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Jerez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Urda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Franco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium Series on Computational Intelligence</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="728" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Data augmentation by pairing samples for images classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Inoue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.02929</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Patchshuffle regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07103</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Dataset augmentation in feature space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05538</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Data augmentation generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04340</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">STaDA: Style transfer as data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chalasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smolic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01056</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bowles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guerrero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bentley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gunn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hammers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Dickie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Hern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wardlaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.10863</idno>
		<title level="m">GAN augmentation: Augmenting training data using generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">The effectiveness of data augmentation in image classification using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.04621</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learn to augment: Joint data augmentation and network optimization for text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning data augmentation strategies for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="566" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Dreaming more data: Class-dependent distributions over diffeomorphisms for learned data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hauberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Freifeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B L</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics. PMLR</title>
		<imprint>
			<biblScope unit="page" from="342" to="350" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">On the importance of visual context for data augmentation in scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dvornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Feature space augmentation for long-tailed data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="694" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Simple copy-paste is a strong data augmentation method for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2918" to="2928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">On feature normalization and data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-N</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">On the generalization effects of linear transformations in data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Valiant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR, 2020</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">420</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Classmix: Segmentation-based data augmentation for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tranheden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Svensson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1369" to="1378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Disentangling factors of variation with cycle-consistent variational auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Veeravasarapu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="829" to="845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Multi-level variational autoencoder: Learning disentangled representations from grouped observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bouchacourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tomioka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08841</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Disentangling factors of variation in deep representation using adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sprechmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5040" to="5048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Group-based learning of disentangled representations with generalizability for novel contents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hosoya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2506" to="2513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Adversarial disentanglement with grouped observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nemeth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04761</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="594" to="611" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1842" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">A simple neural attentive meta-learner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Meta networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2554" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Rapid adaptation with conditionally shifted neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Few-shot image recognition by predicting parameters from activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7229" to="7238" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
