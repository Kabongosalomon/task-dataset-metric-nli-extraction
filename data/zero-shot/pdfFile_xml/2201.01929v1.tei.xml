<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Decompose to Adapt: Cross-domain Object Detection via Feature Disentanglement</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Dongnan</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyi</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Yang</forename><surname>Song</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyu</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Barnett</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Weidong</forename><surname>Cai</surname></persName>
						</author>
						<title level="a" type="main">Decompose to Adapt: Cross-domain Object Detection via Feature Disentanglement</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-domain adaption</term>
					<term>object detection</term>
					<term>feature dis- entanglement</term>
					<term>automatic drive</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances in unsupervised domain adaptation (UDA) techniques have witnessed great success in cross-domain computer vision tasks, enhancing the generalization ability of data-driven deep learning architectures by bridging the domain distribution gaps. For the UDA-based cross-domain object detection methods, the majority of them alleviate the domain bias by inducing the domain-invariant feature generation via adversarial learning strategy. However, their domain discriminators have limited classification ability due to the unstable adversarial training process. Therefore, the extracted features induced by them cannot be perfectly domain-invariant and still contain domainprivate factors, bringing obstacles to further alleviate the crossdomain discrepancy. To tackle this issue, we design a Domain Disentanglement Faster-RCNN (DDF) to eliminate the sourcespecific information in the features for detection task learning. Our DDF method facilitates the feature disentanglement at the global and local stages, with a Global Triplet Disentanglement (GTD) module and an Instance Similarity Disentanglement (ISD) module, respectively. By outperforming state-of-the-art methods on four benchmark UDA object detection tasks, our DDF method is demonstrated to be effective with wide applicability.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-Recent advances in unsupervised domain adaptation (UDA) techniques have witnessed great success in cross-domain computer vision tasks, enhancing the generalization ability of data-driven deep learning architectures by bridging the domain distribution gaps. For the UDA-based cross-domain object detection methods, the majority of them alleviate the domain bias by inducing the domain-invariant feature generation via adversarial learning strategy. However, their domain discriminators have limited classification ability due to the unstable adversarial training process. Therefore, the extracted features induced by them cannot be perfectly domain-invariant and still contain domainprivate factors, bringing obstacles to further alleviate the crossdomain discrepancy. To tackle this issue, we design a Domain Disentanglement Faster-RCNN (DDF) to eliminate the sourcespecific information in the features for detection task learning. Our DDF method facilitates the feature disentanglement at the global and local stages, with a Global Triplet Disentanglement (GTD) module and an Instance Similarity Disentanglement (ISD) module, respectively. By outperforming state-of-the-art methods on four benchmark UDA object detection tasks, our DDF method is demonstrated to be effective with wide applicability.</p><p>Index Terms-domain adaption, object detection, feature disentanglement, automatic drive</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>O BJECT detection, which aims to assign a bounding box and category prediction for each foreground instance, is essential for modern computer vision. Taking advantages from the deep learning techniques, previous object detection methods based on convolutional neural networks (CNN) have achieved appealing performance on various benchmarks <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b4">[5]</ref>. However, these fully-supervised models have been criticized for the lack of generalization ability and suffer from severe performance drop when validated on other unseen datasets, since they tend to bias towards the data distribution of the training domain <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. On the other hand, collecting sufficient annotations for each new domain is impractical in real applications, due to time-consuming and expensive annotation procedure.</p><p>To address this dilemma, unsupervised domain adaptation (UDA) methods have been proposed to transfer the domaininvariant information from the labeled source domain to an unlabeled target domain <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref>. For the UDA object detection methods, the majority of them incorporate the adversarial domain discriminators with the object detection architectures (e.g Faster-RCNN), to alleviate the cross-domain discrepancy by inducing the domain-invariant features generation <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b13">[14]</ref>.</p><p>Despite the state-of-the-art performance of the adversarial learning-based UDA object detection methods, they overlook the entanglement between the domain-shared and domainprivate features in the latent manifold space <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b16">[17]</ref>. Due to the unstable learning process, the decision boundary for the adversarial domain classifiers in these methods is inaccurate <ref type="bibr" target="#b17">[18]</ref>. As such, the extracted features in the typical adversarial UDA methods cannot be perfectly domain-invariant and inevitably contain domain-private factors. Optimizing the UDA models with these entangled features induces the model bias towards the source domain, and further degrades the performance on the target domain. As indicated in <ref type="figure" target="#fig_0">Figure 1</ref>, the Baseline method with the adversarial domain discriminator tends to focus on the source-specific information in the background, which leads to suboptimal adaptation performance.</p><p>Previously, several methods have been proposed to solve the feature entanglement issue in UDA classification <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. They are established based on auto-encoder architectures, incorporated with latent code independence induction mechanism, and classifier entropy regularization to decompose the domain-private factors from the features for classifier learning. However, directly extending the feature disentanglement-based UDA classification methods to cross-domain object detection suffers from challenges. First, <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b16">[17]</ref> decompose the shared and private features in each domain by inducing their distributions to be independent, which always requires the batch size of the features to be sufficiently large to describe the characteristics of the distribution. However, the batch size for UDA object detection is very limited (sometimes even equals to 1 <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[13]</ref>), due to the mandatory larger model sizes than those for classification. Second, these methods ignore the feature entanglement at the local level for each foreground object since their objective is to assign a category label for the whole image. For cross-domain object detection which typically contains multiple object instances in an image, the adaptation ability of these UDA disentanglement-based classification model is limited, resulting from the instance-level feature entanglement. Although PD <ref type="bibr" target="#b20">[21]</ref> was previously proposed for UDA object detection via feature decomposition based on mutual information minimization <ref type="bibr" target="#b16">[17]</ref>, the limited batch size for the object detection framework causes the suboptimal adaptation performance. Subsequently, there currently still lack feature disentanglement UDA methods particularly for cross-domain object detection.</p><p>Motivated by the aforementioned observations, we propose a Domain Disentanglement Faster-RCNN (DDF) method in this work, to improve the typical adversarial learning-based UDA object detection via feature disentanglement. Specifically, our DDF achieves feature disentanglement at the global and local levels. Throughout this paper, the global-level features represent the output of the backbone network, which contain the information of the object structure, and spatial distribution for the whole images. The local-level features represent the region of interests (ROIs) for object localization and classification. Domain-invariant features represent the features containing shared factors between the source and target domains, and the instance-invariant features represent the domain-invariant features at the instance-level for the ROIs. First, we design a Global Triplet Disentanglement (GTD) module jointly optimized with a domain discriminator, which improves the feature adaptation ability at the global level via a triplet feature disentanglement strategy. To further facilitate the feature disentanglement at the local stage, an Instance Similarity Disentanglement (ISD) module is proposed, based on the similarity regularization between the shared and private features for instance objects. Our DDF method is validated on four benchmark UDA object detection tasks and outperforms the state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK A. Unsupervised Domain Adaptation</head><p>Unsupervised domain adaptation (UDA) aims at bridging the gap between an annotated source domain and an unannotated target domain. Typically, UDA methods transfer the knowledge in four ways: 1) directly minimizing the statistical distribution distance between two domains <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>; 2) inducing the domain-invariant feature generation <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b23">[24]</ref>; 3) learning from the synthesized images <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b28">[29]</ref>, and 4) selftraining via pseudo labels <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>. By alleviating the crossdomain discrepancy at the feature and appearance levels, UDA methods have achieved outstanding performance in crossdomain classification <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b26">[27]</ref>, segmentation <ref type="bibr" target="#b31">[32]</ref>- <ref type="bibr" target="#b33">[34]</ref>, and detection <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>. Since our proposed method aims at tackling the feature entanglement issue for cross-domain object detection, only the literature of UDA object detection and feature disentanglement are reviewed in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. UDA for Cross-domain Object Detection</head><p>Recent UDA object detection methods are designed based on the two-stage detector Faster-RCNN and induce the domain-invariant feature learning by domain adaptive modules at the feature and appearance levels. Among the featurelevel adaptation methods, Chen et al. <ref type="bibr" target="#b10">[11]</ref> first proposed to learn domain-invariant features for the global image and local instance features. To align the image-level features with different scales, strong-weak feature alignment was then proposed <ref type="bibr" target="#b12">[13]</ref>. <ref type="bibr" target="#b34">[35]</ref> designed a selective adaptation architecture to focus on the local regions. Later, hierarchical feature alignment with multi-stage domain discriminators further improves the baseline <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>. Additionally, category-aware feature alignment is effective in alleviating the class-imbalance issues during adaptation <ref type="bibr" target="#b37">[38]</ref>- <ref type="bibr" target="#b40">[41]</ref>. In <ref type="bibr" target="#b41">[42]</ref>, a domain randomization strategy is proposed based on the spectrum learning at the frequency space, which has achieved appealing performance on the cross-domain object detection under the domain generalization setting. For the appearance-level adaptation, targetlike synthetic images and pseudo labels are commonly employed <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b42">[43]</ref>. Even though the previous methods alleviated the domain-bias by various feature-level or appearance-level adaptations, few of them have discussed and attempted to explicitly address the entanglement of the domain-shared and domain-private factors, limiting the model's adaptation abilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. UDA with Feature Disentanglement</head><p>Previous feature disentanglement methods decomposed the features by encouraging the independence between them, based on the variation autoencoders (VAE) <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref> and generative adversarial networks (GAN) <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>. By separating the appearance-level (e.g., texture, brightness) and the global-level factors (e.g., object structure, spatial distribution) in the latent feature space, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b47">[48]</ref> achieved compelling performance on multi-modal image synthesis. For UDA classification, <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b18">[19]</ref> have been proposed to decompose the domain-private and domain-shared information using autoencoder-based structures. As essential steps for these methods, the independence induction for the latent feature distribution requires large batch sizes, which is impractical for UDA object detection. In addition, <ref type="bibr" target="#b48">[49]</ref> proposed to decompose the features via the attention mechanism based on uncertainty learning. In <ref type="bibr" target="#b49">[50]</ref>, a variational information bottleneck was designed to filter out the redundant domainprivate information and achieve the disentanglement. However, the feature entanglement issue for each object instance has not been considered among these methods, which is essential for UDA object detection. Therefore, it is not intuitive to directly extend current UDA feature disentanglement methods for cross-domain object detection.</p><p>In <ref type="bibr" target="#b50">[51]</ref>, a Conditional Domain Normalization (CDN) mechanism was proposed for UDA object detection via feature decomposition, by incorporating the source-specific factors with the target features. However, there lacks feature decomposition for the target features, which would cause the model to suffer from domain bias during inference. Although Wu et al. <ref type="bibr" target="#b20">[21]</ref> designed a PD method with progressive feature disentanglement mechanism for UDA object detection, their feature entanglement modules are based on Mutual Information minimization <ref type="bibr" target="#b16">[17]</ref>, which requires batch-wise feature shuffling <ref type="bibr" target="#b51">[52]</ref>. Based on the implementation of <ref type="bibr" target="#b16">[17]</ref>, the features have a batch size of 128, sufficiently large to contain the distribution characteristics of their corresponding domains. When adapting the method to UDA object detection, the disentanglement module in <ref type="bibr" target="#b20">[21]</ref> has to work with small batch sizes and hence could not fully utilize the Mutual Information minimization strategy. To tackle this issue, we propose a DDF method for cross-domain object detection via feature disentanglement at the global and local levels, without requiring a large image batch size for training. Particularly, our DDF method has achieved better performance than PD <ref type="bibr" target="#b20">[21]</ref> (detection mAP: Ours: 39.1; PD: 36.6). This further demonstrates the superiority of our designed feature decomposition modules compared to the typical ones, when the batch sizes for the framework are limited for cross-domain object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DOMAIN DISENTANGLEMENT FASTER-RCNN</head><p>In this section, we present the detailed framework of our Domain Disentanglement Faster-RCNN (DDF) model. Denote a labeled source domain with N s i.i.d images as</p><formula xml:id="formula_0">D s = {(x s i , c s i , b s i )} i=1,2,.</formula><p>..,Ns , where c s i and b s i represent the category labels and bounding box coordinates for all the foreground objects in each image x s i , respectively. Then, an unlabeled target domain with N t i.i.d images is defined as</p><formula xml:id="formula_1">D t = {(x t j )} j=1,2,.</formula><p>..,Nt , with a different data distribution from the D s . Our DDF method aims at transferring the knowledge from the labeled D s to the unlabeled D t and achieving competitive detection performance on the target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Framework Overview</head><p>The overall diagram of our proposed DDF model is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. First, we establish a baseline UDA object detection model, by incorporating a global-level adversarial domain discriminator with a Faster-RCNN detection model. During each training iteration, the source and target images, defined as x s and x t , are fed into the network. First, a backbone is employed to extract the global-level features of the input images, constructed by a basic feature encoder E b with fixed weights, and a domain-shared feature extractor E s with dynamically updated weights.</p><p>To facilitate the feature disentanglement, we design a domain-private feature encoder E p following the E b , to obtain the feature maps containing specific factors of each domain. As shown in <ref type="figure" target="#fig_1">Figure 2</ref> and Equation 1, F s sha. and F t sha.</p><p>represent the domain-shared features across the source and target domains, and F s pri. and F t pri. indicate the domain-private features specific to the source and target domains:</p><formula xml:id="formula_2">F s sha. = E s (E b (x s )), F t sha. = E s (E b (x t )), F s pri. = E p (E b (x s )), F t pri. = E p (E b (x t )).<label>(1)</label></formula><p>For the global-level feature disentanglement, F s sha. , F t sha. , F s pri. , and F t pri. are optimized with the Global Triplet Disentanglement (GTD) module, which aligns the distributions of the domain-shared features between the source and target domains, as well as enlarges the discrepancy between the domain-shared and domain-private features within each domain. Leveraging the GTD module, the global-level domainspecific factors are thus decomposed from the domain-shared features for the detection task learning. At the local level, we employ a Region Proposal Network (RPN) and a ROIAlign layer to extract the local instance features in F s sha. , F t sha. , F s pri. , and F t pri. :</p><formula xml:id="formula_3">I s sha. = M LP ROIAlign F s sha. , RP N (F s sha. ) , I t sha. = M LP ROIAlign F t sha. , RP N (F t sha. ) , I s pri. = M LP ROIAlign F s pri. , RP N (F s pri. ) , I t pri. = M LP ROIAlign F t pri. , RP N (F t pri. ) ,<label>(2)</label></formula><p>where M LP contains a flatten layer followed by three fully connected layers. In Equation 2, I s sha. and I t sha. denote the instance-wise domain-shared features captured from each domain, while I s pri. and I t pri. represent the domain-private ones. Then, the instance-level I s sha. , I t sha. , I s pri. , and I t pri. are fed into the Instance Similarity Disentanglement (ISD) module to facilitate the feature disentanglement at the local stage, based on feature similarity optimization. Eventually, the domainshared features at the instance-level (I s sha. ) are employed for the object classification and location regression learning. The detailed structure of the private components of the domainspecific feature extractor is shown in <ref type="table" target="#tab_0">Table I</ref>. Conv, ReLU k = (3, 3), s = 1, p = 1, in = 256, out = 256 2</p><p>Maxpooling k = (2, 2), s = 2, p = 0 3</p><p>Conv, ReLU k = (3, 3), s = 1, p = 1, in = 256, out = 512 4</p><p>Maxpooling k = (2, 2), s = 2, p = 0 5</p><p>Conv, ReLU k = (3, 3), s = 1, p = 1, in = 512, out = 512 Layer D glb Hyperparameters 1</p><p>Conv, ReLU k = (3, 3), s = 1, p = 1, in = 512, out = 512 2</p><p>Conv, ReLU k = (3, 3), s = 1, p = 1, in = 512, out = 256 3</p><p>Conv, ReLU k = (3, 3), s = 1, p = 1, in = 256, out = 128 4</p><p>Conv, ReLU k = (3, 3), s = 1, p = 1, in = 128, out = 64 5</p><p>Conv k = (3, 3), s = 1, p = 1, in = 64, out = 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Global Triplet Disentanglement</head><p>In previous UDA object detection methods, the invariance between the domain-shared F s sha. and F t sha. is induced by optimizing an adversarial domain discriminator at the global level <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> through:</p><formula xml:id="formula_4">L di = max ? Es min ? D L ce D glb (F s sha. ), 0 + L ce D glb (F t sha. ), 1 ,<label>(3)</label></formula><p>where L ce (?, ?) measures the cross entropy between the feature maps and labels, while ? Es and ? D indicate the parameters for the domain-shared feature extractor E s and domain discriminator D glb , respectively. We denote the domain label as 0 for the source domain and 1 for the target domain. However, as suggested in <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>, obtaining the precise classification decision boundaries for the adversarial discriminators in typical UDA methods is always challenging due to the unstable learning procedure. As such, the features induced by D glb in Equation 3 cannot be perfectly domain-invariant, and some lurking domain-private factors would inevitably bring domain bias into the detection task learning. Although recent autoencoder-based feature disentanglement methods attempt to alleviate this issue for UDA classification tasks <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b16">[17]</ref>, they are suboptimal for UDA object detection architectures. Due to the large model size, the batch size for the typical UDA object detection framework is always set to 1 <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b20">[21]</ref>. However, the feature independence learning paradigms for feature disentanglement in <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b16">[17]</ref> involve the calculations across a batch with average and shuffling, which would become redundant when the batch size is 1.</p><p>To this end, we propose a Global Triplet Disentanglement (GTD) module, which aims to improve the adversarial learning-based feature adaptation at the global level via feature disentanglement. Our GTD module focuses on the triplet optimization among the domain-private and domain-shared features across two domains without batch-wise calculations as <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b16">[17]</ref>, hence mitigating the influence of feature batch size. Detailed illustrations of the GTD module are shown in <ref type="figure" target="#fig_2">Figure 3</ref>. We consider that under the ideal situation, the domain-private F s pri. and F t pri. contain the specific factors for the source and target domains, respectively. Subsequently, the domain discriminator D glb should be able to distinguish them. Based on this assumption, we first introduce a domain-specific classification loss L ds to enhance the classification ability of D glb :</p><formula xml:id="formula_5">L ds = min ? D ,? Ep L ce D glb (F s pri. ), 0 + L ce D glb (F t pri. ), 1 ,<label>(4)</label></formula><p>where ? Ep indicates the parameters of the domain-private encoder E p .</p><p>To further enlarge the discrepancy between the domainshared and private features within each domain, and meanwhile to align the domain-shared features across the domains, we propose a triplet optimization strategy for F s sha. , F t sha. , F s pri. , and F t pri. . Specifically, the triplet feature decomposition loss L tri is defined as:</p><formula xml:id="formula_6">L tri = 1 2 max d(F s sha. , F t sha. ) ? d(F s sha. , F s pri. ) + m, 0 + max d(F s sha. , F t sha. ) ? d(F t sha. , F t pri. ) + m, 0 ,<label>(5)</label></formula><p>where d(f 1 , f 2 ) = ||D glb (f 1 ) ? D glb (f 2 )|| 2 , measuring the square Euclidean distance between the Sof tM ax domain predictions of f 1 and f 2 . We have set the margin value m as 0.25 to ensure the domain probability predictions of the F s sha.</p><p>and F t sha. close to 0.5. In Equation 3, D glb for distinguishing F s sha from F t sha is trained in an adversarial manner with the feature extractor E s . Under the ideal optimization scenario, the features from E s should be able to confuse D glb for being domain-invariant. To this end, the domain category predictions for both F s sha and F t sha are expected to be 0.5, which is consistent with the objective of Equation 5.</p><p>Overall, the GTD module improves the adaptation ability of the adversarial domain discriminator by decomposing the global-level source-specific factors. Meanwhile, the GTD module could be intuitively implemented without any batchwise calculations, which is more practical-friendly to UDA object detection tasks by loosening the restrictions on large batch size. The loss function for the GTD module is defined as:</p><formula xml:id="formula_7">L GT D = L ds + L tri .<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Instance Similarity Disentanglement</head><p>The GTD module promotes the global-level feature adaptation by removing the domain-specific factors. However, there still exist domain-bias issues in the local-level features of each object. In previous work, adversarial domain discriminators are employed to alleviate the discrepancy for the cross-domain instance features at the local levels <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. However, similar to the global-level issues, the local-level adversarial learning-based feature adaptation also suffers from the feature entanglement, due to its inaccurate decision boundary for domain classification.</p><p>To this end, we propose an Instance Similarity Disentanglement (ISD) to facilitate the instance-level feature alignment based on feature similarity optimization. First, we obtain the domain-shared and domain-private features at the local level, following Equation 2. Since the batch sizes of the local-level I s sha. , I t sha. , I s pri. and I t pri. features equal to the number of ROIs, they are sufficiently large for dependencyaware optimization. Based on the assumption that the domainshared factors in each domain should be independent from the domain-private ones <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b16">[17]</ref>, we next propose to enlarge the distribution distance between the domain-shared and domainprivate instance features within each domain. This step could be achieved by minimizing their cosine similarity as:</p><formula xml:id="formula_8">L ISD?intra = sim(I s sha. , I s pri. ) + sim(I t sha. , I t pri. ),<label>(7)</label></formula><p>where sim(?, ?) denotes the cosine similarity between two vectors: sim(a, b) = a T b ||a|| ||b|| . Since I s sha. , I t sha. , I s pri. and I t pri. are acquired after the ReLU activation layers, the cosine similarity between each pair of them is in range [0, 1]. In addition to the intra-domain feature similarity minimization in <ref type="bibr">Equation 7</ref>, we then propose to minimize the feature similarity between the domain-private features across two domains. This is motivated by <ref type="bibr" target="#b52">[53]</ref>: under the ideal disentanglement scenario, each pair of domain-private factors in the manifold space from two different domains should not intersect. Therefore, enlarging the distribution distance between the domain-private I s pri.</p><p>and I t pri. is beneficial to facilitate the instance-level feature disentanglement, which could be mathematically described as minimizing:</p><formula xml:id="formula_9">L ISD?inter = sim(I s pri. , I t pri. ).<label>(8)</label></formula><p>The loss function for the ISD module is defined by integrating the intra-and inter-domain feature disentanglement at the local stage:</p><formula xml:id="formula_10">L ISD = L ISD?intra + L ISD?inter .<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Training and Inference</head><p>Our DDF method is build on Faster-RCNN with a VGG16 and ResNet50 backbone. For model initialization, the backbones are pretrained on the ImageNet classification task <ref type="bibr" target="#b53">[54]</ref> and the weights of the remaining layers are initialized with normal distribution initialization. Following the implementation of <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b39">[40]</ref>, the fixed-weight E b is the model before the conv3 3 layer of the VGG16 backbone, and E s is the rest. The batch size is 1 and we do not use batch normalization due to the small mini-batch size. Each batch contains two images, one from the source domain and the other from the target domain. For all the experiments, the shorter side of the training image is resized to 600, while maintaining the aspect ratio. During training, we employ the SGD optimizer with a momentum of 0.9, and the weight decay is set to 0.0005. The initial learning rate is 0.001 for the first 50K training iterations and decreased to 0.0001 for the last 20K iterations.</p><p>The overall loss function of our proposed DDF is defined as:</p><formula xml:id="formula_11">L ddf = L det + L di + L GT D + L ISD ,<label>(10)</label></formula><p>where L det represents the detection loss function in the standard Faster-RCNN, including the classification and bounding boxes regression loss for the RPN and the final task. Note that we do not include any trade-off weight for each loss in L ddf , which avoids the time-consuming process of hyperparameter fine-tuning. In addition, the overall DDF framework is optimized in an end-to-end manner. The GRL layer between D glb and E s was employed when optimizing Equations 3 and 5. During inference, the target images directly pass through the vanilla Faster-RCNN with off-the-shelf weights for object detection. For evaluation, we report the mean average precision (mAP) with a threshold of 0.5. Our experiment is implemented with PyTorch [55] on one NVIDIA GeForce 1080Ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL DETAILS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset Description</head><p>Our extensive experiments are conducted on four public datasets including Cityscapes <ref type="bibr" target="#b62">[63]</ref>, Foggy Cityscapes <ref type="bibr" target="#b63">[64]</ref>, SIM10K <ref type="bibr" target="#b64">[65]</ref>, and KITTI <ref type="bibr" target="#b65">[66]</ref>.</p><p>CityScapes <ref type="bibr" target="#b62">[63]</ref> dataset contains 5000 images of the real urban scenes collected from 27 cities in different seasons. In this work, the ground truth for UDA object detection is <ref type="table" target="#tab_0">the  TABLE II  QUANTITATIVE COMPARISON WITH STATE-OF-THE-ART METHODS ON THE CITYSCAPES ? FOGGY CITYSCAPES EXPERIMENT. NO DA REPRESENTS THE  FASTER-RCNN TRAINED WITH THE SOURCE IMAGES AND DIRECTLY TESTED ON THE TARGET IMAGES, WITHOUT ANY DOMAIN ADAPTATION. FOR EACH  COMPARISON METHOD, MAP AND GAIN REPRESENT ITS RESULT UNDER "NO DA" SETTING SETTING AND THE CORRESPONDING IMPROVEMENT  ACHIEVED VIA ADAPTATION, RESPECTIVELY. ORACLE DENOTES THE FULLY SUPERVISED FASTER-RCNN TRAINED ON THE TARGET</ref>  tightest bounding boxes for the instance mask annotations, following the previous practices in <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b35">[36]</ref>. The 5000 total images in the original dataset are officially split into training, validation, and testing sets with 2975, 500, and 1525 images, respectively. Foggy CityScapes <ref type="bibr" target="#b63">[64]</ref> dataset simulates foggy scenes in different intensity levels on the CityScapes dataset. Following the previous methods, we employed the images with the highest foggy intensity level. The data split and annotations of the Foggy Cityscapes dataset are the same as the Cityscapes dataset.</p><p>SIM10K <ref type="bibr" target="#b64">[65]</ref> dataset contains 10000 simulated images synthesized by the Grand Theft Auto (GTA) engine, with around 60K bounding box annotations of car instances.</p><p>KITTI <ref type="bibr" target="#b65">[66]</ref> dataset contains 14999 real autonomous driving images obtained from a mid-sized city, with around 80K bounding boxes annotations for object detection study. Following previous works <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>, we employed the training set with 7481 images for all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison Experiments</head><p>1) Adapting from the Normal to Foggy Weather: Enhancing the model generalization ability under different weather conditions is crucial for automatic driving. In this section, we evaluate the model adaptation capability from the urban scene images captured in normal weather to the foggy ones, under the Cityscapes ? Foggy Cityscapes setting. The training set with 2975 images from the Cityscapes and Foggy Cityscapes datasets are employed as the source and target domains, respectively, to train the models. For testing, the models are validated on the validation set of the Foggy Cityscapes dataset with 500 images. Under the same data split, our method is directly compared with the state-of-the-art methods, as shown in <ref type="table" target="#tab_0">Table II</ref>. In addition, we notice that the performance of the vanilla Faster-RCNN without adaptation is different in each comparison method. To this end, we first report the detection performance without adaptation for each method, which is denoted by the mAP and directly copied from the original paper. Next, we report the performance gain of each method (gain = mAP ? mAP ) for a fair comparison, as suggested in <ref type="bibr" target="#b40">[41]</ref>. Note that the performance gain can also be regarded as the adaptation ability for each cross-domain object detection method.</p><p>As illustrated in <ref type="table" target="#tab_0">Table II</ref>, our DDF method has achieved the highest performance gain to the Faster RCNN without UDA. Compared with the Progressive Disentanglement (PD) <ref type="bibr" target="#b20">[21]</ref> and Conditional Domain Normalization (CDN) <ref type="bibr" target="#b50">[51]</ref> methods, which tackle the UDA object detection via feature disentanglement, our method has a better detection performance among most categories (7 out of 8). For the global-level disentanglement in PD <ref type="bibr" target="#b20">[21]</ref>, the calculation for the Mutual Information is suboptimal for decomposing the features in the UDA object detection architectures. In addition, its instance-level feature disentanglement requires reconstruction task learning, which introduces a decoder with auxiliary parameters. On the other hand, our GTD can avoid the influence from the batch size issue, and the ISD module requires no extra parameters, which further demonstrates the superiority and efficiency of our DDF method. During the inference process of CDN <ref type="bibr" target="#b50">[51]</ref>, the target features for detection predictions contain the domain-specific information from both the source and target domains. Therefore, the detection model trained on the source data still suffers from domain bias, which would lead to suboptimal adaptation performance for the target data. By feature disentanglement for both the source and target domains, our DDF method is able to alleviate the influence of the target-specific factors during testing and achieve better adaptation performance.</p><p>Besides, we notice that the Coarse-to-Fine Feature Adaptation (C2F) method <ref type="bibr" target="#b40">[41]</ref> has achieved the best performance under a few categories. Leveraging prototype alignment for the instance-level features, the C2F has a better object classification ability. However, our method can still achieve better overall performance by decomposing the irrelevant factors that cause domain shift. Although the mAP for Prior-DA <ref type="bibr" target="#b58">[59]</ref> has outperformed our DDF method, their mAP is still inferior to ours. MAP indicates the improvement of the proposed UDA method over the vanilla Faster R-CNN without adaption, which can alleviate the influence of the various implementation conditions for the comparison methods and therefore brings fairer comparison. Moreover, we notice that our DDF even outperforms the oracle fully supervised Faster-RCNN over two categories, which further demonstrates the feature adaptation ability of our DDF method.</p><p>To further evaluate the effectiveness of our proposed method with various backbone models, we have also incorporated our DDF with Faster R-CNN using the ResNet50 backbone and compared with the state-of-the-art UDA object detection methods using the same backbone <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b59">[60]</ref>- <ref type="bibr" target="#b61">[62]</ref>. As indicated in <ref type="table" target="#tab_0">Table II</ref>, our DDF method has achieved state-of-the-art detection and adaptation performance, which further demonstrates the robustness of the DDF method under different backbone models. Compared to DIDN <ref type="bibr" target="#b60">[61]</ref> on the disentanglementbased cross-domain object detection via feature reconstruction, our DDF method has shown superior adaptation performance by decomposing the features based on global-level triplet optimization and local-level similarity learning. 2) Adaptation from the Synthetic to Real Scene: Learning from synthetic images is essential for object detection since it alleviates the extensive costs in acquiring the bounding and category annotations for each instance. In this section, we study the UDA object detection for the car object from the synthesized dataset to the real one, i.e., the adaptation from the SIM10K to Cityscapes dataset. During training, the whole SIM10K dataset is employed as the source domain, and the Cityscapes training set with 2975 images as the target domain. The detailed results are shown in <ref type="table" target="#tab_0">Table III</ref>. By outperforming the state-of-the-art methods such as HTCN <ref type="bibr" target="#b35">[36]</ref> and UMT <ref type="bibr" target="#b66">[67]</ref>, the superiority of our proposed DDF method is further demonstrated. Among all the comparison methods, we notice that MAF <ref type="bibr" target="#b11">[12]</ref> has a slightly better performance gain than ours. By outperforming MAF under other settings <ref type="table" target="#tab_0">(Table II, and IV), our DDF method is still more effective.   TABLE IV  QUANTITATIVE COMPARISON WITH STATE-OF-THE-ART METHODS ON THE  UDA OBJECT DETECTION TASKS BETWEEN THE KITTI AND CITYSCAPES  DATASETS. K ? C REPRESENTS ADAPTATION FROM THE KITTI TO THE</ref> CITYSCAPES DATASET, AND VICE VERSA.</p><p>Methods 3) Adaptation between Different Cameras: Due to a large diversity of the hardware devices, there exists a domain shift between two datasets captured through various cameras. In this section, we conduct a UDA car object detection experiment between the KITTI and Cityscapes datasets. For the adaptation from the KITTI to Cityscapes, the whole KITTI dataset is employed as a source domain, and the training set for the Cityscapes dataset is adopted as the target domain. When adapting from the Cityscapes to KITTI, the training set of the Cityscapes is employed as the source domain, and the whole KITTI dataset is used as the target domain. The results are presented in <ref type="table" target="#tab_0">Table IV</ref>. Under the KITTI ? Cityscape setting, our method achieves the best performance, in terms of both the overall mAP and the performance gain. When adapting from Cityscapes to KITTI, we observe that the results for the "No DA" reported in all the other comparison methods <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b68">[69]</ref> are the same. To reach a fair comparison, we directly report the comparisons on the overall mAP for each method, where our DDF also achieves competitive performance.</p><formula xml:id="formula_12">K ? C C ? K Method</formula><p>We have also presented visual comparisons in <ref type="figure" target="#fig_3">Figure 4</ref>, where the Baseline is the basic adaptation method without disentanglement, and DAF is a state-of-the-art UDA object detection method <ref type="bibr" target="#b10">[11]</ref>. It can be seen that our disentanglementbased method generated more true-positive predictions as well as fewer false predictions, further indicating the advantage of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Studies</head><p>In this section, we conduct an ablation analysis of the effectiveness of the proposed modules under the Cityscapes ? Foggy Cityscapes setting. The results are shown in <ref type="table" target="#tab_4">Table V</ref>, where "No DA" represents the Faster-RCNN without domain adaptation and Baseline is the Faster-RCNN with global-level feature alignment (L di in <ref type="figure" target="#fig_2">Equation 3</ref>). The w / o ISD method is implemented by removing L ISD in Equation 10, which can also be treated as Baseline + GTD. Similarly, the w / o GTD method is equivalent to Baseline + ISD that neglects L GT D in Equation <ref type="bibr" target="#b9">10</ref>. Apart from the ablated modules, all the methods have the same implementation details as in Section III-D.</p><p>Aligning the features at the global level, the Baseline outperforms the "No DA" method among all categories. To  further strengthen the model adaptation ability at the global and local levels, the GTD and ISD modules are proposed to bring consistent performance gains under most categories except for "trunk" and "train". As two types of large-scale vehicles, parts of the trains and trunks are more likely to be covered by the fog and thus become obscure and vague for the perception system, compared to the objects from the rest categories. To this end, detecting these two categories under the foggy situation is challenging, which is also proved by the inferior performance achieved by the fully supervised benchmark shown in <ref type="table" target="#tab_0">Table II</ref>. Although solely utilizing the GTD or ISD models incurs a slight performance drop on the "train" class, integrating them can still improve the performance significantly (about 10%). This indicates the cross-domain detection for the "train" class is challenging and requires feature disentanglement at both global and local stages. In addition, we notice that the DDF incorporating ISD and GTD degrades the performance than only using ISD. However, by achieving the best overall detection performance, incorporating GTG with ISD in DDF method is still demonstrated to be superior.</p><p>Moreover, we also include ablation studies on the interdomain ISD (L ISD?inter in Equation 8), intra-domain ISD (L ISD?intra in Equation 7), global-level triplet optimization mechanism (L tri in Equation 5), and the global-level domainspecific classification learning (L ds in Equation 4). The results show performance drop by removing each module, hence demonstrating that all modules are useful. For the globallevel GTD module, we notice the triplet optimization module has a stronger decomposition ability than the domain-specific classifier, as indicated by the larger performance drop when ablated. At the local level, the inter-domain ISD works slightly better than the intra-domain one. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Disentanglement Evidence Analysis</head><p>In this section, we first perform a qualitative analysis on the effectiveness of our disentanglement-based DDF method. For the No DA and Baseline, the visualized features are the outputs of the backbone network. For our DDF method, we visualize the domain-shared features as well as the domainprivate features at the global level. All the features are firstly averaged along their channel dimensions and then normalized to range [0, 255]. Finally, the features are resized and overlapped with the original images for visualization. Note that we only visualize the image-level features. The instance-level features I s sha. , I t sha. , I s pri , and I t pri , on the other hand, are vectors instead of tensors. Therefore, they do not have height and width and cannot be resized and overlaid on the original images for visualization. As shown in <ref type="figure" target="#fig_0">Figures 5 and 1</ref>, the domain-shared features from our DDF at the global stages particularly focus on the instance objects which are crucial for detection task learning. However, the domain-shared features for the Baseline still focus on auxiliary background components, instead of particularly on the foreground instances. Therefore, in the Baseline method, the global-level domain-specific factors are introduced into the detection task learning, which results in the feature entanglement issue and further degrades the model adaptation performance. In addition, we observe the domainprivate features for our DDF particularly concentrate on the information specific to its belonging domain, such as the components that reflect the weather conditions. This further demonstrates our E p and E s successfully extract the private and shared features for the given domains as expected.</p><p>To quantitatively evaluate the disentanglement ability of our DDF method, we calculate the feature distribution discrepancy of the domains under the Cityscape ? Foggy Cityscape setting at both the global and local stages. At the global level, the feature distributions are acquired by average pooling applied on the outputs of the network backbone. For the local-level features, we randomly select 100 foreground object features from each category in each domain. To measure the domain distance, Proxy A-distance (PAD) <ref type="bibr" target="#b69">[70]</ref> and Earth Movers Distance (EMD) <ref type="bibr" target="#b70">[71]</ref> are employed. As shown in <ref type="table" target="#tab_0">Table VI</ref>, our method achieves a shorter feature distribution distance than the Baseline at both the global and local levels. Given a better representation of the domain-invariant features and a lower cross-domain feature discrepancy, our DDF method is proved to be effective in improving the Baseline via feature disentanglement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Model Design Selections</head><p>In this section, we discuss the effectiveness of our DDF methods under different model design choices, including directly considering the similarity between the local-level shared cross-domain features, and introducing the adversarial learning strategies at the local level. The experiments are conducted under the Cityscape ? Foggy Cityscape setting and the results are shown in <ref type="table" target="#tab_0">Table VII</ref>. First, we include a similarity maximization mechanism for the local-level domain-shared features I s sha. and I t sha. , by inducing the cosine similarity between I s sha. and I t sha. to be 1. This selection is referred to as 'ins-simmax' in <ref type="table" target="#tab_0">Table VII</ref>. By directly maximizing the similarity between the cross-domain shared features, the model performance drops significantly under many categories. One of the major reasons is that the cross-domain local-level features are ROIs under several categories, and directly narrowing the distribution gap between them incurs misalignment across different classes.</p><p>Next, we conduct an experiment replacing the similaritybased ISD module of the DDF method at the local level with the same disentanglement strategies as the global level, i.e., using the adversarial learning strategy and triplet optimization. Specifically, the optimization strategies in Equations 3, 4, and 5 are adopted into I s sha. , I t sha. , I s pri. and I t pri. at the local level. The overall method is referred to as 'ins-td' in <ref type="table" target="#tab_0">Table VII</ref>, which achieves less competitive performance than the DDF method with similarity-based feature disentanglement at the local level. Due to the lack of annotations for the target images during training, the instance-level target features might not be accurate without the detection task learning, especially at the early training stage. Therefore, we think the inferior performance of the 'ins-td' method results from the influence of the unstable training process of the adversarial learning strategy. In addition, a similar phenomenon was also observed in <ref type="bibr" target="#b12">[13]</ref> previously: including an adversarial domain discriminator for the local features in a UDA object detection model degrades the overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Computational Complexity Analysis</head><p>In this section, we conduct a computational complexity analysis for each principle module (GTD and ISD) in our proposed DDF method. The computational cost for the ablation studies under the C ? F settings in <ref type="table" target="#tab_4">Table V</ref> is as follows (number of parameters, and training time per iteration): 1) Baseline: 140M , 0.34s/iter; 2) Baseline + GTD: 145M , 0.39s/iter; 3) Baseline + ISD: 145M , 0.51s/iter; and 4) DDF: 145M , 0.54s/iter. For the model size, the auxiliary cost for both ISD and GTD modules is negligible. Although our ISD module brings extra training time, we think it is acceptable given the performance gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this work, we present a novel Domain Disentanglement Faster-RCNN (DDF) method for cross-domain object detection via feature disentanglement. Specifically, we propose a GTD module to decompose the shared and private features within each domain at the global stage, as well as fitting the model batch size restrictions. At the instance level, an ISD module based on inter-and intra-domain similarity optimization is proposed to facilitate the feature disentanglement. Extensive experiments demonstrate the superiority of our DDF method by outperforming state-of-the-art methods on several benchmark UDA object detection tasks. Additionally, the qualitative and quantitative analysis on the disentanglement evidence further indicates the effectiveness of our method on decomposing the domain-specific factors and eliminating the domain-bias from them. In a larger perspective, the feature entanglement issues and the restrictions on model batch size are not limited to cross-domain object detection. With the promising performance on cross-domain object detection, the DDF method can also be extended to other cross-domain object analysis tasks (e.g., segmentation, and tracing) in future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The visualization results on the image-level features. 'No DA' is the Faster R-CNN without adaptation, and 'Baseline' is the Faster R-CNN with basic adversarial domain discriminator. For the No DA and Baseline, the features are collected and visualized from the backbone network. For our method, we simultaneously visualize the global-level domain-shared and domain-private features. All the features and labels are overlapped on the original images. (Best viewed in color and zoom in)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The overall architecture of our proposed DDF method. Please refer to Section III-A for detailed definitions on all the symbols. Note that the M LP module before I s sha. , I t sha. , I s pri. and I t pri. is omitted. (Best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The detailed structure of our proposed Global Triplet Disentanglement (GTD) module in Section III-B. The definitions for all the symbols are the same asFigure 2. (Best viewed in color, and in conjunction withFigure 2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Visual comparison of results achieved under various UDA settings: C ? F : CityScapes ? Foggy CityScapes, S ? C : Sim10K ? CityScapes, K ? C : KITTI ? CityScapes, and C ? K : CityScapes ? KITTI. The bounding box predictions are overlapped on the original images. (Best viewed in color and zoom in)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Qualitative analysis on the disentanglement effectiveness of our DDF method under all UDA settings: C ? F : CityScapes ? Foggy CityScapes, S ? C : Sim10K ? CityScapes, K ? C : KITTI ? CityScapes, and C ? K : CityScapes ? KITTI. The left three columns are the results on the source domain (src), and the right three are on the target domain (tgt). All the features and labels are overlapped on the original images. (Best viewed in color and zoom in)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I DETAILED</head><label>I</label><figDesc>MODEL STRUCTURE OF THE OVERALL ARCHITECTURE. in AND out REPRESENT THE INPUT AND OUTPUT CHANNEL NUMBERS OF EACH LAYER, RESPECTIVELY. k, s, AND p ARE THE KERNEL SIZE, STRIDE, AND PADDING OF EACH LAYER.</figDesc><table><row><cell>Layer</cell><cell>Ep</cell><cell>Hyperparameters</cell></row><row><cell>1</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III QUANTITATIVE</head><label>III</label><figDesc>COMPARISON WITH STATE-OF-THE-ART METHODS ON THE SIM10K ? CITYSCAPES EXPERIMENT.</figDesc><table><row><cell>Methods</cell><cell>mAP</cell><cell>mAP</cell><cell>gain</cell></row><row><cell>No DA</cell><cell>34.0</cell><cell>34.0</cell><cell>?</cell></row><row><cell>Baseline</cell><cell>39.5</cell><cell>34.0</cell><cell>5.5</cell></row><row><cell>DAF [11]</cell><cell>39.0</cell><cell>30.1</cell><cell>8.9</cell></row><row><cell cols="2">SWDA [13] 40.1</cell><cell>34.6</cell><cell>7.7</cell></row><row><cell>MAF [12]</cell><cell>41.1</cell><cell>30.1</cell><cell>11.0</cell></row><row><cell>SCDA [35]</cell><cell>43.0</cell><cell>34.0</cell><cell>9.0</cell></row><row><cell>ATF [14]</cell><cell>42.8</cell><cell>34.6</cell><cell>8.2</cell></row><row><cell>HTCN [36]</cell><cell>42.5</cell><cell>34.6</cell><cell>7.9</cell></row><row><cell>CDTD [37]</cell><cell>42.6</cell><cell>34.6</cell><cell>8.0</cell></row><row><cell>UMT [67]</cell><cell>43.1</cell><cell>34.3</cell><cell>8.8</cell></row><row><cell>C2F [41]</cell><cell>43.8</cell><cell>35.0</cell><cell>8.8</cell></row><row><cell>DDF (ours)</cell><cell>44.3</cell><cell>34.0</cell><cell>10.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V ABLATION</head><label>V</label><figDesc>STUDIES FOR THE GTD AND ISD MODULES ON THE CITYSCAPES ? FOGGY CITYSCAPES EXPERIMENT.</figDesc><table><row><cell>Method</cell><cell cols="2">person rider</cell><cell>car</cell><cell>trunk</cell><cell>bus</cell><cell>train</cell><cell cols="2">moto bicycle mAP</cell></row><row><cell>No DA</cell><cell>23.2</cell><cell cols="3">27.2 32.8 13.0</cell><cell cols="2">23.5 9.3</cell><cell>12.9</cell><cell>25.3</cell><cell>20.9</cell></row><row><cell>Baseline</cell><cell>25.8</cell><cell cols="3">32.8 46.1 21.4</cell><cell cols="3">36.9 24.7 17.9</cell><cell>25.7</cell><cell>28.9</cell></row><row><cell>w / o GTD</cell><cell>33.2</cell><cell cols="3">41.0 51.0 30.0</cell><cell cols="3">43.4 24.0 25.8</cell><cell>35.1</cell><cell>35.4</cell></row><row><cell>w / o L ds</cell><cell>36.6</cell><cell>46.9</cell><cell cols="2">55.2 28.4</cell><cell cols="3">46.3 26.9 28.4</cell><cell>38.7</cell><cell>38.4</cell></row><row><cell>w / o L tri</cell><cell>36.8</cell><cell cols="2">44.7 55.4</cell><cell>26.5</cell><cell>45.1</cell><cell cols="2">27.1 28.8</cell><cell>38.7</cell><cell>37.9</cell></row><row><cell>w / o ISD</cell><cell>35.0</cell><cell cols="3">44.4 51.0 23.4</cell><cell cols="3">42.0 22.3 30.0</cell><cell>38.2</cell><cell>35.8</cell></row><row><cell>w / o L ISD?intra</cell><cell>36.0</cell><cell cols="3">42.2 54.0 27.2</cell><cell cols="3">42.5 25.1 32.2</cell><cell>37.4</cell><cell>37.1</cell></row><row><cell>w / o L ISD?inter</cell><cell>35.7</cell><cell cols="3">44.0 51.5 25.6</cell><cell cols="2">40.5 36.0</cell><cell>28.6</cell><cell>38.5</cell><cell>37.5</cell></row><row><cell>DDF</cell><cell>37.2</cell><cell cols="3">46.3 51.9 24.7</cell><cell cols="3">43.9 34.2 33.5</cell><cell>40.8</cell><cell>39.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI QUANTITATIVE</head><label>VI</label><figDesc>EVALUATION ON THE DOMAIN DISTANCE UNDER THE CITYSCAPES ? FOGGY CITYSCAPES SETTING. G AND I REPRESENT THE FEATURES AT THE GLOBAL AND LOCAL LEVELS, RESPECTIVELY.</figDesc><table><row><cell></cell><cell cols="2">PAD?</cell><cell cols="2">EMD?</cell></row><row><cell></cell><cell>G</cell><cell>I</cell><cell>G</cell><cell>I</cell></row><row><cell>No DA</cell><cell cols="4">1.98 1.69 6.37 10.46</cell></row><row><cell cols="5">Baseline 1.51 0.53 1.83 10.34</cell></row><row><cell>Ours</cell><cell>0.75</cell><cell>0.39</cell><cell>1.38</cell><cell>9.64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VII THE</head><label>VII</label><figDesc>DDF METHOD WITH DIFFERENT MODEL DESIGNS ON THE CITYSCAPES ? FOGGY CITYSCAPES EXPERIMENT. 'INS-SIMMAX' REPRESENTS THE DDF WITH THE DIRECT SIMILARITY MAXIMIZATION BETWEEN THE CROSS-DOMAIN SHARED FEATURES AT THE LOCAL LEVEL. 'INS-TD ' INDICATES THE DDF EMPLOYING TRIPLET DISENTANGLEMENT MODULE AT THE LOCAL LEVEL.</figDesc><table><row><cell>Method</cell><cell>per</cell><cell>rid</cell><cell>car</cell><cell>tru</cell><cell>bus</cell><cell>tra</cell><cell cols="2">moto bic</cell><cell>mAP</cell></row><row><cell cols="4">ins-simmax 32.4 36.6 43.5</cell><cell cols="2">24.1 40.7</cell><cell cols="2">16.5 22.4</cell><cell cols="2">32.2 31.1</cell></row><row><cell>ins-td</cell><cell cols="3">37.6 46.2 51.9</cell><cell cols="2">23.4 43.8</cell><cell cols="2">23.7 34.8</cell><cell cols="2">39.0 37.6</cell></row><row><cell>OG DDF</cell><cell cols="3">37.2 46.3 51.9</cell><cell cols="2">24.7 43.9</cell><cell cols="2">34.2 33.5</cell><cell cols="2">40.8 39.1</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cascade R-CNN: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1521" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Domain adaptive faster R-CNN for object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3339" to="3348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-adversarial faster-rcnn for unrestricted object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6668" to="6677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Strong-weak distribution alignment for adaptive object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6956" to="6965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Domain adaptive object detection via asymmetric tri-way faster-rcnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="309" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="343" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning disentangled semantic representation for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI: proceedings of the conference</title>
		<imprint>
			<publisher>NIH Public Access</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page">2060</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Domain agnostic learning with disentangled representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5102" to="5112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Domain2Vec: Domain embedding for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="756" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised multi-target domain adaptation: An information theoretic approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rudovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3993" to="4002" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Instance-invariant adaptive object detection via progressive disentanglement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A kernel method for the two-sample-problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised visual domain adaptation using subspace alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Habrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sebban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2960" to="2967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adversarial network with multiple classifiers for open set domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shermin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Murshed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multimodal unsupervised image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="172" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">CyCADA: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1989" to="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spa-gan: Spatial attention gan for image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Emami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Aliabadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Chinnam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="391" to="401" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">PDAM: A panoptic-level feature alignment framework for unsupervised domain adaptive instance segmentation in microscopy images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>O&amp;apos;donnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="154" to="165" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cross-domain weakly-supervised object detection through progressive domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Furuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5001" to="5009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cross-view regularization for domain adaptive panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="133" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bidirectional learning for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6936" to="6945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">ADVENT: Adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2517" to="2526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised instance segmentation in microscopy images via panoptic domain adaptation and task re-weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>O&amp;apos;donnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4243" to="4252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adapting object detectors via selective cross-domain alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="687" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Harmonizing transferability and discriminability for adapting object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8869" to="8878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">CDTD: A large-scale cross-domain benchmark for instance-level image-to-image translation and domain adaptive object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maheshwari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Exploring categorical regularization for domain adaptive object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11" to="724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">IFAN: Image-instance full alignment networks for adaptive object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="13" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Cross-domain detection via graph-induced prototype alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="355" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cross-domain object detection through coarse-to-fine feature adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">FSDR: Frequency space domain randomization for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6891" to="6902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Diversify and match: A domain adaptive representation learning paradigm for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">465</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">beta-VAE: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Disentangling factors of variation in deep representation using adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sprechmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5040" to="5048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2172" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Diverse image-to-image translation via disentangled representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="35" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Attending to discriminative certainty for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Kurmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">P</forename><surname>Namboodiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="491" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Informative feature disentanglement for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Adapting object detectors with conditional domain normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="403" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Mutual information neural estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rajeshwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="531" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning to balance specificity and invariance for in and out of domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="301" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Every pixel matters: Center-aware feature alignment for domain adaptive object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y.</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="733" to="748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">RDA: Robust domain adaptation via fourier adversarial attacking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Adaptive object detection with dual multi-label prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="54" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Prior-based domain adaptive object detection for hazy and rainy conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Oza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yasarla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="763" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Exploring object relation in mean teacher for cross-domain detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Domain-invariant disentangled network for generalizable object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8771" to="8780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Uncertainty-aware unsupervised domain adaptation in object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Semantic foggy scene understanding with synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="973" to="992" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Driving in the matrix: Can virtual worlds replace human-generated annotations for real world tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson-Roberson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rosaen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Robotics and Automation</title>
		<meeting>International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Unbiased mean teacher for crossdomain object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4091" to="4101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Domain-specific suppression for adaptive object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9603" to="9612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">MeGA-CDA: Memory guided attention for category-aware unsupervised domain adaptive object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Oza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4516" to="4526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">137</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">The earth mover&apos;s distance as a metric for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rubner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="121" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
