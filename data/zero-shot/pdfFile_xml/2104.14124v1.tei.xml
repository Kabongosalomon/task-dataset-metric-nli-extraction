<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Condensation-Net: Memory-Efficient Network Architecture with Cross-Channel Pooling Layers and Virtual Feature Maps</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-04-29">29 Apr 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tse-Wei</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Motoki</forename><surname>Yoshinaga</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxing</forename><surname>Gao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Canon Information Technology (Beijing) Co., Ltd</orgName>
								<address>
									<addrLine>12A Floor, Yingu Building, No.9 Beisihuanxi Road</addrLine>
									<settlement>Haidian, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Canon Information Technology (Beijing) Co., Ltd</orgName>
								<address>
									<addrLine>12A Floor, Yingu Building, No.9 Beisihuanxi Road</addrLine>
									<settlement>Haidian, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongchao</forename><surname>Wen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Canon Information Technology (Beijing) Co., Ltd</orgName>
								<address>
									<addrLine>12A Floor, Yingu Building, No.9 Beisihuanxi Road</addrLine>
									<settlement>Haidian, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Canon Information Technology (Beijing) Co., Ltd</orgName>
								<address>
									<addrLine>12A Floor, Yingu Building, No.9 Beisihuanxi Road</addrLine>
									<settlement>Haidian, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kinya</forename><surname>Osa</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masami</forename><surname>Kato</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Twchen@ieee</forename><surname>Org</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Device Technology Development Headquarters</orgName>
								<orgName type="institution">Canon Inc</orgName>
								<address>
									<addrLine>30-2, Shimomaruko 3-chome, Ohta-ku</addrLine>
									<postCode>146-8501</postCode>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Condensation-Net: Memory-Efficient Network Architecture with Cross-Channel Pooling Layers and Virtual Feature Maps</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-04-29">29 Apr 2021</date>
						</imprint>
					</monogr>
					<note>arXiv:2104.14124v1 [cs.CV]</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Lightweight convolutional neural networks" is an important research topic in the field of embedded vision. To implement image recognition tasks on a resource-limited hardware platform, it is necessary to reduce the memory size and the computational cost. The contribution of this paper is stated as follows. First, we propose an algorithm to process a specific network architecture (Condensation-Net) without increasing the maximum memory storage for feature maps. The architecture for virtual feature maps saves 26.5% of memory bandwidth by calculating the results of cross-channel pooling before storing the feature map into the memory. Second, we show that cross-channel pooling can improve the accuracy of object detection tasks, such as face detection, because it increases the number of filter weights. Compared with Tiny-YOLOv2, the improvement of accuracy is 2.0% for quantized networks and 1.5% for full-precision networks when the false-positive rate is 0.1. Last but not the least, the analysis results show that the overhead to support the cross-channel pooling with the proposed hardware architecture is negligible small. The extra memory cost to support Condensation-Net is 0.2% of the total size, and the extra gate count is only 1.0% of the total size.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Convolutional Neural Networks (CNNs) are widely used in image and video analysis applications, such as face alignment <ref type="bibr" target="#b19">[20]</ref>, face recognition <ref type="bibr" target="#b16">[17]</ref>, object detection <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>, scene segmentation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>, and so on. In order to implement these algorithms on mobile devices and embedded system platforms, it is necessary to reduce the memory size and the computational cost without decreasing the accuracy.</p><p>Many kinds of algorithms are proposed to handle lightweight networks on low-power devices. These algorithms can be classified into several categories. The first category is "efficient network architecture design," where special operations are applied to reduce the size or the computationcal cost of a network. Howard, et al. propose a network architecture called MobileNets <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16]</ref>, where depthwise separable convolutions are used to reduce the connections between layers. The second category is "bit-width adjustment," where floating-point data are transferred to lowbit data to reduce the memory cost. Rastegari et al. propose a network architecture called XNOR-Net <ref type="bibr" target="#b12">[13]</ref>, where fullprecision filter weights and full-precision feature maps are replaced with 1-bit filter weights and 1-bit feature maps, respectively, so that multiplication operations can be replaced with exclusive NOR (XNOR) operations. The third category is "knowledge distillation," which is a common technique to increase the accuracy of small networks with extra information from other networks. Hinton et al. propose a technique to distill the knowledge from the teacher network into the student network <ref type="bibr" target="#b5">[6]</ref> for training algorithms. When the size of student network is smaller than the teacher network, the technique can be used to increase the accuracy of small networks. In addition to the 3 categories, there are still many other kinds of algorithms for lightweight networks. These techniques can be applied to both hardware and software implementation.</p><p>For hardware implementation in embedded systems, it is important to achieve high performance and high recognition accuracy with compact network models. <ref type="bibr">Boo</ref>   the structured sparsity <ref type="bibr" target="#b0">[1]</ref>, where a rule for look-up tables is applied to the training algorithm. Chen et al. propose a reconfigurable accelerator which contains a Run-Length Coding (RLC) module to compress the feature maps with consecutive zeros <ref type="bibr" target="#b3">[4]</ref>. These techniques can be applied to different kinds of systems to reduce the network size without decreasing the accuracy, but it is difficult to find a systematic way to increase the accuracy of small networks.</p><p>In this paper, we propose a new approach, which is called Condensation-Net, to increase the accuracy of networks with limited hardware resources, including memory cost of feature maps and filter weights. The concept of the proposed method is shown in <ref type="figure" target="#fig_0">Figure 1</ref>, which is separated into 2 parts. The upper part of <ref type="figure" target="#fig_0">Figure 1</ref> shows the convolution process without cross-channel pooling, where the number of input feature maps and the number of output feature maps are N ch,i and N ch,i+1 , respectively. Both the input feature maps and the output feature maps are stored in the memory. The parameter i represents the index of a convolution layer in the network.</p><p>The lower part of <ref type="figure" target="#fig_0">Figure 1</ref> shows the convolution process with cross-channel pooling operations, where the number of output feature maps is ?N ch,i+1 and the number of output feature maps after cross-channel pooling is N ch,i+1 . Since ? is larger than 1, we can use more filter weights to compute the output feature maps than the network architecture in the upper part of <ref type="figure" target="#fig_0">Figure 1</ref>. After computing the output feature maps, cross-channel pooling operations are applied, and ?N ch,i+1 output feature maps will be condensed into N ch,i+1 output feature maps. It is not necessary to store all the output feature maps before cross-channel pooling because the result of cross-channel pooling can be computed sequentially with only a part of output feature maps. That is, we can increase the accuracy of the network by adding filter weights while keeping the number of stored feature maps the same. For embedded computing with limited hardware resources, it is an advantage to improve the accuracy of the network without increasing the memory storage of feature maps.</p><p>There are two main ideas in our approach, cross-channel pooling and virtual feature maps. Cross-channel pooling is a technique commonly used to combine the information of multiple feature maps (channels). Marcos, et al. use cross-channel pooling operations to preserve the rotationinvariant features for texture classification <ref type="bibr" target="#b10">[11]</ref>. Laptev et al. apply an operator called "Transform-Invariant pooling (TI-Pooling)" to the fully-connected layers <ref type="bibr" target="#b8">[9]</ref>. Nguyen et al. propose a sparse temporal pooling network, where a video-level representation is generated via weighted temporal average pooling <ref type="bibr" target="#b11">[12]</ref>. In this paper, we use crosschannel pooling for "condensation," which means to reduce the number of channels while preserving the information. The input feature maps of cross-channel pooling are called "virtual feature maps," which are computed sequentially and NOT stored in the main memory.</p><p>The paper is organized as follows. In Sec. 2, the proposed networks architecture and the algorithm are introduced. The proposed hardware architecture is shown in Sec. 3. The experimental results are discussed in Sec. 4. The conclusions are given in Sec. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Proposed Network: Condensation-Net</head><p>An example of the proposed network, Condensation-Net, is shown in <ref type="figure">Figure 2</ref>. There are 5 sets of feature maps, 2 convolution layers, and 2 cross-channel pooling layers in the network. The 1st set and the 3rd set of feature maps are stored in the memory, but the 2nd set and the 4th set of feature maps, which are the input of the 1st cross-channel pooling layer and the 2nd cross-channel pooling layer, respectively, are not stored in the memory. The 2nd set and the 4th set of feature maps, which are shown in the dotted lines, are also called "virtual feature maps." The 2nd set of feature maps is the output of the 1st convolution layer and is calculated based on the 1st set of feature maps with convolution and activation functions. The 3rd set of feature maps is the output of the 1st cross-channel pooling layer and is calculated based on the 2nd set of feature maps with crosschannel pooling functions. <ref type="figure" target="#fig_1">Figure 3</ref> shows the comparison of spatial pooling operations and cross-channel pooling operations. An illustration of spatial pooling is shown in <ref type="figure" target="#fig_1">Figure 3</ref>(a), where the stride and the window size are both 2 pixels (2 ? 2 pixels) in the spatial domain. The size of input feature map is reduced from 4 ? 4 pixels to 2 ? 2 pixels in the spatial domain. The number of feature maps does not change. An illustration of cross-channel pooling is shown in <ref type="figure" target="#fig_1">Figure 3</ref>(b), where the stride and the window size are both 2 pixels in the channel direction. In this example, the window size for crosschannel pooling has no connection with the spatial domain. The number of input feature maps is 2, and the number of output feature maps is reduced to 1. The resolution of feature maps does not change after cross-channel pooling. The operations of spatial pooling and cross-channel pooling are similar, but the numbers of filter weights required to compute the input feature maps for the 2 pooling algorithms can be different. The number of filter weights for a convolution layer with cross-channel pooling operations might be 2 times larger than the one with spatial pooling operations. The output pixel of cross-channel pooling can be computed based on the max operations, the average operations, the min operations, and so on. <ref type="figure">Figure 4</ref> shows the proposed algorithms. There are 4 levels in the nested loop structure. In the 1st level of loops, each convolution layer of the networks is processed sequentially, and the parameters of the corresponding layer are set. In the 2nd level of loops, each output channel of the layer is processed sequentially. In the 3rd level of loops, each block of the output channel is processed sequentially. In the 4th level of loops, each input channel is processed sequentially. The filter weights of the i-th convolution layer, the j-th output channel, the n-th input channel are set, and the convolution results of ? output blocks are computed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Cross-Channel Pooling</head><p>If cross-channel pooling operations are enabled, the output block to the next layer is calculated and the number of feature maps becomes 1 since the output blocks are reduced by a factor of ? in the channel direction. Otherwise, the output result to the next layer includes the convolution results of ? blocks (channels) and no operations are required. <ref type="figure">Figure 5</ref> shows the relation between the "virtual feature maps" and the feature map memory. There are N ch,i input feature maps (channels) in the upper part of the feature map memory and N ch,i+1 output feature maps in the lower part of the feature map memory. A block is extracted from the input feature maps, and the result of convolution and activation, which is a part of virtual feature maps, is generated. There are ?N ch,i+1 virtual feature maps in total. After cross-channel pooling, the number of output feature maps is reduced to N ch,i+1 , and a block of the output feature maps is generated. The same operation are repeated until all blocks are processed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Virtual Feature Maps</head><p>The memory can store N ch,i channels for the input and N ch,i+1 channels for the output. If all the virtual feature maps are stored, we need an extra storage for ?N ch,i+1 channels. Since the result of cross-channel pooling can be calculated partially, it is not necessary to store all virtual feature maps. By employing the proposed algorithm, only the storage of 1 block is required to compute the result of cross-channel pooling. The results of cross-channel pooling are also called virtual feature maps since it is not necessary to store them in the physical memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Activation Functions Based on Quantization</head><p>The activation functions mentioned in the previous sections can be implemented by using Rectified Linear Unit (ReLU) functions, sigmoid functions, tangent functions, leaky ReLU functions, and so on. When the bit width of feature maps and filter weights are small, the activation functions can also be implemented with quantization functions <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b1">2]</ref>. The zeros in the feature maps can be removed by using cross-channel pooling operations without losing much information especially for low-bit networks (e.g. 1 bit or 2 bits) since the pixels of feature maps contain lots of zeros.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Training Algorithm</head><p>The proposed network with cross-channel pooling can be trained by using the back-propagation algorithm. The following is an example of the training algorithm for crosschannel pooling with the max operations.</p><p>For a cross-channel pooling layer with a stride of 2 pixels and a window size of 2 pixels in the channel direction, the maximum value of 2 input pixels is computed in the  forward-propagation process, and the corresponding channel which has the maximum values is recorded. Then, the gradient value is sent to the recorded channel of the crosschannel pooling layer in the back-propagation process. The steps of forward-propagation process and back-propagation process are repeated until the training results converge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>Proposed Hardware Architecture <ref type="figure" target="#fig_3">Figure 6</ref> shows the proposed hardware architecture. The feature map memory is used to store the input feature maps and the output feature maps, which are the feature maps in 2 consecutive layers in a network. The weight memory is used to store the filter weights, and the layer parameter memory is used to store layer parameters, which indicate the size of filter weights, the interconnections of input and output channels, the stride and the window size of pooling operations, and so on. The convolution results are computed in the convolution cores, and the activation results are computed in the activation unit. The convolution cores and the activation unit are included in the "Convolution Layer Processing Unit (CLPU)," and the result of crosschannel pooling is computed in the "Pooling Layer Processing Unit (PLPU)." The result of cross-channel pooling from the PLPU is a set of blocks of feature maps, which are store in the feature map memory as a part of feature maps.</p><p>When the number of input channel is N ch,i , the convolution unit needs to process N ch,i blocks of input feature maps before generating 1 block of virtual feature map. The convolution unit contains M convolution cores, which are able to process M convolution operations in parallel. To accelerate the computations, it is also feasible to generate multiple virtual feature maps for different channels by using multiple CLPUs. The value of M is set according to the size of network, the requirement of computational speed, and the target size of the hardware. Since the activation unit and the convolution cores work in a pipeline manner, the activation result of the block can be computed when the result of cross-channel pooling is generated. The parameters of PLPU are set according to the layer parameters. When the cross-channel pooling is not enabled, the virtual feature maps are the same as the output feature maps.</p><p>for i = 1, 2, ?, (# of convolution layers) set layer parameters for j = 1, 2, ?, ch,i+1 for m = 1, 2, ?, (# of blocks) for n = 1, 2, ?, ch,i extract the m-th block of the n-th channel set the filter weights for ( i, n, j ) calculate the convolution result of output blocks partially calculate the activation result of output blocks if cross-channel pooling is enabled calculate 1 pooling result and reduce the number of channels by a factor of </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>The experiments results contain 2 parts. The first part is the comparison of accuracy of face detection. The second part is the analysis of the memory cost of the proposed hardware architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparison of Accuracy</head><p>We evaluate the 2 networks shown in <ref type="table" target="#tab_4">Table 1</ref>. The first one is Tiny-YOLOv2, which is a compact network for object detection <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18]</ref>. The second one is the proposed Condensation-Net, where the parameters are extended from Tiny-YOLOv2. In Condensation-Net, the 1st -the 4th convolution layers are replaced by the combinations of convolution layers and cross-channel pooling layers shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The reason to choose the 1st -the 4th convolution layers is that the memory cost of filter weights in these layers is relatively small compared to the total cost. In the 1st convolution layer, the filter size of Condensation-Net is 16? ? 3 ? 3, which means that the number of output channels, N ch,1 , is 16?, and both the width and the height are 3. The number of input channels is 3 since there are 3 channels (RGB) in the input image. For Condensation-net, the value of ? is set to 2 or 4, and Tiny-YOLOv2 can be regarded as a special case of Condensation-Net when ? = 1. The cross-channel pooling layers can be added to different layers according to the requirements of applications.</p><p>The images in the training set of Wider Face <ref type="bibr" target="#b18">[19]</ref> are used to train the 2 networks, Tiny-YOLOv2 and Condensation-Net. We compare the accuracy of the networks on the Face Detection Data Set and Benchmark (FDDB) <ref type="bibr" target="#b7">[8]</ref>, which contains 5,171 faces in 2,845 test images. The ROC curves of the networks are shown in <ref type="figure" target="#fig_4">Figure 7</ref>, which shows that Condensation-Net achieves better detection rate than Tiny-YOLOv2 especially when the number of false positive is small. To further evaluate the performance of the face detector, we compare the detection rate (true-positive rate) when the false-positive rate is   <ref type="table" target="#tab_5">Table 2</ref>.</p><p>As mentioned in Sec. 1, one method to reduce the memory size is quantization. We quantize the network using the Half-Wave Gaussian Quantization (HWGQ) algorithm, where full-precision filter weights and full-precision feature maps are replaced with 1-bit filter weights and 2-bit feature maps, respectively <ref type="bibr" target="#b1">[2]</ref>. For quantized networks and fullprecision networks, Condensation-Net achieves higher accuracy than Tiny-YOLOv2 because Condensation-Net contains more filter weights than Tiny-YOLOv2. It means that the proposed cross-channel pooling layers can be applied to either quantized networks or full-precision networks to increase the accuracy. However, when ? is increased from 2 to 4, the number of filter weights in the 1st -the 4th convolution layer doubles, but the accuracy decreases. The reason can be that some information contained in the parameters, which is essential to increasing the accuracy of face detection, is removed by cross-channel pooling. Similar to spatial pooling operations, where the window size and the stride are set to 2 ? 2 pixels for many applications, the parameter ? = 2 is found to be the optimal value for the proposed cross-channel pooling on face detection tasks according to the experimental result.</p><p>In the spatial pooling layers, we re-use the parameters of Tiny-YOLOv2 and employ the max operations to compute the result of spatial pooling. However, to compare the accuracy, we employ the max operations and the average operations to compute the result of cross-channel pooling in the 1st -the 4th convolution layers. The accuracy of these two kinds of operations is similar, but the max operations achieve slightly better performance than the average operations except for the full-precision network with ? = 2. According to the experimental results, we employ the max operations for cross-channel pooling on face detection tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Analysis of Memory Size</head><p>The comparison of the required memory size of the proposed hardware architecture for different networks is shown in <ref type="table" target="#tab_7">Table 3</ref>. As shown in <ref type="figure" target="#fig_3">Figure 6</ref>, the proposed hardware architecture stores the data of 2 consecutive feature maps and all of the filter weights in the network. The maximum size of input images is 512 ? 512 pixels ?8 bits. For the convolution layers where ? is set to 2 or 4, we only need to store 1/? of feature maps after convolution operations because of cross-channel pooling. The size of the weight memory is 1,924 KB for Tiny-YOLOv2, and the sizes of the weight memory are 1,935 KB and 1,959 KB for Condensation-Net when ? = 2 and ? = 4, respectively. The memory sizes of the feature map memory for Tiny-YOLOv2 and Condensation-Net (? = 2 or ? = 4) are both 4,096 KB. They are exactly the same since we do not have to store the virtual feature maps as mentioned in Sec. 2.2. The numbers of feature maps of Tiny-YOLOv2 and Condensation-Net are different after convolution operations, but they become the same after applying crosschannel pooling to Condensation-Net. The memory sizes of the weight memory for Tiny-YOLOv2 and Condensation-Net are slightly different because Condensation-Net requires more filter weights than Tiny-YOLOv2 to compute the results of convolution. However, as shown in <ref type="table" target="#tab_4">Table 1</ref>, since the numbers of channels in the 1st -the 4th convolution layers are small, the difference of filter weights be- 30 ? 1 ? 1 * The stride in the 1st -the 4th convolution layers (?) in Condensation-Net represents the stride in the channel direction, not in the spatial domain. The stride for the pooling layers represents the stride in the spatial domain, not in the channel direction.  tween the 2 networks is also small. As shown in <ref type="table" target="#tab_5">Table 2</ref> and <ref type="table" target="#tab_7">Table 3</ref>, the total size of the weight memory and the feature map memory for Tiny-YOLOv2 is 6,020 KB, and the detection rate of face detection is 89.87%. By using the proposed techniques, we can increase the detection rate by adding memory storage with negligible costs. The total size of the weight memory and the feature map memory for Condensation-Net (? = 2) is 6,031 KB, but the detection rate of face detection is 91.82%, which is higher than Tiny-YOLOv2. The overhead of the   proposed hardware architecture is 11 KB, which is only 0.2% of the total memory size. <ref type="figure">Figure 8</ref> shows the comparison of the required memory size of all layers. In our hardware architecture, we only need to store the feature map of 2 consecutive layers, but the memory access of feature maps in all layers is inevitable. When ? = 2, the required memory size of Condensation-Net is 9,788 KB, but we can reduce the memory access to 7,740 KB with the virtual feature maps. It means that we can save 26.5% of memory bandwidth. The larger the feature map, the more memory access can be saved.</p><p>The specifications of the proposed hardware architecture, which is designed to process the quantized Condensation-Net (? = 2), are shown in <ref type="table" target="#tab_9">Table 4</ref>   The comparison of hardware architectures for low-bit quantized CNNs is shown in <ref type="table" target="#tab_10">Table 5</ref>. It is difficult to compare the proposed hardware architecture with the related works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21]</ref> because they are implemented on FPGA platforms. Since the proposed architecture is synthesized with the cell-based design library, it generally achieves higher clock frequency than FPGAs. The proposed hardware architecture can achieve 7.72 GOPS / kLUT for 1-bit -2-bit operations. The performance density is higher than the related work <ref type="bibr" target="#b20">[21]</ref> without considering the cost of DSPs in FPGA, and the supported bit width is larger than the related work <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future Work</head><p>In this paper, we propose a new network architecture, Condensation-Net, which combines cross-channel pooling with a specific processing order for virtual feature maps. The experimental results show that the proposed algorithm achieves higher accuracy than Tiny-YOLOv2 for face detection applications, where the dataset of FDDB are used for training and the Wider Face dataset are used for testing. The overhead of memory size to support the cross-channel pooling functions is only 0.2%, and the extra gate count is only 2K gates. Besides, the architecture for virtual feature maps saves 26.5% of memory bandwidth by calculating the results of cross-channel pooling before storing the feature map into the memory.</p><p>The proposed network can be implemented on different hardware platforms. For future work, we plan to apply the proposed method to different applications, including image segmentation and face recognition, and test the accuracy of specific image-recognition tasks. In addition to Tiny-YOLOv2, we will apply cross-channel pooling to other kinds of networks and analyze the tradeoff among the memory cost, the processing speed, and the accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Concept of cross-channel pooling and virtual feature maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of spatial pooling and cross-channel pooling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Proposed algorithm. Relation between virtual feature maps and memory.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Overview of the proposed hardware architecture. 0.1 (1 false positive / 10 test images), which corresponds to ?2, 845 ? 0.1? = 284 faces in 2,845 test images. The comparison results are shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>FullFigure 7 .</head><label>7</label><figDesc>Condensation-Net (Max Pooling, ?=4) Full-Precision Condensation-Net (Max Pooling, ?=2) Full-Precision Condensation-Net (Average Pooling, ?=4) Full-Precision Condensation-Net (Average Pooling, ?=2) False Positives Quantized Condensation-Net (Max Pooling, ?=4) Quantized Condensation-Net (Max Pooling, ?=2) Quantized Condensation-Net (Average Pooling, ?=4) Quantized Condensation-Net (Average Pooling, ?=2) Comparison of ROC curves for (a) full-precision networks and (b) quantized networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>et al. propose an architecture to compress the ternary weights by utilizing</figDesc><table><row><cell></cell><cell>( ch,i ? ch,i+1 ) Weights</cell><cell cols="2">Network Architecture Without Cross-Channel Pooling</cell></row><row><cell>ch,i</cell><cell></cell><cell>ch,i+1</cell></row><row><cell></cell><cell>Convolution +Activation</cell><cell></cell></row><row><cell>Input Feature Maps</cell><cell cols="2">Output Feature Maps</cell></row><row><cell>ch,i</cell><cell>Weights ( ? ch,i ? ch,i+1 )</cell><cell cols="2">Network Architecture With Cross-Channel Pooling ch,i+1 ( &gt;1) ? ch,i+1</cell></row><row><cell></cell><cell>Convolution +Activation</cell><cell>Cross-Channel Pooling (Condensation)</cell></row><row><cell>Input Feature Maps</cell><cell cols="2">Virtual Feature Maps</cell><cell>Output Feature Maps</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Figure 2. Example of the proposed network, Condensation-Net.</figDesc><table><row><cell>Feature Maps (Set 1)</cell><cell cols="2">Virtual Feature Maps (Set 2)</cell><cell cols="2">Feature Maps (Set 3)</cell><cell>Virtual Feature Maps (Set 4)</cell><cell>Feature Maps (Set 5)</cell></row><row><cell cols="2">Conv. Layer 1 (Convolution+ Activation)</cell><cell cols="2">Cross-Channel Pooling Layer 1</cell><cell cols="2">Conv. Layer 2 (Convolution+ Activation)</cell><cell>Cross-Channel Pooling Layer 2</cell></row><row><cell cols="2">4 Pixels</cell><cell>1 Pixel</cell><cell></cell><cell></cell></row><row><cell cols="3">Spatial Pooling</cell><cell></cell><cell></cell></row><row><cell>(4 x 4 Pixels) Input</cell><cell></cell><cell>Output (2 x 2 Pixels)</cell><cell></cell><cell></cell></row><row><cell cols="3">(a) Spatial Pooling</cell><cell></cell><cell></cell></row><row><cell>2 Pixels</cell><cell></cell><cell>1 Pixel</cell><cell></cell><cell></cell></row><row><cell cols="3">Cross-Channel Pooling</cell><cell></cell><cell></cell></row><row><cell>Input (4 x 4 Pixels)</cell><cell></cell><cell cols="2">Output (4 x 4 Pixels)</cell><cell></cell></row><row><cell cols="3">(b) Cross-Channel Pooling</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 .</head><label>1</label><figDesc>Network Architecture and Filter Weights of Tiny-</figDesc><table><row><cell>YOLOv2 and Condensation-Net</cell><cell></cell><cell></cell></row><row><cell cols="3">Quantized Full-Precision</cell></row><row><cell></cell><cell>Network</cell><cell>Network</cell></row><row><cell>Tiny-YOLOv2</cell><cell>89.87%</cell><cell>92.28%</cell></row><row><cell>Condensation-Net</cell><cell></cell><cell></cell></row><row><cell>? = 2, Max Pooling 1</cell><cell>91.82%</cell><cell>93.86%</cell></row><row><cell>? = 2, Avg. Pooling 2</cell><cell>91.13%</cell><cell>93.69%</cell></row><row><cell>? = 4, Max Pooling 1</cell><cell>90.25%</cell><cell>93.61%</cell></row><row><cell>? = 4, Avg. Pooling 2</cell><cell>90.74%</cell><cell>93.56%</cell></row></table><note>1 Cross-channel pooling with the max operations.2 Cross-channel pooling with the average operations.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 .</head><label>2</label><figDesc>Accuracy of Face Detection (False-Positive Rate = 0.1)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 .</head><label>3</label><figDesc>Memory Size of Proposed Hardware Architecture</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 .</head><label>4</label><figDesc>Specifications of Proposed Hardware Architecture the 28-nm CMOS technology library for the experiments. The performance is 320 Multiply-Accumulate operations (MACs) per clock cycle for 2-bit feature maps and 1-bit filter weights, which means that the value of M is set to 320. As shown inFigure 6, since the cross-channel pooling operations can be enabled or disabled in different layers, the hardware architecture can also process Tiny-YOLOv2, which includes no cross-channel pooling operations. The gate counts of the CLPU and the PLPU are 197K and 2K, respectively. The gate count of the PLPU is a small number compared to the gate count of the whole hardware architecture, 199K. It means that we can support cross-channel pooling functions by increasing only 1.0% of the hardware resources. Compared with Tiny-YOLOv2, it takes 1.3 times of processing time to compute the inference result of Condensation-Net. Since the proposed hardware can support both Tiny-YOLOv2 and Condensation-Net, the target networks can be switched according to the target processing time.</figDesc><table><row><cell cols="2">Device Clock</cell><cell>Bit</cell><cell>Performance</cell></row><row><cell></cell><cell></cell><cell>Width</cell><cell>Density</cell></row><row><cell></cell><cell cols="3">(MHz) (bits) (GOPS / kLUT)</cell></row><row><cell>[10] FPGA</cell><cell>90</cell><cell>1</cell><cell>22.40</cell></row><row><cell>[21] FPGA</cell><cell>143</cell><cell>1 -2</cell><cell>4.43</cell></row><row><cell>Ours ASIC</cell><cell>400</cell><cell>1 -2</cell><cell>7.72  *</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* 1 LUT = 6 two-input NAND gates.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 .</head><label>5</label><figDesc>Comparison of Hardware Architectures for Low-Bit Quantized CNNs</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Structured sparse ternary weight coding of deep neural networks for efficient hardware implementations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoonho</forename><surname>Boo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonyong</forename><surname>Sung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Workshop on Signal Processing Systems (SiPS)</title>
		<meeting>IEEE International Workshop on Signal Processing Systems (SiPS)</meeting>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Jian Sun, and Nuno Vasconcelos. Deep learning with low precision by half-wave Gaussian quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.00953</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">DeepLab: Semantic image segmentation with deep convolutional nets, Atrous convolution, and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Hsin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">S</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivienne</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="127" to="138" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A review on deep learning techniques applied to semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Orts-Escolano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergiu</forename><surname>Oprea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Villena-Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Garcia-Rodriguez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06857</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">MobileNets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">FDDB: A benchmark for face detection in unconstrained settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vidit</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno>UM-CS-2010-009</idno>
		<imprint>
			<date type="published" when="2010" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">TI-POOLING: Transformation-invariant pooling for feature learning in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><forename type="middle">M</forename><surname>Buhmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.06318</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A GPU-outperforming FPGA accelerator architecture for binary convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengbo</forename><surname>Ren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.06392</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning rotation invariant convolutional filters for texture classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devis</forename><surname>Tuia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.06720</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Weakly supervised action localization by sparse temporal pooling network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phuc</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05080</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">XNOR-Net: ImageNet classification using binary convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05279</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">YOLO9000: Better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Kaiming He anbd Ross Girshick, and Jian Sun. Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shaoqing Ren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01497</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.04381</idno>
		<title level="m">MobileNetV2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">DeepID3: Face recognition with very deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.00873</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Zulkalnain bin Mohd Yussof, Sani Irwan bin Salim, and Lim Kim Chuan. Fixed point implementation of Tiny-YOLO-v2 using OpenCL on FPGA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">June</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Advanced Computer Science and Applications (IJACSA)</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="506" to="512" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">WIDER FACE: A face detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Joint face detection and alignment using multi-task cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.02878</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Accelerating binarized convolutional neural networks with software-programmable FPGAs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ritchie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeng-Hau</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mani</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajesh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiru</forename><surname>Zhang1</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</title>
		<meeting>ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</meeting>
		<imprint>
			<date type="published" when="2017-02" />
			<biblScope unit="page" from="15" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuchang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuheng</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06160</idno>
		<title level="m">DoReFa-Net: Training low bitwidth convolutional neural networks with low bitwidth gradients</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
