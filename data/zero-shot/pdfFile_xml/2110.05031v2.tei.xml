<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EDFace-Celeb-1M: Benchmarking Face Hallucination with a Million-scale Dataset</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihao</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxu</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Luo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyu</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
						</author>
						<title level="a" type="main">EDFace-Celeb-1M: Benchmarking Face Hallucination with a Million-scale Dataset</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T11:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Face Hallucination</term>
					<term>Face Super-resolution</term>
					<term>Benchmarking</term>
					<term>Million-scale Dataset</term>
					<term>EDFace-Celeb-1M !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent deep face hallucination methods show stunning performance in super-resolving severely degraded facial images, even surpassing human ability. However, these algorithms are mainly evaluated on non-public synthetic datasets. It is thus unclear how these algorithms perform on public face hallucination datasets. Meanwhile, most of the existing datasets do not well consider the distribution of races, which makes face hallucination methods trained on these datasets biased toward some specific races. To address the above two problems, in this paper, we build a public Ethnically Diverse Face dataset, EDFace-Celeb-1M, and design a benchmark task for face hallucination. Our dataset includes 1.7 million photos that cover different countries, with relatively balanced race composition. To the best of our knowledge, it is the largest-scale and publicly available face hallucination dataset in the wild. Associated with this dataset, this paper also contributes various evaluation protocols and provides comprehensive analysis to benchmark the existing state-of-the-art methods. The benchmark evaluations demonstrate the performance and limitations of state-of-the-art algorithms. https://github.com/HDCVLab/EDFace-Celeb-1M</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Human faces contain important identity information and are central to various vision applications, such as face alignment <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, face parsing <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> and face identification <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. However, most of these applications require highquality images as input and the approaches perform less favorably in low-resolution conditions. To alleviate the issue, the task of face hallucination, or face super-resolution, aims to super-resolve low-resolution face images to their high-resolution counterparts, thus facilitating effective face analysis.</p><p>As a special case of Single Image Super-Resolution (SISR) <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, face hallucination is a fundamental and challenging problem in face analysis. Different from general SISR, which deals with super-resolving pixels in arbitrary scenes, the face hallucination task tackles only facial images. Therefore, the facial special prior knowledge in face images could help recover accurate face shape and rich facial details. As a result, the face hallucination methods often achieve better performance than general single image super-resolution ones in terms of higher up-scaling factors. Previous face hallucination methods <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> utilize facial priors to restore high-resolution images by approaches which are typically not end-to-end ones. Currently, a number of deep learning based methods <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref> are proposed to greatly boost the performance of the task of face hallucination, even surpassing human ability.</p><p>Is face hallucination solved? Firstly, many current deep hallucination methods evaluate their methods on non-public synthetic datasets. Especially, they typically download public datasets and synthesize pairs of low-resolution and highresolution images, and then randomly select some faces as training and testing samples. After that, they evaluate their methods and re-train previous methods on their synthesized datasets. However, using this way for evaluation presents two obvious problems. <ref type="bibr" target="#b0">(1)</ref> Given that the division of training/testing groups is random, the following researchers cannot strictly follow and reproduce the previous experiments like <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>. <ref type="bibr" target="#b1">(2)</ref> To verify that the new face hallucination methods outperform the previous methods, researchers have to synthesize new datasets by themselves and re-train previous methods again, which greatly increases the unnecessary workload and reduces the credibility of their results like <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b32">[33]</ref>. Therefore, it is not clear how these algorithms would perform on public face hallucination datasets.</p><p>On the other hand, though it is popular that the current deep face hallucination methods are evaluated on synthesized face datasets, these datasets face the problem of being biased toward specific races, and other races are significantly ignored. This scheme along with these datasets not only fails to accurately evaluate the performance of face hallucination methods, but also will raise the ethical problem. Therefore, a large-scale face hallucination dataset with relatively balanced race composition is necessary for face analysis in the community.</p><p>To address the above problems, in this paper, we introduce an Ethnically Diverse Face dataset called EDFace-Celeb-1M, and design benchmark protocols along with analysis to evaluate and encourage the development of face hallucination algorithms. There exist three key objectives of creating the EDFace-Celeb-1M dataset for face hallucination.</p><p>(1) It should contain a large-scale set of face images in the wild with unconstrained pose, emotion and exposure. Specially, the proposed EDFace-Celeb-1M dataset includes 1.7 million photos of more than 40, 000 unique celebrity subjects. <ref type="bibr" target="#b1">(2)</ref> The dataset should include faces from as many different countries and races as possible to mitigate the race bias in the current face hallucination datasets. More specifically, the EDFace-Celeb-1M dataset contains different race groups including White, Black, Latino and Asian with a relatively balanced composition. (3) It should be publicly available, enabling benchmarking current and future face hallucination methods with a unified dataset protocol to ensure fair and effortless evaluation. In this paper, the proposed dataset is public available with fixed training and testing sets for fair comparison.</p><p>To investigate the performance of current deep face hallucination algorithms on the constructed dataset with relatively balanced race composition, we design and provide concrete evaluation protocols, and evaluate four publicly available face hallucination methods and four SISR methods. We also introduce in detail our experiment setup and report baseline models to benefit and drive future research for the face hallucination task and inspire other related tasks in the computer vision community.</p><p>Our main contributions are summarized as follows.</p><p>? First, to the best of our knowledge, we build the first large-scale publicly available face hallucination dataset with relatively balanced race composition. The dataset includes 1.7 million face images collected from different race groups, providing fixed training and testing groups, pairs of low-resolution and high-resolution images with different scale factors (e.g., 2?, 4?, 8?), and aligned and non-aligned face images, which makes the future comparison more convenient, repeatable and credible. ? Second, we design a benchmark task to evaluate the performance of some current deep face hallucination and SISR methods to super-resolve low-resolution images. By doing so, it is clear how these algorithms perform on public face hallucination datasets (See Section 4). ? Third, we address fundamental questions of face hallucination and obtain several key findings.</p><p>-How well do the current face hallucination and SISR methods perform in the case of different upsampling factors? (See <ref type="table" target="#tab_1">Table 2</ref> and <ref type="figure">Figure 8</ref>,9,10,11) -How do the noise and blur kernels affect the performance of face hallucination methods? (See <ref type="table" target="#tab_1">Table 2</ref> and <ref type="figure">Figure 8</ref>,9,10,11) -Is the size of training data important? (See <ref type="table" target="#tab_3">Table 4</ref> and <ref type="figure" target="#fig_5">Figure 7</ref>) -Using the super-resolved images for the facial analysis tasks like landmark detection and identity preservation, is the gap significant, compared with using the ground truth high-resolution face images? (See <ref type="table" target="#tab_2">Table 3</ref>)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Face Hallucination</head><p>Many approaches have been proposed for face hallucination, which can be classified into two categories: non-deep learning methods and deep learning methods. For the non-deep learning methods, holistic-based <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref> and part-based <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref> techniques are two popular models, which upsample face images via representing faces by parameters and extracting facial regions, respectively. Recently, deep neural networks have been successfully applied to various computer vision tasks including face hallucination. Yu et al. <ref type="bibr" target="#b34">[35]</ref> investigate the Generative Adversarial Network (GAN) to super-resolve face images of very low resolution and create perceptually realistic highresolution face images. Huang et al. <ref type="bibr" target="#b28">[29]</ref> introduce wavelet coefficients prediction into deep networks to generate superresolution face images with different upscaling factors. To train deep face hallucination networks, Zhang et al. <ref type="bibr" target="#b40">[41]</ref> propose a super-identify loss function to measure the difference of identity information. Cao et al. <ref type="bibr" target="#b29">[30]</ref> design a novel attention-aware face hallucination framework and use deep reinforcement learning to optimize its parameters.</p><p>As a domain-specific super-resolution problem, there are also many face hallucination methods that use facial prior knowledge to help super-resolve low-resolution face images. Song et al. <ref type="bibr" target="#b41">[42]</ref> propose a two-stage framework, which firstly generates facial components to represent the basic facial structures and then synthesizes fine-grained facial structures through a component enhancement method.</p><p>Yu et al. <ref type="bibr" target="#b31">[32]</ref> present a multi-task upsampling network to employ the image appearance similarity and exploit the face structure information with the help of the proposed facial component heat maps. Chen et al. <ref type="bibr" target="#b30">[31]</ref> introduce a FSRNet model to make use of facial landmark heat maps and parsing maps. In addition, the attention mechanism <ref type="bibr" target="#b42">[43]</ref> and bi-network <ref type="bibr" target="#b27">[28]</ref> are also applied to make use of facial prior knowledge to train a high-resolution face generator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Single Image Super-Resolution</head><p>The advancement of deep neural networks has achieved great success on image super-resolution, and most of the state-of-the-art SISR methods are based on deep learning <ref type="bibr" target="#b43">[44]</ref>. As a pioneer work of deep SISR methods, Dong et al. <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b44">[45]</ref> propose a Super-Resolution Convolutional Neural Network (SRCNN) to super-resolve low-resolution images by firstly adopting deep learning for SISR. After that, many improvements have been explored. For example, Kim et al. <ref type="bibr" target="#b45">[46]</ref> propose a deeply-recursive CNN to make use of skip connections to train their proposed a Deeply-Recursive Convolutional Network (DRCN). Lim et al. <ref type="bibr" target="#b10">[11]</ref> design an Enhanced Deep Super-Resolution Network (EDSR) to remove redundant modules and combine with multi-scale processing. To reduce the computational cost, many efficient SISR methods are proposed <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>. To make the generated images more realistic, GAN based SISR methods are introduced to improve the perceptual quality of HR images <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>. Recently, Hairs et al. <ref type="bibr" target="#b14">[15]</ref> develop a Deep Back-Project Network (DBPN) to exploit the mutual dependencies with a feedback mechanism. Zhang et al. <ref type="bibr" target="#b12">[13]</ref> introduce dense connections to make use of cues. Residual Channel Attention Networks (RCAN) is proposed in <ref type="bibr" target="#b13">[14]</ref> to introduce a residual-in-residual structure and a channel attention module. More recently, there are many SISR methods utilizing novel attention modules <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b49">[50]</ref>  </p><formula xml:id="formula_0">Dataset Size Public Race HR-LR LFW 13K - ? CelebA 200K - ? CASIA-WebFace 500K - ? FB-DeepFace 4.4M ? - ? VGGFace 2.6M - ? VGGFace2 3.3M - ? UMDFaces 367K - ? FaceScrub 100K - ? MegaFace 1M - ? FairFace 10K ? MS-Celeb-1M 10M - ? Ours 1.7M</formula><p>or feedback mechanisms <ref type="bibr" target="#b50">[51]</ref> to further improve the performance of SISR. Even without considering facial structures, these above methods can also super-resolve low-resolution face images to their corresponding high-resolution versions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Face Datasets</head><p>Currently, there does not exist publicly available face hallucination datasets. In this section, we briefly review some popular face datasets which have been constructed recently. The Labeled Faces in the Wild (LFW) dataset <ref type="bibr" target="#b51">[52]</ref> is created in 2007, and it contains 13, 000 images. In 2014, the CelebA <ref type="bibr" target="#b52">[53]</ref> and CASIA-WebFace <ref type="bibr" target="#b53">[54]</ref> datasets are released, including about 20K and 500K images, respectively. The VGGface <ref type="bibr" target="#b54">[55]</ref> dataset released in 2015 includes 2.6 million images. More recently, Kemelmacher-Shlizerman et al. <ref type="bibr" target="#b55">[56]</ref> assemble a dataset of 4.7 million images to evaluate how face recognition algorithms perform with a very large number of images. Cao et al. <ref type="bibr" target="#b56">[57]</ref> release the VGGface2 dataset. Compared to the VGGface dataset, VGGface2 has 3.3 million images to cover a larger number of identities. The largestscale face dataset is MS-Celeb-1M, which contains 10 million images for training and testing.</p><p>Even though there exist many face datasets, none of them can be directly utilized to evaluate the current face hallucination approaches, due to the following reasons. Firstly, none of these datasets provides large-scale pairs of low-resolution and high-resolution face images. However, the current deep face hallucination methods mainly rely on supervised learning and thus pairs of training face images are necessarily required. Secondly, researchers in the computer vision community have paid increasing attention to the race bias problem. However, these existing datasets are strongly biased toward specific races. Face hallucination models trained on these datasets will generate highresolution face images with inappropriate race information.</p><p>The FairFace <ref type="bibr" target="#b57">[58]</ref> contains face images from different races. However, it includes only 10K images without pairs of face images for the evaluation of face hallucination methods. A summary of current face datasets is listed in <ref type="table" target="#tab_0">Table 1</ref> to give a clear view. To overcome the problem of lacking face hallucination datasets, current face hallucination researchers synthesize pairs of training and testing samples to evaluate the previous methods and their proposed ones. For example, Yu et al. <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b35">[36]</ref> and Kim et al. <ref type="bibr" target="#b42">[43]</ref> train and test their proposed methods on pairs of face images synthesized from CelebA <ref type="bibr" target="#b58">[59]</ref>. Zhang et al. <ref type="bibr" target="#b59">[60]</ref> synthesize pairs of face images from Multi-PIE <ref type="bibr" target="#b60">[61]</ref> and CelebA <ref type="bibr" target="#b58">[59]</ref>. WaveletSR <ref type="bibr" target="#b28">[29]</ref> is evaluated on synthesized face images from VGGface2 <ref type="bibr" target="#b56">[57]</ref>, and Li et al. <ref type="bibr" target="#b61">[62]</ref> train and test their methods on the FFHQ <ref type="bibr" target="#b62">[63]</ref>, VGGface2 <ref type="bibr" target="#b56">[57]</ref> and CelebA <ref type="bibr" target="#b58">[59]</ref> datasets. However, most of these synthesized face hallucination datasets are not public. Meanwhile, given that some of them are randomly split training and testing samples, the face hallucination community faces the following challenge: all the following methods cannot directly use the pre-trained models and their results like (Peak Signal-to-Noise Ratio (PSNR )) and (Structural Similarity Index (SSIM)) values, and thus have to synthesize datasets by themselves again and re-train previous face hallucination methods. Therefore, it increases a lot of unnecessary duplication of works. In order to benchmark current deep face hallucination methods and provide convenience to the future face hallucination research, we provide a large-scale publicly available face hallucination dataset with relatively balanced race composition and contribute several baseline models in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EDFACE-CELEB-1M COLLECTION</head><p>In this section, we provide an overview of the EDFace-Celeb-1M dataset and introduce how it is collected in detail. We build the dataset to benchmark the current deep face hallucination methods and drive the development of the face hallucination task in the future. As mentioned above, we aim to build a publicly available large-scale face hallucination dataset, which provides pairs of low-resolution and high-resolution face images with a fixed setting of training and testing samples.</p><p>The dataset collection processing includes: how a list of candidate identities is obtained, how the candidate images are collected, how to detect the face in images, and how  to synthesize pairs of low-resolution and high-resolution images. In addition, we provide attribute statistics of the proposed dataset such as race and gender.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Stage I: Obtain a Name List</head><p>The first step from scratch is to have a list of subject names whose faces we aim to collect. As mentioned before, race balance is the top priority when we build this dataset. To obtain such a name list with race balance, we split the races into four groups: White, Black, Asian and Latino. Based on this, we collect names for different groups. More specifically, for each group, we collect as many names of celebrities as possible from different countries. The names in our list are from diverse countries. For example, the Asian group includes names of people from East Asian, Southeast Asian, Middle East and India from Asian countries. In addition, it also includes some Asian-Americans. In summary, our EDFace-Celeb-1M dataset contains more than 20, 000 names and the ratio (name lists) of the above four groups is about 31.1% : 19.2% : 19.6% : 18.3%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Stage II: Select Images for Each Identity</head><p>After obtaining a name list, we use the Google Image Search engine to download 100 ? 1000 images for each identity. Moreover, to obtain diverse images with age variations, we further add the keyword young to each subject and further download the corresponding images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Stage III: Face Detection</head><p>We then detect faces in images via the Dlib detector <ref type="bibr" target="#b63">[64]</ref>. In this way, we can obtain a facial image dataset, containing faces of different poses/angles, variations of appearance (like glasses, hat). In addition, we manually remove some non-face images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Stage IV: Synthesize Pairs of Images</head><p>With the above steps, we have obtained about 10M facial images in total. Given the obtained facial images, we construct two subsets. The first one is composed of real-world low resolution facial images, which are dedicated to the qualitative study of existing face hallucination methods, as there is no ground truth available. Specifically, we choose images whose resolution is smaller than 50 ? 50 to compose this subset.</p><p>The second set is dedicated to quantitatively evaluating the existing face hallucination methods, consisting of pairs of high-resolution and low-resolution facial images. To this end, we have to choose high-resolution images and synthesize the corresponding low-resolution images. To be specific, we choose 1.5M face images whose resolution is larger than 128 ? 128 and resize them to 128 ? 128. These images serve as high-resolution images. To synthesize the corresponding low-resolution images, we employ the strategies employed in most of the existing face hallucination methods. More specifically, we simulate the degradation process via specific operations like downsampling. The developed subset includes five different degradation settings, named as 2?, 4?, 4? BD, 4? DN and 8?. The numbers indicate the downsampling factor, "D" stands for downsampling, "B" indicates blur operation, and "N " stands for Gaussian noise that is added to the LR images. The order of letters indicates the order of operations. For example, "BD" means that the blur artifact is applied prior to the downsampling operation. We use bicubic interpolation for downsampling. 2?, 4? and 8? mean that only bicubic downsampling operation is applied. When creating blurry and noisy face images, Gaussian blur and Gaussian noise are added to images. Among the 1.5M image pairs, we choose 1.36M pairs of images for training and 0.14M pairs of images for quantitative testing.</p><p>In summary, by conducting the above four steps, we derive a dataset including two subsets of non-aligned facial images. The first one contains 200K real-world lowresolution images for qualitative testing, and the second one includes 1.36M and 140K pairs of images for training and quantitative testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">The Statistics of EDFace-Celeb-1M</head><p>Lastly, we present the statistics of specific attributes of the developed EDFace-Celeb-1M dataset as follows.</p><p>? First, we show some representative high-resolution face images from the EDFace-Celeb-1M dataset in <ref type="figure" target="#fig_1">Figure 2</ref>. It is obvious that our dataset includes different races including White, Black, Asian and Latino, from different countries. In addition, these faces exhibit evident pose and appearance variations (e.g. glasses). <ref type="figure" target="#fig_0">Figure 1</ref> presents the statistics of the face pose. ? Second, we demonstrate the ratios of races and gender.</p><p>64% of the images of our dataset are from males and the rest 36% images are from females. The ratios of White, Black, Asian and Latino are about 31.1%, 19.2%, 19.6% and 18.3%, respectively, which are relatively balanced compared with existing datasets of face images. ? Third, we also analyze the distribution of age of the celebrities we include in our dataset, and show the results in <ref type="figure" target="#fig_2">Figure 3</ref>. The age of celebrities is estimated by the model from Insightface. Roughly, the majority of age is between 25 and 55, which aligns well with the property of demography. Notably, our dataset includes also celebrities younger than 20 and older than 60. ? Fourth, <ref type="figure" target="#fig_3">Figure 4</ref> presents the statistics of the resolution of face images before the resizing operation. In general, all the ground-truth high-resolution images are resized from images with resolution greater than 128 ? 128. ? Fifth, the proposed dataset provides fixed training and testing subsets. Each subsets includes high-quality images and corresponding low-quality images. All images are labeled as HR and LR with factors( e.g., 2?, 4?, 4 ? BD, 4 ? DN and 8?). Based on the labels and fixed subsets, the dataset can make the reproducibility and the fair comparison easier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we introduce the evaluation protocols and benchmark the existing face hallucination methods and representative SISR methods on the proposed EDFace-Celeb-1M dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluated Methods</head><p>In this benchmark study, we investigate four state-of-theart face hallucination methods (i.e., Deep Iterative Collaboration Network (DICNet) <ref type="bibr" target="#b32">[33]</ref>, Deep Iterative Collaboration GAN (DICGAN) <ref type="bibr" target="#b32">[33]</ref>, Wavelet-based Super-Resolution Network (WaveletSRNet) <ref type="bibr" target="#b28">[29]</ref> and HiFaceGAN <ref type="bibr" target="#b64">[65]</ref>), and four SISR methods (i.e., EDSR <ref type="bibr" target="#b10">[11]</ref>, Holistic Attention Network (HAN) <ref type="bibr" target="#b16">[17]</ref>, Residual Dense Network (RDN) <ref type="bibr" target="#b12">[13]</ref>, and Residual Channel Attention Network (RCAN) <ref type="bibr" target="#b13">[14]</ref>). All methods are based on deep learning. Specifically, DICGAN <ref type="bibr" target="#b32">[33]</ref> is an iterative framework of recurrently estimating landmarks and recovering high-resolution images. Wavelet coefficients are introduced in WaveletSRNet [29] to deal with very-low-resolution facial images. HiFaceGAN <ref type="bibr" target="#b64">[65]</ref> is proposed to formulate the face restoration task as a generation problem guided by semantics, and this problem is addressed by a multi-stage framework containing several units of collaborative suppression and replenishment. For the SISR task, EDSR is an enhanced deep super-resolution network containing several Resblocks. HAN is proposed in <ref type="bibr" target="#b16">[17]</ref> to model the correlation among different convolution layers with a layer attention module and channel-spatial attention module. RDN <ref type="bibr" target="#b12">[13]</ref> is a residual dense network to exploit the hierarchical features from both the local and global perspectives. RCAN <ref type="bibr" target="#b13">[14]</ref> is a deep residual channel attention network with both short and long skip connections. Channel attention is also adopted in this network for better performance. In summary, we select the above eight methods for three criteria. First, these methods achieve high values of the commonly used metrics like PSNR and SSIM. Second, the codes of these methods are publicly available. Third, these methods are proposed for face hallucination or image super-resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>Our dataset has four different degradation settings. Each setting corresponds to pairs of low-resolution and highresolution face images, which are used to train different models. We use the code released from the original publications. For fair comparisons, the learning rate and epoch number for all methods are set as 0.0001 and 20, respectively. All models are trained using V100 GPUs. We conduct the calculation of different metrics in the RGB space to access the results. During the training stage, all models are trained on the training subset and the testing subset is not used. After training, we evaluate the models on the testing subset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Face Super-Resolution</head><p>To evaluate the four methods on face hallucination, we provide the quantitative results of PSNR and SSIM on the EDFace-Celeb-1M dataset in <ref type="table" target="#tab_1">Table 2</ref>. Based on the PSNR and SSIM values, DICNet achieves the best performance on 4?, 4 ? BD, 4 ? DN and 8? degradation settings. RCAN achieves the best performance on 2?. In terms of SSIM, the best performance values on 4?, 4 ? BD, 4 ? DN , and 8? are also obtained by WaveletSR. The <ref type="table" target="#tab_1">Table 2</ref> does not provide the results of DICNet and DICGAN on X2 setting because the two methods do not work on this setting.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Face Alignment</head><p>Face alignment <ref type="bibr" target="#b65">[66]</ref> is an important task in the field of computer vision. Apart from the PSNR and SSIM, we also evaluate the performance of face hallucination methods via face alignment. Specially, we first use a popularly used alignment model from Dlib to estimate the 68 landmarks of high-resolution face images, and the super-resolved face images generated via different methods. Then, we use a metric commonly employed in face alignment, Normalized Root Mean Squared Error (NRMSE), to evaluate the error between the landmarks from the HR (High-Resolution) and SR (Super-Resolution) face images. A smaller NRMSE value indicates better face alignment performance, which corresponds to a better face hallucination method. We can see from <ref type="table" target="#tab_2">Table 3</ref> that the best performance on 2?, 4?, 4 ? BD, 4 ? DN and 8? are obtained by HAN, HAN, RCAN, HAN and RDN, respectively. We have an interesting observation that image SR methods achieve better performance than face hallucination methods regarding the metric of face alignment. This is because image SR methods mainly consider to super-resolve the LR images and ignore the adjustment of high-level information (like landmarks). Therefore, these methods can maintain the original landmarks better. For face hallucination methods, they usually consider the high-level information to help update the super-resolving process, which may cause the change of landmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Face Identity Information</head><p>Facial identity information is important during face hallucination. An ideal face hallucination approach should ensure that the ID information of HR face images and SR face images are the same. However, in the real world, it is difficult for the face hallucination methods to generate SR results that are the same as HR images. In this section, we use a popular face ID information extractor <ref type="bibr" target="#b6">[7]</ref> to extract ID information from HR face images and SR face images generated by different face hallucination methods. We then calculate the cosine distance between them. The larger value indicates that the ID information loss is less, which corresponds to a better face hallucination method. <ref type="table" target="#tab_2">Table 3</ref> shows the results of different models. We can see that the best performance on 2?, 4?, 4 ? BD, 4 ? DN and 8? are obtained by HAN &amp; WaveletSR, RCAN, RCAN, RCAM and RCAN, respectively. <ref type="table" target="#tab_2">Table 3</ref>, we can find that face hallucination methods do not show better performance than image SR methods regarding the metric of face identification. We suspect that both of the two kinds methods do not consider the loss functions about identification information during the training stage. In the future, it may be a meaningful direction for researchers to study face identification information for the task of face hallucination. Finally, we extract similarity scores across different people's images before hallucination, and compare that with extracted similarity scores across different people's images after hallucination (by RCAN method) and their high-resolution versions. The <ref type="figure">Figure 6</ref> shows the similarity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Based on results from</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Comparison on the Real LR Face Images</head><p>In addition, we also show the performance of the evaluated methods in the case of real-world scenarios based on our EDFace-Celeb-1M dataset. Taking a real-world lowresolution face image from our proposed dataset, we process it by different methods to generate SR face images, and the results are shown in <ref type="figure">Figure 5</ref>. We can find that most of the current face hallucination methods can improve the quality of low-resolution images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Impacts of Training Samples</head><p>Given that the proposed EDFace-Celeb-1M is a large-scale public face hallucination dataset, we conduct an experimen- tal study to explore the effect of the size of training samples for face hallucination. <ref type="figure" target="#fig_5">Figure 7</ref> and <ref type="table" target="#tab_3">Table 4</ref> show that models can achieve better performance with an increasing number of training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSIONS</head><p>In this section, we discuss the quality of the dataset and the effects we have made to fairly evaluate the current methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">The Quality of the Dataset</head><p>In summary, we provide an important dataset for the face hallucination community. The literature currently lacks a dataset specific to the face hallucination task and the proposed dataset makes the reproducibility and the fair comparison much easier for further research. To ensure the quality of this dataset, we make several efforts. First, to improve the quality of facial images in our dataset, we use a face detector to obtain facial images. In addition, we also manually remove some low-quality facial images. Second, to improve models' reproducibility on the dataset, we fixed the two training and testing subsets. As a comparison, previous methods randomly choose the two subsets. Third, different from previous methods which only provide one or two degraded settings, the provided dataset provides five degraded settings. In this way, the proposed dataset can better evaluate the performance of different face hallucination methods. Fourth, different from previous methods which ignore the problem of ethnicity, the proposed dataset provides a relatively balanced race composition. Fifth, different from some previous methods which evaluate their methods on relatively small-scale datasets, the provided dataset is the largest publicly available face hallucination dataset. In future, we will consider building more datasets to benefit the development of face hallucination, including using stronger face detectors to better detect faces and providing video sequences to help learn spatio-temporal information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Fairness</head><p>In summary, to fairly evaluate the current methods, this paper makes some significant efforts. First, we build a largescale publicly available dataset and fix the training and testing sets to conduct experiments, rather than randomly choose samples. Second, we use the popular metrics including PSNR and SSIM to compare different methods. Third, to further evaluate the current methods, we design two taskdriven ablation studies including landmark detection and identify preservation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>In this paper, we first propose the largest publicly available face hallucination dataset with relatively balanced race composition. It contains 1.5 million pairs of LR and HR face images for training and testing, and 140K real-world tiny face images for quantitative comparisons. Thanks to the proposed EDFace-Celeb-1M dataset, the following face hallucination can evaluate their methods on a public and fixed division of training and testing samples, which significantly makes the comparison convenient and improves the reliability.</p><p>In addition, given that the current face hallucination methods are evaluated on privately synthesized datasets, we benchmark four public available face hallucination methods, and four SISR methods on the proposed EDFace-Celeb-1M dataset. The proposed dataset will be made publicly available to encourage the development of face hallucination algorithms.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The facial pose statistics of the proposed EDFace-Celeb-1M dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Representative face images from the proposed dataset. These face images exhibit relatively balanced race distribution , evident pose and appearance variations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The age statistics of the proposed EDFace-Celeb-1M dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>The resolution of face images before resizing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Face hallucination results on the real-world images.. From left to right: results of bicubic, DICNet, DICGAN, WaveletSR, HiFaceGAN, EDSR, RDN, RCAN and HAN. An increase in similarity across different people after the hallucination.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Face hallucination results trained on different numbers of training samples. From left to right, the columns show results of the DICNet trained using 0.3M , 0.7M and 1.35M training samples from the EDFace-Celeb-1M dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .Fig. 9 .</head><label>89</label><figDesc>Visual results of BI models (?8) on the EDFace-Celeb-1M dataset. From left to right: HR, results of bicubic, DICNet, DICGAN, WaveletSR, HiFaceGAN, EDSR, RDN, RCAN and HAN. "BI" means the bicubic interpolation. Visual results of BI models (?4) on the EDFace-Celeb-1M dataset. From left to right: HR, results of bicubic, DICNet, DICGAN, WaveletSR, HiFaceGAN, EDSR, RDN, RCAN and HAN. "BI" means the bicubic interpolation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 .</head><label>10</label><figDesc>Visual results of BD models (?4) on the EDFace-Celeb-1M dataset. From left to right: HR, results of bicubic, DICNet, DICGAN, WaveletSR, HiFaceGAN, EDSR, RDN, RCAN and HAN. "BD" means that the blur artifact is applied prior to the downsampling operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 .</head><label>11</label><figDesc>Visual results of DN models (?4) on the EDFace-Celeb-1M dataset. From left to right: HR, results of bicubic, DICNet, DICGAN, WaveletSR, HiFaceGAN, EDSR, RDN, RCAN and HAN. "DN" means that Gaussian noise is added to the LR images after the downsampling operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 Representative face datasets.</head><label>1</label><figDesc>Most of the current public face datasets do not consider the race problem. Meanwhile, none of them provides large-scale pairs of LR and HR samples for evaluating deep hallucination methods.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2 Performance comparison of representative methods for face hallucination on the EDFace-Celeb-1M dataset.</head><label>2</label><figDesc>Results are reported in terms of both PSNR and SSIM. The highest, second highest and lowest results are highlighted in bolded, blue and red, respectively.</figDesc><table><row><cell>Scale</cell><cell cols="9">Metrics DICNet [33] DICGAN [33] WaveletSR [29] HiFaceGAN [65] EDSR [11] RDN [13] RCAN [14] HAN [17]</cell></row><row><cell>2?</cell><cell>PSNR SSIM</cell><cell>--</cell><cell>--</cell><cell>30.60 0.9119</cell><cell>29.68 0.8836</cell><cell>31.23 0.8869</cell><cell>31.39 0.8889</cell><cell>31.42 0.8892</cell><cell>31.41 0.8888</cell></row><row><cell>4?</cell><cell>PSNR SSIM</cell><cell>29.06 0.8453</cell><cell>28.41 0.8261</cell><cell>26.35 0.8211</cell><cell>25.37 0.7727</cell><cell>26.99 0.8035</cell><cell>27.56 0.8153</cell><cell>27.64 0.8161</cell><cell>27.62 0.8168</cell></row><row><cell>4 ? BD</cell><cell>PSNR SSIM</cell><cell>29.68 0.8213</cell><cell>28.58 0.7988</cell><cell>26.52 0.7940</cell><cell>24.23 0.7009</cell><cell>27.22 0.7825</cell><cell>27.83 0.7955</cell><cell>27.89 0.7969</cell><cell>27.88 0.7964</cell></row><row><cell>4 ? DN</cell><cell>PSNR SSIM</cell><cell>27.96 0.8117</cell><cell>27.38 0.7922</cell><cell>25.72 0.7815</cell><cell>22.94 0.6856</cell><cell>26.08 0.7673</cell><cell>26.59 0.7817</cell><cell>26.66 0.7835</cell><cell>26.62 0.7851</cell></row><row><cell>8?</cell><cell>PSNR SSIM</cell><cell>25.29 0.7453</cell><cell>24.64 0.7134</cell><cell>22.33 0.6758</cell><cell>21.88 0.6408</cell><cell>23.24 0.6890</cell><cell>23.74 0.7117</cell><cell>23.77 0.7114</cell><cell>23.73 0.7114</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3 Performance comparison of representative methods for face hallucination on the EDFace-Celeb-1M dataset.</head><label>3</label><figDesc>Results are reported in terms of the errors of face alignment, face parsing and identity information. The highest, second highest and lowest results are highlighted in bolded, blue and red, respectively.</figDesc><table><row><cell>Scale</cell><cell>Metrics</cell><cell cols="8">DICNet [33] DICGAN [33] WaveletSR [29] HiFaceGAN [65] EDSR [11] RDN [13] RCAN [14] HAN [17]</cell></row><row><cell>2?</cell><cell>Alignment Identity</cell><cell>--</cell><cell>--</cell><cell>0.0218 0.9813</cell><cell>0.0223 0.9773</cell><cell>0.0220 0.9800</cell><cell>0.0219 0.9811</cell><cell>0.0220 0.9812</cell><cell>0.0210 0.9813</cell></row><row><cell>4?</cell><cell>Alignment Identity</cell><cell>0.0259 0.8360</cell><cell>0.0272 0.8184</cell><cell>0.0262 0.8338</cell><cell>0.0295 0.7875</cell><cell>0.0264 0.8262</cell><cell>0.0254 0.8490</cell><cell>0.0253 0.8543</cell><cell>0.0250 0.8527</cell></row><row><cell>4 ? BD</cell><cell>Alignment Identity</cell><cell>0.0257 0.7846</cell><cell>0.0277 0.7726</cell><cell>0.0258 0.7055</cell><cell>0.0312 0.6758</cell><cell>0.0261 0.7786</cell><cell>0.0252 0.8045</cell><cell>0.0250 0.8109</cell><cell>0.0251 0.8087</cell></row><row><cell>4 ? DN</cell><cell>Alignment Identity</cell><cell>0.0289 0.7035</cell><cell>0.0312 0.6797</cell><cell>0.0292 0.7857</cell><cell>0.0362 0.5344</cell><cell>0.0298 0.6969</cell><cell>0.0281 0.7291</cell><cell>0.0280 0.7332</cell><cell>0.0279 0.7302</cell></row><row><cell>8?</cell><cell>Alignment Identity</cell><cell>0.0366 0.4830</cell><cell>0.040 0.448</cell><cell>0.0407 0.4305</cell><cell>0.0440 0.3851</cell><cell>0.0399 0.4600</cell><cell>0.0353 0.5094</cell><cell>0.0354 0.5116</cell><cell>0.0354 0.5081</cell></row><row><cell cols="5">Figures 8, 9, 10 and 11 show the visual comparison of</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">different methods on the EDFace-Celeb-1M dataset.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 4 Performance comparison of DICNet [33] trained using different numbers of training samples from the EDFace-Celeb-1M dataset.</head><label>4</label><figDesc>Results are reported in terms of PSNR and SSIM. 0.3M , 0.7M and 1.35M represent the size of training samples. The highest, second highest and lowest results are highlighted in bolded, blue and red, respectively.</figDesc><table><row><cell>Scale</cell><cell>Metrics</cell><cell>0.3M</cell><cell>0.7M</cell><cell>1.35M</cell></row><row><cell>4?</cell><cell>PSNR SSIM</cell><cell cols="2">28.42 0.8265 0.8362 28.78</cell><cell>29.06 0.8453</cell></row><row><cell>4 ? BD</cell><cell>PSNR SSIM</cell><cell cols="2">28.93 0.8053 0.8162 29.36</cell><cell>29.68 0.8213</cell></row><row><cell>4 ? DN</cell><cell>PSNR SSIM</cell><cell cols="2">27.23 0.7986 0.8073 27.68</cell><cell>27.96 0.8117</cell></row><row><cell>8?</cell><cell>PSNR SSIM</cell><cell cols="2">24.63 0.7216 0.7376 24.95</cell><cell>25.29 0.7453</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kaihao</head><p>Zhang is currently pursuing the Ph.D. degree with the College of Engineering and Computer Science, The Australian National University, Canberra, ACT, Australia. His research interests focus on computer vision and deep learning. He has more than 30 referred publications in international conferences and journals, including CVPR, ICCV, ECCV, NeurIPS, AAAI, ACMMM, TPAMI, IJCV, TIP, TMM, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dongxu</head><p>Li is a Ph.D candidate at The Australian National University. His research interests are mainly computer vision and deep learning, including visual sequence representation learning, vision-language learning and multi-modal learning. Before starting PhD, Dongxu obtained his Bachelor degree from The Australian National University with first-class honours in Computing.</p><p>Wenhan Luo is currently an Associate Professor in Sun Yat-sen University. Prior to that, he worked as a research scientist for Tencent and Amazon. He has published over 40 papers in top conferences and leading journals, including ICML, CVPR, ICCV, ECCV, ACL, AAAI, ICLR, TPAMI, IJCV, TIP, etc. He also has been reviewer, senior PC member and Guest Editor for several prestigious journals and conferences. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep convolutional network cascade for facial point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3476" to="3483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Face alignment across large poses: A 3d solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="146" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2d &amp; 3d face alignment problem?(and a dataset of 230,000 3d facial landmarks)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1021" to="1030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-objective convolutional learning for face labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3451" to="3459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Face parsing with roi tanh-warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5654" to="5663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cosface: Large margin cosine loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5265" to="5274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4690" to="4699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Accelerating the superresolution convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="391" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Accurate image superresolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="624" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2472" to="2481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image superresolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="286" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep back-projection networks for super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1664" to="1673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Densely residual laplacian superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Single image super-resolution via a holistic attention network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="191" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Superresolution of face images using kernel pca-based prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rajagopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="888" to="892" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Face hallucination: Theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="115" to="134" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generalized face super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="873" to="886" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Super-resolution of human face image using canonical correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2532" to="2543" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hallucinating face by position-patch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2224" to="2236" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Position-patch based face hallucination using convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="367" to="370" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Structured face hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1099" to="1106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A comprehensive survey to face hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="30" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Face super-resolution via multilayer locality-constrained iterative neighbor embedding and intermediate dictionary learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4220" to="4231" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning face hallucination in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep cascaded bi-network for face hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="614" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Wavelet-srnet: A waveletbased cnn for multi-scale face super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1689" to="1697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention-aware face hallucination via deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="690" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fsrnet: End-to-end learning face super-resolution with facial priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2492" to="2501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Face super-resolution guided by facial component heatmaps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="217" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep face superresolution with iterative collaboration between attentive recovery and landmark estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5569" to="5578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dualpath deep fusion network for face image hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Ultra-resolving face images by discriminative generative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="318" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Super-resolving very low-resolution face images with supplementary attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="908" to="917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hallucinating face by eigentransformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="425" to="434" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Transport-based single frame super resolution of very low resolution face images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Rohde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4876" to="4884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A bayesian approach to alignmentbased image hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Tappen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="236" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hallucinating compressed face images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="597" to="614" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Super-identity convolutional neural network for face hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="183" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Learning to hallucinate face images via component generation and enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00223</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Progressive face super-resolution via attention to facial landmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-S</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08239</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep learning for image superresolution: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Photorealistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Esrgan: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C. Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Enhancenet: Single image super-resolution through automated texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4491" to="4500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Second-order attention network for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="65" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Feedback network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3867" to="3876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Labeled faces in the wild: A database forstudying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mattar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on faces in&apos;Real-Life&apos;Images: detection, alignment, and recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep learning face representation from predicting 10,000 classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1891" to="1898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Learning face representation from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7923</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">The megaface benchmark: 1 million faces for recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4873" to="4882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Vg-gface2: A dataset for recognising faces across pose and age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 13th IEEE international conference on automatic face &amp; gesture recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="67" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Fairface: Face attribute dataset for balanced race, gender, and age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>K?rkk?inen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.04913</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Copy and paste gan: Face hallucination from shaded thumbnails</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7355" to="7364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Multipie</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and vision computing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="807" to="813" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Blind face restoration via deep multi-scale component dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="399" to="415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Dlib-ml: A machine learning toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1755" to="1758" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Hifacegan: Face renovation via collaborative suppression and replenishment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1551" to="1560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">img2pose: Face alignment and detection via 6dof, face pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Albiero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7617" to="7627" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
