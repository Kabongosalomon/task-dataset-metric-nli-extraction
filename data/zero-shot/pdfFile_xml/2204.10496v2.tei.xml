<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multimodal Adaptive Distillation for Leveraging Unimodal Encoders for Vision-Language Tasks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhecan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxuan</forename><surname>You</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multimodal Adaptive Distillation for Leveraging Unimodal Encoders for Vision-Language Tasks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cross-modal encoders for vision-language (VL) tasks are often pretrained with carefully curated vision-language datasets. While these datasets reach an order of 10 million samples, the labor cost is prohibitive to scale further. Conversely, unimodal encoders are pretrained with simpler annotations that are less cost-prohibitive, achieving scales of hundreds of millions to billions. As a result, unimodal encoders have achieved state-of-art (SOTA) on many downstream tasks. However, challenges remain when applying to VL tasks. The pretraining data is not optimal for cross-modal architectures and requires heavy computational resources. In addition, unimodal architectures lack cross-modal interactions that have demonstrated significant benefits for VL tasks. Therefore, how to best leverage pretrained unimodal encoders for VL tasks is still an area of active research. In this work, we propose a method to leverage unimodal vision and text encoders for VL tasks that augment existing VL approaches while conserving computational complexity. Specifically, we propose Multimodal Adaptive Distillation (MAD), which adaptively distills useful knowledge from pretrained encoders to cross-modal VL encoders. Second, to better capture nuanced impacts on VL task performance, we introduce an evaluation protocol that includes Visual Commonsense Reasoning (VCR), Visual Entailment (SNLI-VE), and Visual Question Answering (VQA), across a variety of data constraints and conditions of domain shift. Experiments demonstrate that MAD leads to consistent gains in the low-shot, domain-shifted, and fully-supervised conditions on VCR, SNLI-VE, and VQA, achieving SOTA performance on VCR compared to other single models pretrained with image-text data. Finally, MAD outperforms concurrent works utilizing pretrained vision encoder from CLIP. Code will be made available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract. Abstract. Cross-modal encoders for vision-language (VL) tasks are often pretrained with carefully curated vision-language datasets. While these datasets reach an order of 10 million samples, the labor cost is prohibitive to scale further. Conversely, unimodal encoders are pretrained with simpler annotations that are less cost-prohibitive, achieving scales of hundreds of millions to billions. As a result, unimodal encoders have achieved state-of-art (SOTA) on many downstream tasks. However, challenges remain when applying to VL tasks. The pretraining data is not optimal for cross-modal architectures and requires heavy computational resources. In addition, unimodal architectures lack cross-modal interactions that have demonstrated significant benefits for VL tasks. Therefore, how to best leverage pretrained unimodal encoders for VL tasks is still an area of active research. In this work, we propose a method to leverage unimodal vision and text encoders for VL tasks that augment existing VL approaches while conserving computational complexity. Specifically, we propose Multimodal Adaptive Distillation (MAD), which adaptively distills useful knowledge from pretrained encoders to cross-modal VL encoders. Second, to better capture nuanced impacts on VL task performance, we introduce an evaluation protocol that includes Visual Commonsense Reasoning (VCR), Visual Entailment (SNLI-VE), and Visual Question Answering (VQA), across a variety of data constraints and conditions of domain shift. Experiments demonstrate that MAD leads to consistent gains in the low-shot, domain-shifted, and fully-supervised conditions on VCR, SNLI-VE, and VQA, achieving SOTA performance on VCR compared to other single models pretrained with image-text data. Finally, MAD outperforms concurrent works utilizing pretrained vision encoder from CLIP. Code will be made available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pretrained Text Encoders</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ERNIE-ViL</head><p>In-domain Data Research Goal: How to leverage large-scale pretrained unimodal vision and text encoders to improve performance of pretrained multi-modal VL encoders? <ref type="figure">Fig. 1</ref>. Comparison of data size between different pretraining frameworks. It is noticeable that the data size of pretraining unimodal encoder models is generally much larger than Vision-Language models. <ref type="bibr" target="#b1">[2]</ref>, are all VL tasks requiring solutions that effectively bridge the two modalities, with mechanisms of logic and prior knowledge serving as the link between them. For example, in VCR, logical reasoning capability and common knowledge are useful in answering questions about images, especially when the visual information is ambiguous, rendering the task of connecting questions with their correct answers difficult. Considering this, most recent works that have achieved SOTA tend to capture the logic and prior knowledge with various cross-modal transformer architectures that jointly model both modalities in a unified architecture <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b56">56]</ref>. These models are typically pretrained on large images-caption dataset <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b30">30]</ref>. These curated datasets are all collected carefully by human annotators to ensure the text data consists of visually descriptive and syntactically correct sentences. They typically do not exceed about 10M data points in size. By contrast, obtaining training data for unimodal models requires significantly less effort and manual labor, although the data tends to have large domain gaps against downstream VL tasks. For example, attention-based text models like BERT <ref type="bibr" target="#b12">[12]</ref>, GPT <ref type="bibr" target="#b37">[37]</ref>, RoBERTa <ref type="bibr" target="#b31">[31]</ref>, BEiT <ref type="bibr" target="#b2">[3]</ref>, etc. can conveniently leverage self-supervision or weak-supervision to conduct unimodal pretraining on large text corpus <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b30">30]</ref>. Vision models like ViT <ref type="bibr" target="#b13">[13]</ref> can also be obtained via unimodal supervised pretraining on large image datasets such as JFT-300M <ref type="bibr" target="#b48">[48]</ref>. Furthermore, another line of research has been examining pretraining with paired noisy image-text data (crawled from the web) which is also easier to collect than image-caption pairs. This pretraining is typically combined with a contrastive learning objective to align both modalities into a shared embedding space. In these frameworks, vision, and text streams are all modality-specific encoders and cross-modal fusion layers do not exist but shallow connections via cosine distance. Thus different from the conventional VL frameworks and similar to unimodal pretraining, those contrastive pretraining can also produce pretrained unimodal encoder models like CLIP-V, CLIP-T, etc. These works extend pretraining datasets to a more massive scale, such as CLIP <ref type="bibr" target="#b36">[36]</ref> with 400 million, ALIGN <ref type="bibr" target="#b22">[22]</ref> with 10 billion and BASIC <ref type="bibr" target="#b35">[35]</ref> with 100 billion (the largest so far).</p><p>These pretrained unimodal encoder models are generally pretrained with larger data than pretrained VL models, as shown in <ref type="figure">Fig. 1</ref>  <ref type="bibr" target="#b3">4</ref> . Thus they can produce generalized feature representations and can benefit a wide range of downstream tasks. For instance, CLIP-V and CLIP-T show strong performance for zero-shot classification and object detection tasks <ref type="bibr" target="#b67">[67]</ref>. Due to their complementary model architecture and massive scale pretraining, unimodal encoder models present the potential to benefit VL tasks. However, whether and how to leverage pretrained vision/text encoder models for VL tasks is still an area of active research investigation. Previous methods extensively experimented with directly plugging in pretrained text encoders into VL frameworks <ref type="bibr" target="#b57">[57,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b56">56]</ref>. Nevertheless, those frameworks all require redoing pretraining steps to align the representations. Another recent method <ref type="bibr" target="#b44">[44]</ref> adapts the pretrained vision model from CLIP <ref type="bibr" target="#b36">[36]</ref> but also has to undergo an additional stage of pretraining on millions of data for adaptation. This approach not only increases the computational complexity but also require to have sufficient additional data for adaptation. This is particularly impractical in real-world scenarios with limited data availability and may further vulnerability to domain shift in target task data.</p><p>Current research is continuously pushing the performance of pretrained unimodal encoder models via increasing pretraining data size. Therefore, a natural research question rises. If given the best-pretrained vision and text encoders, what is the efficient solution to integrate them into a pretrained VL model without redoing pretraining steps and impacting inference complexity?</p><p>In this work, we propose a flexible approach to leverage pretrained vision and text models for VL tasks relying only on the task finetuning step. Specifically, we propose our main approach referred to as Multimodal Adaptive Distillation (MAD), which adaptively distills knowledge from pretrained unimodal models to VL task-specific cross-modal architectures per data instance, changing both distillation weights as well as distillation targets, depending on the behavior of the unimodal encoders and VL task-specific cross-modal architecture on the data point. Our framework, first time, proposes multimodal modularized distillation allowing teacher unimodal encoders to come from different frameworks. Our method does not require computationally expensive pretraining steps and maintains the inference complexity of the task model. For evaluation, we propose a new protocol involving multiple VL tasks (VCR, SNLI-VE, and VQA) where evaluation is conducted under a variety of data availability constraints, including zero-shot, low-shot, and fully-supervised settings. For VCR, we additionally use established ways of perturbing the characteristics of the evaluation set to measure model performance under domain-shift. We compare ours with a broad spectrum of baselines fusing pretrained vision and text models and demonstrate superior performance, achieving state-of-art on VCR Q2A for single models pretrained with image-text data, and competitive results on SNLI-VE and VQA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Vision and Language Pretraining: Since the success of text pretraining with attention-based models, such as BERT <ref type="bibr" target="#b12">[12]</ref>, GPT <ref type="bibr" target="#b37">[37]</ref>, etc., those pretrained text encoder models have been broadly employed in different tasks. VL models, such as LXMERT <ref type="bibr" target="#b52">[52]</ref>, VL-BERT <ref type="bibr" target="#b47">[47]</ref>, UNITER <ref type="bibr" target="#b9">[9]</ref>, VILLA <ref type="bibr" target="#b17">[17]</ref>, and others <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b68">68,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b62">62,</ref><ref type="bibr" target="#b28">28]</ref> followed to integrate the pretrained text models into their frameworks. Thus they have to conduct additional VL pretraining (VLP) with image-caption data. All of these pretraining steps utilize additional datasets on the order of 10M samples for various pretraining objectives, including Masked Language Modeling (MLM), Image-Text Matching (ITM), etc. Similarly largescale pretrained vision models can be obtained via vision pretraining frameworks such as ViT <ref type="bibr" target="#b13">[13]</ref> on image dataset including JFT-300M <ref type="bibr" target="#b48">[48]</ref>. Trying to utilize large-scale pretrained vision models, a recent work, CLIP-ViL experiments to <ref type="bibr" target="#b44">[44]</ref> adapt the vision encoder from CLIP. Similarly, this also unavoidably results in redoing all the VLP steps. Both pretrained vision and text models can also be obtained via contrastive pretraining frameworks which reserve modalityindependence <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b35">35]</ref>. These contrastive learning frameworks can also be regarded as a shallow method to fuse pretrained vision and text encoders for VL tasks. They heavily rely on cosine distance between output features of text and vision encoder models. However, without incorporating pretrained vision, text models into a pretrained VL model but a shallow cosine measure, these frameworks have not proved to produce a high performance on highly semantic VL tasks like VCR, etc.</p><p>Currently, there is a lack of a more general framework for fusing large-scale pretrained vision and text models into a pretrained VL structure for improving downstream VL tasks while controlling computational complexity. Existing fusion methods, such as shallow contrastive frameworks, that avoid impacting computation complexity mostly focus on traditional VL tasks such as object classification, image captioning, etc. To our best knowledge, no prior VLP works from this tier have studied the impact on generalization capability by assessing performance on highly-semantic tasks, such as VCR, especially under both lowshot and domain shifted scenarios, which are more reflective of the challenges encountered in practice.</p><p>In this work, we address several of these mentioned gaps. First, we propose an efficient distillation approach to leverage pretrained vision and text encoder models into a pretrained VL structure in a way that doesn't require additional pretraining, and retains inference complexity. Second, we study the impact of utilizing large-scale pretrained vision, text encoders like CLIP-V, CLIP-T, Roberta, ViT, etc. on VL tasks including VCR, SNLI-VE, and VQA under true zero-shot, low-shot, and domain shifted scenarios.</p><p>Generalization of Question Answering: Visual question answering <ref type="bibr" target="#b63">[63,</ref><ref type="bibr" target="#b1">2]</ref> is among the most challenging VL tasks, due to large variation between samples and distribution discrepancy between training and testing datasets. This results in difficulty for models to perform well under zero-shot, low-shot, and domainshifted settings. Despite high accuracy recent models achieve <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b66">66,</ref><ref type="bibr" target="#b64">64]</ref>, other works have begun to uncover the tendency of models to leverage spurious shortcut signals, or memorization of mapping distributions <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b27">27]</ref>. While prior works have begun to explore question answering tasks under zero-shot <ref type="bibr" target="#b53">[53,</ref><ref type="bibr" target="#b34">34]</ref>, low-shot <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b3">4]</ref> and domain shifted <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b65">65,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b38">38]</ref> settings, they are mostly limited within text-only question answering and low-level visual question answering tasks (i.e. VQA <ref type="bibr" target="#b1">[2]</ref>). To the best of our knowledge, none of the prior works have explored true zero-shot and low-shot settings in highly complex visual question answering datasets such as VCR or SNLI-VE, and only one prior work <ref type="bibr" target="#b61">[61]</ref> contributed to VCR with domain shift.</p><p>In this work, we conduct a thorough evaluation with many existing topperforming models on zero-shot, low-shot, and domain-shifted settings.</p><p>Knowledge Distillation: In conventional knowledge distillation <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b60">60,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b33">33</ref>], a larger model serves as the teacher to a smaller student model. The goal is usually to obtain a computationally lighter and more efficient framework but still maintain similar or even higher accuracy <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b49">49]</ref>. However, recently many frameworks with small model complexity may have large pretraining data which results in more generalized feature representations. Crucial values still exist for distillation from those large-scale pretrained models to downstream domain-specific models with potentially larger model complexity. It is shallow to only compare the inference complexity between them ignoring the large pretraining complexity.</p><p>Also, tremendous progress was made in knowledge distillation with unimodal data. For instance, in vision, <ref type="bibr" target="#b54">[54,</ref><ref type="bibr" target="#b15">15]</ref> propose distillation for visual representation learning. In addition, remarkable advances have been made in knowledge distillation for language model compression <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b51">51]</ref>. A wide range of works have explored to supervise the student model by mimicking different components of the teacher, e.g. the distribution of self-attention, intermediate representations of transformer blocks, last layer features, etc. to increase performances <ref type="bibr" target="#b59">[59,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b8">8]</ref>. On the other hand, only one prior explored knowledge distillation in VL transformers <ref type="bibr" target="#b14">[14]</ref> while it mainly follows the conventional methods with unimodal distillation. The distillation is not differentiated between vision and language.</p><p>Further, in most of these scenarios, the feature components leveraged for the distillation loss is fixed, and the weight of the distillation loss is also fixed. Prior works related to attention structures only focus on fixed sequence-to-sequence distillation <ref type="bibr" target="#b49">[49]</ref> and only one prior work first introduced the idea of dynamically adjusting the distillation loss weight on a per-instance level <ref type="bibr" target="#b16">[16]</ref>.</p><p>In this work, our framework, for the first time, proposes multimodal distillation which modulates the distillation between modalities enabling flexible switch of different teacher models for each modality. Our approach is also modelagnostic ignoring the inference complexity comparison between the teacher and the student. The experiments prove that it is effective in all of these scenarios. We also leverage the idea of dynamic distillation weights, but we even go one step further: we dynamically adjust not only the distillation weight for each sample instance but which token features are distilled as well within every sequence.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TS Pair Module</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>In this work, we explore many methods to utilize large-scale pretrained unimodal encoder models to help downstream VL tasks, including direct finetuning, adding adapters on unimodal models, and several proposed forms of distillation. Among these, we find that our proposed knowledge distillation is the most effective, achieving the best performance without having to redo costly pretraining, and without impacting inference complexity. We introduce these methods in the following order: First, a naive approach, Multimodal Distillation (MD); Then, the improved Multimodal Adaptive Distillation (MAD).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multimodal Distillation</head><p>Following <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b49">49]</ref>, we can utilize token features form both vision and language to represent the corresponding modality sequence information. This allows our method to conduct sequence-to-sequence distillation.</p><p>Both the teacher and student model's visual img 5 tokens, in addition to the teacher's text eos token and the student's text cls token, are compared via L1 measure. The final loss is the weighted distillation loss L d summed with the original task loss L t for any specific downstream task. Formally,</p><formula xml:id="formula_0">L f inal?M D = L t + w ? (L d )<label>(1)</label></formula><p>where the distillation loss L d is a sum of the distillation losses between the two modalities:</p><formula xml:id="formula_1">L d = L d,v + L d,t<label>(2)</label></formula><p>where v and t refer to vision and text branches, respectively.</p><formula xml:id="formula_2">L d,v = ?f t,v,img (i j ) ? f s,v,img (? (i j ))? 1 = ?V t ? V s ? 1<label>(3)</label></formula><p>where f t,v,img and f s,v,img refer to the feature extraction regarding img token of the teacher vision encoder, and student model. V t and V s represent the extracted visual token features from the teacher and student models. ? represents the backbone detection network. i j refers to the image input for instance j. For the text modality, we have</p><formula xml:id="formula_3">L d,t = ?f t,t,eos (t j ) ? f s,t,cls (t j )? 1 = ?T t ? T s ? 1 (4)</formula><p>where f t,t,eos refers to the feature extraction of eos token from the teacher text model, and f s,t,cls refers to the feature extraction of cls token from the student model. T t and T s represent the extracted text token features from the teacher and student models. t j refers to the text input for instance j.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multimodal Adaptive Distillation (MAD)</head><p>Improved on top of MD, MAD consists of the following 3 components: Token Selection using an unsupervised language prior, Confidence Weighted Distillation, and Adaptive Finetuning with Distilled Knowledge. Token Selection: Typically, distillation methods <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b49">49]</ref> distill knowledge from the teacher transformer to the student transformer in a sequence-tosequence fashion by using fixed token components of the networks. However, the most semantically relevant tokens can change per instance. Without a dynamic process to determine which components of a network should be distilled, the student model may have a higher risk of learning spurious signals from trivial tokens. Addressing this, we present a hybrid method of performing token selection for distillation. Given a text sequence t j = {w 0 . . . w z }, where z is the length of the sequence, we apply a pretrained Token Selection Module (TSM) f tsm (i j , t j ) to discriminate the semantically meaningful tokens. TSM generates a score, s l for each token w l , obtaining a distribution S j = {s 0 . . . s z } = f tsm (i j , {w 0 . . . w z }). In our implementation of this selective module, two sets of weights are computed via two different approaches, and summed together:</p><formula xml:id="formula_4">S j = S vr |S vr | 1 + S si |S si | 1 (5) where S vr = {s vr0 . . . s vrz } = {cos ?f t,v (i j ) , f s,t (w l )</formula><p>?} represents a scoring between the visual representation and text token representation. In this manner, S vr is the score measuring the visual relevance of each token w l , l ? [0, . . . , z]. Then, S si = f ke (t j ), where S si represents the semantic and syntactic importance of each token related to the context, t j , using a keyword extractor. In practice, we apply a pre-trained keyword extractor <ref type="bibr" target="#b4">[5]</ref>, f ke with n-grams:</p><p>We then rank text tokens based on S j and select m tokens with the highest scores,</p><formula xml:id="formula_5">t ? j , where |t j ? | = m.</formula><p>The corresponding features of both teacher and student model would be compared with an L1 measure to calculate their difference:</p><formula xml:id="formula_6">L dt ? = ?f t,t (t j ? ) ? f s,t (t j ? )? 1<label>(6)</label></formula><p>finally, the outputted loss, L dt ? would be added to the final loss, L final-MAD by the proportionally updated distillation weight, w ? :</p><formula xml:id="formula_7">L final-MAD = L t + w ? ? (L d,v + L d,t + L ? dt ) = L t + w ? ? (L d + L ? dt )<label>(7)</label></formula><p>Confidence Weighting (CW): CLIP <ref type="bibr" target="#b36">[36]</ref> benefits from a large amount of paired language-image training data. While this broad prior knowledge is likely helpful for VL tasks, the degree to which this knowledge is either helpful or potentially hurtful likely changes on an instance level. Given this, we design an approach to toggle the distillation objective depending on the relative confidence of the CLIP teacher and the specific student architecture. To do this, we define the ratio r between maximum confidence scores of the CLIP teacher and base student model:</p><formula xml:id="formula_8">r = f argmax ? L c j f argmax ? L b j<label>(8)</label></formula><p>where L c j represents the logit vector from CLIP and L b j from the base model. Finally, the new adaptive weight w r is defined as:</p><formula xml:id="formula_9">w r = 0, if r ? 1 w ? , if r &gt; 1<label>(9)</label></formula><p>where w r replaces the distillation weights in Eq. 1 and 7. When the ratio is above 1, CLIP is confident with its prediction, thus the distillation weight w ? would be applied accordingly. Otherwise, the distillation value would be set to 0 to prevent from CLIP's interference. Adaptive Finetuning (AF) : After large-scale pretraining, previous work <ref type="bibr" target="#b9">[9]</ref> chooses to first train V+L models with a set of auxiliary tasks to better integrate the modalities, including Masked Language Modeling (MLM), Image-Text Matching (ITM), etc. on downstream datasets. Then the training would finally be followed by the direct finetuning with the downstream target task only. The corresponding loss of this set of auxiliary tasks can be denoted as L adapt (For details of L adapt , please refer to <ref type="bibr" target="#b9">[9]</ref>). Inspired by this, we further propose a two-stage finetuning strategy. With our strategy, we first finetune the base model with L AF on the full downstream data then we conduct the last-step finetuning with L final-MAD . Thus during the Adaptive Finetuning, multimodal distilled knowledge can help align the vision and language modality features beforehand.</p><p>Full Training Steps = 1st step, finetune with L AF 2nd step, finetune with L final-MAD (10)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Teacher Models</head><p>We utilize the popular pretrained visual and text encoders like CLIP-V, CLIP-T, ViT and RoBERTa. Our distillation framework is a generalized modularized multimodal distillation such that the teacher model of each modality can be different. This leads to forming two essential combinations in our experiments: pretrained visual and text encoders come from the same pretraining framework and vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Student Models</head><p>Several recent top-performing pretrained VL models for highly-semantic VL tasks are selected as students, including UNITER <ref type="bibr" target="#b9">[9]</ref>, VL-BERT <ref type="bibr" target="#b47">[47]</ref>, and VILLA <ref type="bibr" target="#b17">[17]</ref>. All of these models represent variations of multi-modal architectures, using different portfolios of objective functions and image-caption datasets for pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Datasets and Evaluations</head><p>In order to evaluate our methods and demonstrate their supremacy. We propose this evaluation protocol under a variety of data availability constraints, including zero-shot, low-shot, domain-shift, and fully-supervised settings. We evaluate our methods on 3 commonly used highly-semantic VL benchmarks: VCR, SNLI-VE, and VQA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Visual Commonsense Reasoning (VCR)</head><p>The VCR benchmark presents images along with a paired question, a set of candidate answers, and a set of candidate rationales <ref type="bibr" target="#b63">[63]</ref>. We include a prior evaluation configuration <ref type="bibr" target="#b61">[61]</ref> that focuses on mitigating shortcuts between question and answers. Shortcuts are shallow signals models can learn to recognize. Learning shortcuts may allow models to link questions to correct answers without deep understanding of the content. We refer to this as "Shortcut Mitigated" (SM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Visual Entailment (SNLI-VE)</head><p>The Stanford Natural Language Inference Visual Entailment (SNLI-VE) task <ref type="bibr" target="#b58">[58]</ref> presents images as a premise, with paired hypothesis test. The goal is to predict whether the image entails or contradicts the hypothesis, or whether neither is the case (neutral). Zero-Shot: We first extract the VL features from the pretrained models for the given image and text premise. The features are then directly measured by cosine distance to produce the similarity between the image and the text premise. The challenge is determining what similarity values should constitute entailment, contradiction, or neither. To accomplish this, without finetuning the model, we further perform k-means of the similarities on the validation set, with k = 3, and use the resultant clusters as anchors for each output decision. Note that this approach, while a form of transductive learning on the validation set, uses no ground truth labels, and does not change any weights of the pretrained models. We apply this procedure to evaluate both pretrained VL encoders and pretrained VL models. Low-Shot: There are only 3 types of relationship between the image premise and the text hypothesis (entailment, neutral, and contradiction). Also, different from VCR, an image can pair with around 5 text premises in SNLI-VE. Therefore, we choose the image-based selection. We also have two settings: (1) We select 100 random images for each class, 300 images in total paired with around 1,500 text premises.</p><p>(2) 1,000 randomly sampled for each class label, then 30,00 samples in total with around 15,000 premises. They both correspond to 0.3% and 3% of SNLI-VE. Each experiment is run more than 4 times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Visual Question Answering (VQA)</head><p>Different from VCR and VE, for every image-question pair in VQA <ref type="bibr" target="#b1">[2]</ref>, questionspecific multiple choices are not provided. Instead, the global set of all possible answer choices for all the questions are provided (more than 3,000). The challenge is then to select the correct answer choice from this set for the given imagequestion pair.</p><p>Zero-Shot: Comparing with VCR and VE, zero-shot on VQA is more challenging due to the large amount of answer choices (refer to supplement). Low-Shot: There is not a clear categorization of VQA questions. Based on our analysis of the first n-gram words of questions, we group and finalize to 8 types in total. In VQA, an image is also paired with several questions, 5.4 on average. Thus, we also rely on image-based sampling. We have two settings of image sampling in low-shot settings: 100 random images per question types and 1,000 random image per question types. After collecting up to 2 paired questions per each selected image, we have two low-shot set: a set with around 1,600 questions and another set with 16,000 questions. They both correspond to 0.3% and 3% of VQA. Each experiment is run more than 4 times. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Visual Commonsense Reasoning (VCR)</head><p>Results on the VCR dataset for Q?A, across several data availability and domain, shifted constraints, are shown in Tab. 1 (for additional metrics of Q2A and Q2AR, please see supplement). These results yield 5 key observations: 1) From the 1st row, pretrained unimodal encoder models are capable of strong zero-shot results in VCR "out-of-the-box": When TE and VE are both CLIP's vision and text encoders, the accuracy is over 58% on short-cut mitigated evaluation (a more difficult domain-shifted task), which is similar to some supervised approaches. 2) Based on the 2nd row, without enough downstream data to finetune, pretrained unimodal encoders may not adapt to the downstream domain well. In fact, new data may disturb the pretrained knowledge and result in even more poor performance than zero-shot.3) Our proposed MAD approach yields significant performance gains under low-shot data regimes across a range of student architectures: up to 52% for VL-BERT, and 47.7% for UNITER. 4) Our proposed MAD approach even benefits fully supervised tasks by 2.1% for VL-BERT and 3.8% for UNITER. 5) Our approach yields even higher gains under low-shot domain-shifted scenarios of shortcut mitigation (SM): up to 71.3% for VL-BERT and 61% for UNITER. 6) With only finetuning and not increasing any computation complexity for the student model, MAD outperforms a prior proposed approach to leverage CLIP's vision encoder and redo the pretraining steps: CLIP-Vil <ref type="bibr" target="#b45">[45]</ref>. 7) The experiments prove that our modularized multimodal distillation method is generalizable and flexible. It can maintain effectiveness regardless of whether the pretrained vision and text encoders come from the same pretraining framework or separate ones. MAD with VILLA delivers high performance on the public leaderboard (Q2A: 79.6% QA2R: 82.9% Q2AR: 66.2%), achieving a new state-of-art of Q2A performance compared to other single models that are pretrained with imagetext data, as well as overall state-of-art comparable performance for QA2R and Q2AR.</p><p>As our approach is model agnostic, ensembles are possible. Combining multiple MAD approaches, further significant gains in performance (Q2A: 80.93%, QA2R: 84.01%) in the fully-sampled standard validation set.  As shown in <ref type="table">Table S3</ref>, we performed an ablation study of individual components of our MD and MAD frameworks, evaluated with VL-BERT as the student model. These results demonstrate 3 key observations: 1) Language distillation, not vision (as studied in previous works), contributes the most performance improvement. However, distilling both vision and language perform better than either one alone. 2) Multimodal distillation with an adaptive mechanism like token selection, and confidence weighting can produce better performance than naive sequence-to-sequence multimodal distillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Head Number</head><p>Layer Number </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VL-BERT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VL-BERT-Distilled</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Visual Entailment (SNLI-VE)</head><p>Results on SNLI-VE are shown in <ref type="table">Table 3</ref>, revealing 3 key observations: 1) Pretrained unimodal encoders can achieve up to 40.02 % zero-shot accuracy on val set of SNLI-VE "out-of-the-box", which is significantly higher than baseline methods like VL-BERT and UNITER. 2) MAD continues to provide accuracy improvements in all settings of zero-shot, low-shot, and fully-supervised conditions, across all student base models, versus both baselines with no distillation, and MD approach. 3) MAD continues to outperform concurrent work CLIP-ViL p under all evaluated conditions.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Visual Question Answering (VQA)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Analysis</head><p>Increased Utilization of Vision Modality: Following <ref type="bibr" target="#b6">[6]</ref>, we measure Modality Importance (MI) of both modalities. After distillation, refer to <ref type="figure" target="#fig_3">Fig. 3</ref>, we observe that the Vision MI is increased in (a) and the MI difference between Vision and Text is decreased in (b).</p><p>Regulating Shortcuts: Additionally, as V+L models are prone to learn trivial shortcuts from questions to correct answers <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b61">61]</ref>, performance degrades when tested on datasets that mitigate these signals. We further seek to qualitatively better understand how our distillation approach might help improve the performance in this setting. <ref type="figure" target="#fig_4">Fig. 4</ref>, shows token attention values from a VL-BERT model on an instance from the VCR dataset, before and after distillation, in addition to the token selection scores of our MAD approach. One can see that trivial tokens such as "is" have the highest attention values prior to distillation. By contrast, the token selection forces emphasis on more meaningful terms, such as "sending," "telegram," etc. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this work, we explore leveraging pretrained unimodal encoders to improve existing pretrained multi-modal encoders for VL tasks. To accomplish this we present a new approach, Multi-modal Adaptive Distillation (MAD), to perform adaptive distillation from pretrianed unimodal encoders to a variety of high performing multi-modal student encoders, including VL-BERT, UNITER, and VILLA. We evaluate this approach over a new comprehensive VL task protocol involving VCR, SNLI-VE, and VQA, covering zero-shot, low-shot, fullysupervised, and domain-shifted settings. Our results demonstrate significant improvements in performance across datasets, tasks, and settings, compared with baselines.   <ref type="table">Table S1</ref>. Complete VCR Evaluation Results. In the first row, we utilize pretrained unimodal encoders of CLIP-T and CLIP-V to conduct zero-shot evaluation. IA represents that we disregard question infor and only we measure the cosine distance between image feature and answer feature when solving Q2A task. IR represents similar procedure between image and rationale features. ? represents evaluation with our methods. entry is displayed under the name, CLIP-TD 7 with test result: Q2A 79.6% QA2R 82.9% Q2AR 66.2%. For more thoroughly comparing our work with others, we also submitted its entry to the other VCR public leaderboard hosted by DARPA. It is ranked as the 4th overall entry with a comparatively weaker base model among our student models, VL-BERT <ref type="bibr" target="#b47">[47]</ref>. Meanwhile, we are also submitting our ensemble models' prediction results to the leaderboard, as shown in the second last row of Tab. S1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Standard Validation Results</head><p>As in Tab. S1, we include comparison of our methods on top of top-performing base models for all three evaluation metrics:</p><p>Q2A, QA2R and Q2AR. Our method consistently improves on top of the base models across all metrics.  Shortcut Mitigation Results For better evaluating our algorithms' performance in settings in real world scenarios where domain shift may often occur. We follow <ref type="bibr" target="#b61">[61]</ref>'s modification and compare our method against others in its modified validation set. The evaluation results are listed in Tab. 1 and Tab. 2 of the main paper submission. The modified example is shown in <ref type="figure" target="#fig_2">Fig. S2</ref>. Motivated by the observation that "The correct option has the most overlap with the question" as showned in <ref type="figure">Fig. ?</ref>? and stated in <ref type="bibr" target="#b61">[61]</ref>, the modification mainly focus on changing the pronouns of the correct and incorrect answer choices. As showned in <ref type="figure" target="#fig_2">Fig. S2</ref>, the correct answers' pronouns are changed to be different from the question and the incorrect answer choices' pronouns are changed to be the same, instead.</p><p>VCR Dataset Question Sub-Types According to <ref type="bibr" target="#b63">[63]</ref>, among VCR questions, 38% fall into explanation (why, how come, etc.), 24% activity (doing, looking, event, etc.), 13% temporal (happened, before, after, etc.), 8% mental (feeling, thinking, etc.), 7% role (relations, occupations, etc.), 5% scene (where, near, etc.), and 5% hypothetical (if, would, could, etc.). Details can be referred to <ref type="bibr" target="#b63">[63]</ref>.</p><p>Further Analysis of Multimodal Distillation Based on our results, we have seen the impact on end task performance of distilling knowledge from largely pretrained unimodal encoders into student VL models. However, a key question still remains: how much the improvement comes from the distillation of each modality respectively?</p><p>Following <ref type="bibr" target="#b6">[6]</ref>, we measure the Modality Importance (MI) of both visual modality and textual modality. This approach sums the attention weights across heads of each modality to understand how much each modality is weighted by the model. <ref type="figure" target="#fig_3">Fig. 3</ref> shows the average the MI values of all the heads for each layer on VL-BERT, both with and without MD, trained on the VCR dataset. One can clearly observe that prior to distillation, the model more heavily weights the text modality as being important to correctly choosing answers. After distillation, both vision and text modalities are more equally considered. This may also explain why the model yields such impressive performance improvements in low-shot and domain shifted scenarios. <ref type="figure" target="#fig_3">Fig. 3</ref>, we plot the MI values of all the heads across 12 layers in VL-BERT Base and VL-BERT Base with MD. It is obvious that, at the last layer, the textual MI heatmap on the right is denser than the visual MI heatmap on the left. This shows a common flaw from existing V+L models that they heavily rely on the textual information than the visual part indicating the shallow understanding of the visual scene in downstream tasks. However, in the bottom row, the difference between the left and right heatmaps is much smaller and the visual MI heatmap at the bottom is also clearly more denser than the one at the top. This could be further verified by 8.b VQA Zero-shot VQA is different from VCR and SNLI-VE in terms of number of answer labels for each sample data. For every image-question pair, VCR provides four answer choices and for every premise-hypothesis pair, only three answer labels are provided. While in VQA, for every image-question pair, more than 2000 answer labels are provided. Thus the format of VQA does not strictly follow the conventional Multiple-Choice-Question (MCQ) format. Given the vast variety of answer labels provided, every question in VQA may potentially map to more than one answer choice. This one-to-many mappings may introduce difficulty in evaluating learning algorithms' ability of answering the VQA questions correctly. Different from <ref type="bibr" target="#b45">[45]</ref> which naively measure the distance between image-question pair against all the answer labels and deliver a performance of almost 0 %. We highly suspect that is the objective solution in terms of evaluating largely pretrained unimodal encoders' generalization in solving VQA tasks.</p><p>In this work, we first apply fixed number of templates embedded with heuristic knowledge and convert every question into statement. Then we further utilize pretrained sentence embedding <ref type="bibr" target="#b39">[39]</ref> to measure the cosine distance between every question against all the answer labels. Based on the ranking of the cosine measurement, we filter all the answer labels to three set of answer candidates: Top 1, Top 3 and Top 10. We utilize CLIP's inference steps and prompt engineering and evaluate zero-shot performance of CLIP-V and CLIP-T across the three set, as shown in Tab. S5.   the authors, which we modified to include our distillation methods. CLIP-ViL p did not originally evaluate on VCR in their pre-print. However, we evaluated it on the VCR dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9.b Implementation Details</head><p>-Token Selectioin: Our ablation experiments with TS distillation show that when the number of tokens selected is 2, the highest performance is obtained, shown in Tab. S3.</p><p>-Confidence Weighting: With Confidence Weighted (CW) knowledge distillation method, during our experiment, we find out that the performance would achieve the optimal gain when the distillation is conducted across all four question-answer pairs instead of question-correct-answer pair only, as in Tab. S3.</p><p>-Adaptive Finetuning (AF) with Contrastive Knowledge: Before the last-step finetuning for the target downstream tasks, we conduct Adaptive Finetuning to adapt the model to the downstream domain. During the Adaptive Finetuning, the model is trained on the full training set. The loss contains the construction loss from the same set of pretraining tasks like Masked Language Modeling (MLM), Image-Text Matching (ITM), etc. as in <ref type="bibr" target="#b9">[9]</ref>. Additionally, we also include Naive Knowledge Distillation thus the final loss also includes the knowledge distillation loss besides the construction loss.</p><p>-VL-BERT: We train our model for 30 epochs with warm-up steps of 1000, SGD optimizer. Initial learning rate is 7.0e?5 and decays by 0.1 at the 14th, 18th and 26th epoch. The gradient accumulation steps is set to be 4 on 8 NVIDIA V100 GPUs (32GB VRAM). The total number of layers for VL-BERT is 24 VL-BERT Large .</p><p>-UNITER: Started with warm up steps of 800, the model is trained with total steps of 8000. With AdamW optimizer, the intial learning rate is set to be 6e?05 with weight decay of 0.01 and batch size of 4000. The gradient accumulation steps is set to be 5 on 4 NVIDIA TITAN RTX GPUs (24GB VRAM).</p><p>-VILLA: Warm up steps is set to be 1000 and total training steps is 10000. The intial learning rate is 6e ? 05 with weight decay of 0.01 and AdamW optimizer.</p><p>The training batch size is 1250. The gradient accumulation steps is set to be 8 on 8 NVIDIA TITAN RTX GPUs (24GB VRAM).</p><p>-CLIP-ViL p : The model is trained for 20 epochs with batch size of 24. The optimizer is AdamW with a peak learning rate of 5 x 10 ?5 .</p><p>10 Baseline Method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10.a Multimodal Distillation</head><p>As illustrated in the top rows of Tab. S3, we experiment with a wide spectrum of disllation weight and realize that the performance is best optimized when the weight is set to be 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10.b Adapters over CLIP</head><p>For more comprehensively exploring different options to utilize the pretrained unimodal encoders for downstream Vision-Language tasks, we also experimented to directly add adapters on top of unimodal encoders. As listed in Tab. S2, we eseentially have experimented with adding either MLP or attention layers on top of CLIP-V and CLIP-T. However, due to the large gap between the size of finetuning data and CLIP's original pretraining data, the adapters' limited capacity fail to adapt the preatrained unimodel encoders for the downstream tasks efficiently.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Structure diagram of Multimodal Distillation (MD) and Multimodal Adaptive Distillation (MAD). MAD is further improved on top of MD and MD is equivalent to MAD in terms of structure when both Confidence Weighting and Token Selection are removed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>(a) Heatmap of MI values for both visual and textual modality. Each cell represents a combination between 12 heads and 12 layers in VL-BERT Base model. The top row corresponds to the original VL-BERT Base model and the bottom row corresponds to VL-BERT Base with Multimodal Distillation. Index starts with 0. (b) Average Modality Importance (MI) values for each layer of VL-BERT with MAD (green) and baseline (blue). Shaded areas represent differences between vision and text modalities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Comparison of attentions from VL-BERT before and after distillation, and Token Selective Module scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Materials 7 Overview</head><label>7</label><figDesc>Section 8.a covers additional details regarding our evaluations on the VCR dataset, including our public leaderboard results (8.a), additional metrics for internal validation results (8.a, 8.a), information about question sub-types (8.a), and further analysis of model behavior before and after distillation (8.a). Section 9 provides additional details regarding training code configurations (9.a) and parameters in all of our experiments (9.b). Section 10 covers additional details regarding our baselines for Multimodal Distillation (MD), Multimodal Adaptive Distillation (MAD) (10.a) and adapters over CLIP (10.b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. S1 .</head><label>S1</label><figDesc>An example of the standard validation set in VCR. The correct answer has the same pronoun as the question while the incorrect answer choices may not.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. S2 .</head><label>S2</label><figDesc>Examples of modified samples in SM validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The dataset includes 290k questions, in reference to 110k unique visual scenes. The questions are constituted into 7 categories based on patterns in the questions. Please see the supplementary material for a full list. Zero-Shot: No training data is used. The pretrained model is directly employed to produce a matching between the image-question pair and a candidate answer. Answers are selected based on which produce the best matches, according to the model's matching measure.Low-Shot:In the low-shot setting, we have 2 training set partitions of varying sizes. Since VCR has 7 types of questions thus (1) we select 100 examples per question category, totalling 700 pairs, or 0.3% of the entire dataset, and (2) 1,000 examples per category, totalling 7,000 pairs, or 3%. Each experiment is run more than 4 times. Standard Evaluation: In this evaluation setting we follow the standard protocol in the benchmark.</figDesc><table /><note>Shortcut Mitigated (SM):</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>VCR distillation ablation experiments using VL-BERT student model. Both vision and text teacher encoder models come from CLIP of ViT-B16. Results of lowshot experiments are averaged over more than 4 times (see supplement for standard deviations). L. represents text distillation and V. represents Vision distillation. SM = Shortcut Mitigated.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>SNLI-VE Results. Baselines represent the original methods without distillation. ? represents our methods. Training data subsampling shown under validation results. The low-shot experiment results are averaged over 4 runs. VQA Results. Baselines represent the original methods without distillation.</figDesc><table><row><cell></cell><cell>Base (Student) Model</cell><cell cols="3">Method Teacher Model</cell><cell></cell><cell>Validation</cell><cell></cell><cell cols="2">Test-Val Test-Std</cell></row><row><cell></cell><cell></cell><cell></cell><cell>VE</cell><cell>TE</cell><cell>0.3%</cell><cell>3%</cell><cell>100%</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">100 SP/C 1000 SP/C</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Adapter CLIP-V CLIP-T</cell><cell>16.14</cell><cell>37.41</cell><cell>51.34</cell><cell></cell><cell></cell></row><row><cell></cell><cell>VL-BERT</cell><cell>Baseline</cell><cell>-</cell><cell>-</cell><cell>35.33</cell><cell>63.29</cell><cell>69.05</cell><cell>71.79</cell><cell>72.22</cell></row><row><cell></cell><cell></cell><cell cols="3">MD? CLIP-V CLIP-T MAD?</cell><cell>36.83 37.12</cell><cell cols="2">64.43 65.71 71.42 70.22</cell><cell></cell><cell></cell></row><row><cell>Only</cell><cell>UNITER</cell><cell>Baseline</cell><cell>-</cell><cell>-</cell><cell>36.46</cell><cell>64.43</cell><cell>71.26</cell><cell>73.82</cell><cell>74.02</cell></row><row><cell>Finetune</cell><cell></cell><cell cols="3">MAD? CLIP-V CLIP-T</cell><cell>39.75</cell><cell cols="2">65.93 71.94</cell><cell></cell><cell></cell></row><row><cell></cell><cell>VILLA</cell><cell>Baseline</cell><cell>-</cell><cell>-</cell><cell>37.18</cell><cell>65.75</cell><cell>72.11</cell><cell>74.69</cell><cell>74.87</cell></row><row><cell></cell><cell></cell><cell>MAD?</cell><cell>CLIP-V</cell><cell cols="2">CLIP-T RoBERTa 39.04 40.16</cell><cell cols="3">66.93 73.02 75.81 66.23 72.43</cell><cell>76.04</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ViT</cell><cell>CLIP-T</cell><cell>38.79</cell><cell>66.10</cell><cell>72.02</cell><cell></cell><cell></cell></row><row><cell>Re-Pretrain</cell><cell>CLIP-ViL</cell><cell></cell><cell>CLIP-V</cell><cell></cell><cell>39.01</cell><cell>66.84</cell><cell>73.91</cell><cell>76.48</cell><cell>76.70</cell></row></table><note>? represents our methods. Training data subsampling shown under validation results. Test results were accomplished using full training set. The low-shot experiment results are averaged over 4 runs.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4</head><label>4</label><figDesc>shows our results from VQA. The key observations from these experiments are as follows: 1) MAD yields performance improvement against baseline and MD under all conditions. 2) Under low-shot conditions, MAD can outperform finetuning approach CLIP-ViL p 3) Under the full-shot setting, MAD can assist the baseline method, VILLA to reach the performance of 73.02 on validation, which is comparable to the performance from CLIP-VIL p . Further experimentation leveraging the model-agnostic MAD to create ensembles achieves 73.93 % on local validation, outperforming CLIP-ViL p .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table S2 .</head><label>S2</label><figDesc>Evaluation of zero-shot and adding adapters performance on CLIP-V, CLIP-T. IQA represents that we group both question and answer together as the text prompt. When calculating the cosine distance in each sample, we measure the distance between the image feature and the text prompt feature.</figDesc><table><row><cell cols="3">Method Teacher Model</cell><cell cols="3">Variation</cell><cell></cell><cell cols="4">Standard Evaluation</cell><cell cols="2">SM Evaluation</cell></row><row><cell></cell><cell>VE</cell><cell>TE</cell><cell></cell><cell cols="2">Prompt</cell><cell></cell><cell cols="6">0% 0.3% 3% 100% 0% 0.3% 3% 100%</cell></row><row><cell cols="3">Zero-shot CLIP-V CLIP-T</cell><cell></cell><cell cols="2">IQA IA</cell><cell></cell><cell>50.27 54.82</cell><cell></cell><cell></cell><cell cols="2">58.3 53.82</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">Adapter Head</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">1 Linear</cell><cell></cell><cell cols="4">30.13 30.39 31.86</cell><cell></cell><cell>28.1 27.43 29.31</cell></row><row><cell cols="3">Adapter CLIP-V CLIP-T</cell><cell></cell><cell cols="2">3 Linear</cell><cell></cell><cell cols="4">30.43 30.86 32.31</cell><cell></cell><cell>28.73 28.32 29.94</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">1 Transformer</cell><cell cols="4">34.41 36.34 36.42</cell><cell></cell><cell>33.35 34.02 35.48</cell></row><row><cell>Method</cell><cell>Variation</cell><cell></cell><cell>V. L.</cell><cell>TS</cell><cell cols="2">CW AF</cell><cell>Distillation Weight</cell><cell cols="4">Token # Standard Evaluation</cell><cell>SM Evaluation</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.3%</cell><cell>3%</cell><cell cols="2">100% 0.3%</cell><cell>3%</cell><cell>100%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">100 SP/C 1000 SP/C</cell><cell></cell><cell>100 SP/C 1000 SP/C</cell></row><row><cell>Baseline</cell><cell></cell><cell></cell><cell>--</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell>30.85</cell><cell>53.48</cell><cell cols="2">75.53 26.37</cell><cell>49.27</cell><cell>71.13</cell></row><row><cell>Unimodal Distillation</cell><cell></cell><cell></cell><cell>Y</cell><cell></cell><cell></cell><cell></cell><cell>0.05</cell><cell></cell><cell>34.24</cell><cell>54.53</cell><cell cols="2">75.92 31.11</cell><cell>51.16</cell><cell>72.24</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Y Y</cell><cell></cell><cell></cell><cell></cell><cell>1.0</cell><cell></cell><cell>33.39</cell><cell>51.57</cell><cell cols="2">75.85 33.22</cell><cell>50.05</cell><cell>71.65</cell></row><row><cell>Multimodal Distillation (MD ?)</cell><cell cols="2">Different Distillation Weight</cell><cell>Y Y Y Y Y Y</cell><cell></cell><cell></cell><cell></cell><cell>0.5 0.1 0.05</cell><cell></cell><cell>34.20 34.47 36.78</cell><cell>52.96 54.72 55.91</cell><cell cols="2">75.89 33.31 76.10 33.64 76.37 34.93</cell><cell>51.17 51.32 52.06</cell><cell>72.10 73.20 73.29</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Y Y</cell><cell></cell><cell></cell><cell></cell><cell>0.01</cell><cell></cell><cell>35.31</cell><cell>53.65</cell><cell cols="2">76.23 31.34</cell><cell>51.23</cell><cell>72.03</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Y Y</cell><cell>Y</cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>36.59</cell><cell>56.49</cell><cell cols="2">76.37 34.70</cell><cell>52.03</cell><cell>72.30</cell></row><row><cell>+ Token Selection</cell><cell cols="2">Different Token # w/o language prior (random selection)</cell><cell>Y Y Y Y Y Y</cell><cell>Y Y Y (Random)</cell><cell></cell><cell></cell><cell>Adaptive 0.05 ? T oken#+2 2</cell><cell>2 3 2</cell><cell>37.21 35.25 30.91</cell><cell>57.64 54.04 53.24</cell><cell cols="2">76.83 36.14 76.16 35.66 75.34 29.02</cell><cell>53.33 51.56 50.43</cell><cell>73.78 72.41 72.13</cell></row><row><cell>+ Confidence Weighting (CW)</cell><cell></cell><cell></cell><cell>Y Y</cell><cell>Y</cell><cell>Y</cell><cell></cell><cell></cell><cell>2</cell><cell>38.28</cell><cell>58.68</cell><cell cols="2">77.15 37.82</cell><cell>54.12</cell><cell>74.24</cell></row><row><cell>+ Adaptive</cell><cell cols="2">w/o distillation (2nd-stage "pretraining")</cell><cell>Y Y</cell><cell>Y</cell><cell cols="2">Y Y</cell><cell>-</cell><cell></cell><cell>38.32</cell><cell>54.13</cell><cell cols="2">76.38 36.42</cell><cell>53.07</cell><cell>72.87</cell></row><row><cell>Finetuning (AF) MAD (S)</cell><cell cols="2">w/ distillation (MAD ?) + Semi-supervision</cell><cell>Y Y Y Y</cell><cell>Y Y</cell><cell cols="2">Y Y Y Y</cell><cell>Adaptive 0.05 ? T oken#+2 2</cell><cell>2 2</cell><cell>39.02 54.43</cell><cell>56.23 61.33</cell><cell cols="2">76.93 38.26 77.61 46.71</cell><cell>55.87 58.18</cell><cell>74.04 74.55</cell></row><row><cell cols="13">Table S3. VCR distillation ablation experiments using VL-BERT student model.</cell></row><row><cell cols="13">Training data subsampling shown under evaluation protocol. SM = Shortcut Miti-</cell></row><row><cell>gated.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">8 Evaluation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">8.a VCR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="13">Public Leaderboard Results We submitted our single model test prediction</cell></row><row><cell cols="13">results to the VCR public leaderboard, which is currently listed as the 12th entry</cell></row><row><cell cols="13">overall (including ensemble models and models without any reference or publi-</cell></row><row><cell cols="13">cation) and achieves State-Of-The-Art (SOTA) performance on VCR compared</cell></row><row><cell cols="13">to other public single models 6 that are pretrained with image-text data. The</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Low-shot For every low-shot experiment across VCR, VQA and SNLI-VE, we average every experiment's result over at least four runs. The corresponding variance is also listed in Tab. S4, Tab. S7 and Tab. S6. +8.27)? 1.32 61.49 (+4.48)? 0.89 41.98 (+12.57)? 1.15 56.85 (+2.7)? 1.23</figDesc><table><row><cell>Base (Student) Model</cell><cell cols="3">Method Teacher Model</cell><cell cols="3">Standard Evaluation</cell><cell></cell><cell></cell><cell cols="2">SM Evaluation</cell></row><row><cell></cell><cell></cell><cell>VE</cell><cell>TE</cell><cell>0.3%</cell><cell>Var</cell><cell>3%</cell><cell>Var</cell><cell>0.3%</cell><cell>Var</cell><cell>3%</cell><cell>Var</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>100 SP/C</cell><cell></cell><cell>1000 SP/C</cell><cell></cell><cell>100 SP/C</cell><cell></cell><cell>1000 SP/C</cell></row><row><cell></cell><cell>Baseline</cell><cell>-</cell><cell>-</cell><cell>30.85</cell><cell>1.04</cell><cell>53.48</cell><cell>0.28</cell><cell>26.37</cell><cell>0.47</cell><cell>49.27</cell><cell>0.83</cell></row><row><cell>VL-BERT</cell><cell cols="3">MD? CLIP-V CLIP-T MAD?</cell><cell cols="8">36.78 40.43 (+9.58)? 0.44 58.98 (+5.5)? 0.51 39.27 (+12.9)? 0.28 54.88 (+5.61)? 0.61 1.48 55.91 0.85 34.93 0.85 52.06 1.35</cell></row><row><cell></cell><cell>Baseline</cell><cell>-</cell><cell>-</cell><cell>34.84</cell><cell>0.87</cell><cell>57.01</cell><cell>0.72</cell><cell>29.41</cell><cell>0.90</cell><cell>54.15</cell><cell>1.34</cell></row><row><cell>VILLA</cell><cell cols="2">MAD? CLIP-V</cell><cell cols="2">CLIP-T RoBERTa 43.11 (42.95</cell><cell>1.14</cell><cell>60.93</cell><cell>1.51</cell><cell>41.97</cell><cell>0.59</cell><cell>55.20</cell><cell>1.07</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table S4 .</head><label>S4</label><figDesc>Low-shot evaluation on VCR with variance. Every low-shot experiment result is averaged over four runs. ? The value in () representing the difference against the baseline.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table S5 .</head><label>S5</label><figDesc>Zero-shot performance of CLIP-V and CLIP-T on VQA.Low-shot For every low-shot experiment across VCR, VQA and SNLI-VE, we average every experiment's result over at least four runs. The corresponding variance is also listed in Tab. S4, Tab. S7 and Tab. S6.</figDesc><table><row><cell></cell><cell cols="6">Question Type Top 1 Top 3 Top 10</cell></row><row><cell></cell><cell></cell><cell cols="2">Overall</cell><cell cols="3">21.77 53.67 71.64</cell></row><row><cell></cell><cell></cell><cell cols="2">Binary</cell><cell cols="3">14.01 41.12 41.12</cell></row><row><cell></cell><cell></cell><cell cols="2">Number</cell><cell cols="3">2.75 50.9 9.62</cell></row><row><cell></cell><cell></cell><cell cols="2">Others</cell><cell cols="3">6.58 10.37 19.63</cell></row><row><cell>Base (Student) Model</cell><cell cols="4">Method Teacher Model</cell><cell cols="3">Validation</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.3 %</cell><cell></cell><cell>3 %</cell></row><row><cell></cell><cell></cell><cell>VE</cell><cell>TE</cell><cell></cell><cell>(100 SP/C)</cell><cell></cell><cell>(1000 SP/C)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Accuracy</cell><cell>Var</cell><cell>Accuracy</cell><cell>Var</cell></row><row><cell></cell><cell>Baseline</cell><cell>-</cell><cell>-</cell><cell></cell><cell>53.28</cell><cell>0.12</cell><cell>62.31</cell><cell>0.46</cell></row><row><cell>VL-BERT</cell><cell cols="4">MD? CLIP-V CLIP-T MAD?</cell><cell cols="3">56.02 56.78 (+3.5)? 1.31 65.37 (+3.06)? 1.09 0.46 64.92 0.74</cell></row><row><cell></cell><cell>Baseline</cell><cell>-</cell><cell>-</cell><cell></cell><cell>58.47</cell><cell>0.76</cell><cell>67.16</cell><cell>0.28</cell></row><row><cell>VILLA</cell><cell cols="2">MAD? CLIP-V</cell><cell cols="5">CLIP-T 59.65 (+1.18)? 0.13 RoBERTa 58.48 0.43 68.86 (+1.70)? 0.66 68.43 0.15</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table S6 .</head><label>S6</label><figDesc>Low-shot evaluation on VQA with variance. Every low-shot experiment result is averaged over four runs. ? The value in () representing the difference against the baseline.8.c SNLI-VELow-shot For every low-shot experiment across VCR, VQA and SNLI-VE, we average every experiment's result over at least four runs. The corresponding variance is also listed in Tab. S4, Tab. S7 and Tab. S6.All the baseline experiment results with models including VL-BERT, UNITER Base, UNITER Large, VILLA and CLIP-ViL p are based on code provided by</figDesc><table><row><cell>Base (Student) Model</cell><cell cols="3">Method Teacher Model</cell><cell cols="3">Validation</cell></row><row><cell></cell><cell></cell><cell>VE</cell><cell>TE</cell><cell>0.3% (100 SP/C)</cell><cell></cell><cell>3% (1000 SP/C)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Accuracy</cell><cell>Var</cell><cell>Accuracy</cell><cell>Var</cell></row><row><cell></cell><cell>Baseline</cell><cell>-</cell><cell>-</cell><cell>35.33</cell><cell>0.31</cell><cell>63.29</cell><cell>0.52</cell></row><row><cell>VL-BERT</cell><cell cols="3">MD? CLIP-V CLIP-T MAD?</cell><cell cols="4">36.83 37.12 (+1.79)? 0.58 65.71 (+2.42)? 1.02 0.38 64.43 0.27</cell></row><row><cell></cell><cell>Baseline</cell><cell>-</cell><cell>-</cell><cell>37.18</cell><cell>0.44</cell><cell>65.75</cell><cell>0.30</cell></row><row><cell>VILLA</cell><cell cols="2">MAD? CLIP-V</cell><cell cols="5">CLIP-T 40.16 (+2.98)? 1.09 66.93(+1.18)? 0.82 RoBERTa 39.04 0.53 66.23 0.62</cell></row><row><cell cols="2">9 Training Details</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>9.a Baselines</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table S7 .</head><label>S7</label><figDesc>Low-shot evaluation on SNLI-VE with variance. Every low-shot experiment result is averaged over four runs. ? The value in () representing the difference against the baseline.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The unit is sample data point. One data point can represent an image in visual pretraining and phrases, sentences in text pretraining.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">visual img token refer to the first token in visual sequence</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">single models with public reference or publication</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">A former name with CLIP's vision and text encoders as the teacher models.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Don&apos;t just assume; look and answer: Overcoming priors for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4971" to="4980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">VQA: Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<title level="m">Beit: Bert pre-training of image transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<title level="m">Language models are few-shot learners</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Yake! keyword extraction from single documents using multiple local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mangaravite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pasquali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jorge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nunes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jatowt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">509</biblScope>
			<biblScope unit="page" from="257" to="289" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.ins.2019.09.013</idno>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S0020025519308588" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Behind the scene: Revealing the secrets of pre-trained vision-and-language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="565" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Fewshotqa: A simple framework for few-shot learning of question answering tasks using pre-trained text-to-text models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natarajan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.01951</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Wasserstein contrastive representation distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16296" to="16305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Uniter: Universal image-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dealing with missing modalities in the visual question answer-difference prediction task through knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1592" to="1601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Beyond question-based biases: Assessing multimodal shortcut learning in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dancette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.03149</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Compressing visuallinguistic model via knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1428" to="1438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Seed: Self-supervised distillation for visual representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.04731</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Born again neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Furlanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1607" to="1616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Large-scale adversarial training for vision-and-language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06195</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Making the v in vqa matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6904" to="6913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Zero-shot detection via vision and language knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<idno>abs/2104.13921</idno>
		<ptr target="https://arxiv.org/abs/2104.13921" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Seeing out of the box: End-to-end pre-training for vision-language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.00849</idno>
		<title level="m">Pixel-bert: Aligning image pixels with text by deep multi-modal transformers</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05918</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Avoiding reasoning shortcuts: Adversarial evaluation, training, and model development for multi-hop qa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07132</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.10351</idno>
		<title level="m">Tinybert: Distilling bert for natural language understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Vilt: Vision-and-language transformer without convolution or region supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.07947</idno>
		<title level="m">Sequence-level knowledge distillation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kovaleva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rumshisky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08593</idno>
		<title level="m">Revealing the dark secrets of bert</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Align before fuse: Vision and language representation learning with momentum distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Gotmare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.07651</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Oscar: Object-semantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="121" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adaptive multi-teacher multi-level knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">415</biblScope>
			<biblScope unit="page" from="106" to="113" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning to specialize with knowledge distillation for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Transfer learning via unsupervised task discovery for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8385" to="8394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.10050</idno>
		<title level="m">Combined scaling for zero-shot transfer learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>abs/2103.00020</idno>
		<ptr target="https://arxiv.org/abs/2103.00020" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.03649</idno>
		<title level="m">Overcoming language priors in visual question answering with adversarial regularization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10084</idno>
		<title level="m">Sentence-bert: Sentence embeddings using siamese bertnetworks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01108</idno>
		<title level="m">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">What do models learn from question answering datasets?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saffari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03490</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cycle-consistency for robust visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6649" to="6658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">How much can clip benefit vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">How much can CLIP benefit vision-and-language tasks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno>CoRR abs/2107.06383</idno>
		<ptr target="https://arxiv.org/abs/2107.06383" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08530</idno>
		<title level="m">Vl-bert: Pre-training of generic visual-linguistic representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Vl-bert: Pre-training of generic visual-linguistic representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SygXPaEYvH" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.09355</idno>
		<title level="m">Patient knowledge distillation for bert model compression</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.14167</idno>
		<title level="m">Contrastive distillation on intermediate representations for language model compression</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02984</idno>
		<title level="m">Mobilebert: a compact task-agnostic bert for resource-limited devices</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">LXMERT: learning cross-modality encoder representations from transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<idno>abs/1908.07490</idno>
		<ptr target="http://arxiv.org/abs/1908.07490" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05546</idno>
		<title level="m">Zero-shot visual question answering</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Contrastive multiview coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="776" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Knowledge distillation and student-teacher learning for visual intelligence: A review and new outlooks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.08587</idno>
		<title level="m">Sgeitl: Scene graph enhanced image-text learning for visual commonsense reasoning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.10904</idno>
		<title level="m">Simvlm: Simple visual language model pretraining with weak supervision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadav</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.06706</idno>
		<title level="m">Visual entailment: A novel task for finegrained image understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.02925</idno>
		<title level="m">Bert-of-theseus: Compressing bert by progressive module replacing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04289</idno>
		<title level="m">Knowledge distillation via adaptive instance normalization</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A case study of the shortcut effects in visual commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kovashka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Ernie-vil: Knowledge enhanced vision-language representations through scene graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: AAAI. vol</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">From recognition to cognition: Visual commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.02636</idno>
		<title level="m">Merlot: Multimodal neural script knowledge models</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Yin and yang: Balancing and answering binary visual questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5014" to="5022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Vinvl: Revisiting visual representations in vision-language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5579" to="5588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.09106</idno>
		<title level="m">Regionclip: Region-based language-image pretraining</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Unified vision-language pre-training for image captioning and vqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: AAAI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13041" to="13049" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
