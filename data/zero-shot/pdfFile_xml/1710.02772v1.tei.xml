<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Smarnet: Teaching Machines to Read and Comprehend Like Human</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheqian</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="laboratory">State Key Lab of CAD&amp;CG</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongqin</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="laboratory">State Key Lab of CAD&amp;CG</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Cao</surname></persName>
							<email>bincao@aidigger.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Eigen Technologies</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
							<email>zhaozhou@zju.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
							<email>dengcai@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="laboratory">State Key Lab of CAD&amp;CG</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Smarnet: Teaching Machines to Read and Comprehend Like Human</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Machine Comprehension (MC) is a challenging task in Natural Language Processing field, which aims to guide the machine to comprehend a passage and answer the given question. Many existing approaches on MC task are suffering the inefficiency in some bottlenecks, such as insufficient lexical understanding, complex question-passage interaction, incorrect answer extraction and so on. In this paper, we address these problems from the viewpoint of how humans deal with reading tests in a scientific way. Specifically, we first propose a novel lexical gating mechanism to dynamically combine the words and characters representations. We then guide the machines to read in an interactive way with attention mechanism and memory network. Finally we add a checking layer to refine the answer for insurance. The extensive experiments on two popular datasets SQuAD and TriviaQA show that our method exceeds considerable performance than most stateof-the-art solutions at the time of submission.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Teaching machines to learn reading comprehension is one of the core tasks in NLP field. Recently machine comprehension task accumulates much concern among NLP researchers. We have witnessed significant progress since the release of large-scale datasets like SQuAD <ref type="bibr" target="#b11">(Rajpurkar et al. 2016)</ref>, MS-MARCO <ref type="bibr" target="#b11">(Nguyen et al. 2016)</ref>, <ref type="bibr">TriviaQA (Joshi et al. 2017)</ref>, <ref type="bibr">CNN/Daily Mail (Hermann et al. 2015</ref>) and Children's Book Test <ref type="bibr" target="#b4">(Hill et al. 2015)</ref>. The essential problem of machine comprehension is to predict the correct answer referring to a given passage with relevant question. If a machine can obtain a good score from predicting the right answer, we can say the machine is capable of understanding the given context.</p><p>Many previous approaches <ref type="bibr" target="#b12">(Seo et al. 2016</ref>) (Gong and Bowman 2017) ) adopt attention mechanisms along with deep neural network tactics and pointer network to establish interactions between the question and the passage. The superiority of these frameworks are to enable questions focus on more relevant targeted areas within passages. Although these works have achieved promising performance for MC task, most of them still suffer from the The majority of the work was done while the first author was interning at the Eigen Technologies . For all the time through, we consider a philosophy question: What will people do when they are having a reading comprehension test? Recall how our teacher taught us may shed some light. As a student, we recite words with relevant properties such as part-of-speech tag, the synonyms, entity type and so on. In order to promote answer's accuracy, we iteratively and interactively read the question and the passage to locate the answer's boundary. Sometimes we will check the answer to ensure the refining accuracy. Here we draw a flow path to depict what on earth the scientific reading skills are in the <ref type="figure" target="#fig_0">Figure 1</ref>. As we see, basic word understanding, iterative reading interaction and attentive checking are crucial in order to guarantee the answer accuracy.</p><p>In this paper, we propose the novel framework named Smarnet with the hope that it can become as smart as humans. We design the structure in the view point of imitating how humans take the reading comprehension test. Specifically, we first introduce the Smarnet framework that exploits fine-grained word understanding with various attribution discriminations, like humans recite words with corresponding properties. We then develop the interactive attention with memory network to mimic human reading procedure. We also add a checking layer on the answer refining in order to ensure the accuracy. The main contributions of this paper are as follows: <ref type="figure">Figure 2</ref>: Fine-grained gating on lexical attributions of words and characters. "POS, NER, TF, EM, Surprisal, QType" refer to part-of-speech tags, named entity tags, term frequency, exact match, surprisal extent, question type.</p><p>? We enrich the word representation with detailed lexical properties. We adopt a fine-grained gating mechanism to dynamically control the amount of word level and character level representations based on the properties of words. ? We guide the machine to read by imitating human's behavior in the reading procedure. We apply the Interactive attention to comprehensively model the question and passage representation. We adopt memory network to enhance the comprehending capacity and accuracy. ? We utilize a checking mechanism with passage self alignment on the revised pointer network. This helps to locate the answer boundary and promote the prediction accuracy for insurance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task Description</head><p>The goal of open-domain MC task is to infer the proper answer from the given text. For notation, given a passage P = {p 1 , p 2 , ..., p m } and a question Q = {q 1 , q 2 , ..., q n }, where m and n are the length of the passage and the question. Each token is denoted as (w i , C i ), where w i is the word embedding extracts from pre-trained word embedding lookups, C i is the char-level matrix representing one-hot encoding of characters. The model should read and comprehend the interactions between P and Q, and predict an answer A based on a continuous sub-span of P .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Smarnet Structure</head><p>The general framework of MC task can be coarsely summarized as a three-layer hierarchical process: Input embedding layer, Interaction modeling layer, answer refining layer. We then introduce our model from these three perspectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Embedding Layer</head><p>Familiar with lexical and linguistic properties is crucial in text understanding. We try to enrich the lexical factors to enhance the word representation. Inspired by Yang et al. ) <ref type="bibr" target="#b10">(Monsalve, Frank, and Vigliocco 2012)</ref>   and , we adopt a more finegrained dynamic gating mechanism to model the lexical properties related to the question and the passage. Here we indicate our embedding method in <ref type="figure">Figure 2</ref>. We design two gates adopted as valves to dynamically control the flowing amount of word-level and character-level representations.</p><p>For the passage word E vp , we use the concatenation of part-of-speech tag, named entity tag, term frequency tag, exact match tag and the surprisal tag. The exact match denote as f em (p i ) = I(p i ? q) in three binary forms: original, lower-case and lemma forms, which indicates whether token p i in the passage can be exactly matched to a question word in q. The surprisal tag measures the amount of information conveyed by a particular word from Surprisal(w t ) = ?log(P (w t |w 1 , w 2 , ..., w t?1 )). The less occurrence of a word, the more information it carries.</p><p>For the question word E vq , we take the question type in place of the exact match information and remain the other features. The type of a question provides significant clue for the answer selection process. For example, the answer for a "when" type question prefers tokens about time or dates while a "why" type question requires longer inference. Here we select the top 11 common question types as illustrated in the diagram. If the model recognize a question's type, then all the words in this question will be assigned with the same QType feature.</p><p>The gates of the passage and the question are computed as follows:</p><formula xml:id="formula_0">g p = ?(W p E vp + b p ) g q = ?(W q E vq + b q )<label>(1)</label></formula><p>where W p , b p , W q , b q are the parameters and ?denotes an element-wise sigmoid function.</p><p>Using the fine-grained gating mechanism conditioned on the lexical features, we can accurately control the information flows between word-level and char-level. Intuitively, the formulation is as follows:</p><formula xml:id="formula_1">h p = g(E w , E c ) = g p ? E w + (1 ? g p ) ? E c h q = g(E w , E c ) = g q ? E w + (1 ? g q ) ? E c<label>(2)</label></formula><p>where ? is the element-wise multiplication operator. when the gate has high value, more information flows from the word-level representation; otherwise, char-level will take the dominating place. It is practical in real scenarios. For example, for unfamiliar noun entities, the gates tend to bias towards char-level representation in order to care richer morphological structure. Besides, we not only utilize the lexical properties as the gating feature, we also concatenate them as a supplement of lexical information. Therefore, the final representation of words are computed as follows:</p><formula xml:id="formula_2">E p = [h p , E vp ] E q = [h q , E vq ]<label>(3)</label></formula><p>where [h, E] is the concatenation function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interaction Modeling Layer</head><p>Recall how people deal with reading comprehension test. When we get a reading test paper, we read the question first to have a preliminary focal point. Then we skim the passage to refine the answer. Sometimes we may not directly ensure the answer's boundary, we go back and confirm the question. After confirming, we scan the passage and refine the right answer we thought. We also check the answer for insurance. Inspired by such a scientific reading procedure, we design the Smarnet with three components: contextual encoding, interactive attention with memory network, answer refining with checking insurance. As is shown in figure 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contextual Encoding</head><p>We use Gated Recurrent Unit (Cho et al. 2014) with bi-directions to model the contextual representations. Here, It is rather remarkable that we do not immediately put the Bi-GRU on the passage words. Instead, we first encode the question and then apply a gate to control the question influence on each passage word, as is shown in the structure (a) and (b). Theoretically, when human do the reading comprehension, they often first read the question to have a rough view and then skim the passage with the impression of the question. No one can simultaneously read both the question and the passage without any overlap. Vice versa, after skimming the passage to get the preliminary comprehension, the whole passage representation is also applied to attend the question again with another gating mechanism, as is shown in the structure (c). This can be explained that people often reread the question to confirm whether they have thoroughly understand it. The outputs of the three steps (a) (b) (c) are calculated as follows:</p><formula xml:id="formula_3">u q t = BiGRU q (u q t?1 , E q t ) Q 1 = ? ? u q n , ? ? u q n (4)</formula><p>where E q t ? R d is the lexical representation from the input layer. u q t ? R d is the hidden state of GRU for the tth question word. Q 1 ? R 2d is the original question semantic embedding obtained from the concatenation of the last hidden states of two GRUs.</p><formula xml:id="formula_4">g q1 = ?(W q1 Q 1 + b q1 ) E p1 = g q1 ? ?Q 1 + (1 ? g q1 ) ? E p u p t = GRU p (u p t?1 , E p1 t ) P 1 = ? ? u p m , ? ? u p m<label>(5)</label></formula><p>where g q1 ? R d is a question gate controlling the question influence on the passage. The larger the g q1 is, the more impact the question takes on the passage word. We reduce the Q 1 dimension through multi-layer perceptron ? since Q1 and E p1 are not in the same dimension. We then put the bi-GRU on top of each passage word to get the semantic representation of the whole passage P 1 .</p><formula xml:id="formula_5">g p1 = ?(W p1 P 1 + b p1 ) E q2 = g p1 ? ?P 1 + (1 ? g p1 ) ? u q t u q t = GRU q (u q t?1 , E q1 t ) Q 2 = [ ? ? u q n , ? ? u q n ]<label>(6)</label></formula><p>where g p1 is a passage gate similar to g q1 . ? is the multilayer perceptron to reduce dimension. Q 2 represents the confirmed question with the knowledge of the context.</p><p>Interactive Attention with Iterative Reasoning The essential point in answer refining lies on comprehensive understanding of the passage content under the guidance of the question. We build the interactive attention module to capture the mutual information between the question and the passage. From human's perspective, people repeatedly and interactively read the question and the passage, narrow the answer's boundary and put more attention on some passage parts where are more relevant to the question. We construct a shared similarity matrix S ? R m?n to attend the relevance between the passage P and the question Q. Each element s ij is computed by the similarity of the ith passage word and the jth question word.</p><p>We signify relevant question words into an attended question vector to collaborate with each context word. Let a j ? R n represent the normalized attention distribution on the question words by tth passage word. The attention weight is calculated by a j = sof tmax(S :j ) ? R n . Hence the attend question vector for all passage words Q ? R 2d is obtained by Q = j a j ? Q :j , where Q :j ? R 2d?n .</p><p>We further utilize Q to form the question-aware passage representation. In order to comprehensively model the mutual information between the question and passage, we adopt a heuristic combining strategy to yield the extension as follows:</p><formula xml:id="formula_6">P t i = BiGRU (?(h q i , Q, h q i ? Q, h q i + Q))<label>(7)</label></formula><p>where P t i ? R 2d denotes the ith question-aware passage word under the tth hop, the ? function is a concatenation function that fuses four input vectors. h q i denotes the hidden state of former ith passage word obtained from BiGRU.</p><p>? denotes element-wise multiplication and + denotes the element-wise plus. Notice that after the concatenation of ?, the dimension can reaches to 8d. We feed the concatenation vector into a BiGRU to reduce the hidden state dimension into 2d. Using BiGRU to reduce dimension provides an efficient way to facilitate the passage semantic information and enable later semantic parsing.</p><p>Naive one-hop comprehension may fail to comprehensively understand the mutual question-passage information. Therefore, we propose a multi-hop memory network which allows to reread the question and answer. In our model, we totally apply two-hops memory network, as is depicted in the structure (c to e) and (f to h). In our experiment we found the two-hops can reach the best performance. In detail, the memory vector stores the question-aware passage representations, the old memory's output is updated through a repeated interaction attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer Selection with Checking Mechanism</head><p>The goal of open-domain MC task is to refine the sub-phrase from the passage as the final answer. Usually the answer span (i, j) is derived by predicting the start p i s and the end p e (j|i) indices of the phrase in the paragraph. In our model, we use two answer refining strategies from different levels of linguistic understanding: one is from original interaction output module and the other is from self-matching alignment. The two extracted answers are then applied into a checking component to final ensure the decision.</p><p>For the original interaction output in structure (h), we directly aggregate the passage vectors through BiGRU. We compute the p i s and p e (j|i) probability distribution under the instruction of (Wang and Jiang 2016) and pointer network <ref type="bibr" target="#b13">(Vinyals, Fortunato, and Jaitly 2015)</ref> by</p><formula xml:id="formula_7">p s1 = sof tmax(w T s1 P 1 i ) p e1 = sof tmax(w T e1 P 1 i )<label>(8)</label></formula><p>where P i is the output of the original interaction. w T s ? R 2d and w T e ? R 2d are trainable weight vectors. For the self-alignment in structure (j), we align the two hops outputs P i1 with P i2 in structure (e) and (h). The purpose of self-alignment aims to analysis the new insights on the passage as the comprehension gradually become clear after iterative hops. For each hop, the reader dynamically collects evidence from former passage representation and encodes the evidence to the new iteration. From human's perspective, each time we reread the passage, we get some new ideas or more solid comprehension on the basis of the former understanding. The self-alignment is computed by</p><formula xml:id="formula_8">P 1 = P 1 n g Pi1 = ?(W P1 P 1 + b P1 ) E Pi2 = g Pi1 ? P 1 + (1 ? g Pi1 ) ? P 2 i P i 2 = BiGRU ( P 2 i?1 , E Pi2 )<label>(9)</label></formula><p>where P 1 ? R 2d is the first hop whole passage vector in the structure (e). We apply a gate mechanism with P 1 to control the evidence flow to the next hop P i2 ? R 2d . The output of self-alignment is computed by</p><formula xml:id="formula_9">p s2 = sof tmax(w T s2 P 2 i ) p e2 = sof tmax(w T e2 P 2 i )<label>(10)</label></formula><p>where p s2 and p e2 are the predicted start and end indices after the self-alignment. For insurance, we obtain two groups of predicted answer span P s1 ? P e1 and P s2 ? P e2 . We then apply a checking strategy to compare the twice answer. This process is quite similar to human's behavior, people often reread the passage and may draw some different answers. Thus they need to compare the alternative answers and finally consider a best one. Here we employ the weighted sum of twice answer prediction indices to make the final decision:</p><formula xml:id="formula_10">p s = p s1 + ?p s2 p e = p e1 + ?p e2 s = argmax(p 1 s , p 2 s , ..., p m s ) e = argmax(p 1 e , p 2 e , ..., p m e )<label>(11)</label></formula><p>where ? ? 1 is a weighted scalar controlling the proportion of the two predicted answers. We set the ? ? 1 as in most cases the latter predicted answer is more accurate comparing to the former one. The final s and e is then judged by the max value through the argmax operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training and Optimization</head><p>We choose the training loss as the sum of the negative log probabilities of the true start and end position by the predicted distributions to train our model:</p><formula xml:id="formula_11">L(?) = ? 1 N N i logp L s (y s i ) + logp L e (y e i )<label>(12)</label></formula><p>where ? denotes all the model coefficients including the neural network parameters and the input gating function parameters, N is the number dataset examples, p L s and p L e are the predicted distributions of the output, y s i and y e i are the true start and end indices of the ith example. The objective function in our learning process is given by:</p><formula xml:id="formula_12">min?(?) = L(?) + ? ? 2 (13)</formula><p>where ? is the trade-off parameter between the training loss and regularization. To optimize the objective, we employ the stochastic gradient descent (SGD) with the diagonal variant of AdaDelta (Zeiler 2012).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Datasets</head><p>In this section we evaluate our model on the task of machine comprehension using the recently released large-scale datasets SQuAD <ref type="table">(</ref> There are some similar settings between these two datasets. Each answer to the question is a segment of text from the corresponding reading context. Two metrics are used to evaluate models: Exact Match (EM) measures the percentage of predictions that match the ground truth answer exactly. F1 score measures the average overlap between the prediction and ground truth answer. Both datasets are randomly partitioned into training set (80%), dev set (10%) and test set (10%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implemental Details</head><p>We preprocess each passage and question using the library of nltk <ref type="bibr" target="#b8">(Loper and Bird 2002)</ref> and exploit the popular pretrained word embedding GloVe with 100-dimensional vectors (Pennington, Socher, and Manning 2014) for both questions and passages. The size of char-level embedding is also set as 100-dimensional and is obtained by CNN filters under the instruction of (Kim 2014). The Gated Recurrent Unit <ref type="bibr" target="#b2">(Cho et al. 2014</ref>) which is variant from LSTM (Hochreiter and Schmidhuber 1997) is employed throughout our model. We adopt the AdaDelta (Zeiler 2012) optimizer for training with an initial learning rate of 0.0005. The batch size is set to be 48 for both the SQuAD and TriviaQA datasets. We also apply dropout (Srivastava et al. 2014) between layers with a dropout rate of 0.2. For the multi-hop reasoning, we set the number of hops as 2 which is imitating human reading procedure on skimming and scanning. During training, we set the moving averages of all weights as the exponential decay rate of 0.999 <ref type="bibr" target="#b9">(Lucas and Saccucci 2000)</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overall Results</head><p>We evaluate the performance of our proposed method based on two evaluation criteria EM and F1 for the MC tasks. We compare our model with other strong competitive methods on the SQuAD leaderboard and TriviaQA leaderboard. <ref type="table" target="#tab_1">Table 1 and Table 2</ref> respectively show the performance of single and ensemble models on the SQuAD leaderboard. The SQuAD leaderboard is very competitive among top NLP researchers from all over the world. We can see the    top record has been frequently broken in order to reach the human's level. Our model was submitted by July 14, 2017, thus we compare our model on the single and ensemble performance against other competitor at that time.</p><p>From the tables 1 and 2 we can see our single model achieves an EM score of 71.415% and a F1 score of 80.160% and the ensemble model improves to EM 75.989% and F1 83.475%, which are both only after the r-net method  at the time of submission. These results sufficiently prove the significant superiority of our proposed model.</p><p>We also compare our models on the recently proposed dataset TriviaQA. <ref type="table" target="#tab_3">Table 3</ref> shows the performance comparison on the test set of TriviaQA. We can see our Smarnet model outperforms the other baselines on both wikipedia domain and web domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Results</head><p>We respectively evaluate the individual contribution of the proposed module in our model. We conduct thorough abla-  <ref type="table" target="#tab_4">table 4 and table 5</ref>. <ref type="table" target="#tab_4">Table 4</ref> shows the different effect of the lexical features. We see the full features integration obtain the best performance, which demonstrates the necessity of combining all the features into consideration. Among all the feature ablations, the Part-Of-Speech, Exact Match, Qtype features drop much more than the other features, which shows the importance of these three features. As the POS tag provides the critical lexical information, Exact Match and Qtype help to guide the attention in the interaction procedure. As for the final ablation of POS and NER, we can see the performance decays over 3% point, which clearly proves the usefulness of the comprehensive lexical information. <ref type="table" target="#tab_5">Table 5</ref> shows the ablation results on the different levels of components. We first replace our input gate mechanism into simplified feature concatenation strategy, the performance drops nearly 2.3% on the EM score, which proves the effectiveness of our proposed dynamic input gating mechanism. We then compare two methods which directly encode the passage words or use the question influence. The result proves that our modification of employing question influence on the passage encoding can boost the result up to 1.3% on the EM score. In our model, we apply two-hops memory network to further comprehend the passage. In the ablation test, we remove the iterative hops of memory network and only remain one interaction round. The result drops 2.6% point on the EM score, which indicate the significance of using memory network mechanism. Finally, we compare the last module of our proposed self-alignment checking with original pointer network. The final result shows the superiority of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameters Tuning</head><p>We conduct two parameters tuning experiments in order to get the optimal performance. <ref type="figure">Figure 4</ref> shows the results on different hops of memory network. We see the number of hops set to 2 can get the best performance comparing to other number of hops. In addition, as the number of hops enlarges, the model is easily to get overfitting on the training set, hence the performance is decrease rather than increase. In <ref type="figure" target="#fig_3">figure 5</ref>, we set different weight of ? into five groups {1.0, 1.25, 1.5, 1.75, 2.0}. The final results show that the proportion of the first answer prediction and the last one reaches to 2:3 can get the most confident answer judgement. The value of ? which is greater than 1, indicating that the latter answer refining takes more insurance on the prediction decision.  <ref type="bibr" target="#b12">(Seo et al. 2016</ref>) use bi-directional attention flow mechanism and a multi-stage hierarchical process to represent the context. Xiong et al. <ref type="bibr" target="#b14">(Xiong, Zhong, and Socher 2016)</ref> propose introduce multi-layer embedding with memory network for full orientation matching on MC task. In our paper, we also adopt the memory network to mimic human behaviors on increasing their understanding by reread the context and the query multi times. We also apply a multi-hop checking mechanism to better refine the true answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>In this paper, we tackle the problem of machine comprehension from the viewpoint of imitating human's ways in having reading comprehension examinations. We propose the Smarnet framework with the hope that it can become as smart as human for the reading comprehension problem. We first introduce a novel gating method with detailed word attributions to fully exploit prior knowledge of word semantic understanding. We then adopt a scientific procedure to guide machines to read and comprehend by using interactive attention and matching mechanisms between questions and passages. Furthermore, we employ the self-alignment with checking strategy to ensure the answer is refined after careful consideration. We evaluate the performance of our method on two large-scale datasets SQuAD and Trivi-aQA. The extensive experiments demonstrate the superiority of our Smarnet framework.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>A scientific reading flow method inefficiency in three perspectives: (1) Comprehensive understanding on the lexical and linguistic level. (2) Complex interactions among questions and passages in a scientific reading procedure. (3) Precise answer refining over the passage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The overview of Smarnet structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Rajpurkar et al. 2016) and TriviaQA (Joshi et al. 2017). SQuAD published by Stanford has obtained a huge attention over the past two years. It is composed of over 100K questions manually annotated by crowd workers on 536 Wikipedia articles. TriviaQA is a newly released opendomain MC dataset which consists of over 650K questionanswer-evidence triples. It is derived by combining 95K Trivia enthusiast authored question-answer pairs with on average six supporting evidence documents per question. The length of contexts in TriviaQA is much longer than SQuAD and models trained on TriviaQA require more cross sentence reasoning to find answers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Performance across different weight of ? tion experiments on the SQuAD dev set, which are recorded on the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>dynamic coattention networks to iteratively infer the answer. Yang et al. (Yang et al. 2016) present a finegrained gating mechanism to dynamically combine wordlevel and character-level representations. Wang et al. (Wang et al. 2017) introduce self-matching attention to refine the gated representation by aligning the passage against itself. Reasoning by Memory Network Multi-hop reasoning combines with Memory networks have shown powerful competence on MC task (Shen et al. 2017) (Dhingra et al. 2017) (Sordoni et al. 2016) (Xiong, Zhong, and Socher 2016) (Hu, Peng, and Qiu 2017) (Gong and Bowman 2017) (Kumar et al. 2016). Theoretically, multi-hop memory networks can repeat computing the attention biases between the query and the context through multiple layers. The memory networks typically maintain memory states which incorporate the information of current reasoning with the previous storage in the memory. Hu et al. (Hu, Peng, and Qiu 2017) utilize a multi-hop answer pointer which allows the network to continue refining the predicted answer span. Gong et al. (Gong and Bowman 2017) adapt the BIDAF (Seo et al. 2016) with multi-hop attention mechanisms and achieve substantial performance. Pan et al. (Pan et al. 2017)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance of ensemble Smarnet model against other strong competitors on the SQuAD. The results are recorded on the submission date on July 14th, 2017.The whole training process takes approximately 14 hours on a single 1080Ti GPU. Furthermore, as the SQuAD and Triv-iaQA are competitive MC benchmark, we train an ensemble model consisting of 16 training runs with the same architecture but identical hyper-parameters. The answer with the highest sum of the confidence score is chosen for each query.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance of single Smarnet model against other strong competitors on the TriviaQA. The results are recorded on the submission date on September 3th, 2017.</figDesc><table><row><cell>Features</cell><cell>EM</cell><cell>F1</cell></row><row><cell>Full</cell><cell cols="2">71.362 80.183</cell></row><row><cell>No f pos</cell><cell cols="2">68.632 78.911</cell></row><row><cell>No f ner</cell><cell cols="2">70.804 79.257</cell></row><row><cell>No f em</cell><cell cols="2">68.486 78.589</cell></row><row><cell>No f surprisal</cell><cell cols="2">70.972 79.021</cell></row><row><cell>No f tf</cell><cell cols="2">70.617 79.701</cell></row><row><cell>No f Qtype</cell><cell cols="2">68.913 78.151</cell></row><row><cell cols="3">No f pos and f ner 67.352 77.239</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Lexical feature ablations on SQuAD dev set.</figDesc><table><row><cell>Model</cell><cell>EM</cell><cell>F1</cell></row><row><cell>Full</cell><cell cols="2">71.362 80.183</cell></row><row><cell>Input concatenation</cell><cell cols="2">69.077 78.531</cell></row><row><cell cols="3">Passage direct encoding 70.012 78.907</cell></row><row><cell>Memory network</cell><cell cols="2">68.774 77.323</cell></row><row><cell cols="3">Self-alignment checking 69.395 79.227</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Component ablations on SQuAD dev set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Work Machine Comprehension Dataset. Benchmark datasets play a vital role in the research advance. Previous humanlabeled datasets on MC task are too small to train data-intensive models (Richardson, Burges, and Renshaw 2013) (Berant et al. 2014) (Yu et al. 2016). Recently, Largescale datasets become available. CNN/Daily Mail (Hermann et al. 2015) and Children's Book Test<ref type="bibr" target="#b4">(Hill et al. 2015)</ref> generated in cloze style offer the availability to train more expressive neural models. The SQuAD(Rajpurkar et al. 2016), TriviaQA (Joshi et al. 2017) and MS-MARCO (Nguyen et al. 2016) datasets provide large and high-quality datasets which extract answers from text spans instead of single entities in cloze style. The open-domain style of MC tasks are more challenging and require different levels of reasoning from multiple sentences. In this paper, we evaluate our Smarnet framework on SQuAD and TriviaQA datasets. Machine Comprehension Models Previous works in MC task adopt deep neural modeling strategies with attention mechanisms, both on cloze style and open domain tasks. Along with cloze style datasets, Chen et al. (Chen, Bolton, and Manning 2016) prove that computing the attention weights with a bilinear term instead of simple dot-product significantly improves the accuracy. Kadlec et al. (Kadlec et al. 2016) sum attention over candidate answer words in the document. Dhingra et al. (Dhingra et al. 2017) iteratively interact between the query and document by a multiplicative gating function. Cui et al. (Cui et al. 2016) compute a similarity matrix with two-way attention between the query and passage mutually. Sordoni et al. (Sordoni et al. 2016) exploit an iterative alternating neural attention to model the connections between the question and the passage. Open-domain machine comprehension tasks are more challenging and have attracted plenty of teams to pursue for higher performance on the leaderboard. Wang et al. (Wang and Jiang 2016) present match-LSTM and use pointer network to generate answers from the passage. Chen et al. (Chen et al. 2017) tackle the problem by using wikipedia as the unique knowledge source. Shen (Shen et al. 2017) adopt memory networks with reinforcement learning so as to dynamically control the number of hops. Seo et al.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Modeling biological processes for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<idno type="arXiv">arXiv:1704.00051</idno>
		<title level="m">Reading wikipedia to answer open-domain questions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoderdecoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolton</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manning ; Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V</forename><surname>Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>A thorough examination of the cnn/daily mail reading comprehension task</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Attention-over-attention neural networks for reading comprehension</title>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Teaching machines to read and comprehend</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The goldilocks principle: Reading children&apos;s books with explicit memory representations</title>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mnemonic reader: Machine comprehension with iterative aligning and multi-hop answer pointing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bajgar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleindienst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno>Kumar et al. 2016</idno>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<meeting><address><addrLine>Kim</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="908" to="918" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Structural embedding of syntactic trees for machine comprehension</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Nltk: The natural language toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<idno>cs.CL/0205028</idno>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Exponentially weighted moving average control schemes: Properties and enhancements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Saccucci</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Lexical surprisal as a general predictor of reading time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Monsalve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">F</forename><surname>Vigliocco ; Monsalve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vigliocco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the European Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="398" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Memen: Multi-layer embedding with memory networks for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<meeting><address><addrLine>Richardson, Burges, and Renshaw</addrLine></address></meeting>
		<imprint>
			<publisher>Pennington, Socher, and Manning</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Squad: 100,000+ questions for machine comprehension of text. Mctest: A challenge dataset for the open-domain machine comprehension of text. In EMNLP</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<idno type="arXiv">arXiv:1611.01603</idno>
		<idno>arXiv:1606.02245</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Reasonet: Learning to stop reading in machine comprehension</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gated self-matching networks for reading comprehension and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fortunato</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Words or characters? fine-grained gating for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01604</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>End-to-end reading comprehension with dynamic answer chunk ranking. Zeiler 2012] Zeiler, M. D. 2012. Adadelta: An adaptive learning rate method. Computer Science</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
