<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint Language Semantic and Structure Embedding for Knowledge Graph Completion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhao</forename><surname>Shen</surname></persName>
							<email>jhshen@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Wang</surname></persName>
							<email>chenguangwang@wustl.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Washington University in St. Louis</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linyuan</forename><surname>Gong</surname></persName>
							<affiliation key="aff2">
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
							<email>dawnsong@berkeley.edu</email>
							<affiliation key="aff2">
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Joint Language Semantic and Structure Embedding for Knowledge Graph Completion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The task of completing knowledge triplets has broad downstream applications. Both structural and semantic information plays an important role in knowledge graph completion. Unlike previous approaches that rely on either the structures or semantics of the knowledge graphs, we propose to jointly embed the semantics in the natural language description of the knowledge triplets with their structure information. Our method embeds knowledge graphs for the completion task via finetuning pre-trained language models with respect to a probabilistic structured loss, where the forward pass of the language models captures semantics and the loss reconstructs structures. Our extensive experiments on a variety of knowledge graph benchmarks have demonstrated the state-of-the-art performance of our method. We also show that our method can significantly improve the performance in a lowresource regime, thanks to the better use of semantics. The code and datasets are available at https://github.com/pkusjh/LASS.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge graphs (KG), such as Wikidata and Freebase <ref type="bibr" target="#b1">(Bollacker et al., 2008)</ref>, consist of factual triplets. KGs have been useful resources for both humans and machines. A triplet in the form of (head entity, relation, tail entity), where the relation involves both head and tail entities, has been used in a great variety of applications, such as question answering <ref type="bibr" target="#b10">(Guu et al.;</ref><ref type="bibr" target="#b11">Hao et al., 2017)</ref> and web search <ref type="bibr" target="#b48">(Xiong et al., 2017)</ref>. Incompleteness has been a longstanding issue in KGs <ref type="bibr">(Carlson et al., 2010)</ref>, impeding their wider adoption in real-world applications.</p><p>KG completion aims to predict a missing entity or relation of a factual triplet. Structural patterns in the existing triplets are useful to predict the missing elements <ref type="bibr" target="#b33">Sun et al., 2019)</ref>. For * Corresponding author example, a composition pattern can be learned to predict the relation grandmother_Of based on two consecutive mother_Of relations. Besides the structure information, semantic relatedness between entities and relations is also critical to infer entities or relations with similar meanings <ref type="bibr" target="#b50">Yao et al., 2019;</ref><ref type="bibr" target="#b39">Wang et al., 2021)</ref>. For example, if a relationship CEO_Of holds between two entities, the relation employee_Of also holds. There are two kinds of KG completion approaches that fall into different learning paradigms. First, the structure-based approaches treat entities and relations as nodes and edges, and use graph embedding methods to learn their representations. Second, the semantic-based approaches encode the text description of entities and relations via language models. While both structures and semantics are vital to KG completion, it is non-trivial for existing methods to process both structural and semantic information.</p><p>In this paper, we propose LASS, a joint language semantic and structure embedding for knowledge graph completion, which incorporates both semantics and structures in a KG triplet. LASS embeds a triplet into a vector space by fine-tuning pre-trained language models (LM) with respect to a structured loss. LASS involves both semantic embedding and structure embedding. The semantic embedding captures the semantics of the triplet, which corresponds to the forward pass of a pre-trained LM over the natural language description of the triplet. The structure embedding aims to reconstruct the structures in the semantic embedding, which corresponds to optimizing a probabilistic structured loss via the backpropagation of the LM. Intuitively, the structured loss treats the relationship between two entities as a translation between embeddings of the entities. LASS outperforms the existing approaches on a collection of KG completion benchmarks. We further evaluate LASS in low-resource settings and find that it is more data-efficient than other methods. The reason is that our method ex-  <ref type="figure">Figure 1</ref>: Overview of LASS. LASS maps a knowledge triplet (Head Entity, Relation, Tail Entity), in short <ref type="bibr">(h, r, t)</ref>, to the corresponding embedding vectors, h, r, t ? R k . LASS embeds KGs for KG completion via fine-tuning pre-trained language models (LM) w.r.t. a probabilistic structured loss, where the forward pass of the LMs captures semantics and the loss reconstructs structures. In particular, LASS consists of semantic embedding and structure embedding. The semantic embedding (leftmost arrow) is generated by a forward pass of the LMs followed by a pooling layer over the natural language description of a triplet.</p><p>[B] (the beginning token) and [S] (the separator token) are special tokens of LMs attached to the description. For example, the textual description of head entity is (x h 1 , ? ? ? , x h n h ). h is calculated as the mean pooling of the corresponding LM outputs (o h 1 , ? ? ? , o h n h ). r and t are calculated similarly. The structure embedding (rightmost arrow) reconstructs KG structures in the semantic embeddings via optimizing a structured loss on top of the LMs through backpropagation. The structured loss is based on a score function f ( h + r ? t 2 2 ), which regards the relationship between two entities corresponds to a translation between the embeddings of the entities. The goal is to minimize the loss function so that h + r ? t when (h, r, t) holds.</p><p>ploits both semantics and structures in the training data.</p><p>The contributions are the following:</p><p>? We design a natural language embedding approach, LASS, that integrates both structural and semantic information of KGs, for KG completion. We train LASS by fine-tuning pre-trained LMs w.r.t. a structured loss, where the forward pass of the LMs captures semantics and the loss reconstructs structures. The method consists of both the KG module and the LM module, which sheds light on the connections between the KGs and deep language representation, and advances the research at the intersection of the two areas.</p><p>? We evaluate LASS on two KG completion tasks, link prediction and triplet classification, and obtain state-of-the-art performance.</p><p>The results suggest that capturing both semantics and structures is critical to understand the KGs. The findings are beneficial to many downstream knowledge-driven applications.</p><p>? We show that we can significantly improve the performance in the low-resource settings over existing approaches, thanks to the improved transfer of knowledge about semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">LASS</head><p>We introduce LASS to embed both semantics and structures of knowledge graphs (KG) with natural language. As shown in <ref type="figure">Figure 1</ref>, LASS incorporates two embeddings: semantic embedding and structure embedding. The semantic embedding captures the semantics in the natural language description of the KG triplets. The structure embedding further reconstructs the structure information of the KGs from the semantic embedding. LASS embeds KG in a vector space by fine-tuning a pre-trained language model (LM) w.r.t. a structured loss, where the forward pass performs semantic embedding and the optimization of structured loss conducts structure embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Semantic Embedding</head><p>A KG of triplets is denoted as G. Each triplet of G is in the form of (h, r, t), where h,t ? E and r ? R. E is the set of entities, and R is the set of relations.</p><p>The semantic similarities between the head entity h, relation r, and tail entity t are crucial to complete a factual triplet. For example, given h = "Bob Dylan" and r = "was born in", the task is to predict a missing t, where the candidates are "Duluth" and "Apple". The semantic similarity between "Bob Dylan" and "Duluth", as well as the similarity between "was born in" and "Duluth" should be larger than their similarities with "Apple" as "Duluth" is the ground-truth answer. Pre-trained LMs capture the rich semantics in natural language via pre-training on large-scale textual corpora. This inspires us to use the semantics stored in the parameters of LMs to encode the semantics of triplets. Formally, for a triplet (h, r, t), both entities (h and t) and relation (r) are represented by their corresponding natural language descriptions. The head entity h is represented as a sequence of tokens, T h = (x h 1 , ? ? ? , x h n h ), describing the entity. Similarly, T t = (x t 1 , ? ? ? , x t nt ) represents the tail entity t. T r = (x r 1 , ? ? ? , x r nr ) denotes the relation r. We generate the semantic embedding via the forward pass of the LMs as shown in <ref type="figure">Figure 1</ref>. The knowledge graph completion tasks require explicit modeling of dependency of the head, relation and tail. For example, both the connections between head and tail, and relation and tail contribute to the prediction of the tail in the link prediction task. Therefore, we use the concatenation of T h , T r , and T t as the input sequence to the LMs, and use the mean pooling over the output representation of every token in T h , T r , and T t from the forward pass of LMs as h, r, t ? R k , where k is the dimension of the embedding vectors.</p><p>More specifically, we construct the input sequence in the following format:  <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref> respectively. The input sequence is then converted to the corresponding input embeddings of the LMs. For example, the input embeddings of BERT are the sum of the token embeddings, the segment embeddings, and the position embeddings. The input embeddings are fed into the LM. We add a mean pooling layer on top of the output layer of the LM and perform mean pooling over the output representation of every token in T h , i.e., (o h 1 , ? ? ? , o h n h ), resulting in h as illustrated in <ref type="figure">Figure 1</ref>. We obtain r and t in the same way. The dimension k equals to the hidden size of the LM.</p><formula xml:id="formula_0">[B] T h [S] T r [S] T t [S],</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Structure Embedding</head><p>Structural information of KGs has been successfully used in the KG completion. Traditional approaches regard the relationship between two en-tities corresponds to a translation between the embeddings of the entities. This is different from the above semantic embedding and the forward pass cannot capture the structure information. We propose to incorporate the structure embedding by fine-tuning the pre-trained LM with a structure loss.</p><p>The goal is to reconstruct structure information in the semantic embedding. The updated embeddings of h, r, and t are still denoted as h, r, and t, which incorporate structure information of KGs while preserve semantic information. We reconstruct structure information in the semantic embeddings via optimizing a probabilistic structured loss, in which the score function of a triplet (h, r, t) is defined by Eq. 1:</p><formula xml:id="formula_1">f (h, r, t) = b ? 1 2 h + r ? t 2 2<label>(1)</label></formula><p>If (h, r, t) holds, we have h + r ? t. We also use f ( h + r ? t 2 2 ) to denote this in Figure 1 for simplicity. The score function is motivated by TransE .</p><p>We define the following probabilistic model based on the score function <ref type="formula" target="#formula_1">(1)</ref>:</p><formula xml:id="formula_2">Pr(h|r, t) = exp(f (h, r, t)) h ?E exp(f (h, r, t))<label>(2)</label></formula><p>Hereh is the corrupted head sampled from the entity set E. Pr(r|h, t) and Pr(t|h, r) have a similar form except that the summation in the denominator is over corrupted relations and tails, respectively. The probabilistic structured loss is defined in Eq. 3. The goal is to minimize the negative log likelihood over the KG:</p><formula xml:id="formula_3">L = ? (h,r,t)?G ( log Pr(h|r, t) + log Pr(r|h, t) + log Pr(t|h, r))<label>(3)</label></formula><p>Optimization Computing the probability in Eq. 2 is computationally inefficient since it requires a forward pass of all possible triplets (h, r, t) to compute the denominator. We use negative sampling <ref type="bibr" target="#b20">(Mikolov et al., 2013)</ref> to make training more efficient. Instead of minimizing ? log Pr(h|r, t) as in Eq. 3, we optimize the loss as is described in Eq. 4 for modeling h.</p><formula xml:id="formula_4">L h = ? log Pr(1|h, r, t) ? nns i Eh i ?E\{h} log Pr(0|h i , r, t)<label>(4)</label></formula><p>where Pr(1|h, r, t) = ?(f (h, r, t)).</p><p>The loss for modeling r and t is similarly defined. Here, hyperparameter n ns is the number of negative samples. Each negatively sampled headh i is drawn uniformly without replacement from the entity set E\{h}. A sample is not treated as a negative sample if it is already a positive example. We have the final structured loss L = (h,r,t)?G (L h + L r + L t ) by adopting the similar negative sampling procedures for relations and tail entities.</p><p>The training of LASS is unified as fine-tuning an LM with respect to a structured loss. The semantic embedding is obtained by the forward pass of the LM. The structure embedding is conducted by optimizing the structured loss through backpropagation of the LM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setup</head><p>Datasets We test the performance of our method on five KG benchmarks built with three KGs: Freebase <ref type="bibr" target="#b1">(Bollacker et al., 2008)</ref>, WordNet <ref type="bibr" target="#b21">(Miller, 1994)</ref> and UMLS <ref type="bibr" target="#b8">(Dettmers et al., 2018)</ref>. Freebase is a large-scale KG containing general knowledge facts. We employ two subsets from Freebase, namely FB15K-237 , and FB13 <ref type="bibr" target="#b30">(Socher et al., 2013)</ref>. WordNet provides semantic knowledge of words. We use two subsets from WordNet, namely WN18RR <ref type="bibr" target="#b8">(Dettmers et al., 2018)</ref>, and WN11 <ref type="bibr" target="#b30">(Socher et al., 2013)</ref>. UMLS is a medical semantic network containing semantic entities and relations. The statistics are summarized in <ref type="table" target="#tab_3">Table 1</ref>. We also provide a detailed description of the datasets in Appendix A.1.  For LASS-BERT BASE and LASS-RoBERTa BASE , the batch size is set to 128, the learning rate is set to 3e-5 with linear warm-up and 0.01 weight decay. We set the batch size to 64 for LASS-BERT LARGE and LASS-RoBERTa LARGE . The number of training epochs is set to 5. The margin b in Eq. 1 is empirically set to 7. We sample 5 negative entities or relations resulting in 15 negative triplets for each positive triplet for the negative sampling.</p><formula xml:id="formula_5">Dataset # Entity # Relation # Train # Dev # Test FB15k-</formula><p>We represent entities and relations as their names or descriptions <ref type="bibr" target="#b50">(Yao et al., 2019)</ref>. For FB15k-237, we used entity descriptions from <ref type="bibr" target="#b47">(Xie et al., 2016)</ref>. For FB13, we use entity descriptions in Wikipedia. For WN18RR, we use definitions of synsets as entity descriptions. For WN11 and UMLS, the entity names are used as the entity descriptions. The relation descriptions are based on the relation names across all the datasets. The input sequence is constructed based on Sec.  .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison Methods</head><p>We compare our method to state-of-the-art methods, including (i) shallow structure embedding: TransE , TransH , TransR , TransD , TransG , TranSparse , DistMult , DistMult-HRS , ConvE <ref type="bibr" target="#b8">(Dettmers et al., 2018)</ref>, <ref type="bibr">ConvKB (Nguyen et al., 2018)</ref>, ComplEx , RotatE <ref type="bibr" target="#b33">(Sun et al., 2019)</ref>, REFE , HAKE , and ComplEx-DURA ; (ii) deep structure embedding: NTN <ref type="bibr" target="#b30">(Socher et al., 2013)</ref>, DO-LORES , KBGAT <ref type="bibr" target="#b22">(Nathani et al., 2019)</ref>, GAATs , NeP-TuNe <ref type="bibr" target="#b31">(Sonkar et al., 2021)</ref>, and ComplEx-N3-RP <ref type="bibr" target="#b7">(Chen et al., 2021)</ref>; (iii) language semantic embedding: TEKE <ref type="bibr" target="#b44">(Wang and Li, 2016)</ref>, KG-BERT <ref type="bibr" target="#b50">(Yao et al., 2019)</ref>, and stAR <ref type="bibr" target="#b39">(Wang et al., 2021)</ref>. We present a detailed technical description of the above methods in Appendix A.2.</p><p>Method WN11 FB13 Avg NTN <ref type="bibr" target="#b30">(Socher et al., 2013)</ref> 86.2 90.0 88.1 TransE  75.9 81.5 78.7 TransH  78.8 83.3 81.1 TransR  85.9 82.5 84.2 TransD  86.4 89.1 87.8 TEKE <ref type="bibr" target="#b44">(Wang and Li, 2016)</ref> 86.1 84.2 85.2 TransG  87.4 87.3 87.4 TranSparse-S  86.4 88.2 87.3 DistMult  87.1 86.2 86.7 DistMult-HRS  88.9 89.0 89.0 AATE  88  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Triplet Classification</head><p>The task of triplet classification judges whether a given triplet (h, r, t) is correct or not. The task is a binary classification task. We use WN11 and FB13 for the task, since only the test sets of the two datasets contain both positive and negative triplets among all the datasets. For the task, we use the score function as defined in Eq. 1 , and set a score threshold. For a triplet, if the score is above the threshold, the triplet is classified as positive, otherwise negative. We set the threshold empirically based on the accuracy on the validation set. As shown in <ref type="table" target="#tab_6">Table 2</ref>, we conclude with the following findings.  We find that our methods consistently produce state-of-the-art results on triplet classification tasks. This indicates that our score function has captured semantics and structures that are crucial for the triplet classification. We also notice that LASS-BERT generates slightly better results compared to LASS-RoBERTa. This is due to RoBERTa removing the NSP objective, however the objective naturally fits in the triplet classification task. LASS-RoBERTa still generates reasonable results. The reason is that the masked LM objective captures the necessary semantics needed for the triplet classification, and LASS is able to preserve the important semantic information.</p><p>In <ref type="table" target="#tab_8">Table 3</ref>, we also show some cases where LASS-BERT BASE makes correct predictions while KG-BERT produces incorrect ones on FB13. Compared to KG-BERT, we find that LASS is more capable in relations that require comprehensive structure information, such as "institution".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Low-Resource Settings</head><p>We additionally test the accuracy of triplet classification in a low-data regime, in particular, when using 5%, 10%, 15%, 20%, and 30% of the training data on WN11 and FB13. The results are shown in <ref type="figure" target="#fig_0">Figure 2</ref>. LASS-BERT LARGE consistently outperforms the state-of-the-art KG-BERT. This indicates that LASS is more data-efficient, as it leverages both semantics and structures in the training data. We also find that LASS is able to produce competitive results with less training data Method FB15k-237 WN18RR UMLS Hits@10 MR Hits@10 MR Hits@10 MR TransE  0.465 357 0.501 3384 0.989 1.84 DistMult  0.419 254 0.49 5110 0.846 5.52 ComplEx  0.428 339 0.51 5261 0.967 2.59 ConvE <ref type="bibr" target="#b8">(Dettmers et al., 2018)</ref> 0.501 244 0.52 4187 0.990 1.51 RotatE <ref type="bibr" target="#b33">(Sun et al., 2019)</ref> 0.533 177 0.571 3340 --HAKE  0.542 -0.582 ---KBGAT <ref type="bibr" target="#b22">(Nathani et al., 2019)</ref> 0.626 210 0.581 1940 --KG-BERT <ref type="bibr" target="#b50">(Yao et al., 2019)</ref> 0.420 153 0.524 97 0.990 1.47 REFE  0.541 -0.561 ---GAATs  0.650 187 0.604 1270 --ComplEx-DURA  0.560 -0.571 ---StAR <ref type="bibr" target="#b39">(Wang et al., 2021)</ref> 0.562 117 0.732 46 0.991 1.49 NePTuNe <ref type="bibr" target="#b31">(Sonkar et al., 2021)</ref> 0.547 -0.557 ---ComplEx-N3-RP <ref type="bibr" target="#b7">(Chen et al., 2021)</ref> 0.  compared to existing methods even with full training data. LASS-BERT LARGE with 5% training data of WN11 outperforms most of the existing methods using full training data. When using 10% training data of FB13, LASS-BERT LARGE is able to perform comparably with KG-BERT with full training data, and outperforms the remaining methods. This is because LASS transfers the knowledge about semantics better to the tasks compared to existing approaches without fully leveraging the KG semantics. The results suggest that LASS is effective in low-resource scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Link Prediction</head><p>Link prediction aims to predict a missing entity given a relation and the other entity, which is evaluated as a ranking problem. We perform link prediction on FB15k-237, WN18RR and UMLS datasets. For each correct triplet (h, r, t), either h or t is corrupted by replacing it with every other entity in the entity set E. These triplets are ranked based on scores produced by Eq. 1 of LASS. The evaluation is under the filtered setting , i.e., removing all the triplets that appear either in the train, dev, or test set. Two common metrics, Mean Rank (MR) and Hits@10 (the proportion of correct entities ranked in the top 10) are used to evaluate the results. A lower MR is better while a higher Hits@10 is better. From the results in <ref type="table" target="#tab_10">Table 4</ref>, we summarize key observations as below. We find all our methods significantly outperform the compared methods in MR, and reach competitive or better Hits@10. LASS-RoBERTa LARGE performs the best on WN18RR, which outperforms the best compared method StAR by 11 units in MR and 5.4% in Hits@10. It also delivers the best MR on FB15k-237. On UMLS, the existing state-of-the-art performance sets a high standard.</p><p>However, LASS-BERT BASE still outperforms others by at least 0.08 unit in MR. The reasons for the improvements are mainly two-fold. (i) LASS is able to capture the structural patterns in the existing triplets to predict the missing ones via the structured loss. Compared to KG-BERT, LASS is able to use the neighboring entities in the KGs for the prediction. (ii) LASS is able to maintain the semantics of the KGs through semantic embedding to avoid unreasonable triplets with high ranks.</p><p>For example, if CEO_Of holds between two entities, the employee_Of also holds, but birth_Place does not hold. This is the main reason that LASS outperforms all structure embedding based methods by a large margin especially in MR. For instance, LASS significantly outperforms TransE, which shares the similar structured loss with LASS.</p><p>Compare to the improvements made on FB15k-237, LASS-RoBERTa LARGE has significantly improved the state-of-the-art results on WN18RR. The main reason leading to such significant improvements is that the pre-trained LMs provide more semantics in the semantic embedding for WordNet as those LMs are trained on textual corpora to capture rela-tionships between words. While WordNet provides the relationships between words, FB15k-237 contains real-world entities and relations, which are less captured by the LMs. We also notice that LASS only produces moderate Hits@10 on FB15k-237. The main reason is that FB15k-237 presents more complex relations between entities compared to other link prediction datasets shown in <ref type="table" target="#tab_3">Table 1</ref>. Therefore, a more complex structured loss is expected for LASS to gain further improvements. We leave it as one of the future explorations. Besides, on FB15k-237 and WN18RR, LASS-BERT LARGE outperforms LASS-BERT BASE , and LASS-RoBERTa LARGE also outperforms LASS-RoBERTa BASE . This confirms the recent findings <ref type="bibr" target="#b26">(Petroni et al., 2019)</ref> that larger LMs store more semantic knowledge in the parameters. We expect further improvements when larger LMs are used with LASS. On UMLS, we observe slightly different trends. This is mainly because UMLS is a relatively small dataset, thus large models can suffer from overfitting. Overall, RoBERTa improves the BERT pre-training procedure from several perspectives. The improved pre-training procedure enables RoBERTa to generate better performance in many downstream tasks. This suggests that an improved pre-training procedure can enrich the semantics learned in the corresponding LMs.</p><p>Both link prediction and triplet classification are core KG completion tasks. The results show that the proposed LASS generalizes well in KG completion tasks. Different from KG-BERT that designs different models for the tasks, our method does not introduce task-specific parameters or losses for different tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Case Study</head><p>We show uncurated examples to illustrate why LASS can yield the above results, especially how the parameters of the LMs capture the semantics and structures. As attention layers are basic building blocks of the LMs, we focus on visualizing the attention weights with different input sequences.</p><p>We use BertViz <ref type="bibr" target="#b38">(Vig, 2019)</ref> to illustrate the attention weights of the LMs. Given an example of a positive triplet, h = "symbololatry, the worship of symbols", r = "hypernym", and t = "veneration, religious zeal", <ref type="figure" target="#fig_2">Figure 3a</ref> shows the attention weights of the last layer of LASS-BERT BASE on WN11. We find that semantically related tokens  attend to each other with relatively high scores. For example, "religious" attends intensively to "worship" and "veneration". As in multi-head self attention <ref type="bibr" target="#b37">(Vaswani et al., 2017)</ref>, different attention heads in different colors attend to different aspects of the input, the heads are then concatenated to compute the final attention weights. The darker the color, the larger the attention score. This demonstrates that the semantic embedding of LASS is effective in capturing the semantics in the natural language description of the triplets. We show another positive example with h = "successfulness, the condition of prospering", r = "hypernym", and t = "luckiness, an auspicious state resulting from favorable outcomes". <ref type="figure" target="#fig_2">Figure 3b</ref> illustrates the attention weights of the last layer of LASS-BERT BASE on WN11. We observe that tokens are highly attended to each other with similar structure roles in the triplet, even though they share fewer semantic similarities. For instance, the attention score between "hypernym" and "condition" is large. There is also a large attention score between "hypernym" and "state". This is because both "condition" and "state" capture the critical structure information of the triplet. The results indicate that the structure embedding of LASS is able to reconstruct the structure information in the semantic embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Error Analysis</head><p>To better understand the limitations of LASS, we perform a detailed analysis of the errors. We use triplet classification as an example. We investigate the errors made by LASS-BERT BASE on WN11 and summarize the errors based on the relations in <ref type="table" target="#tab_11">Table 5</ref>. We find most errors are caused by relations that are hard to be distinguished from each other due to their semantic similarities. For example, "domain topic" and "domain region" are such relations with an unclear semantic boundary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation</head><p>Percentage (%) domain topic 19.8 domain region 10.8 member meronym 9.1 has instance 8.4 has part 8.1 similar to 7.1 part of 6.3 synset domain topic 5.5 type of 4.6 member holonym 3.9 subordinate instance of 3.2 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>Structure Losses There are several directions to further improve LASS. LASS uses the probabilistic structured loss based on the score function of TransE, which learns a single representation for every entity and relation in the same embedding space. However, different relationships expect different entity embeddings. We propose to enable an entity to have distinct distributed representations when involved in different relations. For example, a new score function h r + r ? t r 2 2 models entities and relations in distinct spaces, and performs the translation between entity embeddings in relation space. The idea is in the same spirit as TransH  and TransR . However, a downside of leveraging those losses is that they will bring additional computation overhead. Our method aims to trade off the computation costs and effectiveness. Exploring computation-light methods that involve alternative losses is one of the future investigations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-trained LMs</head><p>We have explored two pretrained LM families: BERT and RoBERTa. There are three possible directions along this line. First, as indicated in the experimental findings, larger LMs often store more semantics, which can improve the semantic embedding module of LASS.</p><p>We propose to examine larger pre-trained LMs, such as GPT-2 <ref type="bibr" target="#b28">(Radford et al., 2019)</ref>, <ref type="bibr">GPT-3 (Brown et al., 2020)</ref>, and Megatron-LM <ref type="bibr" target="#b29">(Shoeybi et al., 2019)</ref>. Incorporating longer language descriptions (e.g., Wikipedia page) of the entities in the knowledge graphs will provide richer knowledge for improved natural language understanding. Second, the fine-tuning procedure of the deep LMs for KG completion tasks, especially link prediction, is still computationally inefficient. Investigating light LM architectures, such as AL-BERT <ref type="bibr" target="#b15">(Lan et al., 2020)</ref>, to speed up the training process, is one of the promising directions. Finally, our proposed method is generally useful for many knowledge-driven downstream NLP tasks (e.g., question answering, factual probing) as well as lowresource NLP tasks. Ensembling our method with autoregressive models (e.g., GPT-2) will enable the method to perform text generation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Pre-trained LMs Pre-trained LMs, such as BERT, have recently been used to obtain stateof-the-art results in many NLP benchmarks <ref type="bibr" target="#b9">(Devlin et al., 2019;</ref><ref type="bibr" target="#b17">Liu et al., 2019)</ref>. These models are usually based on Transformers <ref type="bibr" target="#b37">(Vaswani et al., 2017)</ref> and trained on unlabeled text corpora. They are used to improve downstream tasks via embedding <ref type="bibr" target="#b25">(Peters et al., 2018)</ref>, fine-tuning <ref type="bibr" target="#b27">(Radford et al., 2018)</ref>, or few-shot learning <ref type="bibr" target="#b28">(Radford et al., 2019)</ref>. Fine-tuning bidirectional Transformers is the most widely used scheme in recent NLP applications, and the approach described in this paper is also based on this scheme. The main difference is that we design a structured loss on top of the LMs aiming to capture structures in natural language.</p><p>Knowledge Graph Embedding KG embedding aims to map entities and their relations to a continuous vector space. Traditional KG embedding methods represent each entity or each relation with a fixed vector. For any triplet (h,r,t), they use a scoring function f (h, r, t) to model its likelihood. The scoring function of TransE  is a negative translational distance. It can be augmented with different geometric transformations such as linear projections  or rotations <ref type="bibr" target="#b33">(Sun et al., 2019)</ref>. Other models based on bilinear transformations , and convolutions <ref type="bibr" target="#b8">(Dettmers et al., 2018)</ref>, also show promising results on KG completion benchmarks. Our structured loss is motivated by TransE. The main differences are the following. TransE  treats the relation as a translation of the embeddings from the head to the tail. Therefore h + r ? t when (h, r, t) holds. TransE designs a margin-based ranking loss based on the l 2 norm h + r ? t 2 2 . The key differences between LASS and TransE are: (i) LASS leverages the natural language semantics in LMs, while TransE does not; (ii) LASS is a probabilistic structured loss, and is more computationally efficient and data-efficient compared to TransE. The main advantage of the probabilistic loss is that we eliminate the norm calculation that TransE requires to prevent the training process from trivially minimizing its loss by increasing the embeddings of entities or relations. The ranking loss of TransE calculates the loss of some training examples as zeros, which will not contribute to the optimization procedure. Our probabilistic loss makes use of all the training examples. Besides, we introduce corrupted relations in the loss, which provides more flexibility in incorporating the KG structure.</p><p>Traditional KG embedding approaches aforementioned regard entities and relations as basic units, without using any extra information. However, studies <ref type="bibr" target="#b30">(Socher et al., 2013;</ref><ref type="bibr" target="#b42">Wang et al., 2014a;</ref><ref type="bibr" target="#b47">Xie et al., 2016)</ref> show that a KG model that models the natural language descriptions of entities and relations usually outperforms those methods that only model the structure of knowledge triplets. <ref type="bibr" target="#b26">Petroni et al. (2019)</ref> use LMs as virtual KGs to answer factual questions. ERNIE <ref type="bibr" target="#b54">(Zhang et al., 2019b)</ref> integrates structural KGs into pre-trained models to improve knowledge-driven NLP tasks. By contrast, we aim to combine both the structures and semantics of the KGs via a unified optimization procedure for the task of KG completion. KG-BERT <ref type="bibr" target="#b50">(Yao et al., 2019</ref>) models KG completion tasks as sentence classification tasks and solves them by fine-tuning pre-trained LMs. There are several key differences between our LASS and KG-BERT <ref type="bibr" target="#b50">(Yao et al., 2019)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose a new embedding method that leverages both semantics and structures of the knowledge graphs for the task of knowledge graph completion, and offers additional benefits in lowresource settings. The method maps a knowledge graph triplet to an embedding space via fine-tuning language models, where the forward pass captures semantics and the loss reconstructs structures. Our method has shown significant improvements on knowledge graph completion benchmarks. The implementation has made no modifications to the language model architectures. The results suggest that the learned embeddings are generally useful in downstream knowledge-driven applications, and potentially useful for more natural language understanding tasks. We hope our results will foster further research in this direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Ethical Considerations</head><p>We hereby acknowledge that all of the co-authors of this work are aware of the provided ACM Code of Ethics and honor the code of conduct. The followings give the aspects of both our ethical considerations and our potential impacts to the community. This work uses pre-trained LMs for knowledge graph completion. The risks and potential misuse of LMs are discussed in <ref type="bibr" target="#b3">Brown et al. (2020)</ref>. There are potential undesirable biases in the datasets, such as unfaithful descriptions from Wikipedia. We do not anticipate the production of harmful outputs after using our model, especially towards vulnerable populations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Environmental Considerations</head><p>We use BERT and RoBERTa as our pre-trained LMs. According to the estimation in <ref type="bibr" target="#b32">Strubell et al. (2019)</ref>, pre-training a base model costs 1,507 kWh?PUE and emits 1,438 lb CO 2 , while pretraining a large model requires 4 times the resources of a base model. In addition, our finetuning takes less than 1% gradient-steps of the number of steps of pre-training. Therefore, our energy cost and CO 2 emissions are relatively small. Besides, the results in the low-resource settings show that our method has better sampling efficiency. This indicates that we can further reduce energy consumption when training with fewer data. types provide the structure for the network and represent important relationships in the biomedical domain.</p><formula xml:id="formula_6">h, t ? R k , r ? R d , M r (? r ) ? R k?d , M 1 r ? 1 r , M 2 r ? 2 r ? R k?d DistMult r, h, t h, r, t ? R k ConvKB concat(g([h, r, t] * ?))w h, r, t ? R k ComplEx ( r, h, t ) h, r, t ? C k ConvE ?(vec(?([r, h] * ?))W), t h, r, t ? R k RotatE ? h ? r ? t 2 h, r, t ? C k , |r i | = 1 REFE ?arctanh( ? h, Ref(r) ? c t ) h, r, t ? R k HAKE RotatE ? sin((h + r ? t)/2) 1 h, r, t ? R k ComplEx-DURA ComplEx ? h, r 2 ? t 2 h, r, t ? C k</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.2 Triplet Classification</head><p>? WN11 and FB13 are subsets of WordNet and FreeBase respectively for triplet classification, where <ref type="bibr" target="#b30">Socher et al. (2013)</ref> randomly switch entities from correct testing triplets resulting in a total of doubling the number of test triplets with an equal number of positive and negative examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Comparison Methods</head><p>We compare LASS to three types of knowledge graph completion methods: shallow structure embedding, deep structure embedding, and language semantic embedding. 1 methods proposes a scoring function regarding a knowledge triplet, without using the natural language descriptions or names of entities or relations. The scoring functions are shown in <ref type="table" target="#tab_13">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.2 Deep Structure Embedding</head><p>? NTN (Neural Tensor Network) <ref type="bibr" target="#b30">(Socher et al., 2013)</ref> models entities across multiple dimensions by a bilinear tensor neural layer.</p><p>? DOLORES  is based on bidirectional LSTMs and learns deep representations of entities and relations from constructed entity-relation chains.</p><p>? KBGAT proposes an attention-based feature embedding that captures both entity and relation features in any given entity's neighborhood, and additionally encapsulates relation clusters and multi-hop relations <ref type="bibr" target="#b22">(Nathani et al., 2019)</ref>.</p><p>? GAATs integrates an attenuated attention mechanism in a graph neural network to assign different weights in different relation paths and acquire the information from the neighborhoods .</p><p>? NePTuNe takes advantage of both TuckER and NTN by carefully crafted nonlinearities and a shared core tensor intrinsic to the Tucker decomposition <ref type="bibr" target="#b31">(Sonkar et al., 2021)</ref>.</p><p>? ComplEx-N3-RP introduces an auxiliary training task to predict relation types as a selfsupervised objective. <ref type="bibr" target="#b7">(Chen et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.3 Language Semantic Embedding</head><p>? TEKE <ref type="bibr" target="#b44">(Wang and Li, 2016)</ref> takes advantage of the context information in a text corpus. The textual context information is incorporated to expand the semantic structure of the knowledge graph and each relation is enabled to own different representations for different head and tail entities.</p><p>? AATE  is a text-enhanced knowledge graph representation learning method, which can represent a relation/entity with different representations in different triples by exploiting additional textual information.</p><p>? KG-BERT <ref type="bibr" target="#b50">(Yao et al., 2019)</ref> considers triples in knowledge graphs as textual sequences, where each textual sequence is a concatenation of text descriptions of the head entity, the relation, and the tail entity. Then KG-BERT treats the knowledge graph completion task as a text binary classification task, and then solves it by fine-tuning a pre-trained BERT.</p><p>? StAR <ref type="bibr" target="#b39">(Wang et al., 2021)</ref> partitions each triplet into two asymmetric parts as in translationbased graph embedding approach, and encodes both parts into contextualized representations by a Siamese-style textual encoder (BERT or RoBERTa).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>-Large (ours) LanE-BERT-Base (ours) LanE-RoBERTa-Large (ours) LanE-RoBERTa-Base (ours) Triplet classification accuracy in a low-resource regime: training with different proportions of the corresponding training datasets on WN11 and FB13.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of attention weights of the last layer of LASS-BERTBASE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:2209.08721v1 [cs.CL] 19 Sep 2022</figDesc><table><row><cell>f (</cell><cell>h</cell><cell>+</cell><cell>r</cell><cell>?</cell><cell>t</cell><cell>2 2 )</cell></row><row><cell>[B]</cell><cell>o h 1 ? ? ? o h n h</cell><cell>[S]</cell><cell>o r 1 ? ? ? o r nr</cell><cell>[S]</cell><cell>o t 1 ? ? ? o t nt</cell><cell>[S]</cell></row><row><cell>Semantics</cell><cell></cell><cell cols="3">Language Model</cell><cell></cell><cell>Structures</cell></row><row><cell>[B]</cell><cell>x h 1 ? ? ? x h n h</cell><cell>[S]</cell><cell>x r 1 ? ? ? x r nr</cell><cell>[S]</cell><cell>x t 1 ? ? ? x t nt</cell><cell>[S]</cell></row><row><cell></cell><cell>Head Entity</cell><cell></cell><cell>Relation</cell><cell></cell><cell>Tail Entity</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Statistics of knowledge graphs. Implementation Details We use two families of LMs with LASS. First, we adopt both BERT BASE and BERT LARGE from (Devlin et al., 2019) with LASS, namely LASS-BERT BASE and LASS-BERT ) on each KG dataset via fine-tuning the corresponding LMs. The training hyperparameters are set as follows.</figDesc><table /><note>LARGE . Second, RoBERTa family (Liu et al., 2019) is used, namely LASS-RoBERTa BASE and LASS-RoBERTa LARGE . We train LASS with AdamW (Loshchilov and Hutter, 2019</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>2.1. For LASS-BERT BASE and LASS-BERT LARGE , we use a character-level BPE vocabulary. [B] is replaced with [CLS], and [S] is replaced with [SEP]. For LASS-RoBERTa BASE and LASS-RoBERTa LARGE , we use a byte-level BPE vocabulary, and [B] and [S] are replaced with BOS and EOS respectively. We implement LASS using the Transformers package</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Triplet classification accuracy on WN11 and FB13.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc>Samples of LASS's correct predictions on FB13, where KG-BERT (Yao et al., 2019) outputs wrong predictions. Label means a gold positive triplet. indicates a gold negative triplet.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Link prediction results on FB15k-237, WN18RR and UMLS.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 :</head><label>5</label><figDesc>Analysis of most common errors of LASS-BERTBASE categorized by relations on WN11.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>: (i) LASS reconstructs the structures of KGs via structure embedding, while KG-BERT does not; (ii) LASS unifies the link prediction and triplet classification under the same architecture, while KG-BERT designs different architectures for different tasks; (iii) LASS works with two families of LMs, while KG-BERT only works with BERT BASE . LASS is not particularly designed for BERT, shedding light on understanding the role of semantics in LMs for KG comple-tion.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 6 :</head><label>6</label><figDesc>The score functions f r (h, t) of shallow structure embedding models for knowledge graph embedding, where ? denotes the generalized dot product, ? denotes the Hadamard product, ? denotes activation function and * denotes 2D convolution. ? denotes conjugate for complex vectors, and 2D reshaping for real vectors in the ConvE model. Ref(?) denotes the reflection matrix induced by rotation parameters ?. ? c is M?bius addition that provides an analogue to Euclidean addition for hyperbolic space.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We refer the readers to<ref type="bibr" target="#b14">(Ji et al., 2021)</ref> for a more comprehensive review of the knowledge graph completion methods.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We would like to thank the anonymous reviewers for their suggestions and comments. This material is in part based upon work supported by Berkeley DeepDrive and Berkeley Artificial Intelligence Research.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Experimental Setup Details</head><p>We describe additional details of our experimental setup including datasets and comparison methods in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Datasets</head><p>We introduce the link prediction and triplet classification datasets as below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.1 Link Prediction</head><p>? FB15k-237. Freebase is a large collaborative knowledge graph consisting of data composed mainly by its community members. It is an online collection of structured data harvested from many sources, including individual and user-submitted wiki contributions <ref type="bibr" target="#b24">(Pellissier Tanon et al., 2016)</ref>. FB15k is a selected subset of Freebase that consists of 14,951 entities and 1,345 relationships . FB15K-237 is a variant of FB15K where inverse relations and redundant relations are removed, resulting in 237 relations .</p><p>? WN18RR. WordNet is a lexical database of semantic relations between words in English. WN18 ) is a subset of Word-Net which consists of 18 relations and 40,943 entities. WN18RR is created to ensure that the evaluation dataset does not have inverse relations to prevent test leakage <ref type="bibr" target="#b8">(Dettmers et al., 2018)</ref>.</p><p>? UMLS. UMLS semantic network <ref type="bibr" target="#b19">(McCray, 2003)</ref> is an upper-level ontology of Unified Medical Language System. The semantic network, through its 135 semantic types, provides a consistent categorization of all concepts represented in the UMLS. The 46 links between the semantic Method Score Function</p><p>TranSparse-S ? M r (? r ) h + r ? M r (? r ) t 2 1/2 ? M 1 r ? 1 r h + r ? M 2 r ? 2 r t 2 1/2</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Accurate text-enhanced knowledge graph representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="745" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Dur?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ilya Sutskever, and Dario Amodei</title>
		<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
	<note>NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Estevam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Hruschka</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Toward an architecture for neverending language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1306" to="1313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Low-dimensional hyperbolic knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ines</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adva</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6901" to="6914" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Relation prediction as an auxiliary training objective for improving multi-relational graph representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AKBC</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional 2d knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1811" to="1818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Traversing knowledge graphs in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<biblScope unit="page" from="318" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An end-to-end model for question answering over knowledge base with cross-attention combining global knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchao</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding via dynamic mapping matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-IJCNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="687" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Knowledge graph completion with adaptive sparse transfer matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="985" to="991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A survey on knowledge graphs: Representation, acquisition, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoxiong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pekka</forename><surname>Marttinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S Yu</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNNLS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="494" to="514" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Albert: A lite bert for selfsupervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2181" to="2187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized bert pretraining approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An upper-level ontology for the biomedical domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexa</forename><forename type="middle">T</forename><surname>Mccray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comparative and Functional Genomics</title>
		<imprint>
			<biblScope unit="page" from="80" to="84" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Wordnet: A lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning attentionbased embeddings for relation prediction in knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Nathani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jatin</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charu</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Kaul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4710" to="4723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A novel embedding model for knowledge base completion based on convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tu</forename><forename type="middle">Dinh</forename><surname>Dai Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dat</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinh</forename><surname>Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Phung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="327" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">From freebase to wikidata: The great migration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Thomas Pellissier Tanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Vrande?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schaffert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lydia</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pintscher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1419" to="1428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Language models as knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2463" to="2473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training. Ope-nAI blog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>OpenAI blog</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Megatron-lm: Training multibillion parameter language models using model parallelism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Neptune: Neural powered tucker network for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Sonkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arzoo</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Energy and policy considerations for deep learning in NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananya</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mc-Callum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3645" to="3650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rotate: Knowledge graph embedding by relational rotation in complex space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVSC-WS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Representing text for joint embedding of text and knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pallavi</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1499" to="1509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Th?o</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A multiscale visualization of attention in the transformer model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Vig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="37" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Structure-Augmented Text Representation Learning for Efficient Knowledge Graph Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1737" to="1748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dolores: Deep contextualized knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AKBC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Knowledge Graph Embedding via Graph Attenuated Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bicheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqian</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="5212" to="5224" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Knowledge graph and text jointly embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1591" to="1601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1112" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Text-enhanced representation learning for knowledge graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1293" to="1299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Transg: A generative model for knowledge graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2316" to="2325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Representation learning of knowledge graphs with entity descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2659" to="2665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Explicit semantic ranking for academic search via knowledge graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1271" to="1279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Kg-bert: Bert for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengsheng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Duality-induced regularizer for tensor factorization based knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanqiu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="21604" to="21615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning Hierarchy-Aware Knowledge Graph Embeddings for Link Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanqiu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3065" to="3072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding with hierarchical relation structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzhen</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3198" to="3207" />
		</imprint>
	</monogr>
	<note>Fen Lin, and Qing He</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Ernie: Enhanced language representation with informative entities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1441" to="1451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">A.2.1 Shallow Structure Embedding</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">TransR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Transe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020) are methods based only on the structure of the knowledge graphs. DistMult-HRS</title>
		<editor>ComplEx-DURA (Zhang et al.,</editor>
		<meeting><address><addrLine>ComplEx</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>ConvE (Dettmers. is an extension of Dist-Mult which is combined with a three-layer hierarchical relation structure (HRS) loss. Each of these</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
