<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Person Transfer GAN to Bridge Domain Gap for Person Re-Identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
							<email>wgao@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
							<email>qi.tian@utsa.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Texas at San Antonio</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Person Transfer GAN to Bridge Domain Gap for Person Re-Identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although the performance of person Re-Identification (ReID) has been significantly boosted, many challenging issues in real scenarios have not been fully investigated, e.g., the complex scenes and lighting variations, viewpoint and pose changes, and the large number of identities in a camera network. To facilitate the research towards conquering those issues, this paper contributes a new dataset called MSMT17 with many important features, e.g., 1) the raw videos are taken by an 15-camera network deployed in both indoor and outdoor scenes, 2) the videos cover a long period of time and present complex lighting variations, and 3) it contains currently the largest number of annotated identities, i.e., 4,101 identities and 126,441 bounding boxes. We also observe that, domain gap commonly exists between datasets, which essentially causes severe performance drop when training and testing on different datasets. This results in that available training data cannot be effectively leveraged for new testing domains. To relieve the expensive costs of annotating new training samples, we propose a Person Transfer Generative Adversarial Network (PTGAN) to bridge the domain gap. Comprehensive experiments show that the domain gap could be substantially narrowed-down by the PTGAN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person Re-Identification (ReID) targets to match and return images of a probe person from a large-scale gallery set collected by camera networks. Because of its important applications in security and surveillance, person ReID has been drawing lots of attention from both academia and industry. Thanks to the development of deep learning and the availability of many datasets, person ReID performance has been significantly boosted. For example, the Rank-1 accuracy of single query on Market1501 <ref type="bibr" target="#b38">[39]</ref> has been improved from 43.8% <ref type="bibr" target="#b20">[21]</ref> to 89.9% <ref type="bibr" target="#b30">[31]</ref>. The Rank-1 accuracy on CUHK03 <ref type="bibr" target="#b19">[20]</ref> labeled dataset has been improved from <ref type="bibr" target="#b18">19</ref>.9% <ref type="bibr" target="#b19">[20]</ref> to 88.5% <ref type="bibr" target="#b27">[28]</ref>. A more detailed review of current approaches will be given in Sec. 2. <ref type="figure">Figure 1</ref>: Illustration of the domain gap between CUHK03 and PRID. It is obvious that, CUHK03 and PRID present different styles, e.g., distinct lightings, resolutions, human race, seasons, backgrounds, etc., resulting in low accuracy when training on CUHK03 and testing on PRID.</p><p>Although the performance on current person ReID datasets is pleasing, there still remain several open issues hindering the applications of person ReID. First, existing public datasets differ from the data collected in real scenarios. For example, current datasets either contain limited number of identities or are taken under constrained environments. The currently largest DukeMTMC-reID <ref type="bibr" target="#b40">[41]</ref> contains less than 2,000 identities and presents simple lighting conditions. Those limitations simplify the person ReID task and help to achieve high accuracy. In real scenarios, person ReID is commonly executed within a camera network deployed in both indoor and outdoor scenes and processes videos taken by a long period of time. Accordingly, real applications have to cope with challenges like a large number of identities and complex lighting and scene variations, which current algorithms might fail to address.</p><p>Another challenge we observe is that, there exists domain gap between different person ReID datasets, i.e., training and testing on different person ReID datasets results in severe performance drop. For example, the model trained on CUHK03 <ref type="bibr" target="#b19">[20]</ref> only achieves the Rank-1 accuracy of 2.0% when tested on PRID <ref type="bibr" target="#b9">[10]</ref>. As shown in <ref type="figure">Fig. 1</ref>, the domain gap could be caused by many reasons like different lighting conditions, resolutions, human race, seasons, back-grounds, etc. This challenge also hinders the applications of person ReID, because available training samples cannot be effectively leveraged for new testing domains. Since annotating person ID labels is expensive, research efforts are desired to narrow-down or eliminate the domain gap.</p><p>Aiming to facilitate the research towards applications in realistic scenarios, we collect a new Multi-Scene Multi-Time person ReID dataset (MSMT17). Different from existing datasets, MSMT17 is collected and annotated to present several new features. 1) The raw videos are taken by an 15camera network deployed in both the indoor and outdoor scenes. Therefore, it presents complex scene transformations and backgrounds. 2) The videos cover a long period of time, e.g., four days in a month and three hours in the morning, noon, and afternoon, respectively in each day, thus present complex lighting variations. 3) It contains currently the largest number of annotated identities and bounding boxes, i.e., 4,101 identities and 126,441 bounding boxes. To our best knowledge, MSMT17 is currently the largest and most challenging public dataset for person ReID. More detailed descriptions will be given in Sec. 3.</p><p>To address the second challenge, we propose to bridge the domain gap by transferring persons in dataset A to another dataset B. The transferred persons from A are desired to keep their identities, meanwhile present similar styles, e.g., backgrounds, lightings, etc., with persons in B. We model this transfer procedure with a Person Transfer Generative Adversarial Network (PTGAN), which is inspired by the Cycle-GAN <ref type="bibr" target="#b41">[42]</ref>. Different from Cycle-GAN <ref type="bibr" target="#b41">[42]</ref>, PT-GAN considers extra constraints on the person foregrounds to ensure the stability of their identities during transfer. Compared with Cycle-GAN, PTGAN generates high quality person images, where person identities are kept and the styles are effectively transformed. Extensive experimental results on several datasets show PTGAN effectively reduces the domain gap among datasets.</p><p>Our contributions can be summarized into three aspects. 1) A new challenging large-scale MSMT17 dataset is collected and will be released. Compared with existing datasets, MSMT17 defines more realistic and challenging person ReID tasks. 2) We propose person transfer to take advantages of existing labeled data from different datasets. It has potential to relieve the expensive data annotations on new datasets and make it easy to train person ReID systems in real scenarios. An effective PTGAN model is presented for person transfer. 3) This paper analyzes several issues hindering the applications of person ReID. The proposed MSMT17 and algorithms have potential to facilitate the future research on person ReID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>This work is closely related with descriptor learning in person ReID and image-to-image translation by GAN. We briefly summarize those two categories of works in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Descriptor Learning in Person ReID</head><p>Deep learning based descriptors have shown substantial advantages over hand-crafted features on most of person ReID datasets. Some works <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b40">41]</ref> learn deep descriptors from the whole images with classification models, where each person ID is treated as a category. Some other works <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b5">6]</ref> combine verification models with classification models to learn descriptors. Hermans et al. <ref type="bibr" target="#b8">[9]</ref> show that triplet loss effectively improves the performance of person ReID. Similarly, Chen et al. <ref type="bibr" target="#b0">[1]</ref> propose the quadruplet network to learn representations.</p><p>The above works learn global descriptors and ignore the detailed cues which might be important for distinguishing persons. To explicitly utilize local cues, Cheng et al. <ref type="bibr" target="#b1">[2]</ref> propose a multi-channel part-based network to learn a discriminative descriptor. Wu et al. <ref type="bibr" target="#b31">[32]</ref> discover hand-crafted features could be complementary with deep features. They divide the global image into five fixed-length regions. For each region, a histogram descriptor is extracted and concatenated with the global deep descriptor. Though the above works achieve good performance, they ignore the misalignment issue caused by fixed body part division. Targeting to solve this issue, Wei et al. <ref type="bibr" target="#b30">[31]</ref> utilize Deepercut <ref type="bibr" target="#b10">[11]</ref> to detect three coarse body regions and then learn an globallocal-alignment descriptor. In <ref type="bibr" target="#b37">[38]</ref>, more fine-grained part regions are localized and then fed into the proposed Spindle Net for descriptor learning. Similarly, Li et al. <ref type="bibr" target="#b17">[18]</ref> adopt Spatial Transform Networks (STN) <ref type="bibr" target="#b12">[13]</ref> to detect latent part regions and then learn descriptors on those regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Image-to-Image Translation by GAN</head><p>Since GAN proposed by Goodfellow et al. <ref type="bibr" target="#b6">[7]</ref>, many variants of GAN <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b41">42]</ref> have been proposed to tackle different tasks, e.g., natural style transfer, super-resolution, sketch-to-image generation, image-to-image translation, etc. Among them, imageto-image translation has attracted lots of attention. In <ref type="bibr" target="#b11">[12]</ref>, Isola et al. propose conditional adversarial networks to learn the mapping function from input to output images. However, this method requires paired training data, which is hard to acquire in many tasks <ref type="bibr" target="#b41">[42]</ref>. Targeting to solve the unpaired image-to-image translation task, Zhu et al. <ref type="bibr" target="#b41">[42]</ref> propose cycle consistency loss to train unpaired data. Also, the works <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b13">14]</ref> propose a similar framework to solve the task. Our proposed PTGAN is similar to Cycle-GAN <ref type="bibr" target="#b41">[42]</ref> in that, it also performs image-to-image translation. Differently, extra constraints on person identity are applied to ensure the transferred images can be used for model training. Zheng et al. <ref type="bibr" target="#b40">[41]</ref> adopt GAN to generate new samples for data augmentation in person ReID. Their work differs from ours in both motivation and methodology. As far as we know, this is an early work on person transfer by GAN for person ReID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MSMT17 Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview of Previous Datasets</head><p>Current person ReID datasets have significantly pushed forward the research on person ReID. As shown in Table 1, DukeMTMC-reID <ref type="bibr" target="#b40">[41]</ref>, CUHK03 <ref type="bibr" target="#b19">[20]</ref>, and Market-1501 <ref type="bibr" target="#b38">[39]</ref> involve larger numbers of cameras and identities than VIPeR <ref type="bibr" target="#b7">[8]</ref> and PRID <ref type="bibr" target="#b9">[10]</ref>. The enough training data makes it possible to develop deep models and show their discriminative power in person ReID. Although current algorithms have achieved high accuracy on those datasets, person ReID is far from being solved and widely applied in real scenarios. Therefore, it is necessary to analyze the limitations of existing datasets.</p><p>Compared with the data collected in real scenarios, current datasets present limitations in four aspects: 1) The number of identities and cameras are not large enough, especially when compared with the real surveillance video data. In <ref type="table" target="#tab_0">Table 1</ref>, the largest dataset contains only 8 cameras and less than 2,000 identities. 2) Most of existing datasets cover only single scene, i.e., either indoor or outdoor scene.</p><p>3) Most of existing datasets are constructed from short-time surveillance videos without significant lighting changes. 4) Their bounding boxes are generated either by expensive hand drawing or out-dated detectors like Deformable Part Model (DPM) <ref type="bibr" target="#b3">[4]</ref>. Those limitations make it necessary to collect a larger and more realistic dataset for person ReID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Description to MSMT17</head><p>Targeting to address above mentioned limitations, we collect a new Multi-Scene Multi-Time person ReID dataset (MSMT17) by simulating the real scenarios as much as possible. We utilize an 15-camera network deployed in campus. This camera network contains 12 outdoor cameras and 3 indoor cameras. We select 4 days with different weather conditions in a month for video collection. For each day, 3 hours of videos taken in the morning, noon, and afternoon, respectively, are selected for pedestrian detection and annotation. Our final raw video set contains 180 hours of videos, 12 outdoor cameras, 3 indoor cameras, and 12 time slots. Faster RCNN <ref type="bibr" target="#b25">[26]</ref> is utilized for pedestrian bounding box detection. Three labelers go through the detected bounding boxes and annotate ID label for 2 months. Finally, 126,441 bounding boxes of 4,101 identities are annotated. Some statistics on MSMT17 are shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. Sample images from MSMT17 are shown and compared in <ref type="figure" target="#fig_0">Fig. 2</ref>. Compared with existing datasets, we summarize the new features in MSMT17 into the following aspects:</p><p>1) Larger number of identities, bounding boxes, and cameras. To our best knowledge, MSMT17 is currently the largest person ReID dataset. As shown by the comparison in <ref type="table" target="#tab_0">Table 1</ref>, MSMT17 contains 126,441 bounding boxes, 4,101 identities, which are significantly larger than the ones in previous datasets.</p><p>2) Complex scenes and backgrounds. MSMT17 contains the largest number of cameras, i.e., 15 cameras placed in different locations. It is also constructed with both indoor and outdoor videos, which has not been considered in previous datasets. Those considerations result in complex backgrounds and scene variations, also make MSMT17 more appealing and challenging.</p><p>3) Multiple time slots result in severe lighting changes. MSMT17 is collected with 12 time slots, i.e., morning, noon, and afternoon in four days. It better simulates the real scenarios than previous datasets, but brings severe lighting changes. 4) More reliable bounding box detector. Compared with hand drawing and DPM detector, Faster RCNN <ref type="bibr" target="#b25">[26]</ref> is a better choice for bounding box detection in real applications, e.g., easier to implement and more accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Evaluation Protocol</head><p>We randomly divide our dataset into training set and testing set, respectively. Different from dividing the two parts   equally in previous datasets, we set the training and testing ratio as 1:3. We use this setting because of the expensive data annotation in real scenarios, and thus want to encourage more efficient training strategies. Finally, the training set contains 32,621 bounding boxes of 1,041 identities, and the testing set contains 93,820 bounding boxes of 3,060 identities. From the testing set, 11,659 bounding boxes are randomly selected as query images and the other 82,161 bounding boxes are used as gallery images. Similar with most of previous datasets, we utilize the Cumulated Matching Characteristics (CMC) curve to evaluate the ReID performance. For each query bounding box, multiple true positives could be returned. Therefore, we also regard person ReID as a retrieval task. mean Average Precision (mAP) is thus also used as the evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Person Transfer GAN</head><p>To better leverage the training set of dataset A in person ReID tasks on dataset B, we propose to bridge the domain gap by transferring persons in A to B. As illustrated in <ref type="figure">Fig. 1</ref>, different datasets present distinct styles due to multiple reasons such as backgrounds, lighting conditions, resolutions, etc. Imagine that, if persons in A were captured by the cameras of B, the style of those person images would be consistent with the style of B. Our person transfer tries to simulate this procedure, i.e., learning a transfer function to 1) ensure the transferred person images show similar styles with the target dataset, and 2) keep the appearance and identity cues of the person during transfer.</p><p>This transfer task seems easy, e.g., can be finished by cropping the person foregrounds from A and paste them on the backgrounds on B. However, it is difficult to deal with multiple reasons of domain gap in a rule-based algorithm. Moreover, there could be complicated style variations on B, e.g., different backgrounds and lighting conditions between two cameras of PRID in <ref type="figure">Fig. 1</ref>. Our algorithm is inspired by the popularity of GAN models, which have been proven effective in generating the desired image samples. We hence design a Person Transfer GAN (PTGAN) to perform person transfer from A to B.</p><p>Based on the above discussions, PTGAN is constructed to satisfy two constraints, i.e., the style transfer and person identity keeping. The goal of style transfer is to learn the style mapping functions between different person datasets. The goal of person identity keeping is to ensure the identity of one person remains unchanged after transfer. Because different transferred samples of one person are regarded as having the same person ID, the constraint on person identity is important for person ReID training. We thus formulate the loss function of PTGAN as, i.e.,</p><formula xml:id="formula_0">L P T GAN =L Style + ? 1 L ID ,<label>(1)</label></formula><p>where L Style denotes the style loss and L ID denotes the identity loss, and ? 1 is the parameter for the trade-off between two losses. ReID datasets do not contain paired person images, i.e., images of the same person from different datasets. Therefore, the style transfer can be regarded as an unpaired image-to-image translation task. Because of the good performance of Cycle-GAN in unpaired image-to-image translation task, we employ Cycle-GAN to learn the style mapping functions between dataset A and B. Suppose G repre-sents the style mapping function from A to B and G represents the style mapping function from B to A. D A and D B are the style discriminators for A and B, respectively. The objective function of style transfer learning can be formulated as follows:</p><formula xml:id="formula_1">L Style =L GAN (G, D B , A, B) + L GAN (G, D A , B, A) + ? 2 L cyc (G, G),<label>(2)</label></formula><p>Where L GAN represents the standard adversarial loss <ref type="bibr" target="#b6">[7]</ref>, and L cyc represents the cycle consistency loss <ref type="bibr" target="#b41">[42]</ref>. For more details of those loss functions, please refer to the Cycle-GAN <ref type="bibr" target="#b41">[42]</ref>.</p><p>Solely considering style transfer may result in ambiguous person ID labels in transferred person images. We thus compute the identity loss to ensure the accuracy of person ID labels in the transferred data. The person identity loss is computed by first acquiring the foreground mask of a person, then evaluating the variations on the person foreground before and after person transfer. Given the data distribution of A as a ? p data (a) and the data distribution of B as b ? p data (b). The objective function of identity loss can be formulated as follows:</p><formula xml:id="formula_2">L ID =E a?p data (a) [||(G(a) ? a) M (a)|| 2 ] + E b?p data (b) [||(G(b) ? b) M (b)|| 2 ],<label>(3)</label></formula><p>where G(a) represents the transferred person image from image a, and M (a) represents the foreground mask of person image a.</p><p>Because of its good performance on segmentation task, we use PSPNet <ref type="bibr" target="#b36">[37]</ref> to extract the mask on person images. On video surveillance data with moving foregrounds and fixed backgrounds, more accurate and efficient foreground extraction algorithms can be applied. It can be observed that, PTGAN does not require person identity labels on the target dataset B. The style discriminator D B can be trained with unlabled person images on B. Therefore, PTGAN is well-suited to real scenarios, where the new testing domains have limited or no labeled training data.</p><p>We show some sample results generated by PTGAN in <ref type="figure" target="#fig_3">Fig. 4</ref>. Compared with Cycle-GAN, PTGAN generates images with substantially higher quality. For example, the appearance of person is maintained and the style is effectively transferred toward the one on PRID camera1. The shadows, road marks, and backgrounds are automatically generated and are similar with the ones on PRID camera1. It is also interesting to observe that, PTGAN still works well with the noisy segmentation results generated by PSPNet. This implies that, PTGAN is also robust to the segmentation errors. More detailed evaluation of PTGAN will be given in Sec. 5.4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>In addition to the MSMT17, four widely used person ReID datasets are employed in our experiments.</p><p>DukeMTMC-reID <ref type="bibr" target="#b40">[41]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation Details</head><p>PTGAN uses similar network architecture with the one in Cycle-GAN <ref type="bibr" target="#b41">[42]</ref>. For the generator network, two stride-2 convolutions, 9 residual blocks, and two stride-1 2 fractionally-strided convolutions are designed. Two parts are included in the discriminator network. PatchGAN <ref type="bibr" target="#b11">[12]</ref> is adopted as one part. The PatchGAN classifies whether a 70 ? 70 patch in an image is real or fake. For the other part, L 2 distance between the transferred image and input image is computed on the foreground person. Adam solver <ref type="bibr" target="#b14">[15]</ref> is adopted in PTGAN. For the generator network, the learning rate is set as 0.0002. The learning rate is set as 0.0001 for the discriminator network. We set ? 1 = 10, and ? 2 = 10. The size of input image is 256?256. Finally, we train PTGAN for 40 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Performance on MSMT17</head><p>As described in Sec. 3, MSMT17 is challenging but close to the reality. This section verifies this claim by testing existing algorithms on MSMT17.</p><p>We go through the state-of-the-art works published in 2017 and 2016. Among those works, the GLAD proposed by Wei et al. <ref type="bibr" target="#b30">[31]</ref> achieves the best performance on Market, and the PDC proposed by Su et al. <ref type="bibr" target="#b27">[28]</ref> achieves the best performance on CUHK03. <ref type="bibr" target="#b0">1</ref> We thus evaluate those two methods on MSMT17 with the codes and models provided by their authors. In most of person ReID works, GoogLeNet <ref type="bibr" target="#b28">[29]</ref> is commonly used as the baseline model. We thus also use GoogLeNet <ref type="bibr" target="#b28">[29]</ref> as our baseline.</p><p>We summarize the experimental results in <ref type="table" target="#tab_2">Table 2</ref>. As shown in the table, the baseline only achieves mAP of 23% on MSMT17, which is significantly lower than its mAP of 51.7% on Market <ref type="bibr" target="#b5">[6]</ref>. It is also obvious that, PDC <ref type="bibr" target="#b27">[28]</ref> and GLAD <ref type="bibr" target="#b30">[31]</ref> substantially outperform the baseline performance by considering extra part and regional features. However, the best performance achieved by GLAD, e.g., mAP of 34%, is still substantially lower than its reported performance on other datasets, e.g., 73.9% on Market. The above experiments clearly show the challenges of MSMT17.</p><p>We also show some sample retrieval results in <ref type="figure" target="#fig_4">Fig. 5</ref>. From the samples, we can conclude that although challenging, the ReID task defined by MSMT17 is realistic. Note that, in real scenarios distinct persons may present similar clothing cues, and images of same person may present different lightings, backgrounds, and poses. As shown in <ref type="figure" target="#fig_4">Fig. 5</ref>, the false positive samples do show similar appearances with the one of query person. Some true positives present distinct lightings, poses, and backgrounds from the query. Therefore, we believe MSMT17 is a valuable dataset to facilitate the future research on person ReID. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Performance of Person Transfer</head><p>Person transfer is performed from dataset A to B. The transferred data is hence used for training on B. To ensure there is enough transferred data for training on B, we test person transfer in two cases, i.e., 1) transferring from a large A to a small B, and 2) transferring from a large A to a large B. In the following experiments, we use the training set provided by A for person transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Transfer from Large Dataset to Small Dataset</head><p>This part tests the performance of transferred person data from CUHK03 and Martket to a small dataset PRID. As shown in <ref type="figure">Fig. 1</ref>, person images captured by two cameras on PRID show different styles. Therefore, we perform person transfer to those two cameras, i.e., PRID-cam1 and PRID-cam2, respectively.</p><p>We first perform person transfer from CUHK03 to PRID-cam1 and PRID-cam2. Samples of the transferred person images to PRID-cam1 are shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. We additionally show samples of transferred person images from CUHK03 to PRID-cam2 in <ref type="figure" target="#fig_5">Fig. 6</ref>. It is clear that, the transferred person images to those two cameras show different styles, which are consistent with the ones on PRID. We also transfer Market to PRID-cam1 and PRID-cam2, respectively. Samples of the transferred person images from Market are shown in <ref type="figure" target="#fig_6">Fig. 7</ref>, where similar results can be observed as the ones in <ref type="figure" target="#fig_3">Fig. 4</ref> and <ref type="figure" target="#fig_5">Fig. 6</ref>, respectively.</p><p>To further evaluate whether the domain gap is reduced through PTGAN. We conduct comparisons between GoogLeNet trained with the training sets on CUHK03 and Market, and GoogLeNet trained on their transferred training sets, respectively. The experimental results are summarized in <ref type="table" target="#tab_3">Table 3</ref>. As shown in the table, GoogLeNet  trained on the CUHK03, only achieves the Rank-1 accuracy of 2.0% on PRID, which implies substantial domain gap between CUHK03 and PRID. With training data transferred by PTGAN, GoogLeNet achieves a significant performance boost, e.g., the Rank-1 accuracy is improved from 2.0% to 37.5%, the Rank-10 accuracy is improved from 11.5% to 72.5%. Similar improvements can be observed from the results on Martket, e.g., the Rank-1 accuracy is significantly improved from 5.0% to 33.5% after person transfer. The substantial performance improvements clearly indicate the shrunken domain gap. Moreover, this experiment shows that even without using labeled data on PRID, we can achieve reasonable performance on it using training data from other datasets. From <ref type="table" target="#tab_3">Table 3</ref>, we also observe an interesting phenomenon, i.e., combining the transferred datasets on two cameras results in better performance. This might be due to two reasons: 1) the combined dataset has more training samples, thus helps to train a better deep network, and 2) it enables the learning of style differences between two cameras. In the combined dataset, each person image has two transferred samples on camera1 and camera2, respectively with different styles. Because those two samples have the same person ID label, this training data enforces the network learning to gain robustness to the style variations between camera1 and camera2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Transfer from Large Dataset to Large Dataset</head><p>This part simulates a more challenging scenario commonly existing in real applications, i.e., the available training data on a large testing set is not provided. We thus test the performance of PTGAN by conducting person transfer among three large datasets, i.e., Duke, Market, and CUHK03, respectively.</p><p>The large person ReID dataset commonly contains a large number of cameras, making it expensive to perform person transfer to each individual camera. Therefore, different from the experimental settings in Sec. 5.4.1, we do not distinguish different cameras and directly transfer person images to the target dataset with one PTGAN. Obviously, this is not an optimal solution for person transfer. Our experimental results are summarized in <ref type="figure" target="#fig_7">Fig. 8</ref>. It is obvious that GoogLeNet trained on transferred datasets works better than the one trained on the original training sets. Sample transferred images are presented in <ref type="figure" target="#fig_8">Fig. 9</ref>. It is obvious that, although we use a simple transfer strategy, PTGAN still generates high quality images. Possible better solutions for person transfer to large datasets will be discussed as our future work in Sec. 6.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Performance of Person Transfer on MSMT17</head><p>We further test PTGAN on MSMT17. We use the same strategy in Sec. 5.4.2 to conduct person transfer. As shown in <ref type="table" target="#tab_4">Table 4</ref>, the domain gaps between MSMT17 and the other three datasets are effectively narrowed-down by PT-GAN. For instance, the Rank-1 accuracy is improved by 4.7%, 6.8%, and 3.7% after performing person transfer from Duke, Market, and CUHK03, respectively.</p><p>In real scenarios, the testing set is commonly large and has limited number of labeled training data. We hence test the validity of person transfer in such case. We first show the person ReID performance using different portions of training data on MSMT17 in <ref type="table" target="#tab_5">Table 5</ref>. From the comparison between <ref type="table" target="#tab_4">Table 4</ref> and <ref type="table" target="#tab_5">Table 5</ref>, it can be observed that 10% of MSMT17 training set gets similar performance with the transferred training set from Duke, e.g., both achieve the Rank-1 accuracy of about 11.5%?11.8%. Therefore, 16,522 transferred images from Duke achieves similar performance with 2,602 annotated images on MSMT17. We can roughly estimate that 6.3 transferred images are equivalent to 1 annotated image. This thus effectively relieves the cost of data annotation on new datasets. The transferred data is then combined with the training set on MSMT17. As shown in <ref type="table" target="#tab_5">Table 5</ref>, the Rank-1 accuracy is constantly improved by 1.9%, 5.1%, and 2.4%, respectively by combining the transferred data from Duke, Market, and CUHK03,  respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and Discussions</head><p>This paper contributes a large-scale MSMT17 dataset. MSMT17 presents substantially variants on lightings, scenes, backgrounds, human poses, etc., and is currently the largest person ReID dataset. Compared with existing datasets, MSMT17 defines a more realistic and challenging person ReID task.</p><p>PTGAN is proposed as an original work on person transfer to bridge the domain gap among datasets. Extensive experiments show PTGAN effectively reduces the domain gap. Different cameras may present different styles, making it difficult to perform multiple style transfer with one mapping function. Therefore, the person transfer strategy in Sec. 5.4.2 and Sec. 5.5 is not yet optimal. This also explains why PTGAN learned on each individual target camera performs better in Sec. 5.4.1. A better strategy is to consider the style differences among cameras to get more stable mapping functions. Our future work would continue to study more effective and efficient person transfer strategies for large datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Comparison of person images in CUHK03, Mar-ket1501, DukeMTMC-reID, and MSMT17. Each column shows two sample images of the same identity. It is obvious that, MSMT17 presents a more challenging and realistic person ReID task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>The number of identities and bounding boxes in each time slot (a) The number of identities and bounding boxes on each camera (c) The number of identities across different, i.e., 1-15, cameras</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Statistics of MSMT17.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Comparison of the transferred images by PTGAN and Cycle-GAN from CUHK03 to PRID-cam1. The second row shows the segmentation results by PSPNet. The pink regions are segmented as person body regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Sample person ReID results generated by the method of GLAD [31] on MSMT17.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Sample transferred person images from CUHK03 to PRID-cam2. Each sample shows an image from CUHK03 in the first column, and the transferred image in the second column.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Sample transferred person images from Market to PRID-cam1 and PRID-cam2. Images in the first column are from Market. Transferred images to PRID-cam1 and PRID-cam2 are shown in the second and third columns, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Rank-1 and Rank-10 accuracies of GoogLeNet on CUHK03, Market, and Duke. The subscripts C, M a, and D denote the transferred target dataset is CUHK03, Market, and Duke, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Illustration of the transferred person images toDuke. The images in first row are from Duke. The images in second and third rows are transferred images from Market to Duke. Obviously, those images have the similar styles, e.g., similar backgrounds and lightings, etc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison between MSMT17 and other person ReID datasets.</figDesc><table><row><cell>Dataset</cell><cell>MSMT17</cell><cell>Duke [41, 27]</cell><cell>Market [39]</cell><cell>CUHK03 [20]</cell><cell>CUHK01 [19]</cell><cell>VIPeR [8]</cell><cell>PRID [10]</cell><cell>CAVIAR [3]</cell></row><row><cell>BBoxes</cell><cell>126,441</cell><cell>36,411</cell><cell>32,668</cell><cell>28,192</cell><cell>3,884</cell><cell>1,264</cell><cell>1,134</cell><cell>610</cell></row><row><cell>Identities</cell><cell>4,101</cell><cell>1,812</cell><cell>1,501</cell><cell>1,467</cell><cell>971</cell><cell>632</cell><cell>934</cell><cell>72</cell></row><row><cell>Cameras</cell><cell>15</cell><cell>8</cell><cell>6</cell><cell>2</cell><cell>10</cell><cell>2</cell><cell>2</cell><cell>2</cell></row><row><cell>Detector</cell><cell>Faster RCNN</cell><cell>hand</cell><cell>DPM</cell><cell>DPM, hand</cell><cell>hand</cell><cell>hand</cell><cell>hand</cell><cell>hand</cell></row><row><cell>Scene</cell><cell>outdoor, indoor</cell><cell>outdoor</cell><cell>outdoor</cell><cell>indoor</cell><cell>indoor</cell><cell>outdoor</cell><cell>outdoor</cell><cell>indoor</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>is composed of 1,812 identities and 36,411 bounding boxes. 16,522 bounding boxes of 702 identities are used for training. The rest identities are included in the testing set. DukeMTMC-reID is also denoted as Duke for short. Market-1501 [39] contains 1,501 identities and 32,668 bounding boxes. The training set contains 12,936 bounding boxes of 751 identities. The rest 750 identities are included in the testing set. Market-1501 is also denoted as Market for short. CUHK03 [20] consists of 1,467 identities and 28,192 bounding boxes generated by both DPM and hand. Following the work [33], 26,264 bounding boxes of 1,367 identities are used for training, and 1,928 bounding boxes of 100 identities are used for testing. PRID [10] is composed of 934 identities from two cameras. Our experiments use the bounding boxes of 200 persons shared by both cameras as testing set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The performance of the state-of-the-art methods on MSMT17. R-1 represents the Rank-1 accuracy.</figDesc><table><row><cell>Methods</cell><cell>mAP</cell><cell>R-1</cell><cell>R-5</cell><cell>R-10</cell><cell>R-20</cell></row><row><cell>GoogLeNet [29]</cell><cell>23.0</cell><cell>47.6</cell><cell>65.0</cell><cell>71.8</cell><cell>78.2</cell></row><row><cell>PDC [28]</cell><cell>29.7</cell><cell>58.0</cell><cell>73.6</cell><cell>79.4</cell><cell>84.5</cell></row><row><cell>GLAD [31]</cell><cell>34.0</cell><cell>61.4</cell><cell>76.8</cell><cell>81.6</cell><cell>85.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance of GoogLeNet tested on PRID but trained with different training sets. * denotes the transferred dataset. For instance, the subscript cam1 represents the transferred target dataset PRID-cam1. "cam1/cam2" means using images in PRID-cam1 as query set and images from PRID-cam2 as gallery set.</figDesc><table><row><cell>Training Set</cell><cell cols="2">cam1/cam2 R-1 R-10</cell><cell cols="2">cam2/cam1 R-1 R-10</cell></row><row><cell>CUHK03</cell><cell>2.0</cell><cell>11.5</cell><cell>1.5</cell><cell>11.5</cell></row><row><cell>CUHK03  *  cam1 CUHK03  *  cam2 CUHK03  *  cam1 + CUHK03  *  cam2</cell><cell>18.0 17.5 37.5</cell><cell>43.5 53.0 72.5</cell><cell>6.5 22.5 37.5</cell><cell>24.0 54.0 69.5</cell></row><row><cell>Market</cell><cell>5.0</cell><cell>26.0</cell><cell>11.0</cell><cell>40.0</cell></row><row><cell>Market  *  cam1 Market  *  cam2 Market  *  cam1 + Market  *  cam2</cell><cell>17.5 10.0 33.5</cell><cell>50.5 31.5 71.5</cell><cell>8.5 10.5 31.0</cell><cell>28.5 37.5 70.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>The performance of GoogLeNet tested on MSMT17. The subscript M S denotes the transferred target dataset MSMT17.</figDesc><table><row><cell></cell><cell cols="2">Duke Duke  *  M S</cell><cell>Market</cell><cell>Market  *  M S</cell><cell cols="2">CUHK03 CUHK03  *  M S</cell></row><row><cell>R-1</cell><cell>7.1</cell><cell>11.8</cell><cell>3.4</cell><cell>10.2</cell><cell>2.8</cell><cell>6.5</cell></row><row><cell>R-10</cell><cell>17.4</cell><cell>27.4</cell><cell>10.0</cell><cell>24.4</cell><cell>8.6</cell><cell>17.2</cell></row><row><cell>mAP</cell><cell>1.9</cell><cell>3.3</cell><cell>1.0</cell><cell>2.9</cell><cell>0.7</cell><cell>1.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>The performance of GoogLeNet for weakly supervised learning on MSMT17.</figDesc><table><row><cell>Training Set</cell><cell>R-1</cell><cell>R-10</cell><cell>mAP</cell></row><row><cell>MSMT (1%)</cell><cell>0.9</cell><cell>3.6</cell><cell>0.2</cell></row><row><cell>MSMT (2.5%)</cell><cell>2.0</cell><cell>7.4</cell><cell>0.5</cell></row><row><cell>MSMT (5%)</cell><cell>6.3</cell><cell>18.1</cell><cell>1.9</cell></row><row><cell>MSMT (10%)</cell><cell>11.5</cell><cell>26.9</cell><cell>3.7</cell></row><row><cell>Duke + MSMT17 (10%)</cell><cell>16.1</cell><cell>33.1</cell><cell>5.5</cell></row><row><cell>Duke  *  M S + MSMT17 (10%)</cell><cell>18.0</cell><cell>36.4</cell><cell>6.2</cell></row><row><cell>Market + MSMT17 (10%)</cell><cell>12.6</cell><cell>28.5</cell><cell>4.4</cell></row><row><cell>Market  *  M S + MSMT17 (10%)</cell><cell>17.7</cell><cell>35.9</cell><cell>6.0</cell></row><row><cell>CUHK03 + MSMT17 (10%)</cell><cell>11.9</cell><cell>28.3</cell><cell>4.1</cell></row><row><cell>CUHK03  *  M S + MSMT17 (10%)</cell><cell>14.3</cell><cell>31.7</cell><cell>4.6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The work<ref type="bibr" target="#b22">[23]</ref> reports better performance, but it is trained on an augmented data including training sets from three datasets.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Beyond triplet loss: a deep quadruplet network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Person re-identification by multi-channel parts-based cnn with improved triplet loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Custom pictorial structures for re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stoppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deep transfer learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05244</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Viewpoint invariant pedestrian recognition with an ensemble of localized features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Person re-identification by descriptive and discriminative classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beleznai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SCIA</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deepercut: A deeper, stronger, and faster multiperson pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Imageto-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07004</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning to discover cross-domain relations with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.05192</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kinga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04802</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Precomputed real-time texture synthesis with markovian generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning deep context-aware features over body and latent parts for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Human reidentification with transferred metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Coupled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Hydraplus-net: Attentive deep features for pedestrian analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.05396</idno>
		<title level="m">Generative adversarial text to image synthesis</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multicamera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Posedriven deep convolutional model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Generative image modeling using style and structure adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Glad: Global-local-alignment descriptor for pedestrian retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An enhanced deep feature representation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning deep feature representations with domain guided dropout for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attribute2image: Conditional image generation from visual attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Unsupervised dual learning for image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Gong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02510</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pixellevel domain transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Paek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Spindle net: Person re-identification with human body region guided feature decomposition and fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">A discriminatively learned cnn embedding for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05666</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unpaired imageto-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
