<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">WinoGAViL: Gamified Association Benchmark to Challenge Vision-and-Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bitton</surname></persName>
							<email>yonatan.bitton@mail.huji.ac.il</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The Hebrew University of Jerusalem</orgName>
								<orgName type="institution" key="instit2">Ben Gurion University ? University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitzan</forename><surname>Bitton-Guetta</surname></persName>
							<email>nitzangu@bgu.ac.il</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The Hebrew University of Jerusalem</orgName>
								<orgName type="institution" key="instit2">Ben Gurion University ? University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Yosef</surname></persName>
							<email>ron.yosef@mail.huji.ac.il</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The Hebrew University of Jerusalem</orgName>
								<orgName type="institution" key="instit2">Ben Gurion University ? University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Elovici</surname></persName>
							<email>elovici@bgu.ac.il</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The Hebrew University of Jerusalem</orgName>
								<orgName type="institution" key="instit2">Ben Gurion University ? University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
							<email>mbansal@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The Hebrew University of Jerusalem</orgName>
								<orgName type="institution" key="instit2">Ben Gurion University ? University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
							<email>gabriel.stanovsky@mail.huji.ac.il</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The Hebrew University of Jerusalem</orgName>
								<orgName type="institution" key="instit2">Ben Gurion University ? University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
							<email>roy.schwartz1@mail.huji.ac.il</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The Hebrew University of Jerusalem</orgName>
								<orgName type="institution" key="instit2">Ben Gurion University ? University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">WinoGAViL: Gamified Association Benchmark to Challenge Vision-and-Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While vision-and-language models perform well on tasks such as visual question answering, they struggle when it comes to basic human commonsense reasoning skills. In this work, we introduce WinoGAViL: an online game of vision-andlanguage associations (e.g., between werewolves and a full moon), used as a dynamic evaluation benchmark. Inspired by the popular card game Codenames, a "spymaster" gives a textual cue related to several visual candidates, and another player tries to identify them. Human players are rewarded for creating associations that are challenging for a rival AI model but still solvable by other human players. We use the game to collect 3.5K instances, finding that they are intuitive for humans (&gt;90% Jaccard index) but challenging for state-of-the-art AI models, where the best model (ViLT) achieves a score of 52%, succeeding mostly where the cue is visually salient. Our analysis as well as the feedback we collect from players indicate that the collected associations require diverse reasoning skills, including general knowledge, common sense, abstraction, and more. We release the dataset, the code and the interactive game, allowing future data collection that can be used to develop models with better association abilities.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Humans can intuitively reason about how a cue is associated with an image <ref type="bibr">[1,</ref><ref type="bibr">2,</ref><ref type="bibr">3]</ref>. For example, in <ref type="figure">Figure 1</ref>, the word werewolf may be intuitively associated with images of a puppy and a full moon. These reasoning skills go beyond object detection and similarity and require rich cultural and world knowledge. Cognitive studies suggest that this kind of associative thinking involves connecting distant concepts in the human memory, organized as a network of interconnected ideas <ref type="bibr">[4,</ref><ref type="bibr">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>. On the other hand, vision-and-language models often fail when faced with tasks that require commonsense reasoning and cultural knowledge <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>, motivating the construction of a challenging high quality vision-and-language benchmark.</p><p>In this work, we introduce a Gamified Association benchmark to challenge Vision-and-Language models (WinoGAViL). Inspired by Winograd Schema Challenge <ref type="bibr" target="#b12">[13]</ref>, we suggest WinoGAViL as a benchmark for multimodal machine commonsense reasoning and association abilities. Similar to the Codenames game, 2 each instance in WinoGAViL is composed of a textual cue, a number k, and a set of candidate images. The task is to select the k images most associated with the cue. We refer to the <ref type="figure">Figure 1</ref>: Top: An association instance from the WinoGAViL benchmark. The task is to choose the top k images that suit the cue word. In this example, the top k=2 images that suit the cue werewolf are surrounded by red bounding boxes. Bottom: Game Setup-a new association instance generation. A spymaster (Alice) composes a new association given a set of images that is challenging for the rival AI model but easy for other human players. (a) Alice generates a cue word for a subset of the images; (b) A rival AI model makes a prediction based on the given cue, and Alice is rewarded inversely to the model performance; (c) Three human solvers also try to solve the task and the spymaster is rewarded according to their performance. cue and the associated images as an association instance. For example, in <ref type="figure">Figure 1</ref>, the pictures of a puppy and a moon are (arguably) the ones most associated with the cue werewolf out of the given candidates. <ref type="figure">Figure 2</ref>: The spymaster screen for an example collected via the WinoGAViL benchmark. The spymaster submitted the cue 'pogonophile' (a lover of beards), and associated it with the three images surrounded by red bounding boxes. Model predictions are marked with V for success and X for failure. In this example the spymaster has managed to partially fool the AI model, while three other humans are able to solve it perfectly.</p><p>We develop an online game to collect novel and challenging associations. The game is used to collect data for this work, but more importantlyto serve as a dynamic source for additional data in the future. As exemplified in <ref type="figure">Figure 1</ref>, a "spymaster" first composes a new association cue given a set of images. A rival AI model (CLIP RN50 <ref type="bibr" target="#b13">[14]</ref>) then predicts the given association, and the spymaster is rewarded inversely to its performance, motivating the spymaster to make the cue challenging. Lastly, three human players attempt to solve the association task. The spymaster is rewarded according to their performance, motivating the spymaster to compose associations that are solvable by humans and, thus, ideally more natural than examples designed to fool a model. We use crowdworkers to collect 3.5K test instances. See <ref type="figure">Figure 2</ref> for a collected example.</p><p>We evaluate several state-of-the-art models on WinoGAViL data. We find that our game allows the collection of associations that are easy for humans (&gt;90% Jaccard index) and challenging for models (?52%), even those that are orders of magnitude larger than the model used to create the game. Our analysis shows that models succeed mostly where the cue is visually salient. Finally, we compare our collected data with data we collected via an alternative data generation baseline that relies on SWOW <ref type="bibr" target="#b14">[15]</ref>, a hand-crafted resource of textual associations. Our results show that while the two approaches are relatively easy for humans, data generated by WinoGAViL is much more challenging to machines, highlighting the value of our gamified data collection framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The WinoGAViL Benchmark</head><p>We start by presenting the game as a framework for collecting challenging associations ( ?2.1). Second, we describe how we crowd-source a test set using the game ( ?2.2). Finally, we analyze the collected dataset and provide statistics ( ?2.3).</p><p>Throughout this paper we use the Jaccard index, which is the intersection of selected candidates divided by the union of selected candidates. <ref type="bibr">3</ref> This metric does not reward random guesses highly. The random expected Jaccard index is 38%, 34%, 24%, 17% with 5/6/10/12 candidates respectively. For example, in <ref type="figure">Figure 1c</ref> the Jaccard index ('Human score') of the solvers is 100%, since the intersection of the selections is the same as the union. In <ref type="figure">Figure 1b</ref> the AI model selection is 1/3, so the Jaccard index ('Model score') is 33%: there are three images in the union, and one image in the intersection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Game</head><p>This section describes the WinoGAViL game environment. Besides collecting the data presented in this paper, the game can also serve as a dynamic source of new data in the future. The game setup is described below in sequential order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">A spymaster creates a challenging association. A spymaster composes a new association</head><p>instance given a random set of images sampled from the web (see details below). We experiment with sets of 5, 6, 10 or 12 images. The spymaster then submits a single-word cue and selects the subset of associated images. The goal is for the association to be solvable by humans but not by the AI model. For example in <ref type="figure">Figure 1</ref>, the spymaster composes the cue werewolf and associates it with the images of the puppy and the moon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>A rival AI model makes a prediction. We then feed the association instance to a rival AI model, and report the model score. For example, in <ref type="figure">Figure 2</ref>, the model predicts correctly one candidate (the image of the bison), and the total number of candidates involved is 5 :the three images the user selected and the two images falsely predicted by the model. Therefore, the model's Jaccard index is 1/5=20%. The spymaster is rewarded inversely to the model performance, so their "fool-the-AI" score is (100 -'model score') = 80%.</p><p>3. Three human players validate the created association. We then give the association to three human validators, who are rewarded according to their Jaccard index for solving the association. Importantly, the spymaster's association "solvable-by-humans" score is determined by the average score of the three solvers. For example, in <ref type="figure">Figure 1</ref> all players solve the created associations perfectly; therefore, the spymaster's association "solvable-byhumans" score is 100%.</p><p>Each player alternates between spymaster and solver roles. Each new association instance created by the spymaster is assigned to three solvers. Once the spymaster creates an association instance, their role changes to a solver responsible for solving other players' associations. This balanced approach ensures that all new associations are automatically validated by three other players.</p><p>Rival AI model. We use CLIP <ref type="bibr" target="#b13">[14]</ref>, with a textual prompt of "A/An [cue]". We intentionally use a small version of CLIP (RN50), so we could evaluate the generated data with larger models. Our experiments ( ?3) show that this data is indeed challenging for orders-of-magnitude larger models. Future versions of the game will use newer and stronger models, that are likely to further improve the data quality.</p><p>Image extraction. We start with a corpus of English concepts obtained from SWOW <ref type="bibr" target="#b14">[15]</ref>. <ref type="bibr">4</ref> We collect an image for each concept from Google Images Download. We filter images of written words using an OCR model <ref type="bibr" target="#b15">[16]</ref>. We removed images containing the query text using an OCR model (e.g., OCR prediction "brary" for search query "library"). We extract the top image based on google ranking (?2% of the images are filtered). We also manually filter and verify that there are no inappropriate images. The result is a set of 3K images.</p><p>WinoGAViL game properties. WinoGAViL's main goal is to serve as a dynamic benchmark that remains relevant as the field advances. To achieve this, we publicly release the WinoGAViL web game, allowing dynamic data collection. The players who create associations observe the AI model predictions in real-time. Players switch roles, validating each created association as part of the game. We use rewards to motivate players to create high-quality data according to our metrics. Players are rewarded for both fooling the AI model and making the associations solvable by other humans, preventing the data from becoming unnatural and biased towards only fooling the AI model. The publicly released game includes a player dashboard and a leaderboard. All of these aim to motivate the players to compete with the AI model and with each other, leading to enhanced user engagement and high-quality data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Human Annotation</head><p>We hire Amazon Mechanical Turk workers to play the WinoGAViL game. We develop qualification tests to select high-quality annotators and collect the annotators' demographic information. Spymasters screen example is presented in <ref type="figure" target="#fig_0">Figure 3</ref>; See Appendix A for more details. <ref type="bibr">5</ref> We have several options for the total number of candidates: 5, 6, 10 or 12. With more candidates, the task naturally becomes harder. The spymasters are allowed to select between 2-5 images. Full annotation results and statistics are presented in <ref type="table" target="#tab_1">Table 1</ref>. The scores of both humans and models is the Jaccard index of between their created associations instances. The annotation task includes three steps, elaborated below.</p><p>First, we create new associations by asking three spymasters to create two different cues and associated candidates for a given set of images. The created association should fool the AI model but still be solvable by other humans. To reinforce it, the spymasters receive a bonus payment if their "solvableby-humans" score is at least 80%, which grows according to their "fool-the-AI" score, see full details of the bonus in Appendix A, Section A.4.1. The first row in <ref type="table" target="#tab_1">Table 1</ref> presents the number of generated associations, and the second row presents the average model score (or 100-"fool-the-AI score"). The low model scores indicate that the spymasters succeeded in creating data that fools the AI model.</p><p>Second, we take the associations created via the game and ask three annotators to solve them. We compute an average Jaccard index of the three solvers for each instance. The third row in <ref type="table" target="#tab_1">Table 1</ref> presents the average human score (or the spymaster's "solvable-by-humans" score), indicating that the spymasters were able to create data that is solvable by other humans.  Finally, we select the WinoGAViL test set. To obtain the final test instances, we select associations solved with a mean Jaccard index of at least 80%. The threshold can be lowered to receive more data of lower quality or raised to receive less data of higher quality. Note that in order to reduce the dependence on a specific model, we do not use the model scores in the data selection, i.e., instances that can be solved by the AI model are not automatically excluded, only the solvable-by-humans score is considered in the discarding decision. The last row in <ref type="table" target="#tab_1">Table 1</ref> presents the final number of instances accumulated in the dataset. The annotators were paid an average of 14 USD per hour for the annotation tasks (including bonuses). The total project annotation budget was 2,000 USD. The annotators received daily feedback on their performances, scores, and the bonuses they won. We denote the data created by the Wino-GAViL game by WinoGAViL dataset. In ?3 we show that this data is easy for humans and challenging for state-of-the-art models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">WinoGAViL Analysis</head><p>Reasoning skills. We analyze the different skills required to solve the WinoGAViL dataset. We randomly sample 320 samples of WinoGAViL dataset and manually annotate these skills, observing the patterns required for humans to solve each association. <ref type="table" target="#tab_0">Table 2</ref> presents some of the observed patterns, required skills, and frequencies. Appendix A, <ref type="table" target="#tab_7">Table 8</ref> presents the full table and <ref type="figure" target="#fig_4">Figure 6</ref> presents examples of the visual associations. We see that solving WinoGAViL dataset requires diverse commonsense skills.</p><p>Players feedback. We collected qualitative and quantitative feedback from the crowdworkers. <ref type="table" target="#tab_2">Table 3</ref> presents quantitative questions and ratings, showing our game is recommended as an online game, is fun and has an intuitive user interface. We also asked the spymasters open questions about how seeing the AI model prediction and the performance bonus affected them. They mostly responded that these decisions were effective-"I used the model's guesses to make my associations better. I went after associations that the model frequently got wrong." and "bonus keep motivation up when it was hard to come up with connections". Full qualitative responses (open text) are presented in Section A.4.2 at Appendix A.</p><p>Section A.5 in Appendix A includes additional analysis, for example annotator statistics with demographic information and average performance, and generated cues statistics including richness ratings of the created cues, ratings for abstract and concrete cues, and more. 3 Experiments</p><p>In this section, we provide an extensive evaluation of WinoGAViL dataset. First, we show the value of our game, by comparing it to an alternative data generation baseline based on SWOW <ref type="bibr" target="#b16">[17]</ref>, an existing resource of textual associations. We then evaluate human and models performance on both datasets and provide analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Extracting the SWOW Baseline Dataset</head><p>We describe an alternative data generation baseline based on the SWOW ("Small World of Words") dataset. <ref type="bibr" target="#b5">6</ref> SWOW is an ongoing project where participants are presented with a cue word and asked to respond with the first three words that come to mind. We use a common representation of SWOW as a graph network. <ref type="bibr" target="#b6">7</ref> We select random distractors that are not associated with the cue in the SWOW graph. We combine the distractors to the association instances from SWOW and create 1,200 multiplechoice instances with 5 or 6 candidates. Each concept's image is obtained from the extracted images ( ?2.1). Note that SWOW is based on textual associations, which were provided by humans given a cue, making it textual and non-adversarial, whereas WinoGAViL is based on visual and adversarial associations, where humans create a new cue given a set of images. <ref type="figure">Figure 4</ref> illustrates this difference. As we did in the WinoGAViL game, we validate with human annotation and only keep instances with a mean Jaccard score of at least 80%. Human performance is 85%, so most association instances are retained. The final dataset, denoted SWOW vision baseline dataset, is composed of 1,000 instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation Setup</head><p>We experiment with state-of-the-art models and compare them to humans on the WinoGAViL dataset and the SWOW vision baseline dataset. On the WinoGAViL dataset we compare cases with 5-6 candidates and cases with 10-12 candidates. We use the Jaccard index as an evaluation metric ( ?2).</p><p>Humans. We sample 10% of the test sets and validate it with new annotators who were not involved in any previous annotation tasks. We require three different annotators to solve each instance and report their average Jaccard score as the final human prediction. Annotator agreement is measured two different ways: by comparing the Jaccard index of the annotators' selections with the ground-truth labels, and by comparing the Jaccard index between the three annotators' selections. The standard deviations are 6.3, 7.5, and 5, and the Jaccard index is 80, 81, and 89 for the cases with 10-12 candidates, 5-6 candidates, and SWOW, respectively, indicating high agreement.</p><p>Zero-shot models. We evaluate several diverse state-of-the-art vision-and-language models. In all cases described below (except CLIP-ViL), the model encodes the text and the image and produces a matching score for each (cue, image) pair. We take the k (number of associations) images with the top scores (For example, the top k=3 model predictions in <ref type="figure">Figure 2</ref>). <ref type="bibr" target="#b7">8</ref> 1. CLIP <ref type="bibr" target="#b13">[14]</ref> is pre-trained with a contrastive objective that can be used without directly optimizing for the task. We use four versions of models with different amounts of parameters: RN50, ViT-B/32, ViT-L/14 and RN50x64/14 with 100M, 150M, 430M and 620M parameters respectively (RN50 was used during data collection).</p><p>(a) An association from WinoGAViL dataset, collected by our interactive game. Spymaster composes a new association given a set of images, aiming to fool a rival AI model. The spymaster has created the cue horn and selected the two images surrounded by bounding boxes. This association instance cannot be solved without the specific image information (TVs usually don't have horns). Cues are assigned in a visual and adversarial manner.</p><p>(b) An association from SWOW vision baseline dataset, which is automatically extracted based on the SWOW dataset. Annotators receive a cue (e.g., stork) and provide three associations. We take the textual annotations, add distractors and extract images for each given association. This association could be solved without the visual information (stork is correlated with the concepts of bird and baby). Cues are assigned in a textual and non-adversarial manner. <ref type="figure">Figure 4</ref>: WinoGAViL dataset vs. SWOW vision baseline dataset generation process.</p><p>2. CLIP-ViL <ref type="bibr" target="#b17">[18]</ref>, with 290M parameters, is a pre-trained vision-and-language model that uses CLIP as a visual backbone, rather than CNN based visual encoders that are trained on a small set of manually annotated data. We use the image-text matching objective, where a classification head predicts a score indicating whether the candidate image and the cue match each other. 3. ViLT <ref type="bibr" target="#b18">[19]</ref>, with 111M parameters, incorporates text embeddings into a Vision Transformer (ViT). 4. X-VLM <ref type="bibr" target="#b19">[20]</ref>, with 216M parameters, is pre-trained with multi-grained vision language alignments and fine-tuned for image-text retrieval (Flickr30 <ref type="bibr" target="#b20">[21]</ref>) tasks, achieving state-ofthe-art results on several benchmarks.</p><p>Supervised models. We join a line of benchmarks that introduce a test set, without predefined train splits <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>. We believe that in order to solve associations, a machine must map knowledge to new, unknown cases without extensive training <ref type="bibr" target="#b23">[24]</ref>. Nonetheless, for completeness, we also consider fine-tuning models on the associations data. We add a binary classifier on top of the pretrained embeddings to classify whether a given (cue, image) pair is associative or not. We use CLIP (VIT-B/32) model, concatenate the textual cue embedding to the visual image embedding, followed by a classifier that produces a score in [0, 1], where 1 is labeled 'associative'. We use the Adam optimizer <ref type="bibr" target="#b24">[25]</ref> with a learning rate of 0.001, batch size of 128, and train for 7 epochs. Since we do not propose a training/validation/test split, we repeat five experiments with different random seeds where we sample a unified training set of 9,326 (cue,image) pairs for both the candidates cases. We then sample a separate test (10%) and validation (10%) sets with non-overlapping images, and report the average results, comparing the supervised and zero-shot models on the same sampled test sets. 9 <ref type="figure" target="#fig_3">Figure 5</ref>: Examples for different association categories and results for each category Zero-shot results on WinoGAViL dataset and the SWOW vision baseline dataset are presented in <ref type="table" target="#tab_3">Table 4</ref>. <ref type="table" target="#tab_1">Table 10</ref> (Appendix A) shows full statistics and performance for the different number of candidates and created associations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results and Model Analysis</head><p>The game allows collection of associations that are easy for humans and challenging for models. Performance on the data collected via the game is 15-52% with 10-12 candidates, and 47-55% with 5-6 candidates. All models' performances are far below human performance (90% and 92%, see last row). We highlight that although our rival AI model is CLIP with RN50, the created data is still challenging even for models order-of-magnitude larger. We also see a significant performance drop with most models when increasing the number of candidates without hurting human accuracy, indicating that humans are robust to the increased difficulty level while models struggle with it.</p><p>The game creates more challenging associations compared to the SWOW based method. The highest model performance on the SWOW vision baseline dataset is 74%, and on the WinoGAViL dataset is 55%, both with the same number of candidates <ref type="bibr">(5 &amp; 6)</ref>. CLIP-ViL achieves lower results, especially in the 10 &amp; 12 case. The reason could be that CLIP-ViL uses the ITM pre-training objective (image-text matching), whereas X-VLM and ViLT are fine-tuned for image-text retrieval. CLIP is also pre-trained, but with a different contrastive pre-training objective that may be more useful for this task. The results indicate the value of our game in collecting associations that are much more challenging than the SWOW-based method. Training is effective given more distractors. Fine-tuning results are presented in <ref type="table" target="#tab_4">Table 5</ref>. The relatively low performance indicates that models struggle to capture the information required to solve challenging associations from supervised data. Interestingly, we see that training did not change with 5 &amp; 6 candidates, but did improve performance by 7% with 10 &amp; 12 candidates, indicating that the model is only able to exploit supervised data in particularly hard cases, with lower random chance of success.</p><p>Model performance varies between different association types. We provide a fine-grained model analysis of different association types. We hypothesize that models perform better on association instances that require direct visual detection, as these models' training objectives are similar to these kind of tasks. We sampled ?1K cue-image pairs of the instances created via the game with 10-12 candidates for analysis. Three of the authors identified the following six categories: (a) Visually salient: the cue is the main visually salient item in the image; (b) Visually non-salient: the cue is visual but non-salient, and specific to the particular given image; (c) Concept related: the cue is related to the image concept, not necessarily to the particular given image; (d) Activity: the cue is an activity depicted in the image, e.g., the cue is jumping, and the image shows people jumping; (e) Counting: the cue is a number or amount of items depicted in the image (e.g., two with an image of two people); (f) Colors: the cue indicates a color that is depicted in the image (e.g., white); Examples are presented in <ref type="figure" target="#fig_3">Figure 5</ref>. Additional details, annotations guidelines, full examples for each category and screenshots from the annotation task are provided in Appendix A, Section A.7. We define the final category as the annotators' majority vote, that was reached in 98% of the cases, and discarded the other 2%. We evaluate CLIP ViT-B/32 on the association instances and report the accuracy per category which is the proportion of the model success in each given category. Results are presented in <ref type="figure" target="#fig_3">Figure 5</ref>. We find that model performance is highest in the visually salient and colors category, degrades in concept related, and activities, and is much worse in the visually non-salient and counting categories. The results suggest a lack of common sense reasoning capabilities. We release the annotated data in the project website for future research. Performance of textual models is close to vision-and-language models, but still far from human. Another approach for tackling WinoGAViL is using textual models, when transferring the visual modality to textual modality with image captions, receiving a fulltextual dataset. We take OFA <ref type="bibr" target="#b25">[26]</ref>, a state-of-the-art image captioning model, and extract image captions for each of the image candidates. We use the three leading models for semantic search in Sentence Transformers <ref type="bibr" target="#b26">[27]</ref>, which are Distilled RoBERTa, <ref type="bibr" target="#b27">[28]</ref> and MPNet <ref type="bibr" target="#b28">[29]</ref> (two versions, the original model, and a model fine-tuned for semantic search). <ref type="bibr" target="#b9">10</ref> Results are presented in <ref type="table" target="#tab_6">Table 7</ref>. We see that the results are better than chance level, a bit lower than the textual cue and visual candidates' version (ViLT, one line prior to the last), but still far from human performance. These results hint that WinoGAViL cannot be trivially solved by mapping the images to text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Associations and Codenames. Several works have studied the popular Codenames game in the context of natural language processing <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>, which is also related to works on semantic relatedness <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>. In the context of associations, a recent work have proposed to use the SWOW resource to evaluate pre-trained word embedding <ref type="bibr" target="#b16">[17]</ref>, and some works evaluate models with a CNN-based visual components <ref type="bibr">[1,</ref><ref type="bibr">2]</ref>. We expand these ideas to evaluate state-of-the-art vision-andlanguage pre-trained models.</p><p>Commonsense. Commonsense reasoning is a topic with increasing interest lately <ref type="bibr" target="#b35">[36]</ref>. Many commonsense reasoning tasks have been proposed, both in NLP <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref>, and Computer Vision <ref type="bibr" target="#b42">[43,</ref><ref type="bibr">44]</ref>, including works that require understanding social cues <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b8">9]</ref>. In the text domain, a number of Winograd Schema Challenge Datasets have been proposed as alternatives for the Turing test <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>. In the vision-and-language domain Thrush et al. <ref type="bibr" target="#b9">[10]</ref> have proposed a dataset that tests compositional reasoning in vision-and-language models with the task of matching a caption with its correct image. WinoGAViL also measures vision-and-language reasoning, but focuses on commonsense-based image-cue associations, and primarily serves as a dynamic benchmark as playing the game allows future data collection. Human-and-model-in-the-loop. Models are often used in dataset collection to reduce dataset biases or to create adversarial instances <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50]</ref>, which might limit the created instances to be effected by the used model. For example, in works that create adversarial visual question answering instances <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52]</ref>, human annotators are prompted to fool the model iteratively for each instance, receiving online feedback from the model, and their annotation is allowed to be submitted only after they succeed or after a certain number of trails. In contrast, in our work, the annotators have only one chance to trick the AI model for a given instance. They cannot iteratively 'squeeze' the model to produce an adversarial example. Thus, the generated data is less dependent on the particular AI model since the model is only used to motivate the human player to fool it. In particular, we do not use the models' predictions to choose the test set instances.</p><p>Gamification. Gamification was previously used for several purposes, such as data collection <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55]</ref>, education <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b56">57]</ref>, and beat-the-AI tasks for AI model evaluation <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b59">60]</ref>. Talmor et al. <ref type="bibr" target="#b11">[12]</ref> proposed a gamification framework to collect question answering instances. Kiela et al. <ref type="bibr" target="#b60">[61]</ref> proposed a dynamic benchmark that supports human-and-model-in-the-loop. We propose a game that serves as a dynamic benchmark of vision-and-language associations, gamifying both human interactions with an AI model and human interactions with other humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Limitations and Conclusions</head><p>Despite our efforts to filter inappropriate concepts and images, some players may feel harmed when they are exposed to new generated cues, or when seeing an image that have passed the automatic and manual filtering. Players are able to mark such cases (with a designated 'report' button), leading to immediate removal until further examination. Additionally, players will agree to a consent form when they register. When designing the game, we had several choices to make, including the bonus reward and the AI model interaction. Future work will thoroughly explore the impact of these choices.</p><p>We introduced an online game to collect challenging associations. We demonstrated its effectiveness by collecting a dataset that it is easy for humans and challenging for state-of-the-art models. We provided an extensive evaluation of the game and collected dataset. We hope the WinoGAViL benchmark will drive the development of models with better commonsense and association abilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.1 Dataset Supplementary Materials 1. Dataset documentation, metadata, and download instructions: https://winogavil. github.io/download.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Intended uses: we hope our benchmark will be used by researchers to evaluate machine learning models. We hope that our benchmark will be played by users, leading to new associations collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Author statement:</head><p>We bear all responsibility in case of violation of right in using our benchmark. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Licenses</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>Hosting &amp; preservation: our website is deployed and all data is accessible and available. We encourage researchers to send us model predictions on the created test sets. We will update a model and players leader-board with this results periodically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Code repository: https://github.com/WinoGAViL/WinoGAViL-experiments</head><p>A.2 Reasoning Skills <ref type="table" target="#tab_7">Table 8</ref> lists the full reasoning and observed patterns annotated to solve the WinoGAViL dataset ( ?2.3), and <ref type="figure" target="#fig_4">Figure 6</ref> shows an example of each visual pattern we annotated.  <ref type="figure" target="#fig_4">(Figure 6b</ref>) 6%</p><p>Humor/Sarcasm Cue is related to Association in a funny way pigpen is a dirty place, tide can make it cleaner <ref type="figure" target="#fig_4">(Figure 6e</ref>) 1% a man that looks neglected is described as trim Analogy Cue can be seen/used like/with Association, although they are from a different concept map TV antenna looks like a horn <ref type="figure" target="#fig_4">(Figure 6d</ref>) 4%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual Similarity</head><p>Association is visually similar to the Cue The sponge shape is similar to a box <ref type="figure" target="#fig_4">(Figure 6a</ref>) 20%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstraction</head><p>Cue is related to Association in an abstract way discovery is when a bulb turns on (I got it!) ( <ref type="figure" target="#fig_4">Figure 6c</ref>) 5% Generalization bread dough becomes fresh bread when baked 8% raven is a bird that can be found in a backyard A.3 Human Annotation <ref type="figure">Figure 7</ref> shows an example of the Mechanical Turk user-interface. Section A.4 describe the annotator qualifications we required. Section A.4.1 describes the designed bonus reward, aiming to receive generated data that is challenging for models and easy for humans. Section A.4.2 describes the player feedback we collected. Finally, Section A.5 describes additional analysis such as players statistics and the generated textual cues analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Qualifications</head><p>The basic requirements for our annotation task is percentage of approved assignments above 98%, more than 5,000 approved HITs, the location from the US, UK, Australia or New Zealand. To be a 'solver' or a 'spymaster', we required additional qualification tests: We selected 10 challenging examples from SWOW based dataset as qualification test. In each qualification test, a new worker entered demographic information: age, gender, level of education and whether he is a native English speaker. To be qualified as a 'solver', we accepted annotators that received a mean jaccard score over 80%. To be qualified as a 'creator', we require "fool-the-AI" score above 40%, and "solvable-byhumans" score above 80%. To obtain "solvable-by-humans" score, we sent the created associations to solvers (who have passed to solve qualification). The players received instructions, presented in 8 and could do an interactive practice in the project website. <ref type="bibr" target="#b10">11</ref> . We do not collect or publish players personal information. We presented anonymous demographic statistics, and we do not publish the demographic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4.1 Bonus Reward</head><p>If the score is between <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b59">60)</ref>, the bonus is 0.03$. If the score is between <ref type="bibr" target="#b59">[60,</ref><ref type="bibr">67)</ref>, the bonus is 0.07$. If the score is between [67,80), the bonus is 0.18$. Finally, if the score is at least 80, the bonus is 0.27$. The payment can thus reach up to 0.61$ for a single annotation when creating two cues for the same image instances that completely fool the AI model and are still solvable by humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4.2 Players Feedback</head><p>Here we list some of the open text feedback we received from our crowd workers. It is not cherrypicked -we chose five representative responses with positive and negative insights.</p><p>Q: Describe what did you like and dislike while performing the task. Spymasters:</p><p>1. I liked the chance to improve my creativity and brainstorm. It was fun.</p><p>2. I liked the mental challenge, especially on the larger 10-12 ones. It was frustrating when the AI clearly guessed and got it right on the 5-6.</p><p>3. I liked that I got immediate feedback and it was something different than what I usually do on mturk. I did not like that sometimes it seemed like the objects had nothing in common and it took me too long to think of a word to try and associate the objects.</p><p>4. I liked that it was a very creative-focused task, even more so on the creator's side. It was fun to think of what I could come up with to link these words/images and fool the AI/other people.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>Creating was exponentially harder for me than the solving. I felt frustration and I kind of felt stupid because I struggled with it. (But the solving was a blast.) Solvers:</p><p>1. I liked how easy and straightforward they were, and that they were also super fun and different from other typical HITs I have done. The only thing I disliked was probably the pay but it was not a big deal.</p><p>2. I like the fact that I got to be creativity. Nothing to dislike about this task.</p><p>3. I liked that the correct answers were sometimes abstract and required a little thinking.</p><p>4. I liked that it was a puzzle. I really enjoy puzzles. I did not like that some of them seemed unsolvable. But all in all, I enjoyed it and did much more than I usually do. Annotators statistics. <ref type="table" target="#tab_8">Table 9</ref> in Appendix A presents statistics for the Amazon Mechanical Turk workers that were involved in WinoGAViL annotation, both as spymasters and as solvers. A total of 58 crowd workers, mostly English native speakers (?95%), of a variety of ages , genders, and levels of education (high school to graduate school). <ref type="figure">Figure 9</ref> in Appendix A presents the spymaster's score plots, which include the number of annotations, fool-the-AI score, and solvable-by-humans score for each spymaster. Generated cues statistics. For the final 3,568 test instances, 2,215 different cues were collected. We measure the concreteness of cue words using the concreteness dataset described <ref type="bibr" target="#b61">[62]</ref>, in which human annotated concreteness scores on a scale of 1-5 were collected. This dataset covers over 88% of the collected cues, indicating a 12% upper bound for out-of-vocabulary words. We see a diversity of both abstract and concrete generated cues in <ref type="figure" target="#fig_6">Figure 11</ref>, Appendix A. Additionally, we measure how often different annotators compose the same cues for the same group of images. Since we asked three different annotators to provide two different cues for each group of images, we have six annotations for each image group. We find that almost always (98%) they combine different cues. <ref type="table" target="#tab_1">Table 10</ref> show results for all cases of generated data, with different number of candidates and generated associations. We observe that spymasters usually selected two associations, and that performance (both human and model) are similar between 5 and 6, and between 10 and 12. When comparing human to model performance, we see that the generated data is challenging for models and easy for humans.  <ref type="table" target="#tab_0">087  90  48  3  259  88  51  4  43  100  57   10   2  338  87  37  3  83  93  35  4  5  92  29   12   2  328  90  37  3  84  93  33  4</ref> 16 100 28</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Full Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 Model Analysis on Different Association Types</head><p>A sample of 1,000 associations were annotated by three different annotators. We defined the final category as the annotators' majority vote, that was reached in 98% of the cases, and discarded the other 2%. We reported the accuracy per category, which is the proportion of the model success in each given category. The full annotation guidelines are available in the following link: https://github.com/WinoGAViL/WinoGAViL-experiments/tree/main/ assets/association_types_annotations_guidelines.pdf The annotated data is available in the following link: https://github.com/WinoGAViL/WinoGAViL-experiments/blob/ main/assets/model_analysis_on_different_association_types.csv We show examples of the annotated data categories in <ref type="figure" target="#fig_0">Figures 12,13,14,15, 16, 17</ref>. A screenshot from the annotation task is presented in <ref type="figure" target="#fig_7">Figure 18</ref>.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8 Multimodal Evaluation</head><p>The SWOW vision baseline dataset has four options of text-image modalities, so we evaluate all cases of models: vision-and-language, textual only and visual only.</p><p>Computer vision models when both the cue and candidates are visual we evaluate ViT <ref type="bibr" target="#b62">[63]</ref>, Swin Transformer <ref type="bibr" target="#b63">[64]</ref>, DeiT <ref type="bibr" target="#b64">[65]</ref> and ConvNeXt <ref type="bibr" target="#b65">[66]</ref>. <ref type="bibr" target="#b11">12</ref> Visual associations are more difficult than textual <ref type="table" target="#tab_1">Table 11</ref> shows results for the different modalities. The performance is the highest in the all-text version, decreases when one of the cues or candidates are images, and the worst when both are images. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>A screenshot from a spymaster screen in Amazon Mechanical Turk.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Cue in the image deer &amp; snowman looks like they stare (Figure 6b) 6% Analogy Cue can look like Association, despite being from different concept maps deer &amp; TV antenna looks like a horn (Figure 6d) 4% Visual Similarit Association is visually similar to the Cue The sponge shape is similar to a box (Figure 6a) 20%</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>:</head><label></label><figDesc>Code is licensed under the MIT license https://opensource.org/licenses/ MIT. Dataset is licensed under CC-BY 4.0 license https://creativecommons.org/ licenses/by/4.0/legalcode.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>5 .</head><label>5</label><figDesc>I liked trying to figure out what the creator was thinking Q: Are there additional reasoning skills you feel that were required from you? Spymasters: 11 https://winogavil.github.io/beat-the-ai A.5 Additional Analysis</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Visual Reasoning Skills Examples (a) A screenshot from a solver screen in Amazon Mechanical Turk. Basic payment is 0.03$. (b) A screenshot from a spymaster screen in Amazon Mechanical Turk. Basic payment is 0.07$.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :Figure 8 :Figure 9 :Figure 10 :</head><label>78910</label><figDesc>Examples of the Mechanical Turk user-interface, which referred the crowd workers to the WinoGAViL website A screenshot of the instructions given to the annotators. Spymasters fool-the-AI and solvable-by-human scores. Each point represents a spymaster. The best spymaster on the top right achieved fool-the-AI score of 62 and solvable-by-humans score of 87 on the case of 5 &amp; 6 candidates; and a fool-the-AI score of 70 and solvable-by-humans score of 90 on the case of 10 &amp; 12 candidates A screenshot from the player dashboard, aiming to increase players motivation. It contains different statistics measuring the performance in beating the AI, creating novel associations, and solving other player's associations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 11 :</head><label>11</label><figDesc>Generated cues concreteness distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 18 :</head><label>18</label><figDesc>A screenshot from the task of annotating different analogy types.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Some of the skills and observed patterns required to solve WinoGAViL associations. Each association instance may require multiple skills.</figDesc><table><row><cell>Skill</cell><cell>Observed Pattern</cell><cell>Description</cell><cell>Example</cell><cell>%</cell></row><row><cell></cell><cell>Attribute</cell><cell>Cue has attributes of Association</cell><cell>iguana has green color</cell><cell>14%</cell></row><row><cell></cell><cell></cell><cell>Cue is Association</cell><cell>miners are dirty</cell><cell></cell></row><row><cell>Non-Visual</cell><cell>Use-Of</cell><cell>Cue uses the Association Association is used in relation to Cue</cell><cell>miner uses tractor tupperware is used to store food</cell><cell>9%</cell></row><row><cell></cell><cell>General</cell><cell>Cue is a name for Association</cell><cell>ford is a name of a car</cell><cell>13%</cell></row><row><cell></cell><cell>Knowledge</cell><cell>Association is used in a relation to Cue</cell><cell>Oats improve horses performance</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell># Candidates</cell><cell cols="2">5 &amp; 6 10 &amp; 12</cell></row><row><cell># Generated Associations</cell><cell cols="2">4,482 1,500</cell></row><row><cell>% Avg. Model Score</cell><cell>50%</cell><cell>35%</cell></row><row><cell>% Avg. Human Score</cell><cell>84%</cell><cell>80%</cell></row><row><cell cols="3"># ?80% Avg. Human Score 2,714 854</cell></row></table><note>WinoGAViL collection statistics. Small dif- ferences exist between 5 and 6 candidates, and be- tween 10 and 12 candidates, so we analyze these groups together. Compared to humans, the model struggles with increased number of candidates.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Players feedback collected from the crowdworkers players (scale of 1-5)Rate for the following skills how much you found them required while performing the taskRoleVisual Reasoning General Knowledge Associative Thinking Commonsense Abstraction Divergent ThinkingRoleInterest in play and recommend it as an online game Level of enjoyment while doing the task How clear was the UI</figDesc><table><row><cell>Spymaster</cell><cell>4.4</cell><cell>3.6</cell><cell>4.5</cell><cell>3.9</cell><cell>4.3</cell><cell>4.5</cell></row><row><cell>Solver</cell><cell>4.4</cell><cell>4</cell><cell>4.7</cell><cell>4.3</cell><cell>4.1</cell><cell>4.1</cell></row><row><cell>Spymaster</cell><cell>3.8</cell><cell></cell><cell></cell><cell>3.7</cell><cell></cell><cell>4.7</cell></row><row><cell>Solver</cell><cell>4.1</cell><cell></cell><cell></cell><cell>4.4</cell><cell></cell><cell>4.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Zero-shot models performance on the SWOW vision baseline dataset and the WinoGAViL dataset.</figDesc><table><row><cell cols="4">Numbers indicates Jaccard score (0-</cell></row><row><cell cols="4">100%). Bold numbers indicate best models per-</cell></row><row><cell cols="4">formances and lowest human performance. The</cell></row><row><cell cols="4">associations collected via the game are difficult for</cell></row><row><cell>all models to solve.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>Game</cell><cell></cell><cell>SWOW</cell></row><row><cell># Candidates</cell><cell cols="2">10 &amp; 12 5 &amp; 6</cell><cell>5 &amp; 6</cell></row><row><cell>CLIP-RN50x64/14</cell><cell>38</cell><cell>50</cell><cell>70</cell></row><row><cell>CLIP-VIT-L/14</cell><cell>40</cell><cell>53</cell><cell>74</cell></row><row><cell>CLIP-VIT-B/32</cell><cell>41</cell><cell>53</cell><cell>74</cell></row><row><cell>CLIP-RN50</cell><cell>35</cell><cell>50</cell><cell>73</cell></row><row><cell>CLIP-ViL</cell><cell>15</cell><cell>47</cell><cell>66</cell></row><row><cell>ViLT</cell><cell>52</cell><cell>55</cell><cell>59</cell></row><row><cell>X-VLM</cell><cell>46</cell><cell>53</cell><cell>68</cell></row><row><cell>Humans</cell><cell>90</cell><cell>92</cell><cell>95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table><row><cell cols="2">Supervised models perfor-</cell></row><row><cell cols="2">mance. Results are mean and stan-</cell></row><row><cell cols="2">dard deviation of the Jaccard index</cell></row><row><cell cols="2">of five experiments, each time sam-</cell></row><row><cell cols="2">pling different test set. Training is</cell></row><row><cell cols="2">effective given more distractors.</cell></row><row><cell cols="2"># Candidates 10 &amp; 12 5 &amp; 6</cell></row><row><cell>Zero-Shot</cell><cell>42 ? 3 53 ? 2</cell></row><row><cell>Supervised</cell><cell>49 ? 3 52 ? 1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table><row><cell cols="4">Results for different association categories and</cell></row><row><cell cols="4">results for each category. The model (CLIP ViT-B/32)</cell></row><row><cell cols="4">is stronger when the cue is visually salient in the image</cell></row><row><cell cols="4">(a), but weaker in the other cases, especially in visually</cell></row><row><cell>non-salient associations.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4"># Items % Model % Humans</cell></row><row><cell>Visually salient</cell><cell>67</cell><cell>75</cell><cell>98</cell></row><row><cell>Visually non-salient</cell><cell>379</cell><cell>36</cell><cell>93</cell></row><row><cell>Concept related</cell><cell>426</cell><cell>65</cell><cell>92</cell></row><row><cell>Activity</cell><cell>24</cell><cell>42</cell><cell>96</cell></row><row><cell>Counting</cell><cell>25</cell><cell>36</cell><cell>97</cell></row><row><cell>Colors</cell><cell>14</cell><cell>79</cell><cell>96</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Results of textual models when using textual image captions for the candidates. ViLT performance (textual cue and visual candidates) performance appear one line prior the the last. Image-to-text might be beneficial, but still far from human performance.</figDesc><table><row><cell>Model</cell><cell>Game</cell><cell></cell><cell>SWOW</cell></row><row><cell># Candidates</cell><cell cols="2">10 &amp; 12 5 &amp; 6</cell><cell>5 &amp; 6</cell></row><row><cell>MPNet</cell><cell>39</cell><cell>52</cell><cell>72</cell></row><row><cell>MPNet QA</cell><cell>47</cell><cell>55</cell><cell>75</cell></row><row><cell>Distil RoBERTa</cell><cell>37</cell><cell>50</cell><cell>65</cell></row><row><cell>ViLT (V&amp;L)</cell><cell>52</cell><cell>55</cell><cell>59</cell></row><row><cell>Humans</cell><cell>90</cell><cell>92</cell><cell>95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Some of the observed patterns and reasoning skills required to solve WinoGAViL associations. Each association instance may require multiple skills</figDesc><table><row><cell>Skill</cell><cell>Observed Pattern</cell><cell>Description</cell><cell>Example</cell><cell>%</cell></row><row><cell></cell><cell>Kind-Of</cell><cell>Cue is a kind of Association</cell><cell>a bathtub is a shower</cell><cell>4%</cell></row><row><cell></cell><cell></cell><cell>Association and Cue are kinds of Something</cell><cell>a croissant &amp; bread are pastries</cell><cell></cell></row><row><cell></cell><cell>Attribute</cell><cell>Cue has attributes of Association</cell><cell>iguana has green color</cell><cell>14%</cell></row><row><cell></cell><cell></cell><cell>Cue is Association</cell><cell>miners are dirty</cell><cell></cell></row><row><cell></cell><cell>Use-Of</cell><cell>Cue uses the Association</cell><cell>miner uses tractor</cell><cell>9%</cell></row><row><cell></cell><cell></cell><cell>Association is used in relation to Cue</cell><cell>tupperware is used to store food</cell><cell></cell></row><row><cell></cell><cell>General Knowledge</cell><cell>Cue is a name for Association</cell><cell>ford is a name of a car</cell><cell>13%</cell></row><row><cell></cell><cell></cell><cell>Association is used in a relation to Cue</cell><cell>oats for horses increase their performance</cell><cell></cell></row><row><cell>Non-Visual</cell><cell>Word Sense Meaning</cell><cell></cell><cell>skin ? object: iguanas have scales, but it is also used to measure weight</cell><cell>3%</cell></row><row><cell></cell><cell></cell><cell>Cue has word sense meaning with Association</cell><cell>visible trail ? body part: comets have tail, but it is also an animal body part</cell><cell>3%</cell></row><row><cell></cell><cell>Locations</cell><cell>The location of a Cue is Association</cell><cell>comet is in the sky</cell><cell>5%</cell></row><row><cell></cell><cell></cell><cell>Cue and Associations are located Somewhere</cell><cell>polar bears live in an ice environment</cell><cell></cell></row><row><cell></cell><cell>Outcome</cell><cell>Cue is an outcome of Association</cell><cell>oboe creates music</cell><cell>6%</cell></row><row><cell></cell><cell></cell><cell>Association is an outcome of Cue</cell><cell>birth &amp; baby is the outcome of a pregnancy</cell><cell></cell></row><row><cell></cell><cell>Activity</cell><cell>Associations perform a Cue in the image</cell><cell>deer &amp; snowman looks like they stare</cell><cell></cell></row><row><cell>Visual</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">WinoGAViL Workers Statistics</cell></row><row><cell></cell><cell cols="2">Solvers Creators</cell></row><row><cell># Workers</cell><cell>41</cell><cell>18</cell></row><row><cell># Avg. Annotations</cell><cell>567</cell><cell>332</cell></row><row><cell cols="2">% Avg. Performance (5-6 candidates split) 85.1</cell><cell>fool-the-AI: 50 solvable-by-humans: 83</cell></row><row><cell>Avg. Age</cell><cell cols="2">41 ?10 43 ?9</cell></row><row><cell># High School Education</cell><cell>13</cell><cell>6</cell></row><row><cell># Bachelor Education</cell><cell>19</cell><cell>11</cell></row><row><cell># Master Education</cell><cell>8</cell><cell>1</cell></row><row><cell>% Native English Speakers</cell><cell>98</cell><cell>95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>WinoGAViL dataset Human and model (CLIP RN50) for different candidates and distractors</figDesc><table><row><cell cols="5"># Candidates # Associations (k) # Items % Human Performance % Model Performance</cell></row><row><cell>5</cell><cell>2 3</cell><cell>1,091 234</cell><cell>90 92</cell><cell>52 57</cell></row><row><cell></cell><cell>2</cell><cell>1,</cell><cell></cell><cell></cell></row><row><cell>6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 :</head><label>11</label><figDesc>Results on the multi-modal versions of SWOW baseline dataset. Visual associations are more difficult than textual</figDesc><table><row><cell>Model type</cell><cell>Model</cell><cell>Key</cell><cell cols="2">Modalities Candidates</cell><cell>Jaccard Index</cell></row><row><cell></cell><cell>CLIP-ViT-L/14</cell><cell>Text Image</cell><cell></cell><cell>Text Image Text Image</cell><cell>86 74 79 65</cell></row><row><cell>Vision and Language</cell><cell>ViLT</cell><cell>Text Image</cell><cell></cell><cell>Image Text</cell><cell>58 59</cell></row><row><cell></cell><cell>LiT</cell><cell>Text Image</cell><cell></cell><cell>Image Text</cell><cell>37 40</cell></row><row><cell></cell><cell>X-VLM</cell><cell>Text Image</cell><cell></cell><cell>Image Text</cell><cell>68 70</cell></row><row><cell></cell><cell>ViT</cell><cell></cell><cell></cell><cell></cell><cell>61</cell></row><row><cell>Vision</cell><cell>Swin DeiT</cell><cell></cell><cell>Image</cell><cell></cell><cell>59 53</cell></row><row><cell></cell><cell>ConvNeXt</cell><cell></cell><cell></cell><cell></cell><cell>56</cell></row><row><cell></cell><cell>MPNet</cell><cell></cell><cell></cell><cell></cell><cell>88</cell></row><row><cell>Text Transformers</cell><cell>MPNet QA Distil RoBERTa</cell><cell></cell><cell>Text</cell><cell></cell><cell>91 77</cell></row><row><cell>Text Word2Vec</cell><cell>Spacy</cell><cell></cell><cell>Text</cell><cell></cell><cell>91</cell></row><row><cell></cell><cell>CLIP-ViT-L/14</cell><cell></cell><cell></cell><cell></cell><cell>87</cell></row><row><cell></cell><cell>MPNet MPNet QA</cell><cell></cell><cell>Text</cell><cell></cell><cell>88 90</cell></row><row><cell></cell><cell>Distil RoBERTa</cell><cell></cell><cell></cell><cell></cell><cell>73</cell></row><row><cell></cell><cell>CLIP-ViT-L/14</cell><cell></cell><cell></cell><cell></cell><cell>55</cell></row><row><cell></cell><cell>MPNet MPNet QA</cell><cell>Text</cell><cell cols="2">Synthesized Text</cell><cell>72 76</cell></row><row><cell>Text</cell><cell>Distil RoBERTa</cell><cell></cell><cell></cell><cell></cell><cell>66</cell></row><row><cell></cell><cell>CLIP-ViT-L/14</cell><cell></cell><cell></cell><cell></cell><cell>81</cell></row><row><cell></cell><cell>MPNet MPNet QA</cell><cell cols="2">Synthesized Text</cell><cell>Text</cell><cell>77 78</cell></row><row><cell></cell><cell>Distil RoBERTa</cell><cell></cell><cell></cell><cell></cell><cell>73</cell></row><row><cell></cell><cell>CLIP-ViT-L/14</cell><cell></cell><cell></cell><cell></cell><cell>61</cell></row><row><cell></cell><cell>MPNet MPNet QA</cell><cell cols="3">Synthesized Text</cell><cell>64 64</cell></row><row><cell></cell><cell>Distil RoBERTa</cell><cell></cell><cell></cell><cell></cell><cell>67</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://en.wikipedia.org/wiki/Jaccard_index.4  We removed words that are potentially offensive using https://pypi.org/project/ profanity-filter/.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We note associations can be subjective and culture-dependent. In Section 3 we show high agreement between our annotators.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">licensed under a Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported License. 7 https://smallworldofwords.org/en/project/explore<ref type="bibr" target="#b7">8</ref> We ran the zero-shot experiments on a MacBook Pro laptop (CPU) in &lt;6 hours.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">Code for reproducing these experiments is available in this link. We ran the supervised experiments with a single NVIDIA RTX2080 GPU, all experiments ran in &lt;24 hours.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">https://www.sbert.net/docs/pretrained_models.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">The exact versions we took are the largest pretrained versions available in timm library: ViT Large patch32-384, Swin Large patch4 window7-224, DeiT Base patch16 384, ConvNeXt Large.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Moran Mizrahi for a feedback regarding the players survey. We would also like to thank Jaemin Cho, Tom Hope, Yonatan Belinkov, Inbal Magar and Aviv Shamsian. This work was supported in part by the Center for Interdisciplinary Data Science Research at the Hebrew University of Jerusalem, and a research grant no. 2088 from the Israeli Ministry of Science and Technology.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Visual and affective grounding in language and mind</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><surname>Simon De Deyne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Collell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Perfors</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Visual and affective multimodal models of word meaning in language and mind</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><forename type="middle">J</forename><surname>Simon De Deyne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Collell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Perfors</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Explicit retrieval of visual and non-visual properties of concrete entities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonietta</forename><forename type="middle">Gabriella</forename><surname>Liuzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Peeters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>De Deyne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerrit</forename><surname>Storms</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Vandenberghe</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">Organization for Human Brain Mapping</title>
		<imprint>
			<publisher>OHBM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Brain connectivity-based prediction of real-life creativity is mediated by semantic memory structure. bioRxiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcela</forename><surname>Ovando-Tellez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yoed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Kenett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Benedek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Belo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theophile</forename><surname>Beranger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuelle</forename><surname>Bieth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Volle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Forward flow and creative thought: Assessing associative cognition and its role in divergent thinking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beaty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><forename type="middle">S</forename><surname>Daniel C Zeitlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoed N</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kenett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Thinking Skills and Creativity</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">100859</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unveiling the nature of interaction between semantics and phonology in lexical access based on multilayer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orr</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yoed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orr</forename><surname>Kenett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nichol</forename><surname>Oxenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De Deyne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shlomo</forename><surname>Vitevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Havlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Large-scale network representations of semantics in the mental lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simon De Deyne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yoed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kenett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Anaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Faust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Navarro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychology of Aesthetics Creativity and the Arts</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Aging Lexicon Consortium, et al. New perspectives on the aging lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Dirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De Deyne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="686" to="698" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">From recognition to cognition: Visual commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00688</idno>
		<ptr target="http://openaccess.thecvf.com/content_CVPR_2019/html/Zellers_From_Recognition_to_Cognition_Visual_Commonsense_Reasoning_CVPR_2019_paper.html.1" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-16" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Winoground: Probing vision and language models for visio-linguistic compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tristan</forename><surname>Thrush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Bartolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Candace</forename><surname>Ross</surname></persName>
		</author>
		<idno>abs/2204.03162</idno>
		<ptr target="https://arxiv.org/abs/2204.03162.1" />
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Do androids laugh at electric sheep?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><surname>Marasovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jena</forename><forename type="middle">D</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Mankoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.06293</idno>
	</analytic>
	<monogr>
		<title level="m">humor&quot; understanding&quot; benchmarks from the new yorker caption contest</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Commonsenseqa 2.0: Exposing the limits of ai through gamification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ori</forename><surname>Yoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berant</surname></persName>
		</author>
		<idno>abs/2201.05320</idno>
		<ptr target="https://arxiv.org/abs/2201.05320.1" />
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The winograd schema challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hector</forename><surname>Levesque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernest</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leora</forename><surname>Morgenstern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirteenth international conference on the principles of knowledge representation and reasoning</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v139/radford21a.html.2" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<editor>Marina Meila and Tong Zhang</editor>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<date type="published" when="2021-07" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The &quot;small world of words&quot; english word association norms for over 12,000 cue words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><forename type="middle">J</forename><surname>Simon De Deyne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Perfors</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gert</forename><surname>Brysbaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Storms</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior research methods</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Character region awareness for text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngmin</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bado</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwalsuk</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00959</idno>
		<ptr target="http://openaccess.thecvf.com/content_CVPR_2019/html/Baek_Character_Region_Awareness_for_Text_Detection_CVPR_2019_paper.html.4" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-16" />
			<biblScope unit="page" from="9365" to="9374" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SWOW-8500: Word association task for intrinsic evaluation of word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avijit</forename><surname>Thawani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biplav</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anil</forename><surname>Singh</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-2006</idno>
		<ptr target="https://aclanthology.org/W19-2006" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for NLP</title>
		<meeting>the 3rd Workshop on Evaluating Vector Space Representations for NLP<address><addrLine>Minneapolis, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">How much can clip benefit vision-and-language tasks? ArXiv preprint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liunian</forename><forename type="middle">Harold</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<idno>abs/2107.06383</idno>
		<ptr target="https://arxiv.org/abs/2107.06383.7" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Vilt: Vision-and-language transformer without convolution or region supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bokyung</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ildoo</forename><surname>Kim</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v139/kim21k.html.7" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<editor>Marina Meila and Tong Zhang</editor>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<date type="published" when="2021-07" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="5583" to="5594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Multi-grained vision language pre-training: Aligning texts with visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinsong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2111.08276.7" />
		<imprint/>
	</monogr>
	<note>ArXiv preprint, abs/2111.08276, 2021</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">C</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lazebnik</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.303</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2015.303.7" />
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision, ICCV 2015</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2641" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gender bias in coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Naradowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2002</idno>
		<ptr target="https://aclanthology.org/N18-2002" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Wino-X: Multilingual Winograd schemas for commonsense reasoning and coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Emelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.670</idno>
		<ptr target="https://aclanthology.org/2021.emnlp-main.670" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Online and Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Abstraction and analogy-making in artificial intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of the New York Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">1505</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="101" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6980.7" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<idno>abs/2202.03052</idno>
		<ptr target="https://arxiv.org/abs/2202.03052.9" />
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sentence-BERT: Sentence embeddings using Siamese BERT-networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1410</idno>
		<ptr target="https://aclanthology.org/D19-1410.9" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3982" to="3992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mpnet: Masked and permuted pre-training for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/c3a690be93aa602ee2dc0ccab5b7b67e-Abstract.html.9" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<editor>Hugo Larochelle, Marc&apos;Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Comparing models of associative meaning: An empirical investigation of reference in simple language games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Judy Hanwen Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjarke</forename><surname>Hofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Felbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levy</surname></persName>
		</author>
		<idno>doi: 10.18653/ v1/K18-1029</idno>
		<ptr target="https://aclanthology.org/K18-1029.9" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Conference on Computational Natural Language Learning</title>
		<meeting>the 22nd Conference on Computational Natural Language Learning<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="292" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cooperation and codenames: Understanding natural language processing via codenames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Ruzmaykin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Summerville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment</title>
		<meeting>the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="160" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Computing semantic relatedness using wikipediabased explicit semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaul</forename><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJcAI</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1606" to="1611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Wikirelate! computing semantic relatedness using wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><forename type="middle">Paolo</forename><surname>Ponzetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1419" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Evaluating WordNet-based measures of lexical semantic relatedness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Budanitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
		<idno type="DOI">10.1162/coli.2006.32.1.13</idno>
		<ptr target="https://aclanthology.org/J06-1003.9" />
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13" to="47" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semantic relatedness using salient semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samer</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<ptr target="http://www.aaai.org/ocs/index.php/AAAI/AAAI11/paper/view/3616.9" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2011</title>
		<editor>Wolfram Burgard and Dan Roth</editor>
		<meeting>the Twenty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2011<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The curious case of commonsense intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Academy of Arts &amp; Sciences</title>
		<imprint>
			<biblScope unit="page" from="139" to="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">ExplaGraphs: An explanation graph generation task for structured commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swarnadeep</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prateek</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno>doi: 10. 18653/v1/2021.emnlp-main.609</idno>
		<ptr target="https://aclanthology.org/2021.emnlp-main.609.9" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Online and Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7716" to="7740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">SWAG: A large-scale adversarial dataset for grounded commonsense inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1009</idno>
		<ptr target="https://aclanthology.org/D18-1009.9" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">HellaSwag: Can a machine really finish your sentence?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1472</idno>
		<ptr target="https://aclanthology.org/P19-1472.9" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">ATOMIC: an atlas of machine commonsense for if-then reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Ronan Le Bras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Allaway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Roof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33013027</idno>
		<ptr target="https://doi.org/10.1609/aaai.v33i01.33013027.9" />
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019-02-01" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="3027" to="3035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Lebras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<ptr target="https://aaai.org/ojs/index.php/AAAI/article/view/6239.9" />
	</analytic>
	<monogr>
		<title level="m">The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="7432" to="7439" />
		</imprint>
	</monogr>
	<note>The Thirty-Fourth AAAI Conference on Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Do neural language representations learn physical commonsense? ArXiv preprint, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1908.02899.9" />
		<imprint>
			<date type="published" when="1908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Video2Commonsense: Generating commonsense descriptions to enrich video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Gokhale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratyay</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitta</forename><surname>Baral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yezhou</forename><surname>Yang</surname></persName>
		</author>
		<idno>doi: 10. 18653/v1/2020.emnlp-main.61</idno>
		<ptr target="https://aclanthology.org/2020.emnlp-main.61" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="840" to="860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning common sense through visual abstraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmay</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.292</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2015.292.9" />
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="2542" to="2550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">What is more likely to happen next? video-and-language future event prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.706</idno>
		<ptr target="https://aclanthology.org/2020.emnlp-main.706.9" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8769" to="8784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Winogrande: An adversarial winograd schema challenge at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
		<ptr target="https://aaai.org/ojs/index.php/AAAI/article/view/6399.9" />
	</analytic>
	<monogr>
		<title level="m">The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="8732" to="8740" />
		</imprint>
	</monogr>
	<note>The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">A review of winograd schema challenge datasets and approaches. ArXiv preprint, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vid</forename><surname>Kocijan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernest</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leora</forename><surname>Morgenstern</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2004.13831.9" />
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Adversarial filters of dataset biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Ronan Le Bras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v119/bras20a.html.10" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="1078" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning the difference that makes A difference with counterfactually-augmented data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><forename type="middle">Chase</forename><surname>Lipton</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Sklgs0NFvr.10" />
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Adversarial NLI: A new benchmark for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.441</idno>
		<ptr target="https://aclanthology.org/2020.acl-main.441.10" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4885" to="4901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Adversarial vqa: A new benchmark for evaluating the robustness of vqa models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2042" to="2051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Human-adversarial visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasha</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vedanuj</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Magana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tristan</forename><surname>Thrush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Galuba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Quizz: targeted crowdsourcing with a billion (potential) users</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Panagiotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Ipeirotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gabrilovich</surname></persName>
		</author>
		<idno type="DOI">10.1145/2566486.2567988</idno>
		<ptr target="https://doi.org/10.1145/2566486.2567988.10" />
	</analytic>
	<monogr>
		<title level="m">23rd International World Wide Web Conference, WWW &apos;14</title>
		<editor>Chin-Wan Chung, Andrei Z. Broder, Kyuseok Shim, and Torsten Suel</editor>
		<meeting><address><addrLine>Seoul, Republic of Korea</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="143" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Labeling images with a computer game</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Von Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Dabbish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI conference on Human factors in computing systems</title>
		<meeting>the SIGCHI conference on Human factors in computing systems</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="319" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Fool me twice: Entailment from Wikipedia gamification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Eisenschlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jannis</forename><surname>Bulian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>B?rschinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.32</idno>
		<ptr target="https://aclanthology.org/2021.naacl-main.32.10" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="352" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Game on! experiential learning with tabletop games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Experiential Library</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="103" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Cultural and Academic Experiences of Black Male Graduates from a Historically Black College and University</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charea Lacherie Bustamante</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
		<respStmt>
			<orgName>Northcentral University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Beat the AI: Investigating adversarial human annotation for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Bartolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alastair</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00338</idno>
		<ptr target="https://aclanthology.org/2020.tacl-1.43.10" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="662" to="678" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Beat the machine: Challenging humans to find a predictive model&apos;s &quot;unknown unknowns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Attenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panos</forename><surname>Ipeirotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Foster</forename><surname>Provost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Data and Information Quality (JDIQ)</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Evaluating visual conversational agents via cooperative human-ai games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithvijit</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deshraj</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viraj</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifth AAAI Conference on Human Computation and Crowdsourcing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Dynabench: Rethinking benchmarking in NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Bartolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atticus</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertie</forename><surname>Vidgen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grusha</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratik</forename><surname>Ringshia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tristan</forename><surname>Thrush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeerak</forename><surname>Waseem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.324</idno>
		<ptr target="https://aclanthology.org/2021.naacl-main.324.10" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4110" to="4124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brysbaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><forename type="middle">Beth</forename><surname>Warriner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Kuperman</surname></persName>
		</author>
		<title level="m">Concreteness ratings for 40 thousand generally known english word lemmas. Behavior research methods</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=YicbFdNTTy.27" />
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v139/touvron21a.html.27" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<editor>Marina Meila and Tong Zhang</editor>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<date type="published" when="2021-07-24" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2201.03545.27" />
		<imprint/>
	</monogr>
	<note>A convnet for the 2020s. ArXiv preprint, abs/2201.03545, 2022</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">You had to go more for abstract, metaphorical, or otherwise really &apos;out there&apos; associations to get past it. Solvers: 1. This is probably covered under &quot;general knowledge&quot; but I found that a lot of answers required a basic understanding of Pop Culture references</title>
		<imprint/>
	</monogr>
	<note>I find things like common sense and general knowledge mattered less for creating than when solving, because the AI was very good at cracking anything using general knowledge</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luck</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>which is separate from general knowledge</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Q: Did seeing the model&apos;s predictions affect you in any way? If so, how? (For spymasters only) 1. I was impressed at some of the AI ideas</title>
		<imprint/>
	</monogr>
	<note>admired the programmers and learning</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Yes, it helped but it was also kind of discouraging as it seemed like the AI was able to guess nearly all of my associations, which made me feel like I had even more limited options</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">I used the model&apos;s guesses to make my associations better. I went after associations that the model frequently got wrong</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Yes, it either increased my confidence or made me think harder about cues</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Sometimes the model was very off especially in detecting emotions</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Q: Have you been affected by the performance bonus? In what way? (For spymasters only) 1. It was nice to have a little extra pay. It helped to keep my motivation up when it was hard to come up with connections</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">The bonus did make me sometimes give up on making a &quot;good&quot; cue and make a &quot;performance&quot; cue. Performance cue being a cue that utilizes a quirk of the AI that I know and almost guarantee that it will get wrong and will generally be easy for humans to guess. But it&apos;s not a creative or interesting cue</title>
		<imprint/>
	</monogr>
	<note>Notable words are human, male and female or sometimes features like eyes, noses, ears, hands, etc</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Yes, it made me try harder to fool the AI</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">The performance bonus motivated me to try harder to beat the AI, so I could justify the time investment</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Not really, it wasn&apos;t enough of a bonus for me to be motivated to do more Q: Anything else that you want to say?</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">It was fun and I hope the best for this project! If you make an online game I would 100% suggest a leaderboard for &quot;creators&quot; for people to create the cues. Introduce categories so people can focus on specific things. If you&apos;re also so inclined, build something to work with Twitch.tv so streamers can play with their audience</title>
	</analytic>
	<monogr>
		<title level="m">similar tasks for you in the near future! 2</title>
		<imprint/>
	</monogr>
	<note>There are some pictionary like games that do this where the streamer draws and the people in chat try to guess</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">This would be a super interesting online if you include things like leaderboards for creators, categories, more images (although be sure to get rights to images!) and letting people rate the cues. I can definitely see game like this being popular with streamers on Twitch</title>
		<imprint/>
	</monogr>
	<note>tv to play with their audience (streamer https://twitch.tv/itshafu is pretty known to like games like these and sometimes streams her playing code names with other streamers</note>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">This was something different to do and was fun, thank you for the opportunity. I also really appreciated how you communicated with us!</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">I liked creating, more than solving, even though I think I was a better solver than creator</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
