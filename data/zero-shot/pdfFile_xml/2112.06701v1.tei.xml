<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Anchor Retouching via Model Interaction for Robust Object Detection in Aerial Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Qixiang</forename><surname>Geng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongqi</forename><surname>Wei</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><forename type="middle">A</forename><surname>Vorontsov</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Mingqiang</roleName><forename type="first">Ekaterina</forename><forename type="middle">L</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Wei</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Zhou</surname></persName>
						</author>
						<title level="a" type="main">Anchor Retouching via Model Interaction for Robust Object Detection in Aerial Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Object detection</term>
					<term>dynamic enhancement anchor</term>
					<term>aerial observation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Object detection has made tremendous strides in computer vision. Small object detection with appearance degradation is a prominent challenge, especially for aerial observations. To collect sufficient positive/negative samples for heuristic training, most object detectors preset region anchors in order to calculate Intersection-over-Union (IoU) against the groundtruthed data. In this case, small objects are frequently abandoned or mislabeled. In this paper, we present an effective Dynamic Enhancement Anchor (DEA) network to construct a novel training sample generator. Different from the other state-ofthe-art techniques, the proposed network leverages a sample discriminator to realize interactive sample screening between an anchor-based unit and an anchor-free unit to generate eligible samples. Besides, multi-task joint training with a conservative anchor-based inference scheme enhances the performance of the proposed model while reducing computational complexity. The proposed scheme supports both oriented and horizontal object detection tasks. Extensive experiments on two challenging aerial benchmarks (i.e., DOTA and HRSC2016) indicate that our method achieves state-of-the-art performance in accuracy with moderate inference speed and computational overhead for training. On DOTA, our DEA-Net which integrated with the baseline of RoI-Transformer surpasses the advanced method by 0.40% mean-Average-Precision (mAP) for oriented object detection with a weaker backbone network (ResNet-101 vs ResNet-152) and 3.08% mean-Average-Precision (mAP) for horizontal object detection with the same backbone. Besides, our DEA-Net which integrated with the baseline of ReDet achieves the state-ofthe-art performance by 80.37%. On HRSC2016, it surpasses the previous best model by 1.1% using only 3 horizontal anchors. The source code and the training set are made publicly available at: https://github.com/QxGeng/DEA-Net Index Terms-Object detection, dynamic enhancement anchor, aerial observation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>O BJECT detection is one of the fundamental and challenging problems in computer vision.   <ref type="bibr" target="#b6">[7]</ref>) and regression bounding-boxes (b) and (d) in an anchor-free method (e.g. FCOS <ref type="bibr" target="#b9">[10]</ref>). Anchor-free regression boundingboxes (yellow boxes) have better consistency in small object detection than the anchor proposals (red boxes), but for large objects with large aspect ratios, the anchor proposals have better consistency. Green boxes are the ground truth.</p><p>often captured from horizontal perspectives, aerial images are typically taken from a bird's-eye view at a high altitude, suggesting that objects in aerial images usually are of a small size and diverse orientations with complex background <ref type="bibr" target="#b0">[1]</ref>. A large number of detectors <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> have been designed for aerial observations, most of which are based on a twostage detector (e.g., Fast R-CNN <ref type="bibr" target="#b5">[6]</ref> and Faster R-CNN <ref type="bibr" target="#b6">[7]</ref>) or a one-stage detector (e.g., RetinaNet <ref type="bibr" target="#b7">[8]</ref> and YOLO <ref type="bibr" target="#b8">[9]</ref>). Region anchors are designed as the regression references and the classification candidates to predict the proposals in two-stage detectors or final bounding boxes in one-stage detectors. Most of the anchor-based detectors utilize a uniform anchoring scheme, and then positive and negative samples are selected through Intersection-over-Union (IoU) with groundtruth. For example, the anchor boxes with IoU &gt; 0.5 are treated as positive samples and IoU &lt; 0.3 as negative samples <ref type="bibr" target="#b6">[7]</ref>. In practice, such a strategy may cause two main problems: (i) Anchor quantization errors and noisy training samples</p><p>We take Faster-RCNN <ref type="bibr" target="#b6">[7]</ref> as an example. If the base anchor arXiv:2112.06701v1 [cs.CV] 13 Dec 2021 size is set to 32 and the IoU threshold is set to 0.5, objects with area &lt; 32 2 ? 0.5 (512 pixels) will be excluded from the positive proposals. As shown in <ref type="figure" target="#fig_0">Figure 1</ref> (a), the anchor box (red) and the ground truth (green) have a large quantization discrepancy. This discrepancy will lead to much confusion for both box localization and classification. On the other hand, empirical evidence shows that objects with an area &lt; 512 pixels occupy approximately 30% of an aerial image of DOTA <ref type="bibr" target="#b0">[1]</ref>, where inaccurate anchor-boxes or misclassification of the samples lead to unstable convergence of the model.</p><p>(ii) Mismatch between the pyramid levels and the samples This is based on the consensus that upper feature maps have more semantic information suitable for detecting big instances whereas lower feature maps have more fine-grained details suitable for detecting small instances. Integrated within a feature pyramid, large anchor proposals are typically associated with upper feature maps, and small anchor proposals are associated with lower feature maps. Inaccurate bounding boxes with large background areas would cause a mismatch between the feature pyramid levels and the training samples, largely affecting the model training. In other words, the selected feature level to train each sample may not be correct.</p><p>To deal with these issues, image feature pyramids with more levels can be used to better detect small objects. Another common solution is to enlarge the quantity of the anchors with diverse sizes and aspect ratios. These two solutions have evident drawbacks -both of them lead to significant computational overhead, especially when processing largescale aerial images or training the network with a heavy backbone.</p><p>As shown in <ref type="figure" target="#fig_0">Figure 1</ref> (b), the regression bounding-boxes in an anchor-free detector (e.g. FCOS <ref type="bibr" target="#b9">[10]</ref>) can be potentially leveraged as positive region proposals because they are free from anchor quantization errors. On the other hand, compared with the anchor-based detectors, the anchor-free detectors usually fail to generate an accurate bounding box when the objects are of a large size and an extreme aspect ratio <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b9">[10]</ref>, just like the example shown in <ref type="figure" target="#fig_0">Figure 1 (d)</ref>. Most anchor-based detector (including the baseline Faster R-CNN) regresses from the anchor box with four offsets between the anchor box and the object box, while FCOS regresses from one point with four distances to the bound of the object. It means that for a positive sample, the regression's starting status of Faster R-CNN is a box while FCOS is a point. The box itself contains prior of the shape, and the regression is only the two small offset of X and Y directions. In contrast, FCOS needs to independently return the offsets of +/-X and Y , four directions, from a start point, without any shape prior. The regression error in any direction would greatly affect the shape of the box. Especially for large objects with a large aspect ratio. This observation has been reported in <ref type="bibr" target="#b12">[13]</ref>.</p><p>For the anchor-based approaches, the anchor quantization errors can be ignored for large objects. The anchor boxes are designed to discretize all possible instance boxes into a finite number of boxes with predefined locations, scales, and aspect ratios. We need more anchors with a smaller size and denser layouts or more angles in arbitrary-oriented detection to cover small objects, which may lead to extensive computation cost and imbalanced problems of positive and negative samples. Achieving spatial alignment with small ground-truth objects is challenging and prone to the miss of the corresponding positive anchors based on this strategy. Inspired by the above observations, in this paper, we propose an effective Dynamic Enhancement Anchor (DEA) network to enhance the learning of small objects efficiently. The overall architecture is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The DEA head serves each level of the feature pyramid consisting of an anchor-based module, an anchor-free module, and a sample discriminator. The sample discriminator merges the complementary anchorbased and the anchor-free proposals and generates representative and informative samples with accurate locations and sizes, while avoiding positive/negative confusion. Besides, multitask joint training with a conservative anchor-based inference scheme enhances the performance of the model while avoiding complexity augmentation. We conduct extensive experiments on both oriented and horizontal object detection tasks. Experiments on the aerial image benchmarks DOTA <ref type="bibr" target="#b0">[1]</ref> and HRSC2016 <ref type="bibr" target="#b13">[14]</ref> show that our proposed DEA-Net makes substantial improvement, compared to the baseline methods, and achieves state-of-the-art performance in accuracy (i.e., 80.37% mean-Average-Precision (mAP) (+0.14%) and 90.56% mean-Average-Precision (mAP) (+1.10%)) for oriented object detection tasks. Besides, experiments on DOTA <ref type="bibr" target="#b0">[1]</ref> for horizontal object detection achieves state-of-the-art performance in accuracy with 78.43% mean-Average-Precision (mAP) (+3.08%). By combining the anchor-based and anchor-free branch efficiently, our method maintains a fair inference speed and training computational overhead.</p><p>To our knowledge, this is the first time to simultaneously consider the impact of both the object's scale and aspect ratio, and then distinguish and process them separately for training. In summary, our main contributions consist of: (1) An effective sample generator based on DEA head to enhance the performance of detecting small objects by combining the advantages of anchor-base and anchor-free models. (2) A novel and robust DEA-Net, which can achieve the start-of-the-art oriented and horizontal object detection performance in aerial images.</p><p>The remainder of this paper is organized as follows. In Section II, we discuss the related work. In Section III, we describe the proposed method in detail. The experimental results are presented and discussed in Section IV, and the conclusions and future work are given in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Anchor-based and anchor-free models</head><p>The current mainstream detectors can be divided into two categories: (1) Anchor-based methods <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> and (2) anchor-free methods <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b9">[10]</ref>. In anchor-based methods, the network is trained to regress the offsets between the anchors and ground truth bounding boxes. However, these methods take advantages of the task-oriented settings of anchors, leading to complex parameter tuning. Moreover, since the scales and aspect ratios of anchors are fixed, it has the difficulty to handle the objects with large shape variations, especially for small objects. Anchor-free methods directly regress the bounding box without using preset anchors. They usually have a streamlined network structure due to discarding of the dense anchors. However, they also meet difficulties in learning large variations of the bounding boxes without prior knowledge. DETR <ref type="bibr" target="#b14">[15]</ref> utilizes self-attention to build a novel detection architecture, whose detection precision can compete against those of the two-stage object detectors, but it has the weakness in detecting small objects with high computational overheads in the published literature. Due to the dilemma of the above methods, an emerging line of work attempts to design a detector by combining anchor-based and anchor-free methods. GA-RPN <ref type="bibr" target="#b15">[16]</ref> constructs a region proposal network in an anchor-free manner to predict the proposals for Faster R-CNN. FSAF <ref type="bibr" target="#b16">[17]</ref> attaches an anchorfree module at each feature pyramid level to select appropriate features of each object for RetinaNet. SFace <ref type="bibr" target="#b17">[18]</ref> attaches an anchor-free module to an anchor-based detector and combines the outputs of two modules to improve the performance of the detector. Different from the other state of the arts such as <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> and <ref type="bibr" target="#b17">[18]</ref>, we focus on the collaboration of anchor-based and anchor-free methods from the perspective of sample discrimination. It makes interactive sample screening invulnerable to the diversity of the scale distributions B. Improved anchor-based detection models</p><p>The recent improvement of the anchor selection strategies mainly focuses on two aspects: (1) Weight the predicted anchor boxes to distinguish the potential importance and quality differences. MetaAnchor <ref type="bibr" target="#b18">[19]</ref> and soft anchor-point object detection (SAPD) <ref type="bibr" target="#b19">[20]</ref> belong to this type. MetaAnchor directly weights the generated anchors, while SAPD leverages both soft-weighted anchor points and soft-selected pyramid levels. (2) Propose a refining anchor box assignment strategy. For example, Dynamic Anchor Feature Selection (DAFS) <ref type="bibr" target="#b20">[21]</ref> uses an Anchor Refinement Module (ARM) to adjust the locations and sizes of anchors, and filter out negative anchors and then select new pixels in a feature map for each refined anchor. In <ref type="bibr" target="#b3">[4]</ref>, the authors define the dynamic anchor with a matching degree to evaluate both spatial and feature alignment for anchor assignment. Nevertheless, all the above methods ignore the influence of the object's scale and aspect ratio on anchor assignment. In remote sensing scenarios, for example, we observe that small objects are frequently abandoned or mislabeled due to the predefined anchor sampling interval and the Intersection-over-Union (IoU) rule, which could potentially destroy the original sample distribution in the feature space. On the other hand, anchor-free based detectors often fail to generate an accurate bounding box for large objects with large aspect ratio. Different from <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b3">[4]</ref>, the significant differences of our method are (1) consider the impact of both the object's scale and aspect ratio simultaneously, and (2) distinguish between different scales and aspect ratios, and then use appropriate strategies to process them separately. The anchor-free module is utilized to generate more positive samples of small objects which are ignored in the anchor-based module. For some objects of a large size and extreme aspect ratios, we preserve the anchors in the anchor-based module which have higher IoUs as the positive samples. Compared with the existing methods (weighting the anchor <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref> or refinement assignment strategy <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b3">[4]</ref>), our idea has also been experimentally proven to be effective. In particular, it is more suitable for object detection in remote sensing images, because in remote sensing images, small-sized (such as vehicles) and large-sized objects with extreme aspect ratios (such as ports, bridges) are very common. The work ATSS <ref type="bibr" target="#b21">[22]</ref> is a comparative analysis of the Retinanet and FCOS, which proposes a general training strategy to serve them separately so this method naturally does not increase any overhead. In contrast, our method involves model design, a training scheme (relies on a sample discriminator for interactive sample screening and with multi-task joint training), and an inference scheme (a conservative anchor-based scheme to freezing the anchor-free branch to suppress computational complexity). Such a comprehensive scheme improves the performance in remote sensing scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Object detection in aerial images</head><p>Object detection in aerial images often faces a large number of small objects with arbitrary orientations in complex environments. Detecting objects with oriented bounding boxes is a non-trivial extension of horizontal object detection, which are mostly built on anchor-based detectors <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b1">[2]</ref>. R 3 Det <ref type="bibr" target="#b25">[26]</ref> adopts cascade regression to refine the predicted boxes. DCL <ref type="bibr" target="#b4">[5]</ref> utilizes a densely coded label encoding mechanism for angle classification. SCRDet <ref type="bibr" target="#b2">[3]</ref> improves the performance of small objects by reducing the anchor strides to preset smaller and more anchors, which incurs extensive computational costs. DAL <ref type="bibr" target="#b3">[4]</ref> utilizes a comprehensive scheme for spatial alignment, feature alignment ability, and regression uncertainty for label assignment. RoI Transformer <ref type="bibr" target="#b1">[2]</ref> applies spatial transformations on RoIs and learn the transformation parameters under the supervision of oriented bounding box (OBB) annotations, which is with lightweight and can be easily embedded into detectors for oriented object detection. In our work, our method is based on the RoI transformer to deal with OBB, we constrain random discarding and positive/negative confusion of small objects and produce qualified training samples with accurate locations and scopes without introducing complicated modules. Our experiments also confirm that it is unnecessary to preset a large number of specially designed anchors with large computational overheads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE PROPOSED METHOD</head><p>In this section, we introduce the technical details of our proposed Dynamic Enhancement Anchor Network (DEA-Net) and instantiate our Dynamic Enhancement Anchor (DEA) module by showing how to apply the scheme to the object detectors with a feature pyramid for object detection in aerial images. Specifically, we first introduce the details of our proposed DEA module, and then introduce the sample discriminator which facilitates the learning of small objects. Then, we show the details of our overall network architecture. Finally, we show how to join training with inference of our DEA-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dynamic Enhancement Anchor</head><p>In the literature, anchor-based methods find it difficult to fully learn small objects by selecting positive and negative samples for training through the examination of Intersectionover-Union (IoU) overlap. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, in the anchorbased module, if the IoU between the preset anchors and the ground-truth boxes of small objects is lower than the threshold of the positive samples, the samples are treated as discarded samples (i.e., ?1 as sample label) or negative samples (i.e., 0 as sample label). The training of anchor-based detectors for small objects is not sufficient because of the lack of positive samples, severely affecting the detection performance.</p><p>As aforementioned, the regression bounding-boxes in the anchor-free detector usually have higher IoU for the small objects than the anchor-based detector. However, for some objects of a large size and extreme aspect ratios, anchor-free detectors may have poor performance. Therefore, instead of utilizing the anchor-free method to replace the anchor-based method to generate samples for training, we combine their advantages to deal with positive sample selection of different scales.</p><p>We construct an anchor-free sample generation module that shares the feature pyramid with the anchor-based module and integrate it with an anchor-based detector. Via an interactive sample screening procedure in the sample discriminator, the anchor-free module is utilized to generate more positive samples of small objects which are ignored in the anchor-based module. For some objects of a large size and extreme aspect ratios, we preserve the anchors in the anchor-based module which have higher IoUs as the positive samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Interactive Sample Screening</head><p>Current studies <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b17">[18]</ref> have reported that the designed anchor boxes are the key to successful anchor-based detectors, and the detection performance is sensitive to the size, aspect ratio and the number of the anchor boxes. Therefore, the anchors in these anchor-based detectors must be carefully tuned for each specific task on different datasets. For example, to deal with the challenge of small object detection, one needs to design smaller anchors beforehand and densely locate them on the input image. This handling leads to extensive computational costs and the imbalanced problem. Therefore, our proposed dynamic enhancement method aims to use the preset number of the preset anchors to improve the detection performance of small objects with less computational cost.</p><p>Algorithm 1 describes how the proposed sample discriminator works with an input image. For each ground-truth box g = [x, y, w, h, c] on the input image, where (x, y) is the lefttop corner of the box, (w, h) are the box width and height, respectively, and c is the class label, our anchor-free branch will generate prediction vectors:</p><formula xml:id="formula_0">V = [v m,n t , v m,n l , v m,n b , v m,n r , c m,n ],<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">v m,n t , v m,n l , v m,n b</formula><p>, v m,n r are the distances between the current pixel location (m, n) and the top, left, bottom and right boundaries of the box, and c m,n is the prediction class Algorithm 1 Sample discriminator Input:</p><p>G is the set of ground-truth boxes P is the set of feature pyramid levels A is the set of anchor boxes of the RPN outputs V is the set of predicted vector of anchor-free branch T P is the threshold of positive samples T N is the threshold of negative samples Output:</p><p>S E is the set of enhancement samples S P is the set of positive samples S N is the set of negative samples 1: for each ground-truth box g ? G do <ref type="bibr">2:</ref> for each feature pyramid level p i ? P do calculate the Intersection-over-Union (IoU) between g and b j ? B: IB g = IoU (b j , g); <ref type="bibr">6:</ref> calculate the Intersection-over-Union (IoU) between g and a i ? A:</p><formula xml:id="formula_2">IA g = IoU (a i , g); 7:</formula><p>if IB j g ? T P and IB j g ? IA i g then 8:</p><formula xml:id="formula_3">S E = S E B j g 9:</formula><p>else if IA i g ? T P and IA i g ? IB j g then 10:</p><formula xml:id="formula_4">S P = S P A i g 11: else if IA i g ? T N then 12: S N = S N A i g 13:</formula><p>end if 14: end for 15: S P = S P S E 16: return S E , S P , S N ; label. We first decode the prediction vectors V to form the bounding-boxes B:</p><formula xml:id="formula_5">B = [x m,n , y m,n , w m,n , h m,n , c m,n ],<label>(2)</label></formula><p>where</p><formula xml:id="formula_6">x m,n = m ? v m,n l , y m,n = n ? v m,n t , w = v m,n l + v m,n r , h = v m,n t + v m,n b .</formula><p>Then, we calculate the IoU between the regressed boundingboxes B and the ground-truth g, labeled as IB g , and the IoU between the anchors of the RPN output in the anchor-based module A and the ground-truth g, labeled as IA g . Afterward, we select samples as follows:</p><formula xml:id="formula_7">(IB j g ? IA i g ) ? (IB j g ? T P ),<label>(3)</label></formula><p>where T P is the threshold of the positive samples (i.e., 0.5 in this paper). It denotes the bounding-box in the anchor-free module have better consistency than the anchor proposal, and we assign box B j g to the enhanced samples S E with</p><formula xml:id="formula_8">(IA i g ? T P ) ? (IA i g ? IB j g ).<label>(4)</label></formula><p>We assign anchor A i g as positive samples S P . If we have where T N is the threshold of the negative samples (i.e., 0.3 in this paper), we assign anchors A i g as negative samples S N . Finally, we add the enhancement samples S E to the positive samples S P .</p><formula xml:id="formula_9">IA i g ? T N ,<label>(5)</label></formula><p>The threshold of positive and negative samples affects the accuracy of the detection. Because if the threshold is higher, the numbers of positive samples will decrease. If the threshold is lower, the quality of the samples will decrease. We keep this hyperparameter that is widely used in the baseline just like Faster R-CNN, and we find that doing so would potentially cause the problem of insufficient positive samples. The introduction of DEA alleviates the above-mentioned risks, and it directly and independently generates more qualified positive samples. As shown in <ref type="figure" target="#fig_4">Figure 4</ref>, we study the IoU distributions of the samples generated by Faster R-CNN <ref type="bibr" target="#b6">[7]</ref> and our DEA-net. Statistics shows that our DEA network provides more positive samples with higher IoUs compared with the original anchor-based detector.</p><p>In our method, the positive samples dynamically generated by the DEA module are all horizontal bounding box, which are the same as the anchor preset in the anchor-based branch. Then, we leverage a sample discriminator to realize interactive sample screening between the anchor-based branch and DEA branch to generate eligible samples. Finally, we utilize RoI-Transformer <ref type="bibr" target="#b1">[2]</ref> to obtain the feature of rotated objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Network Architecture</head><p>We build a Dynamic Enhancement Anchor network (DEA-Net) for both oriented and horizontal object detection tasks in aerial images. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the overall architecture of our DEA module integrated with Faster RCNN <ref type="bibr" target="#b6">[7]</ref> and RoI-Transformer <ref type="bibr" target="#b1">[2]</ref> for oriented object detection. We deploy ResNet <ref type="bibr" target="#b26">[27]</ref> as the backbone, which has been pre-trained on the ImageNet <ref type="bibr" target="#b27">[28]</ref>. Then, we construct a multi-scale feature pyramid <ref type="bibr" target="#b28">[29]</ref> in the top-down pathway from the backbone  network with levels from P 2 to P 6 and P i has 1/2 i resolution of the input image. Then, we construct a DEA head to P i , which contains the anchor-based module and our proposed DEA module. We construct the anchor-based module following the technique reported in <ref type="bibr" target="#b6">[7]</ref>, including the RPN head network to generate horizontal region proposals. For the DEA module, we construct an anchor-free module following the approach shown in <ref type="bibr" target="#b9">[10]</ref>. We add four convolutional layers after the feature maps P i created by the standard FPN for classification, centralization and regression. We decode the prediction vectors of the anchor-free module to form bounding-boxes and then we select better positive samples from the two modules through the sample discriminator. Finally, for the task of oriented object detection, we build the rotated head inspired by RoI-Transformer <ref type="bibr" target="#b1">[2]</ref> which transforms the horizontal proposals to the rotated ones for arbitrary-oriented detection and a standard Faster R-CNN <ref type="bibr" target="#b6">[7]</ref> is used for horizontal object detection. The anchor-free and anchor-based modules work jointly in a multi-task style and share the features at each pyramid level.</p><p>A recent work for improving one-stage detectors is to introduce an individual prediction branch to estimate the quality of localization, where the predicted quality facilitates the classification to improve detection performance <ref type="bibr" target="#b29">[30]</ref>. The authors compared IoU-branch and centerness-branch, and believed that IoU-branch performs consistently better than centerness as a measurement of localization quality. The convincing reason is that centerness scores are much smaller than IoU scores, which cause the final scores of bounding boxes are potentially small and then removed by NMS. In our method, we utilize DEAbranch (an anchor-free branch with centerness loss) to assist the training process of the anchor-based detector to generate eligible training sample according to the ground truth sample rather than generating the final output score. This avoids the divergence of the two branches in the inference stage. We introduce centerness loss as a part of loss for training the DEA branch, and we select samples based on the IoU between regressed bounding-boxes and the ground-truth, not based on the centerness score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Training and Inference</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-task joint training</head><p>Integrated with Faster RCNN <ref type="bibr" target="#b6">[7]</ref>, our DEA module is trained jointly with the anchor-based module in a multi-task style, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. We define L ab as the total loss of the anchor-based module, and L af as the total loss of the anchor-free module. We combine the losses from the anchor-based and anchor-free modules as the loss of the entire network. Then, the total optimization loss for the whole network is</p><formula xml:id="formula_10">L = L ab + L af ,<label>(6)</label></formula><p>For the multi-task loss in the anchor-based detection module, following <ref type="bibr" target="#b6">[7]</ref>, we optimize the target of the detection by regressing anchor boxes. The loss function for each anchor can be formulated as:</p><formula xml:id="formula_11">L ab ({p i }, {t i }) = L ab cls (p i , p * i ) + p * i L ab reg (t i , t * i ) (7)</formula><p>where the classification loss L ab cls is the cross entropy loss, p i is the predicted probability of anchor i being an object and p * i represents its ground-truth label (p * i = 1 for positive samples and p * i = 0 for negative samples). The regression loss L ab reg is smooth L1 loss <ref type="bibr" target="#b5">[6]</ref>, t i is the vector of the predicted box and t * i represents the ground-truth box. For the anchor-free module, following <ref type="bibr" target="#b9">[10]</ref>, the loss function for each location can be formulated as: </p><p>where classification loss L af cls is focal loss <ref type="bibr" target="#b7">[8]</ref>, p m,n is the prediction of class labels and p * m,n represents the ground-truth label. The regression loss L af reg is IoU loss <ref type="bibr" target="#b30">[31]</ref>. 1 {p * m,n &gt;0} is the indicator function, being 1 if p * m,n &gt; 0 and 0 otherwise. t m,n is a vector of the predicted box and t * m,n represents the ground-truth. The center-ness loss L af center is the cross entropy loss. Our DEA-Net utilizes the anchor-free and the anchor-based modules to jointly train the network to strengthen its feature representation ability and provide high-quality samples to the training task. During the inference stage, we feed the images to the anchor-based module whilst freezing the anchor-free module. This is mainly due to the fact that the anchor-free module has relatively poor consistency in locating boundingboxes, especially for the objects of a large aspect ratio. Freezing the anchor-free module could also avoid complicated fusion computation to control the computational overhead for the inference. We use the confidence score 0.05 and set the threshold of non-maximum suppression to be 0.1 to generate the final detection results. We demonstrate the effectiveness of the proposed scheme in the following ablation experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>In essence, our method is still anchor-based two-stage detection pipeline. One critical problem we solve is the "anchor over-quantization" problem, which would cause small targets to be ignored in the anchor laying process, thereby breaking the original statistical distribution of training samples and finally affecting the performance of the trained model. Instead of increasing the anchor laying density (which would greatly increase training overhead), we design a sample discriminator in the training stage. Unlike the solutions that combine anchorbased and anchor-free methods to detect + fusion, the proposed sample discriminator (as shown in <ref type="figure" target="#fig_1">Figure 2</ref> and Algorithm 1) comprehensively evaluates the consistency of anchor-boxes (produced in the anchor-based branch) and the inferred box produced by the DEA branch (an anchor-free branch) with ground truth. As demonstrated in <ref type="figure" target="#fig_2">Figure 3</ref>, small targets that are split into negative samples by anchor boxes are completely retained in the DEA module, free from the quantization errors of anchors. In the sample discriminator, these target regions located by the DEA module are further regarded as positive samples to compensate for quantization errors in the anchorbased branch, as shown in <ref type="figure" target="#fig_4">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL WORK A. Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>DOTA <ref type="bibr" target="#b0">[1]</ref> is one of the large datasets for object detection in aerial images with both oriented and horizontal bounding box annotations. It contains 2, 806 aerial images with 188, 282 annotated instances from different sensors and platforms. The image size ranges from around 800?800 to 4, 000?4, 000 pixels and contains objects exhibiting in a wide variety of scales, orientations, and shapes. DOTA contains 15 object categories, including Plane (PL), Baseball diamond (BD), Bridge (BR), Ground track field (GTF), Small vehicle (SV), Large vehicle (LV), ship (SH), Tennis court (TC), Basketball court (BC), Storage tank (ST), Soccer-ball field (SBF), Roundabout (RA), Harbor (HA), Swimming pool (SP), and Helicopter (HC). In our experiments, following <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>, 3/6 of the original images are randomly selected to form the training set, 1/6 as the validation set, and 2/6 as the testing set. HRSC2016 <ref type="bibr" target="#b13">[14]</ref> is a challenging dataset for ship detection in aerial images with large aspect ratios and arbitrary orientations. These images were collected from Google Earth, which contain 1061 images and more than 20 categories of ships in various appearances. The image size ranges from 300 ? 300 to 1500 ? 900. In our work, following <ref type="bibr" target="#b13">[14]</ref>, the training, validation, and test sets include 436, 181, and 444 images, respectively. For HRSC2016, only oriented object detection can be carried out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image size</head><p>For DOTA and HRSC2016, we generate a series of 1, 024? 1, 024 patches from the original images with a stride of 824 for training, validation, and testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline setup</head><p>We use the standard two-stage detector Faster R-CNN <ref type="bibr" target="#b6">[7]</ref> as the baseline. It utilizes ResNet-101 as backbone. FPN <ref type="bibr" target="#b28">[29]</ref> is adopted to construct a feature pyramid. Predefined horizontal anchors are set on each feature level, i.e., P 2 -P 6. Here, we do not use any rotation anchor. For oriented object detection, we add the rotated head developed in RoI-Transformer <ref type="bibr" target="#b1">[2]</ref> which transforms the horizontal proposals to the rotated ones. For a fair comparison, all the experimental data and parameter settings are strictly consistent as those reported in <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b13">[14]</ref>.</p><p>To verify the universality of our approach, we also embed our approach to ReDet <ref type="bibr" target="#b31">[32]</ref> which incorporates rotationequivariant networks into the detector to extract rotationequivariant features. It uses the ReResNet-50 <ref type="bibr" target="#b31">[32]</ref> as backbone, and FPN <ref type="bibr" target="#b28">[29]</ref> is adopted to construct a feature pyramid. And then it also adds the rotated head developed in RoI-Transformer <ref type="bibr" target="#b1">[2]</ref> for arbitrary-oriented detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyper-parameters</head><p>For the hyper-parameters, following <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, in DOTA and HRSC2016, only three horizontal anchors are set with the aspect ratios of {1/2, 1, 2}, the base anchor scale is set as {8 2 }, and the anchor strides of each level of the feature pyramid are set to be {4, 8, 16, 32, 64}.</p><p>For the positive and negative sample selection, following <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b1">[2]</ref>, we set the threshold of the positive samples as T P = 0.5 and the threshold of the negative samples as T N = 0.3.</p><p>We set ? = 2 and ? = 0.25 for the focal loss in L af cls .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details</head><p>In order to verify the effectiveness of our method, we perform ablation studies on the DOTA dataset, and avoid utilizing any bells-and-whistles training strategy and data augmentation in the ablation study.</p><p>For the peer comparison on DOTA and HRSC2016, like <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, we only conduct rotation augmentation using 4 angles (0, 90, 180, 270) to simply avoid the imbalance between different categories.</p><p>Stochastic gradient descent is used as the optimizer. The initial learning rate is set to 0.005 and divided by 10 at each decay step. Weight decay and momentum are set to 0.0001 and 0.9, respectively. Following <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b13">[14]</ref>, the total iterations for DOTA and HRSC2016 are 80k and 20k, respectively. We train the models on RTX 2080Ti with a batch size of 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation and metrics</head><p>Following <ref type="bibr" target="#b0">[1]</ref>, the standard mean Average Precision (mAP) is used as the primary evaluation metric for accuracy. Moreover, to verify the model efficiency, the model Parameters (#Params), and GFLOPs / FPS are also taken into consideration. The results of DOTA reported in our work are obtained by submitting our predictions to the official DOTA evaluation server 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ablation Study</head><p>Our ablation study is carried out on DOTA <ref type="bibr" target="#b0">[1]</ref> for oriented object detection with ResNet-101 <ref type="bibr" target="#b26">[27]</ref>, which aims to: (1) verify the effectiveness of our method on different backbone networks; (2) verify the effectiveness of our proposed units integrated with the baseline; (3) verify the effectiveness of the inference schemes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness and efficiency on different backbones</head><p>In <ref type="table" target="#tab_2">Table I</ref>, we show the experimental results of different backbone networks with our proposed units on the test set of DOTA. We use mean Average Precision (mAP) to examine our proposed module with ResNet-50, ResNet-101, and ResNet-152, respectively. Note that for aerial image, the object detection using oriented bounding box (OBB) is much more important but more difficult than using horizontal bounding box (HBB), that is why in <ref type="table" target="#tab_2">Table I</ref> we perform ablation study on OBB task rather than HBB. We observe that adding our proposed module to the backbone increases mAP by 0.52%, 0.90%, and 0.43%.</p><p>In <ref type="table" target="#tab_2">Table II</ref>, we report the model #Params and GFLOPs / FPS for the evaluation of model efficiency. It is clear that using our proposed DEA module increases a little computational cost. For example, average increases on these three backbones are around 4.47 M model #Params with around 13.91 GFLOPs, and with around 2 FPS reduction. Considering the model performance and the amount of the calculation, in the following experiments, we select ResNet-101 as our backbone network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of the proposed units</head><p>In <ref type="table" target="#tab_2">Table I</ref>, we show the performance of our DEA module integrated with three backbones for 15 categories of DOTA. We witness that our module can bring improvements for the 1 https://captain-whu.github.io/DOTA/ bounding box mAP by 0.90%. We can observe that our module has large improvements on small objects. Specifically, for SV (Small vehicle), our method can increase AP by 4.19%, and for ST (Storage Tank), AP can be increased by 5.85%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Efficiency of our proposed units</head><p>We also compare the efficiency and effectiveness of our proposed method against those of the method of presetting more small anchors, as shown in <ref type="table" target="#tab_2">Table III</ref>. The Hyper-parameters settings of the comparison experiment are as follows: <ref type="bibr" target="#b0">(1)</ref> The base anchor scale of the baseline and our method is {8 2 }, and we set the base anchor scale of the method of presetting more small anchors as {2 2 ,4 2 ,8 2 }.</p><p>(2) The aspect ratios of our method and the method of presetting more small anchors are both {1/2, 1, 2}. It is clear that the method of presetting more anchors increases more computational cost than the proposed method. The GFLOPs increase of 21.67 (+Anchor) vs 13.91 (+DEA), with the FPS reduction of 3.2 (+Anchor) vs 2.1 (+DEA). This is because in the method of presetting more anchors, the number of anchors has tripled and these anchors would participate in the calculation of the horizontal bounding boxes and the rotated bounding boxes. In terms of the performance of the two methods, the method of presetting more anchors would indeed improve the performance of some small objects (e.g., small vehicle). Our method has more improvements on small objects, because our DEA module can dynamically generate positive samples which match objects better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of the inference schemes</head><p>We also compare the effectiveness of different inference schemes on DOTA for horizontal object detection after having trained the anchor-based and anchor-free baselines with our proposed method, shown in <ref type="table" target="#tab_2">Table IV</ref>. Compared with the two baselines, our DEA-Net of freezing the anchor-free module can increase mAP by 0.96% and 6.40%. When we fuse the outputs of the anchor-based and anchor-free modules, mAP can have a minor improvement (0.96% vs 1.07% and 6.40% vs 6.51%), compared with the former. Meanwhile, fusing the two modules for inference, the inference speed becomes clearly slower (10.8 FPS vs 6.2 FPS). That is why after having trained the DEA-net, we freeze the anchor-free branch and only utilize the anchor-based module for inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualizations</head><p>We show some of the visual comparisons for oriented object detection between the baseline and the proposed method in <ref type="figure" target="#fig_7">Figure 5</ref>. The proposed method achieves notably better precision for small object detection, such as small vehicles, storage tank, ship, and airplane. We also show some visualized comparisons for horizontal object detection on DOTA in <ref type="figure">Figure 9</ref>. Our proposed method also achieves better performance both in the case of small and large objects with horizontal bounding boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparisons with state-of-the-arts</head><p>Results on DOTA Inference schemes FPS mAP (%) Anchor-based baseline Faster RCNN <ref type="bibr" target="#b6">[7]</ref> 13.0 73.89 Anchor-free baseline FCOS <ref type="bibr" target="#b9">[10]</ref> 11.0 68.45 Ours (DEA-Net) freezing anchor-free branch 10.8 74.85 +0.96/+6.40 Ours (DEA-Net) anchor-free + anchor-based fusion 6.2 74.96 +1.07/+6.51   We compare the proposed approach with some state-of-theart methods on the test set of DOTA, as shown in <ref type="table" target="#tab_5">Table V</ref>. When our approach integrated with RoI-Transformer <ref type="bibr" target="#b1">[2]</ref>, our DEA-Net achieves 77.77% mAP for oriented object detection and 78.43% mAP for horizontal object detection, and outperforms many advanced methods. Of these 15 categories, DEA-Net ranks at the top for 4 categories for oriented object detection and 10 categories for horizontal object detection. Moreover, DEA-Net surpasses the advanced method by 0.40 mAP for oriented object detection with a weaker backbone network (ResNet-101 vs ResNet-152) and 3.08 mAP for horizontal object detection with the same backbone. Visualization results on the test set of DOTA are shown in <ref type="figure">Figure 6</ref>. DEA-Net can accurately predict the categories and have satisfactory performance on small objects, such as small vehicle, storage tank and ship.</p><p>We also compare the proposed approach with some newest methods on the test set of DOTA, as shown in <ref type="table" target="#tab_2">Table VI</ref>. To verify the universality of our approach, we embed our approach to one of these current detectors ReDet <ref type="bibr" target="#b31">[32]</ref>, which is a state-of-the-art rotation detector that explicitly encodes rotation equivariance and rotation invariance. We integrate our DEA module with ReDet and conduct data augmentation following the way in <ref type="bibr" target="#b31">[32]</ref> (i.e., multi-scale data and random rotation), our method achieves 80.37% mAP for oriented object detection, and of these 15 categories, it ranks at the top for 5 categories. Results on HRSC2016  The comparisons with the other state-of-the-art methods on the test set of HRSC2016 <ref type="bibr" target="#b13">[14]</ref> are shown in <ref type="table" target="#tab_2">Table VII</ref>. We can observe that our method achieves the state-of-theart performance in mAP by 90.56%, which surpasses the previous best model by 1.1%. Particularly, in our experiments, our DEA-Net uses only 3 horizontal anchors with the aspect ratios of {1/2, 1, 2}, but outperforms the other frameworks with a large number of anchors. We show some of the visual comparisons for oriented object detection between the baseline and the proposed method on HRSC2016 in <ref type="figure">Figure 8</ref>. Our proposed method also achieves better precision for objects with large aspect ratio. The experiments show that it is critical to effectively utilize the predefined anchors and select high-quality samples where our DEA module can regress the bounding boxes at the locations of the objects without presetting a large number of rotated anchors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>The object detection using oriented bounding box (OBB)  is much more difficult than using horizontal bounding box (HBB). <ref type="table" target="#tab_2">Table V and Table VII</ref> also support this phenomenon.</p><p>In <ref type="table" target="#tab_5">Table V</ref>, for OBB task the proposed methods outperform SOTA by 0.4% but for HBB it outperforms SOTA by 3.08%.</p><p>In <ref type="table" target="#tab_2">Table V and Table VII</ref>, we can also find that, for OBB task, recent SOTA method can have only &lt; 1% gain compared with the previous state of the art methods, for example, R3Det-DCL <ref type="bibr" target="#b4">[5]</ref> (CVPR 2021) 77.37% vs DAL <ref type="bibr" target="#b3">[4]</ref> (AAAI 2021) 76.95% on DOTA. We also list some failed visualization comparisons of baseline and our method, as shown in <ref type="figure" target="#fig_8">Figure 7</ref>. The results show that our method, similar to the baseline, cannot generate accurate bounding boxes when detecting objects with extreme aspect ratios (e.g., harbor). This may be because the DEA module fail to generate accurate positive samples when the objects are of a large size and an extreme aspect ratio, while the anchor-based branch (e.g., the baseline) also cannot regress accurate bounding boxes when the aspect ratio of preset anchors are not match the objects of extreme aspect ratio. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS AND FUTURE WORK</head><p>In this work, a simple yet effective DEA module was proposed to facilitate the learning of small objects. We implemented our DEA module on the standard object detection backbone network with a feature pyramid network (i.e., DEA-Net) and conducted extensive experiments on both oriented and horizontal object detection in aerial images. Experimental results on the challenging DOTA and HRSC2016 indicated that our proposed DEA-Net could achieve state-of-the-art performance in accuracy with moderate computational overhead.</p><p>In the future, we will extend the proposed DEA-Net to a broader range of natural scenes. Besides, exploring how to use DEA-Net for semantic and panoramic segmentation is also a promising direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A SOME VISUALIZED COMPARISON OF OBJECT DETECTION</head><p>WITH ORIENTED BOUNDING BOX ON HRSC2016. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Comparison of anchor proposals (a) and (c) in an anchor-based method (e.g. Faster-RCNN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The architecture of our proposed Dynamic Enhancement Anchor Network (DEA-Net) for oriented object detection. The DEA head serves each level of the feature pyramid to generate higher-quality training samples, including an anchor-based module, an anchor-free module, and a sample discriminator. RoI transformer<ref type="bibr" target="#b1">[2]</ref> processes the horizontal and the oriented proposals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Dynamic enhancement anchor module to provide better positive samples of small objects. Specifically, in the original anchor-based module, the preset anchors are assigned as negative samples (0) or discarded samples (?1). However, our DEA module can generate high-quality bounding-boxes of higher Intersection-over-Union (IoU), assigned as positive samples(1)for better training the network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3 :</head><label>3</label><figDesc>decoder predicted vector V to bounding-boxes B: B = Decoder(V);</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>The Intersection-over-Union (IoU) distributions of the baseline (Faster R-CNN<ref type="bibr" target="#b6">[7]</ref>) and our DEA-net. The x-axis represents the IoU between the samples and the ground-truth boxes. The y-axis represents the average number of samples on each image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>L</head><label></label><figDesc>af ({p m,n }, {t m,n }) = L af cls (p m,n , p * m,n ) +1 {p * m,n &gt;0} L af reg (t m,n , t * m,n ) +1 {p * m,n &gt;0} L af center (t m,n , t * m,n )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>( 3 )</head><label>3</label><figDesc>The anchor strides of each feature map of the two methods are both {4, 8, 16, 32, 64}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 .</head><label>5</label><figDesc>Comparison against the baseline (Faster RCNN<ref type="bibr" target="#b6">[7]</ref>+RoI-Transformer [2]) on DOTA<ref type="bibr" target="#b0">[1]</ref> for oriented object detection with ResNet-101<ref type="bibr" target="#b26">[27]</ref>. Blue boxes indicate the results of the baseline and pink boxes are the results of our proposed DEA-Net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Some failed visualized comparisons of baseline and our method. The figures with blue boxes are the results of the baseline (Faster R-CNN + RoI-Transformer) and pink boxes are the results of the proposed DEA-Net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .Fig. 9 .</head><label>89</label><figDesc>Some visualized comparison of object detection with oriented bounding box on HRSC2016. The figures with blue boxes are the results of the baseline (RoI-Transformer + Faster R-CNN) and pink boxes are the results of our proposed DEA-Net. APPENDIX B SOME VISUALIZED COMPARISONS OF OBJECT DETECTION WITH HORIZONTAL BOUNDING BOX ON DOTA. Some visualized comparisons of object detection with horizontal bounding box on DOTA. The figures with blue boxes are the results of the baseline (Faster R-CNN) and pink boxes are the results of our proposed DEA-Net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Tremendous successes have been achieved on object detection with the development of deep Convolution Neural Networks (DCNNs) in recent years. Different from the objects in natural scenes which are D. Liang, Q. Geng, Z. Wei, and Mingqiang Wei are with the College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, MIIT Key Laboratory of Pattern Analysis and Machine Intelligence, Collaborative Innovation Center of Novel Software Technology and Industrialization, Nanjing 211106, China. E-mail: {liangdong, gengqx, weizongqi, mingqiangw}@nuaa.edu.cn D. A. Vorontsov and E. L. Kim are with the National Research Lobachevsky State University of Nizhny Novgorod, Nizhny Novgorod, Russia. E-mail: vorontsovda@mail.ru; kim@phys.unn.ru H. Zhou is with the School of Computing and Mathematical Sciences, University of Leicester, Leicester LE1 7RH, United Kingdom. E-mail: hz143@leicester.ac.uk</figDesc><table /><note>? Corresponding author: Dong Liang.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>77.05 51.94 64.98 77.98 76.83 87.02 90.78 82.75 82.84 55.56 62.70 73.92 67.74 58.59 73.26 87.97 79.14 51.13 65.43 79.87 78.24 87.66 90.59 83.28 85.65 53.72 63.39 73.83 69.50 57.32 73.78 +0.52 88.53 77.70 51.59 68.80 74.02 76.85 86.98 90.24 84.89 77.68 53.91 63.56 75.88 69.48 55.50 73.06 88.32 79.18 52.03 69.50 78.21 77.98 87.76 90.21 85.12 83.53 54.35 62.08 73.52 70.62 56.94 73.96 +0.90 88.56 77.71 54.03 72.76 74.15 77.48 87.17 90.17 76.39 83.95 45.68 64.50 76.22 69.53 53.41 72.78 88.43 79.21 51.28 69.46 78.17 79.19 87.21 89.89 78.20 85.98 45.94 63.56 74.77 70.97 55.83 73.21 +0.43</figDesc><table><row><cell>R-50 R-101 R-152 +DEA PL</cell><cell>BD</cell><cell>BR GTF SV</cell><cell>LV</cell><cell>SH</cell><cell>TC</cell><cell>BC</cell><cell>ST SBF RA HA</cell><cell>SP</cell><cell>HC mAP (%)</cell></row><row><cell>88.25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I THE</head><label>I</label><figDesc>EFFECTIVENESS OF OUR PROPOSED METHOD WITH DIFFERENT BACKBONE NETWORKS ON THE TEST SET OF DOTA [1] FOR ORIENTED OBJECT DETECTION. "+ DEA" INDICATES THE IMPLEMENTATION OF OUR PROPOSED MODULE ON THE BACKBONE NETWORKS.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>.70 51.59 68.80 74.02 76.85 86.98 90.24 84.89 77.68 53.91 63.56 75.88 69.48 55.50 289.19 / 12.7 74.12 73.06 88.41 80.14 53.79 70.70 77.82 76.98 86.98 90.75 83.90 81.13 51.95 61.41 74.89 69.15 59.27 310.86 / 9.5 74.13 73.82 88.32 79.18 52.03 69.50 78.21 77.98 87.76 90.21 85.12 83.53 54.35 62.08 73.52 70.62 56.94 303.10 / 10.6 78.89 73.96TABLE III THE COMPARISON OF EFFICIENCY BETWEEN OUR PROPOSED METHOD AND THE METHOD OF PRESET MORE SMALL ANCHORS ON DOTA [1] FOR ORIENTED OBJECT DETECTION. "BASELINE" INDICATES THE FASTER RCNN WITH THE BACKBONE OF RESNET-101. "+ ANCHOR" INDICATES THE IMPLEMENTATION OF MORE SMALL ANCHORS ON THE BASELINE NETWORKS. "+ DEA" INDICATES THE IMPLEMENTATION OF OUR PROPOSED MODULE ON THE BASELINE NETWORKS.</figDesc><table><row><cell>Baseline +Anchor +DEA PL</cell><cell>BD</cell><cell>BR GTF SV</cell><cell>LV</cell><cell>SH</cell><cell>TC</cell><cell>BC</cell><cell>ST SBF RA HA</cell><cell>SP</cell><cell cols="2">HC GFLOPs / FPS #Params (M) mAP (%)</cell></row><row><cell cols="2">88.53 77</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">R-50 R-101 R-152 +DEA GFLOPs / FPS #Params (M)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>211.30 / 14.8</cell><cell>55.13 6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>225.21 / 12.5</cell><cell>59.90</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>289.19 / 12.7</cell><cell>74.12</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>303.10 / 10.6</cell><cell>78.89</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>367.17 / 11.1</cell><cell>89.77</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>381.08 / 9.2</cell><cell>94.54</cell></row></table><note>TABLE II THE EFFICIENCY OF OUR PROPOSED METHOD WITH DIFFERENT BACKBONE NETWORKS ON THE TEST SET OF DOTA [1] FOR ORIENTED OBJECT DETECTION. "+ DEA" INDICATES THE IMPLEMENTATION OF OUR PROPOSED MODULE ON THE BACKBONE NETWORKS.Inference with anchor-based module</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV THE</head><label>IV</label><figDesc>EFFECTIVENESS OF DIFFERENT INFERENCE SCHEMES WITH OUR DEA-NET ON DOTA [1] FOR HORIZONTAL OBJECT DETECTION IN AERIAL IMAGES. RESNET-101 [27] IS THE BACKBONE. 83.46 55.14 71.69 79.59 83.08 88.10 90.88 87.09 86.73 63.99 65.14 75.81 78.01 68.69 77.77 +0.40 33.87 22.73 34.88 38.73 32.02 52.37 61.65 48.54 33.91 29.27 36.83 36.44 38.26 11.61 39.20 R-FCN [41] (NIPS 2016) R-101 79.33 44.26 36.58 53.53 39.38 34.15 47.29 45.66 47.74 65.84 37.92 44.23 47.23 50.64 34.90 47.24 FR-H [1] (CVPR 2018) R-101 80.32 77.55 32.86 68.13 53.66 52.49 50.04 90.41 75.05 59.59 57.00 49.81 61.69 56.46 41.85 60.46 FPN [29] (CVPR 2017) R-101 88.70 75.10 52.60 59.20 69.40 78.80 84.50 90.60 81.30 82.60 52.50 62.10 76.60 66.30 60.10 72.00 ICN [35] (ACCV 2018) R-101 90.00 77.70 53.40 73.30 73.50 65.00 78.20 90.80 79.10 84.80 57.20 62.10 73.50 70.20 58.10 72.50 SCRDet [3] (ICCV 2019) R-101 90.18 81.88 55.30 73.29 72.09 77.65 78.06 90.91 82.44 86.39 64.53 63.45 75.77 78.21 60.11 75.35 Ours (RoI Trans + DEA) R-101 89.18 83.34 58.94 71.69 80.23 83.97 88.26 90.88 87.09 87.44 64.24 65.04 76.40 80.88 68.81 78.43 +3.08</figDesc><table><row><cell cols="3">Methods Backbone PL</cell><cell>BD</cell><cell>BR GTF SV</cell><cell>LV</cell><cell>SH</cell><cell>TC</cell><cell>BC</cell><cell>ST SBF RA HA</cell><cell>SP</cell><cell>HC mAP (%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Oriented object detection</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">FR-O [1] (CVPR 2018) R-101</cell><cell cols="9">79.09 69.12 17.17 63.49 34.20 37.16 36.20 89.19 69.60 58.96 49.40 52.52 46.69 44.80 46.30</cell><cell>52.93</cell></row><row><cell cols="2">R-DFPN [33] (ISO4 2018) R-101</cell><cell cols="9">80.92 65.82 33.77 58.94 55.77 50.94 54.78 90.33 66.34 68.66 48.73 51.76 55.10 51.32 35.88</cell><cell>57.94</cell></row><row><cell cols="2">R 2 CNN [34] (preprint 2017) R-101</cell><cell cols="9">80.94 65.67 35.34 67.44 59.92 50.91 55.81 90.67 66.92 72.39 55.06 52.23 55.14 53.35 48.22</cell><cell>60.67</cell></row><row><cell cols="2">RRPN [23] (TMM 2018) R-101</cell><cell cols="9">88.52 71.20 31.66 59.30 51.85 56.19 57.25 90.81 72.84 67.38 56.69 52.84 53.08 51.94 53.58</cell><cell>61.01</cell></row><row><cell cols="2">ICN [35] (ACCV 2018) R-101</cell><cell cols="9">81.36 74.30 47.70 70.32 64.89 67.82 69.98 90.76 79.06 78.02 53.64 62.90 67.02 64.17 50.23</cell><cell>68.16</cell></row><row><cell cols="2">RoI Trans [2] (CVPR 2019) R-101</cell><cell cols="9">88.64 78.52 43.44 75.92 68.81 73.68 83.59 90.74 77.27 81.46 58.39 53.54 62.83 58.93 47.67</cell><cell>69.56</cell></row><row><cell cols="2">CAD-Net [36] (TGRS 2019) R-101</cell><cell cols="9">87.80 82.40 49.40 73.50 71.10 63.50 76.70 90.90 79.20 73.30 48.40 60.90 62.00 67.00 62.20</cell><cell>69.90</cell></row><row><cell cols="2">DRN [37] (CVPR 2020) H-104</cell><cell cols="9">88.91 80.22 43.52 63.35 73.48 70.69 84.94 90.14 83.85 84.11 50.12 58.41 67.62 68.60 52.50</cell><cell>70.70</cell></row><row><cell cols="2">O 2 -DNet [38] (ISPRS 2020) H-104</cell><cell cols="9">89.31 82.14 47.33 61.21 71.32 74.03 78.62 90.76 82.23 81.36 60.93 60.17 58.21 66.98 61.03</cell><cell>71.04</cell></row><row><cell cols="2">SCRDet [3] (ICCV 2019) R-101</cell><cell cols="9">89.98 80.65 52.09 68.36 68.36 60.32 72.41 90.85 87.94 86.86 65.02 66.68 66.25 68.24 65.21</cell><cell>72.61</cell></row><row><cell cols="2">R 3 Det [26] (preprint 2019) R-152</cell><cell cols="9">89.49 81.17 50.53 66.10 70.92 78.66 78.21 90.81 85.26 84.23 61.81 63.77 68.16 69.83 67.17</cell><cell>73.74</cell></row><row><cell cols="2">CSL [39] (ECCV 2020) R-152</cell><cell cols="9">90.25 85.53 54.64 75.31 70.44 73.51 77.62 90.84 86.15 86.69 69.60 68.04 73.83 71.10 68.93</cell><cell>76.17</cell></row><row><cell>DAL [4] (AAAI 2021)</cell><cell>R-50</cell><cell cols="9">89.69 83.11 55.03 71.00 78.30 81.90 88.46 90.89 84.97 87.46 64.41 65.65 76.86 72.09 64.35</cell><cell>76.95</cell></row><row><cell cols="2">R 3 Det-DCL [5] (CVPR 2021) R-152</cell><cell cols="9">89.26 83.60 53.54 72.76 79.04 82.56 87.31 90.67 86.59 86.98 67.49 66.88 73.29 70.56 69.99</cell><cell>77.37</cell></row><row><cell cols="2">Ours (RoI Trans + DEA) R-101</cell><cell cols="6">89.18 Horizontal object detection</cell><cell></cell><cell></cell></row><row><cell cols="2">SSD [40] (ECCV 2016) R-101</cell><cell cols="9">44.74 11.21 6.22 6.91 2.00 10.24 11.34 15.59 12.56 17.94 14.73 4.55 4.55 0.53 1.01</cell><cell>10.94</cell></row><row><cell cols="2">YOLOv2 [9] (CVPR 2016) R-101</cell><cell>76.90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V COMPARISONS</head><label>V</label><figDesc>WITH OTHER STATE-OF-THE-ART METHODS ON THE TEST SET OF DOTA [1] FOR BOTH ORIENTED AND HORIZONTAL OBJECT DETECTION IN AERIAL IMAGES. "OURS" MEANS THE IMPLEMENTATION OF THE DEA MODULE ON THE BASELINE MODEL. "R-" IN THE BACKBONE COLUMN DENOTES RESNET [27], AND "H-" DENOTES THE HOURGLASS NETWORK [42]. 83.60 57.74 81.95 79.94 83.19 89.11 90.78 84.87 87.81 70.30 68.25 78.30 77.01 69.58 79.42 ReDet [32] (CVPR 2021) ReR-50 88.81 82.48 60.83 80.82 78.34 86.06 88.31 90.87 88.77 87.03 68.65 66.90 79.26 79.71 74.67 80.10 GWD [44] (ICML 2021) R-152 89.66 84.99 59.26 82.19 78.97 84.83 87.70 90.21 86.54 86.85 73.47 67.77 76.92 79.22 74.92 80.23 Ours (ReDet + DEA) ReR-50 89.92 83.84 59.65 79.88 80.11 87.96 88.17 90.31 88.93 88.46 68.93 65.94 78.04 79.69 75.78 80.37 +0.14</figDesc><table><row><cell cols="3">Methods Backbone PL</cell><cell>BD</cell><cell>BR GTF SV</cell><cell>LV</cell><cell>SH</cell><cell>TC</cell><cell>BC</cell><cell>ST SBF RA HA</cell><cell>SP</cell><cell>HC mAP (%)</cell></row><row><cell>S 2 A-Net [43] (TGRS 2021)</cell><cell>R-50</cell><cell>88.89</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI COMPARISONS</head><label>VI</label><figDesc>WITH SOME CURRENT STATE-OF-THE-ART METHODS ON THE TEST SET OF DOTA [1] FOR ORIENTED OBJECT DETECTION IN AERIAL IMAGES. "OURS" MEANS THE IMPLEMENTATION OF THE DEA MODULE ON THE BASELINE MODEL. "R-" IN THE BACKBONE COLUMN DENOTES RESNET [27], AND "RER-" DENOTES ROTATION-EQUIVARIANT RESNET [32].</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VII COMPARISONS</head><label>VII</label><figDesc>WITH OTHER STATE-OF-THE-ART METHODS ON THE TEST SET OF HRSC2016 [14] FOR ORIENTED OBJECT DETECTION IN AERIAL IMAGES. "R-" IN THE BACKBONE COLUMN DENOTES THE RESNET [27], AND "V-" DENOTES THE VGG NETWORK [48]. MAP IS OBTAINED ON THE VOC 2007 EVALUATION METRIC.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank Professor Sheng-Jun Huang from NUAA and Professor Gui-Song Xia from Wuhan University for their help discussion and comments. This work was supported by AI+ Project of NUAA (XZA20003), Natural Science Foundation of China (62172218, 61772268, 62172212), Natural Science Foundation of Jiangsu Province (BK20190065).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dota: A large-scale dataset for object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3974" to="3983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning roi transformer for oriented object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2849" to="2858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scrdet: Towards more robust detection for small, cluttered and rotated objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8232" to="8241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dynamic anchor learning for arbitrary-oriented object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2355" to="2363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dense label encoding for boundary discontinuity free rotation detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15" to="819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1137" to="1149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional onestage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Corner proposal network for anchor-free, two-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="399" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A high resolution optical satellite image dataset for ship recognition and some new baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition Applications and Methods (ICPRAM</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="324" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Region proposal by guided anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2965" to="2974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Feature selective anchor-free module for single-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="840" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Sface: An efficient network for face detection in large scale variations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06559</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">in arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Metaanchor: Learning to detect objects with customized anchors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="318" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Soft anchor-point object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="91" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dynamic anchor feature selection for single-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6609" to="6618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9759" to="9768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Arbitrary-oriented scene text detection via rotation proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Multimedia (TMM)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3111" to="3122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ship rotated bounding box space for ship extraction from high-resolution optical satellite images with complex backgrounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Geoscience and Remote Sensing Letters (GRSL)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1074" to="1078" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Textboxes++: A single-shot oriented scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on image processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3676" to="3690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">R3det: Refined single-stage detector with feature refinement for rotating object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05612</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>in arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unitbox: An advanced object detection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM international conference on Multimedia (MM)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="516" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Redet: A rotation-equivariant detector for aerial object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2786" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Automatic ship detection in remote sensing images from google earth of complex scenes based on multiscale rotation dense feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Remote Sensing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">132</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">R2cnn: rotational region cnn for orientation robust scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09579</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>in arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Towards multi-class object detection in unconstrained remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Azimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bahmanyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>K?rner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Reinartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="150" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cad-net: A context-aware detection network for objects in remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Geoscience and Remote Sensing Letters (GRSL)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="15" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dynamic refinement network for oriented and densely packed object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Oriented objects as pairs of middle lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="268" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Arbitrary-oriented object detection with circular smooth label</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="677" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via regionbased fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Align deep features for oriented object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Geoscience and Remote Sensing (TGRS)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Rethinking rotated object detection with gaussian wasserstein distance loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="830" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Toward arbitrary-oriented ship detection with rotated region proposal and discrimination networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Geoscience and Remote Sensing Letters (GRSL)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1745" to="1749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Rotation-sensitive regression for oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5909" to="5918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Gliding vertex on the horizontal bounding box for multi-oriented object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1452" to="1459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
