<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pseudo-LiDAR from Visual Depth Estimation: Bridging the Gap in 3D Object Detection for Autonomous Driving</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
								<address>
									<settlement>Ithaca</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
								<address>
									<settlement>Ithaca</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Garg</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
								<address>
									<settlement>Ithaca</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
								<address>
									<settlement>Ithaca</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Campbell</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
								<address>
									<settlement>Ithaca</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
								<address>
									<settlement>Ithaca</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pseudo-LiDAR from Visual Depth Estimation: Bridging the Gap in 3D Object Detection for Autonomous Driving</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3D object detection is an essential task in autonomous driving. Recent techniques excel with highly accurate detection rates, provided the 3D input data is obtained from precise but expensive LiDAR technology. Approaches based on cheaper monocular or stereo imagery data have, until now, resulted in drastically lower accuracies -a gap that is commonly attributed to poor image-based depth estimation. However, in this paper we argue that it is not the quality of the data but its representation that accounts for the majority of the difference. Taking the inner workings of convolutional neural networks into consideration, we propose to convert image-based depth maps to pseudo-LiDAR representations -essentially mimicking the LiDAR signal. With this representation we can apply different existing LiDAR-based detection algorithms. On the popular KITTI benchmark, our approach achieves impressive improvements over the existing state-of-the-art in image-based performance -raising the detection accuracy of objects within the 30m range from the previous state-of-the-art of 22% to an unprecedented 74%. At the time of submission our algorithm holds the highest entry on the KITTI 3D object detection leaderboard for stereo-image-based approaches. Our code is publicly available at https: /</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Reliable and robust 3D object detection is one of the fundamental requirements for autonomous driving. After all, in order to avoid collisions with pedestrians, cyclist, and cars, a vehicle must be able to detect them in the first place.</p><p>Existing algorithms largely rely on LiDAR (Light Detection And Ranging), which provide accurate 3D point clouds of the surrounding environment. Although highly precise, alternatives to LiDAR are desirable for multiple reasons. First, LiDAR is expensive, which incurs a hefty premium for autonomous driving hardware. Second, over-reliance on a single sensor is an inherent safety risk and it would be advantageous to have a secondary sensor to fall-back onto in case of an outage. A natural candidate are images from stereo or monocular cameras. Optical cameras are highly affordable (several orders of magnitude cheaper than Li-DAR), operate at a high frame rate, and provide a dense depth map rather than the 64 or 128 sparse rotating laser beams that LiDAR signal is inherently limited to.</p><p>Several recent publications have explored the use of monocular and stereo depth (disparity) estimation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b34">35]</ref> for 3D object detection <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b32">33]</ref>. However, to-date the main successes have been primarily in supplementing LiDAR approaches. For example, one of the leading algorithms <ref type="bibr" target="#b17">[18]</ref> on the KITTI benchmark <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> uses sensor fusion to improve the 3D average precision (AP) for cars from 66% for LiDAR to 73% with LiDAR and monocular images. In contrast, among algorithms that use only images, the state-of-the-art achieves a mere 10% AP <ref type="bibr" target="#b32">[33]</ref>.</p><p>One intuitive and popular explanation for such inferior performance is the poor precision of image-based depth estimation. In contrast to LiDAR, the error of stereo depth estimation grows quadratically with depth. However, a visual comparison of the 3D point clouds generated by LiDAR and a state-of-the-art stereo depth estimator <ref type="bibr" target="#b2">[3]</ref> reveals a high quality match (cf. <ref type="figure" target="#fig_0">Fig. 1</ref>) between the two data modalities -even for faraway objects.</p><p>In this paper we provide an alternative explanation with significant performance implications. We posit that the major cause for the performance gap between stereo and LiDAR is not a discrepancy in depth accuracy, but a poor choice of representations of the 3D information for ConvNet-based 3D object detection systems operating on stereo. Specifically, the LiDAR signal is commonly represented as 3D point clouds <ref type="bibr" target="#b24">[25]</ref> or viewed from the topdown "bird's-eye view" perspective <ref type="bibr" target="#b35">[36]</ref>, and processed accordingly. In both cases, the object shapes and sizes are invariant to depth. In contrast, image-based depth is densely estimated for each pixel and often represented as additional image channels <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b32">33]</ref>, making far-away objects smaller and harder to detect. Even worse, pixel neighborhoods in this representation group together points from far-away regions of 3D space. This makes it hard for convolutional networks relying on 2D convolutions on these channels to reason about and precisely localize objects in 3D.</p><p>To evaluate our claim, we introduce a two-step approach for stereo-based 3D object detection. We first convert the estimated depth map from stereo or monocular imagery into a 3D point cloud, which we refer to as pseudo-LiDAR as it mimics the LiDAR signal. We then take advantage of existing LiDAR-based 3D object detection pipelines <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b24">25]</ref>, which we train directly on the pseudo-LiDAR representation. By changing the 3D depth representation to pseudo-LiDAR we obtain an unprecedented increase in accuracy of image-based 3D object detection algorithms. Specifically, on the KITTI benchmark with IoU (intersection-overunion) at 0.7 for "moderately hard" car instances -the metric used in the official leaderboard -we achieve a 45.3% 3D AP on the validation set: almost a 350% improvement over the previous state-of-the-art image-based approach. Furthermore, we halve the gap between stereobased and LiDAR-based systems.</p><p>We evaluate multiple combinations of stereo depth estimation and 3D object detection algorithms and arrive at remarkably consistent results. This suggests that the gains we observe are because of the pseudo-LiDAR representation and are less dependent on innovations in 3D object detection architectures or depth estimation techniques.</p><p>In sum, the contributions of the paper are two-fold. First, we show empirically that a major cause for the performance gap between stereo-based and LiDAR-based 3D object detection is not the quality of the estimated depth but its representation. Second, we propose pseudo-LiDAR as a new recommended representation of estimated depth for 3D object detection and show that it leads to state-of-the-art stereobased 3D object detection, effectively tripling prior art. Our results point towards the possibility of using stereo cameras in self-driving cars -potentially yielding substantial cost reductions and/or safety improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>LiDAR-based 3D object detection. Our work is inspired by the recent progress in 3D vision and LiDAR-based 3D object detection. Many recent techniques use the fact that LiDAR is naturally represented as 3D point clouds. For example, frustum PointNet <ref type="bibr" target="#b24">[25]</ref> applies PointNet <ref type="bibr" target="#b25">[26]</ref> to each frustum proposal from a 2D object detection network. MV3D <ref type="bibr" target="#b6">[7]</ref> projects LiDAR points into both bird-eye view (BEV) and frontal view to obtain multi-view features. Vox-elNet <ref type="bibr" target="#b36">[37]</ref> encodes 3D points into voxels and extracts features by 3D convolutions. UberATG-ContFuse <ref type="bibr" target="#b17">[18]</ref>, one of the leading algorithms on the KITTI benchmark <ref type="bibr" target="#b11">[12]</ref>, performs continuous convolutions <ref type="bibr" target="#b29">[30]</ref> to fuse visual and BEV LiDAR features. All these algorithms assume that the precise 3D point coordinates are given. The main challenge there is thus on predicting point labels or drawing bounding boxes in 3D to locate objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stereo-and monocular-based depth estimation.</head><p>A key ingredient for image-based 3D object detection methods is a reliable depth estimation approach to replace LiDAR. These can be obtained through monocular <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13]</ref> or stereo vision <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">21]</ref>. The accuracy of these systems has increased dramatically since early work on monocular depth estimation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b28">29]</ref>. Recent algorithms like DORN <ref type="bibr" target="#b9">[10]</ref> combine multi-scale features with ordinal regression to predict pixel depth with remarkably low errors. For stereo vision, PSMNet <ref type="bibr" target="#b2">[3]</ref> applies Siamese networks for disparity estimation, followed by 3D convolutions for refinement, resulting in an outlier rate less than 2%. Recent work has made these methods mode efficient <ref type="bibr" target="#b30">[31]</ref>, enabling accurate disparity estimation to run at 30 FPS on mobile devices.</p><p>Image-based 3D object detection. The rapid progress on stereo and monocular depth estimation suggests that they could be used as a substitute for LiDAR in image-based 3D object detection algorithms. Existing algorithms of this flavor are largely built upon 2D object detection <ref type="bibr" target="#b27">[28]</ref>, imposing extra geometric constraints <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b31">32]</ref> to create 3D proposals. <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b32">33]</ref> apply stereo-based depth estimation to obtain the true 3D coordinates of each pixel. These 3D coordinates are either entered as additional input channels into a 2D detection pipeline, or used to extract hand-crafted features. Although these methods have made remarkable progress, the state-of-the-art for 3D object detection performance lags behind LiDAR-based methods. As we discuss in Section 3, this might be because of the depth representation used by these methods.</p><p>Despite the many advantages of image-based 3D object recognition, there remains a glaring gap between the stateof-the-art detection rates of image and LiDAR-based approaches (see <ref type="table" target="#tab_0">Table 1</ref> in Section 4.3). It is tempting to attribute this gap to the obvious physical differences and its implications between LiDAR and camera technology. For example, the error of stereo-based 3D depth estimation grows quadratically with the depth of an object, whereas for Time-of-Flight (ToF) approaches, such as LiDAR, this relationship is approximately linear.</p><p>Although some of these physical differences do likely contribute to the accuracy gap, in this paper we claim that a large portion of the discrepancy can be explained by the data representation rather than its quality or underlying physical properties associated with data collection.</p><p>In fact, recent algorithms for stereo depth estimation can generate surprisingly accurate depth maps <ref type="bibr" target="#b2">[3]</ref> (see <ref type="figure" target="#fig_0">figure 1</ref>). Our approach to "close the gap" is therefore to carefully remove the differences between the two data modalities and align the two recognition pipelines as much as possible. To this end, we propose a two-step approach by first estimating the dense pixel depth from stereo (or even monocular) imagery and then back-projecting pixels into a 3D point cloud. By viewing this representation as pseudo-LiDAR signal, we can then apply any existing LiDAR-based 3D object detection algorithm. <ref type="figure">Fig. 2</ref> depicts our pipeline.</p><p>Depth estimation. Our approach is agnostic to different depth estimation algorithms. We primarily work with stereo disparity estimation algorithms <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">21]</ref>, although our approach can easily use monocular depth estimation methods.</p><p>A stereo disparity estimation algorithm takes a pair of left-right images I l and I r as input, captured from a pair of cameras with a horizontal offset (i.e., baseline) b, and outputs a disparity map Y of the same size as either one of the two input images. Without loss of generality, we assume the depth estimation algorithm treats the left image, I l , as reference and records in Y the horizontal disparity to I r for each pixel. Together with the horizontal focal length f U of the left camera, we can derive the depth map D via the following transform,</p><formula xml:id="formula_0">D(u, v) = f U ? b Y (u, v) .<label>(1)</label></formula><p>Pseudo-LiDAR generation. Instead of incorporating the depth D as multiple additional channels to the RGB images, as is typically done <ref type="bibr" target="#b32">[33]</ref>, we can derive the 3D location (x, y, z) of each pixel (u, v), in the left camera's coordinate system, as follows,</p><formula xml:id="formula_1">(depth) z = D(u, v) (2) (width) x = (u ? c U ) ? z f U (3) (height) y = (v ? c V ) ? z f V ,<label>(4)</label></formula><p>where (c U , c V ) is the pixel location corresponding to the camera center and f V is the vertical focal length. By back-projecting all the pixels into 3D coordinates, we arrive at a 3D point cloud {(x (n) , y (n) , z (n) )} N n=1 , where N is the pixel count. Such a point cloud can be transformed into any cyclopean coordinate frame given a reference viewpoint and viewing direction. We refer to the resulting point cloud as pseudo-LiDAR signal.</p><p>LiDAR vs. pseudo-LiDAR. In order to be maximally compatible with existing LiDAR detection pipelines we apply a few additional post-processing steps on the pseudo-LiDAR data. Since real LiDAR signals only reside in a certain range of heights, we disregard pseudo-LiDAR points beyond that range. For instance, on the KITTI benchmark, following <ref type="bibr" target="#b35">[36]</ref>, we remove all points higher than 1m above the fictitious LiDAR source (located on top of the autonomous vehicle). As most objects of interest (e.g., cars and pedestrians) do not exceed this height range there is little information loss. In addition to depth, LiDAR also returns the reflectance of any measured pixel (within [0,1]). As we have no such information, we simply set the reflectance to 1.0 for every pseudo-LiDAR points. <ref type="figure" target="#fig_0">Fig 1 depicts</ref> the ground-truth LiDAR and the pseudo-LiDAR points for the same scene from the KITTI dataset <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. The depth estimate was obtained with the pyramid stereo matching network (PSMNet) <ref type="bibr" target="#b2">[3]</ref>. Surprisingly, the pseudo-LiDAR points (blue) align remarkably well to true LiDAR points (yellow), in contrast to the common belief that low precision image-based depth is the main cause of inferior 3D object detection. We note that a LiDAR can capture &gt; 100, 000 points for a scene, which is of the same order as the pixel count. Nevertheless, LiDAR points are distributed along a few (typically 64 or 128) horizontal beams, only sparsely occupying the 3D space.</p><p>3D object detection. With the estimated pseudo-LiDAR points, we can apply any existing LiDAR-based 3D object detectors for autonomous driving. In this work, we consider those based on multimodal information (i.e., monocular images + LiDAR), as it is only natural to incorporate the original visual information together with the pseudo-LiDAR data. Specifically, we experiment on AVOD <ref type="bibr" target="#b16">[17]</ref> and frustum PointNet <ref type="bibr" target="#b24">[25]</ref>, the two top ranked algorithms</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stereo/Mono depth</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LiDAR-based detection</head><p>Stereo/Mono images Depth estimation Depth map Pseudo LiDAR 3D object detection Predicted 3D boxes <ref type="figure">Figure 2</ref>: The proposed pipeline for image-based 3D object detection. Given stereo or monocular images, we first predict the depth map, followed by back-projecting it into a 3D point cloud in the LiDAR coordinate system. We refer to this representation as pseudo-LiDAR, and process it exactly like LiDAR -any LiDAR-based detection algorithms can be applied.</p><p>with open-sourced code on the KITTI benchmark. In general, we distinguish between two different setups: a) In the first setup we treat the pseudo-LiDAR information as a 3D point cloud. Here, we use frustum Point-Net <ref type="bibr" target="#b24">[25]</ref>, which projects 2D object detections <ref type="bibr" target="#b18">[19]</ref> into a frustum in 3D, and then applies PointNet <ref type="bibr" target="#b25">[26]</ref> to extract point-set features at each 3D frustum.</p><p>b) In the second setup we view the pseudo-LiDAR information from a Bird's Eye View (BEV). In particular, the 3D information is converted into a 2D image from the top-down view: width and depth become the spatial dimensions, and height is recorded in the channels. AVOD connects visual features and BEV LiDAR features to 3D box proposals and then fuses both to perform box classification and regression.</p><p>Data representation matters. Although pseudo-LiDAR conveys the same information as a depth map, we claim that it is much better suited for 3D object detection pipelines that are based on deep convolutional networks. To see this, consider the core module of the convolutional network: 2D convolutions. A convolutional network operating on images or depth maps performs a sequence of 2D convolutions on the image/depth map. Although the filters of the convolution can be learned, the central assumption is two-fold: (a) local neighborhoods in the image have meaning, and the network should look at local patches, and (b) all neighborhoods can be operated upon in an identical manner. These are but imperfect assumptions. First, local patches on 2D images are only coherent physically if they are entirely contained in a single object. If they straddle object boundaries, then two pixels can be co-located next to each other in the depth map, yet can be very far away in 3D space. Second, objects that occur at multiple depths project to different scales in the depth map. A similarly sized patch might capture just a side-view mirror of a nearby car or the entire body of a far-away car. Existing 2D object detection approaches struggle with this breakdown of assumptions and have to design novel techniques such as feature pyramids <ref type="bibr" target="#b18">[19]</ref> to deal with this challenge.  In contrast, 3D convolutions on point clouds or 2D convolutions in the bird's-eye view slices operate on pixels that are physically close together (although the latter do pull together pixels from different heights, the physics of the world implies that pixels at different heights at a particular spatial location usually do belong to the same object). In addition, both far-away objects and nearby objects are treated exactly the same way. These operations are thus inherently more physically meaningful and hence should lead to better learning and more accurate models.</p><p>To illustrate this point further, in <ref type="figure" target="#fig_2">Fig. 3</ref> we conduct a simple experiment. In the left column, we show the original depth-map and the pseudo-LiDAR representation of an image scene. The four cars in the scene are highlighted in color. We then perform a single 11 ? 11 convolution with a box filter on the depth-map (top right), which matches the receptive field of 5 layers of 3 ? 3 convolutions. We then convert the resulting (blurred) depth-map into a pseudo-LiDAR representation (bottom right). From the figure, it becomes evident that this new pseudo-LiDAR representation suffers substantially from the effects of the blurring.</p><p>The cars are stretched out far beyond their actual physical proportions making it essentially impossible to locate them precisely. For better visualization, we added rectangles that contain all the points of the green and cyan cars. After the convolution, both bounding boxes capture highly erroneous areas. Of course, the 2D convolutional network will learn to use more intelligent filters than box filters, but this example goes to show how some operations the convolutional network might perform could border on the absurd.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate 3D-object detection with and without pseudo-LiDAR across different settings with varying approaches for depth estimation and object detection. Throughout, we will highlight results obtained with pseudo-LiDAR in blue and those with actual LiDAR in gray.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Setup</head><p>Dataset. We evaluate our approach on the KITTI object detection benchmark <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>, which contains 7,481 images for training and 7,518 images for testing. We follow the same training and validation splits as suggested by Chen et al. <ref type="bibr" target="#b4">[5]</ref>, containing 3,712 and 3,769 images respectively. For each image, KITTI provides the corresponding Velodyne LiDAR point cloud, right image for stereo information, and camera calibration matrices.</p><p>Metric. We focus on 3D and bird's-eye-view (BEV) 1 object detection and report the results on the validation set. Specifically, we focus on the "car" category, following <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b33">34]</ref>. We follow the benchmark and prior work and report average precision (AP) with the IoU thresholds at 0.5 and 0.7. We denote AP for the 3D and BEV tasks by AP 3D and AP BEV , respectively. Note that the benchmark divides each category into three cases -easy, moderate, and hard -according to the bounding box height and occlusion/truncation level. In general, the easy case corresponds to cars within 30 meters of the ego-car distance <ref type="bibr" target="#b35">[36]</ref>.</p><p>Baselines. We compare to MONO3D <ref type="bibr" target="#b3">[4]</ref>, 3DOP <ref type="bibr" target="#b4">[5]</ref>, and MLF <ref type="bibr" target="#b32">[33]</ref>. The first is monocular and the second is stereobased. MLF <ref type="bibr" target="#b32">[33]</ref> reports results with both monocular <ref type="bibr" target="#b12">[13]</ref> and stereo disparity <ref type="bibr" target="#b20">[21]</ref>, which we denote as MLF-MONO and MLF-STEREO, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Details of our approach</head><p>Stereo disparity estimation. We apply PSMNET <ref type="bibr" target="#b2">[3]</ref>, DISPNET <ref type="bibr" target="#b20">[21]</ref>, and SPS-STEREO <ref type="bibr" target="#b34">[35]</ref> to estimate dense disparity. The first two approaches are learning-based and we use the released models, which are pre-trained on the <ref type="bibr" target="#b0">1</ref> The BEV detection task is also called 3D localization. Scene Flow dataset <ref type="bibr" target="#b20">[21]</ref>, with over 30,000 pairs of synthetic images and dense disparity maps, and fine-tuned on the 200 training pairs of KITTI stereo 2015 benchmark <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b21">22]</ref>. We note that, MLF-STEREO <ref type="bibr" target="#b32">[33]</ref> also uses the released DISP-NET model. The third approach, SPS-STEREO <ref type="bibr" target="#b34">[35]</ref>, is nonlearning-based and has been used in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>DISPNET has two versions, without and with correlations layers. We test both and denote them as DISPNET-S and DISPNET-C, respectively.</p><p>While performing these experiments, we found that the 200 training images of KITTI stereo 2015 overlap with the validation images of KITTI object detection. That is, the released PSMNET and DISPNET models actually used some validation images of detection. We therefore train a version of PSMNET using Scene Flow followed by finetuning on the 3,712 training images of detection, instead of the 200 KITTI stereo images. We obtain pseudo disparity ground truth by projecting the corresponding LiDAR points into the 2D image space. We denote this version PSMNET . Details are included in the Supplementary Material.</p><p>The results with PSMNET in <ref type="table" target="#tab_2">Table 3</ref> (fined-tuned on 3,712 training data) are in fact better than PSMNET (finetuned on KITTI stereo 2015). We attribute the improved accuracy of PSMNET on the fact that it is trained on a larger training set. Nevertheless, future work on 3D object detection using stereo must be aware of this overlap. Monocular depth estimation. We use the state-of-the-art monocular depth estimator DORN <ref type="bibr" target="#b9">[10]</ref>, which is trained by the authors on 23,488 KITTI images. We note that some of these images may overlap with our validation data for detection. Nevertheless, we decided to still include these results and believe they could serve as an upper bound for monocular-based 3D object detection. Future work, however, must be aware of this overlap.</p><p>Pseudo-LiDAR generation. We back-project the estimated depth map into 3D points in the Velodyne LiDAR's coordinate system using the provided calibration matrices. We disregard points with heights larger than 1 in the system. 3D Object detection. We consider two algorithms: Frustum PointNet (F-POINTNET) <ref type="bibr" target="#b24">[25]</ref> and AVOD <ref type="bibr" target="#b16">[17]</ref>. More specifically, we apply F-POINTNET-v1 and AVOD-FPN. Both of them use information from LiDAR and monocular images. We train both models on the 3,712 training data from scratch by replacing the LiDAR points with pseudo-LiDAR data generated from stereo disparity estimation. We use the hyper-parameters provided in the released code.</p><p>We note that AVOD takes image-specific ground planes as inputs. The authors provide ground-truth planes for training and validation images, but do not provide the procedure to obtain them (for novel images). We therefore fit the ground plane parameters with a straight-forward application of RANSAC <ref type="bibr" target="#b8">[9]</ref> to our pseudo-LiDAR points that fall into a certain range of road height, during evaluation. Details are included in the Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experimental results</head><p>We summarize the main results in <ref type="table" target="#tab_0">Table 1</ref>. We organize methods according to the input signals for performing detection. Our stereo approaches based on pseudo-LiDAR significantly outperform all image-based alternatives by a large margin. At IoU = 0.7 (moderate) -the metric used to rank algorithms on the KITTI leaderboard -we achieve double the performance of the previous state of the art. We also observe that pseudo-LiDAR is applicable and highly beneficial to two 3D object detection algorithms with very different architectures, suggesting its wide compatibility.</p><p>One interesting comparison is between approaches using pseudo-LiDAR with monocular depth (DORN) and stereo depth (PSMNET ). While DORN has been trained with almost ten times more images than PSMNET (and some of them overlap with the validation data), the results with PSMNET dominate. This suggests that stereo-based detection is a promising direction to move in, especially considering the increasing affordability of stereo cameras.</p><p>In the following section, we discuss key observations and conduct a series of experiments to analyze the performance gain through pseudo-LiDAR with stereo disparity.</p><p>Impact of data representation. When comparing our results using DISPNET-S or DISPNET-C to MLF-STEREO <ref type="bibr" target="#b32">[33]</ref> (which also uses DISPNET as the underlying stereo engine), we observe a large performance gap (see <ref type="table">Table.</ref> 2). Specifically, at IoU= 0.7, we outperform MLF-STEREO by at least 16% on AP BEV and 16% on AP 3D . The later is equivalent to a 160% relative improvement. We attribute this improvement to the way in which we represent the resulting depth information. We note that both our approach and MLF-STEREO <ref type="bibr" target="#b32">[33]</ref> first back-project pixel depths into 3D point coordinates. MLF-STEREO construes the 3D coordinates of each pixel as additional feature maps in the frontal view. These maps are then concatenated with RGB channels as the input to a modified 2D object detection pipeline based on Faster-RCNN <ref type="bibr" target="#b27">[28]</ref>. As we point out earlier, this has two problems. Firstly, distant objects become smaller, and detecting small objects is a known hard problem <ref type="bibr" target="#b18">[19]</ref>. Secondly, while performing local computations like convolutions or ROI pooling along height and width of an image makes sense to 2D object detection, it will operate on 2D pixel neighborhoods with pixels that are far apart in 3D, making the precise localization of 3D objects much harder (cf. <ref type="figure" target="#fig_2">Fig. 3</ref>).</p><p>By contrast, our approach treats these coordinates as pseudo-LiDAR signals and applies PointNet <ref type="bibr" target="#b25">[26]</ref> (in F-POINTNET) or use a convolutional network on the BEV projection (in AVOD). This introduces invariance to depth, since far-away objects are no longer smaller. Furthermore, convolutions and pooling operations in these representations put together points that are physically nearby.</p><p>To further control for other differences between MLF-STEREO and our method we ablate our approach to use the same frontal depth representation used by MLF-STEREO. AVOD fuses information of the frontal images with BEV LiDAR features. We modify the algorithm, following <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b32">33]</ref>, to generate five frontal-view feature maps, including 3D pixel locations, disparity, and Euclidean distance to the camera. We concatenate them with the RGB channels while disregarding the BEV branch in AVOD, making it fully dependent on the frontal-view branch. (We make no additional architecture changes.) The results in <ref type="table" target="#tab_1">Table 2</ref> reveal a staggering gap between frontal and pseudo-LiDAR results. We found that the frontal approach struggles with inferring object depth, even when the five extra maps have provided sufficient 3D information. Again, this might be because 2d convolutions put together pixels from far away depths, making accurate localization difficult. This experiment suggests that the chief source of the accuracy improvement is indeed the pseudo-LiDAR representation.</p><p>Impact of stereo disparity estimation accuracy. We compare PSMNET <ref type="bibr" target="#b2">[3]</ref> and DISPNET <ref type="bibr" target="#b20">[21]</ref> on pseudo-LiDAR-based detection accuracies. On the leaderboard of KITTI stereo 2015, PSMNET achieves 1.86% disparity error, which far outperforms the error of 4.32% by DISPNET-C.</p><p>As shown in <ref type="table" target="#tab_2">Table 3</ref>, the accuracy of disparity estimation does not necessarily correlate with the accuracy of object detection. F-POINTNET with DISPNET-C even outperforms F-POINTNET with PSMNET. This is likely due to two reasons. First, the disparity accuracy may not reflect the depth accuracy: the same disparity error (on a pixel) can lead to drastically different depth errors dependent on the pixel's true depth, according to Eq. (1). Second, different detection algorithms process the 3D points differently: AVOD quantizes points into voxels, while F-POINTNET directly processes them and may be vulnerable to noise.</p><p>By far the most accurate detection results are obtained by PSMNET , which we trained from scratch on our own KITTI training set. These results seem to suggest that significant further improvements may be possible through endto-end training of the whole pipeline.</p><p>We provide results using SPS-STEREO <ref type="bibr" target="#b34">[35]</ref> and further analysis on depth estimation in the Supplementary Material.</p><p>Comparison to LiDAR information. Our approach significantly improves stereo-based detection accuracies. A key remaining question is, how close the pseudo-LiDAR   detection results are to those based on real LiDAR signal.</p><p>In <ref type="table" target="#tab_0">Table 1</ref>, we further compare to AVOD and F-POINTNET when actual LiDAR signal is available. For fair comparison, we retrain both models. For the easy cases with IoU = 0.5, our stereo-based approach performs very well, only slightly worse than the corresponding LiDAR-based version. However, as the instances become harder (e.g., for cars that are far away), the performance gaps resurfaces -although not nearly as pronounced as without pseudo-LiDAR. We also see a larger gap when moving to IoU = 0.7. These results are not surprising, since stereo algorithms are known to have larger depth errors for far-away objects, and a stricter metric requires higher depth precision. Both observations emphasize the need for accurate depth estimation, especially for far-away distances, to bridge the gap further. A key limitation of our results may be the low resolution of the 0.4 MegaPixel images, which cause far away objects to only consist of a few pixels.</p><p>Pedestrian and cyclist detection. We also present results on 3D pedestrian and cyclist detection. These are much more challenging tasks than car detection due to the small sizes of the objects, even given LiDAR signals. At an IoU threshold of 0.5, both AP BEV and AP 3D of pedestrians and cyclists are much lower than that of cars at IoU 0.7 <ref type="bibr" target="#b24">[25]</ref>. We also notice that none of the prior work on image-based methods report results in this category. <ref type="table" target="#tab_3">Table 4</ref> shows our results with F-POINTNET and compares to those with LiDAR, on the validation set. Compared to the car category (cf. Table 1), the performance gap is significant. We also observe a similar trend that the gap becomes larger when moving to the hard cases. Nevertheless, <ref type="figure">Figure 4</ref>: Qualitative comparison. We compare AVOD with LiDAR, pseudo-LiDAR, and frontal-view (stereo). Groundtruth boxes are in red, predicted boxes in green; the observer in the pseudo-LiDAR plots (bottom row) is on the very left side looking to the right. The frontal-view approach (right) even miscalculates the depths of nearby objects and misses far-away objects entirely. Best viewed in color. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results on the test set</head><p>We report our results on the car category on the test set in <ref type="table" target="#tab_4">Table 5</ref>. We see a similar gap between pseudo-LiDAR and LiDAR as on the validation set, suggesting that our approach does not simply over-fit to the "validation data." We also note that, at the time we submit the paper, we are at the first place among all the image-based algorithms on the KITTI leaderboard. Details and results on the pedestrian and cyclist categories are in the Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Visualization</head><p>We further visualize the prediction results on validation images in <ref type="figure">Fig. 4</ref>. We compare LiDAR (left), stereo pseudo-LiDAR (middle), and frontal stereo (right). We used PSMNET to obtain the stereo depth maps. LiDAR and pseudo-LiDAR lead to highly accurate predictions, especially for the nearby objects. However, pseudo-LiDAR fails to detect far-away objects precisely due to inaccurate depth estimates. On the other hand, the frontal-view-based approach makes extremely inaccurate predictions, even for nearby objects. This corroborates the quantitative results we observed in <ref type="table" target="#tab_1">Table 2</ref>. We provide additional qualitative results and failure cases in the Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion and Conclusion</head><p>Sometimes, it is the simple discoveries that make the biggest differences. In this paper we have shown that a key component to closing the gap between image-and LiDARbased 3D object detection may be simply the representation of the 3D information. It may be fair to consider these results as the correction of a systemic inefficiency rather than a novel algorithm -however, that does not diminish its importance. Our findings are consistent with our understanding of convolutional neural networks and substantiated through empirical results. In fact, the improvements we obtain from this correction are unprecedentedly high and affect all methods alike. With this quantum leap it is plausible that image-based 3D object detection for autonomous vehicle will become a reality in the near future. The implications of such a prospect are enormous. Currently, the LiDAR hardware is arguably the most expensive additional component required for robust autonomous driving. Without it, the additional hardware cost for autonomous driving becomes relatively minor. Further, image-based object detection would also be beneficial even in the presence of Li-DAR equipment. One could imagine a scenario where the LiDAR data is used to continuously train and fine-tune an image-based classifier. In case of our sensor outage, the image-based classifier could likely function as a very reliable backup. Similarly, one could imagine a setting where high-end cars are shipped with LiDAR hardware and continuously train the image-based classifiers that are used in cheaper models.</p><p>Future work. There are multiple immediate directions along which our results could be improved in future work:</p><p>First, higher resolution stereo images would likely significantly improve the accuracy for faraway objects. Our results were obtained with 0.4 megapixels -a far cry from the state-of-the-art camera technology. Second, in this paper we did not focus on real-time image processing and the classification of all objects in one image takes on the order of 1s. However, it is likely possible to improve these speeds by several orders of magnitude. Recent improvements on real-time multi-resolution depth estimation <ref type="bibr" target="#b30">[31]</ref> show that an effective way to speed up depth estimation is to first compute a depth map at low resolution and then incorporate high-resolution to refine the previous result. The conversion from a depth map to pseudo-LiDAR is very fast and it should be possible to drastically speed up the detection pipeline through e.g. model distillation <ref type="bibr" target="#b0">[1]</ref> or anytime prediction <ref type="bibr" target="#b14">[15]</ref>. Finally, it is likely that future work could improve the state-of-the-art in 3D object detection through sensor fusion of LiDAR and pseudo-LiDAR. Pseudo-LiDAR has the advantage that its signal is much denser than LiDAR and the two data modalities could have complementary strengths. We hope that our findings will cause a revival of image-based 3D object recognition and our progress will motivate the computer vision community to fully close the image/LiDAR gap in the near future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>In this Supplementary Material, we provide details omitted in the main text.</p><p>? Section A: additional details on our approach (Section 4.2 of the main paper).</p><p>? Section B: results using SPS-STEREO <ref type="bibr" target="#b34">[35]</ref> (Section 4.3 of the main paper).</p><p>? Section C: further analysis on depth estimation (Section 4.3 of the main paper).</p><p>? Section D: additional results on the test set (Section 4.4 of the main paper).</p><p>? Section E: additional qualitative results (Section 4.5 of the main paper).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional Details of Our Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Ground plane estimation</head><p>As mentioned in the main paper, AVOD <ref type="bibr" target="#b16">[17]</ref> takes image-specific ground planes as inputs. A ground plane is parameterized by a normal vector w = [w x , w y , w z ] ? R 3 and a ground height h ? R. We estimate the parameters according to the pseudo-LiDAR points {p (n) = [x (n) , y (n) , z (n) ] } N n=1 (see Section 3 of the main paper). Specifically, we consider points that are close to the camera and fall into a certain range of possible ground heights:</p><formula xml:id="formula_2">(width) 15.0 ? x ? ?15.0,<label>(5)</label></formula><formula xml:id="formula_3">(height) 1.86 ? y ? 1.5,<label>(6)</label></formula><formula xml:id="formula_4">(depth) 40.0 ? z ? 0.0.<label>(7)</label></formula><p>Ideally, all these points will be on the plane: w p + h = 0. We fit the parameters with a straight-forward application of RANSAC <ref type="bibr" target="#b8">[9]</ref>, in which we constraint w y = ?1. We then normalize the resulting w to have a unit 2 norm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Pseudo disparity ground truth</head><p>We train a version of PSMNET <ref type="bibr" target="#b2">[3]</ref> (named PSMNET ) using the 3,712 training images of detection, instead of the 200 KITTI stereo images <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b21">22]</ref>. We obtain pseudo disparity ground truth as follows: We project the corresponding LiDAR points into the 2D image space, followed by applying Eq. (1) of the main paper to derive disparity from pixel depth. If multiple LiDAR points are projected to a single pixel location, we randomly keep one of them. We ignore those pixels with no depth (disparity) in training PSMNET.  B. Results Using SPS-STEREO <ref type="bibr" target="#b34">[35]</ref> In <ref type="table" target="#tab_5">Table 6</ref>, we report the 3D object detection accuracy of pseudo-LiDAR with SPS-STEREO <ref type="bibr" target="#b34">[35]</ref>, a non-learningbased stereo disparity approach. On the leaderboard of KITTI stereo 2015, SPS-STEREO achieves 3.84% disparity error, which is worse than the error of 1.86% by PSMNET but better than 4.32% by DISPNET-C. The object detection results with SPS-STEREO are on par with those with PSM-NET and DISPNET, even if it is not learning-based.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Further Analysis on Depth Estimation</head><p>We study how over-smoothing the depth estimates would impact the 3D object detection accuracy. We train AVOD <ref type="bibr" target="#b16">[17]</ref> and F-POINTNET <ref type="bibr" target="#b24">[25]</ref> using pseudo-LiDAR with PSMNET . During evaluation, we obtain oversmoothed depth estimates using an average kernel of size 11 ? 11 on the depth map. <ref type="table" target="#tab_6">Table 7</ref> shows the results: oversmoothing leads to degraded performance, suggesting the importance of high quality depth estimation for accurate 3D object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Additional Results on the Test Set</head><p>We report the results on the pedestrian and cyclist categories on the KITTI test set in <ref type="table" target="#tab_7">Table 8</ref>. For F-POINTNET which takes 2D bounding boxes as inputs, <ref type="bibr" target="#b24">[25]</ref> does not provide the 2D object detector trained on KITTI or the detected 2D boxes on the test images. Therefore, for the car category we apply the released RRC detector <ref type="bibr" target="#b26">[27]</ref> trained on KITTI (see <ref type="table" target="#tab_4">Table 5</ref> in the main paper). For the pedestrian and cyclist categories, we apply Mask R-CNN <ref type="bibr" target="#b13">[14]</ref> trained on MS COCO <ref type="bibr" target="#b19">[20]</ref>. The detected 2D boxes are then inputted into  F-POINTNET <ref type="bibr" target="#b24">[25]</ref>. We note that, MS COCO has no cyclist category. We thus use the detection results of bicycles as the substitute.</p><p>On the pedestrian category, we see a similar gap between pseudo-LiDAR and LiDAR as the validation set (cf. <ref type="table" target="#tab_3">Table 4</ref> in the main paper). However, on the cyclist category we see a drastic performance drop by pseudo-LiDAR. This is likely due to the fact that cyclists are relatively uncommon in the KITTI dataset and the algorithms have over-fitted. For F-POINTNET, the detected bicycles may not provide accurate heights for cyclists, which essentially include riders and bicycles. Besides, the detected bicycles without riders are false positives to cyclists, hence leading to a much worse accuracy.</p><p>We note that, so far no image-based algorithms report 3D results on these two categories on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Additional Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1. LiDAR vs. pseudo-LiDAR</head><p>We include in <ref type="figure" target="#fig_3">Fig. 5</ref> more qualitative results comparing the LiDAR and pseudo-LiDAR signals. The pseudo-LiDAR points are generated by PSMNET . Similar to <ref type="figure" target="#fig_0">Fig. 1</ref> in the main paper, the two modalities align very well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. PSMNET vs. PSMNET</head><p>We further compare the pseudo-LiDAR points generated by PSMNET and PSMNET. The later is trained on the 200 KITTI stereo images with provided denser ground truths. As shown in <ref type="figure" target="#fig_5">Fig. 6</ref>, the two models perform fairly similarly for nearby distances. For far-away distances, however, the pseudo-LiDAR points by PSMNET start to show notable deviation from LiDAR signal. This result suggest that significant further improvements could be possible through learning disparity on a large training set or even end-to-end training of the whole pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3. Visualization and failure cases</head><p>We provide additional visualization of the prediction results (cf. Section 4.5 of the main paper). We consider AVOD with the following point clouds and representations.</p><p>? LiDAR ? frontal-view (stereo): with PSMNET <ref type="bibr" target="#b2">[3]</ref> We note that, as DORN <ref type="bibr" target="#b9">[10]</ref> applies ordinal regression, the predicted monocular depth are discretized.</p><p>As shown in <ref type="figure" target="#fig_6">Fig. 7</ref>, both LiDAR and pseudo-LiDAR (stereo or mono) lead to accurate predictions for the nearby objects. However, pseudo-LiDAR detects far-away objects less precisely (mislocalization: gray arrows) or even fails to detect them (missed detection: yellow arrows) due to in-accurate depth estimates, especially for the monocular depth. For example, pseudo-LiDAR (mono) completely misses the four cars in the middle. On the other hand, the frontal-view (stereo) based approach makes extremely inaccurate predictions, even for nearby objects.</p><p>To analyze the failure cases, we show the precision-recall (PR) curves on both 3D object and BEV detection in <ref type="figure">Fig. 8</ref>. The pseudo-LiDAR-based detection has a much lower recall compared to the LiDAR-based one, especially for the moderate and hard cases (i.e., far-away or occluded objects). That is, missed detections are one major issue that pseudo-LiDAR-based detection needs to resolve.</p><p>We provide another qualitative result for failure cases in <ref type="figure">Fig. 9</ref>. The partially occluded car is missed detected by AVOD with pseudo-LiDAR (the yellow arrow) even if it is close to the observer, which likely indicates that stereo disparity approaches suffer from noisy estimation around occlusion boundaries.  <ref type="figure">Figure 8</ref>: Precision-recall curves. We compare the precision and recall on AVOD using pseudo-LiDAR with PSMNET (top) and using LiDAR (bottom) on the test set. We obtain the curves from the KITTI website. We show both the 3D detection results (left) and the BEV detection results (right). AVOD using pseudo-LiDAR has a much lower recall, suggesting that missed detections are one of the major issues of pseudo-LiDAR-based detection. <ref type="figure">Figure 9</ref>: Qualitative comparison and failure cases. We compare AVOD with LiDAR and pseudo-LiDAR (stereo). Ground-truth boxes are in red; predicted boxes in green. The observer in the pseudo-LiDAR plots (bottom row) is on the bottom side looking to the top. The pseudo-LiDAR-based detection misses the partially occluded car (the yellow arrow), which is a hard case. Best viewed in color.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Pseudo-LiDAR signal from visual depth estimation. Top-left: a KITTI street scene with super-imposed bounding boxes around cars obtained with LiDAR (red) and pseudo-LiDAR (green). Bottom-left: estimated disparity map. Right: pseudo-LiDAR (blue) vs. LiDAR (yellow)the pseudo-LiDAR points align remarkably well with the LiDAR ones. Best viewed in color (zoom in for details.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>We apply a single 2D convolution with a uniform kernel to the frontal view depth map (top-left). The resulting depth map (top-right), after back-projected into pseudo-LiDAR and displayed from the bird's-eye view (bottomright), reveals a large depth distortion in comparison to the original pseudo-LiDAR representation (bottom-left), especially for far-away objects. We mark points of each car instance by a color. The boxes are super-imposed and contain all points of the green and cyan cars respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Pseudo-LiDAR signal from visual depth estimation. Top-left: a KITTI street scene with super-imposed bounding boxes around cars obtained with LiDAR (red) and pseudo-LiDAR (green). Bottom-left: estimated disparity map. Right: pseudo-LiDAR (blue) vs. LiDAR (yellow)the pseudo-LiDAR points align remarkably well with the LiDAR ones. Best viewed in color (zoom in for details).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>?</head><label></label><figDesc>pseudo-LiDAR (stereo): with PSMNET [3]? pseudo-LiDAR (mono): with DORN<ref type="bibr" target="#b9">[10]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>PSMNET vs. PSMNET . Top: a KITTI street scene. Left column: the depth map and pseudo-LiDAR points (from the bird's-eye view) by PSMNET, together with a zoomed-in region. Right column: the corresponding results by PSMNET . The observer is on the very right side looking to the left. The pseudo-LiDAR points are in blue; LiDAR points are in yellow. The pseudo-LiDAR points by PSMNET have larger deviation at far-away distances. Best viewed in color (zoom in for details).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative comparison and failure cases. We compare AVOD with LiDAR, pseudo-LiDAR (stereo), pseudo-LiDAR (monocular), and frontal-view (stereo). Ground-truth boxes are in red; predicted boxes in green. The observer in the pseudo-LiDAR plots (bottom row) is on the very left side looking to the right. The mislocalization cases are indicated by gray arrows; the missed detection cases are indicated by yellow arrows. The frontal-view approach (bottom-right) makes extremely inaccurate predictions, even for nearby objects. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>3D object detection results on the KITTI validation set. We report APBEV / AP3D (in %) of the car category, corresponding to average precision of the bird's-eye view and 3D object box detection. Mono stands for monocular. Our methods with pseudo-LiDAR estimated by PSMNET<ref type="bibr" target="#b2">[3]</ref> (stereo) or DORN<ref type="bibr" target="#b9">[10]</ref> (monocular) are in blue. Methods with LiDAR are in gray. Best viewed in color.</figDesc><table><row><cell>IoU = 0.5</cell><cell>IoU = 0.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell cols="4">Detection Disparity Representation AP BEV / AP 3D</cell></row><row><cell cols="2">MLF [33] DISPNET</cell><cell>Frontal</cell><cell>19.5 / 9.8</cell></row><row><cell>AVOD</cell><cell cols="3">DISPNET-S Pseudo-LiDAR 36.3 / 27.0</cell></row><row><cell>AVOD</cell><cell cols="3">DISPNET-C Pseudo-LiDAR 36.5 / 26.2</cell></row><row><cell>AVOD</cell><cell>PSMNET</cell><cell>Frontal</cell><cell>11.9 / 6.6</cell></row><row><cell>AVOD</cell><cell>PSMNET</cell><cell cols="2">Pseudo-LiDAR 56.8 / 45.3</cell></row></table><note>Comparison between frontal and pseudo-LiDAR repre- sentations. AVOD projects the pseudo-LiDAR representation into the bird-eye's view (BEV). We report APBEV / AP3D (in %) of the moderate car category at IoU = 0.7. The best result of each col- umn is in bold font. The results indicate strongly that the data representation is the key contributor to the accuracy gap.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison of different combinations of stereo disparity and 3D object detection algorithms, using pseudo-LiDAR. We report APBEV / AP3D (in %) of the moderate car category at IoU = 0.7. The best result of each column is in bold font.</figDesc><table><row><cell></cell><cell cols="2">Detection algorithm</cell></row><row><cell>Disparity</cell><cell>AVOD</cell><cell>F-POINTNET</cell></row><row><cell cols="2">DISPNET-S 36.3 / 27.0</cell><cell>31.9 / 23.5</cell></row><row><cell cols="2">DISPNET-C 36.5 / 26.2</cell><cell>37.4 / 29.2</cell></row><row><cell>PSMNET</cell><cell>39.2 / 27.4</cell><cell>33.7 / 26.7</cell></row><row><cell>PSMNET</cell><cell>56.8 / 45.3</cell><cell>51.8 / 39.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>3D object detection on the pedestrian and cyclist cat-</figDesc><table><row><cell cols="4">egories on the validation set. We report APBEV / AP3D at IoU =</cell></row><row><cell cols="4">0.5 (the standard metric) and compare F-POINTNET with pseudo-</cell></row><row><cell cols="4">LiDAR estimated by PSMNET (in blue) and LiDAR (in gray).</cell></row><row><cell>Input signal</cell><cell>Easy</cell><cell>Moderate</cell><cell>Hard</cell></row><row><cell></cell><cell cols="2">Pedestrian</cell><cell></cell></row><row><cell>Stereo</cell><cell cols="3">41.3 / 33.8 34.9 / 27.4 30.1 / 24.0</cell></row><row><cell cols="4">LiDAR + Mono 69.7 / 64.7 60.6 / 56.5 53.4 / 49.9</cell></row><row><cell></cell><cell>Cyclist</cell><cell></cell><cell></cell></row><row><cell>Stereo</cell><cell cols="3">47.6 / 41.3 29.9 / 25.2 27.0 / 24.9</cell></row><row><cell cols="4">LiDAR + Mono 70.3 / 66.6 55.0 / 50.9 52.0 / 46.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>3D object detection results on the car category on the test set. We compare pseudo-LiDAR with PSMNET (in blue) and LiDAR (in gray). We report APBEV / AP3D at IoU = 0.7. ?: Results on the KITTI leaderboard.</figDesc><table><row><cell>Input signal</cell><cell>Easy</cell><cell>Moderate</cell><cell>Hard</cell></row><row><cell></cell><cell>AVOD</cell><cell></cell><cell></cell></row><row><cell>Stereo</cell><cell cols="3">66.8 / 55.4 47.2 / 37.2 40.3 / 31.4</cell></row><row><cell cols="4">?LiDAR + Mono 88.5 / 81.9 83.8 / 71.9 77.9 / 66.4</cell></row><row><cell></cell><cell cols="2">F-POINTNET</cell><cell></cell></row><row><cell>Stereo</cell><cell cols="3">55.0 / 39.7 38.7 / 26.7 32.9 / 22.3</cell></row><row><cell cols="4">?LiDAR + Mono 88.7 / 81.2 84.0 / 70.4 75.3 / 62.2</cell></row><row><cell cols="4">our approach has set a solid starting point for image-based</cell></row><row><cell cols="3">pedestrian and cyclist detection for future work.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Comparison of different stereo disparity methods on pseudo-LiDAR-based detection accuracy with AVOD. We report APBEV / AP3D (in %) of the moderate car category at IoU = 0.7.</figDesc><table><row><cell>Method</cell><cell>Disparity</cell><cell>AP BEV / AP 3D</cell></row><row><cell></cell><cell>SPS-STEREO</cell><cell>39.1 / 28.3</cell></row><row><cell></cell><cell>DISPNET-S</cell><cell>36.3 / 27.0</cell></row><row><cell cols="2">AVOD DISPNET-C</cell><cell>36.5 / 26.2</cell></row><row><cell></cell><cell>PSMNET</cell><cell>39.2 / 27.4</cell></row><row><cell></cell><cell>PSMNET</cell><cell>56.8 / 45.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table><row><cell cols="3">The impact of over-smoothing the depth estimates</cell></row><row><cell cols="3">on the 3D detection results. We evaluate pseudo-LiDAR with</cell></row><row><cell cols="3">PSMNET . We report APBEV / AP3D (in %) of the moderate car</cell></row><row><cell cols="3">category at IoU = 0.7 on the validation set.</cell></row><row><cell></cell><cell cols="2">Detection algorithm</cell></row><row><cell>Depth estimates</cell><cell>AVOD</cell><cell>F-POINTNET</cell></row><row><cell>Non-smoothed</cell><cell>56.8 / 45.3</cell><cell>51.8 / 39.8</cell></row><row><cell>Over-smoothed</cell><cell>53.7 / 37.8</cell><cell>48.3 / 31.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>3D object detection results on the pedestrian and cyclist categories on the test set. We compare pseudo-LiDAR with PSMNET (in blue) and LiDAR (in gray). We report APBEV / AP3D at IoU = 0.5 (the standard metric). ?: Results on the KITTI leaderboard.</figDesc><table><row><cell>Method</cell><cell>Input signal</cell><cell>Easy</cell><cell>Moderate</cell><cell>Hard</cell></row><row><cell></cell><cell cols="2">Pedestrian</cell><cell></cell><cell></cell></row><row><cell>AVOD</cell><cell>Stereo</cell><cell cols="3">27.5 / 25.2 20.6 / 19.0 19.4 / 15.3</cell></row><row><cell>F-POINTNET</cell><cell>Stereo</cell><cell cols="3">31.3 / 29.8 24.0 / 22.1 21.9 / 18.8</cell></row><row><cell>AVOD</cell><cell cols="4">?LiDAR + Mono 58.8 / 50.8 51.1 / 42.8 47.5 / 40.9</cell></row><row><cell cols="5">F-POINTNET  ?LiDAR + Mono 58.1 / 51.2 50.2 / 44.9 47.2 / 40.2</cell></row><row><cell></cell><cell></cell><cell>Cyclist</cell><cell></cell><cell></cell></row><row><cell>AVOD</cell><cell>Stereo</cell><cell cols="3">13.5 / 13.3 9.1 / 9.1 9.1 / 9.1</cell></row><row><cell>F-POINTNET</cell><cell>Stereo</cell><cell cols="3">4.1 / 3.7 3.1 / 2.8 2.8 / 2.1</cell></row><row><cell>AVOD</cell><cell cols="4">?LiDAR + Mono 68.1 / 64.0 57.5 / 52.2 50.8 / 46.6</cell></row><row><cell cols="5">F-POINTNET  ?LiDAR + Mono 75.4 / 72.0 62.0 / 56.8 54.7 / 50.4</cell></row><row><cell cols="2">Input</cell><cell cols="3">Pseudo-Lidar (Bird's-eye View)</cell></row><row><cell cols="2">Depth Map</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Pseudo-LiDAR (Stereo)Front-View (Stereo)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research is supported in part by grants from the National Science Foundation (III-1618134, III-1526012, IIS-1149882, IIS-1724282, and TRIPODS-1740822), the Office of Naval Research DOD (N00014-17-1-2175), and the Bill and Melinda Gates Foundation. We are thankful for generous support by Zillow and SAP America Inc. We thank Gao Huang for helpful discussion.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bucilu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep manta: A coarse-to-fine many-task network for joint 2d and 3d vehicle analysis from monocular image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chabot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chaouch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rabarisoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Teuli?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chateau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pyramid stereo matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-R</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">3d object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3d object proposals using stereo imagery for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1259" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2366" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Multi-scale dense convolutional networks for efficient prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>abs/1703.09844</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Depth extraction from video using non-parametric sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint 3d proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep continuous fusion for multi-sensor 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">3d bounding box estimation using deep learning and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ko?eck?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Robust object proposals reranking for object detection in autonomous driving using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Jeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Processing: Image Communication</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="110" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Accurate single stage detector using recurrent rolling convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Make3d: Learning 3d scene structure from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="824" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep parametric continuous convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><forename type="middle">M A</forename><surname>Pokrovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Anytime stereo image depth estimation on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.11408</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Subcategoryaware convolutional neural networks for object proposals and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-level fusion based 3d object detection from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pointfusion: Deep sensor fusion for 3d bounding box estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Efficient joint segmentation, occlusion labeling, stereo and flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pixor: Real-time 3d object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
