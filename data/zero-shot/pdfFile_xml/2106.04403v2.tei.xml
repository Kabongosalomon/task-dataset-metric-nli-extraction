<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SynthRef: Generation of Synthetic Referring Expressions for Object Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Kazakos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universitat Polit?cnica de Catalunya</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">National Technical University of Athens</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carles</forename><surname>Ventura</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Universitat Oberta de Catalunya</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M?riam</forename><surname>Bellver</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universitat Polit?cnica de Catalunya</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Barcelona Supercomputing Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carina</forename><surname>Silberer</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">University of Stuttgart</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Gir?-I-Nieto</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universitat Polit?cnica de Catalunya</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Barcelona Supercomputing Center</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SynthRef: Generation of Synthetic Referring Expressions for Object Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances in deep learning have brought significant progress in visual grounding tasks such as language-guided video object segmentation.</p><p>However, collecting large datasets for these tasks is expensive in terms of annotation time, which represents a bottleneck. To this end, we propose a novel method, namely SynthRef, for generating synthetic referring expressions for target objects in an image (or video frame), and we also present and disseminate the first large-scale dataset with synthetic referring expressions for video object segmentation. Our experiments demonstrate that by training with our synthetic referring expressions one can improve the ability of a model to generalize across different datasets, without any additional annotation cost. Moreover, our formulation allows its application to any object detection or segmentation dataset. Project site:</p><p>https://imatge-upc. github.io/synthref/</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Visual grounding tasks provide challenging benchmarks for artificial intelligence systems, as they must combine vision and language effectively. Among them, we focus on referring video object segmentation, in which a language query defines which instance to segment from a video sequence. In particular, we define referring expressions (REs) as linguistic phrases that allow the unique identification of an individual object (the referent) in a discourse or scene. (cf., <ref type="bibr" target="#b11">Reiter and Dale 1992;</ref><ref type="bibr" target="#b10">Qiao et al. 2020)</ref>. One of the biggest challenges for this task is the lack of relatively large annotated datasets since a tremendous amount of time and human effort is required for annotation.</p><p>Using referring expressions to identify objects in the real world lies at the core of human communication. Their use for segmenting objects in images has been previously addressed <ref type="bibr" target="#b3">(Hu et al., 2016;</ref><ref type="bibr" target="#b8">Liu et al., 2017;</ref><ref type="bibr" target="#b19">Yu et al., 2018;</ref><ref type="bibr" target="#b18">Ye et al., 2019;</ref><ref type="bibr" target="#b1">Chen et al., 2019)</ref> and has benefited from large scale datasets, such as RefCOCO <ref type="bibr" target="#b5">(Kazemzadeh et al., 2014)</ref>. However, fewer works have explored the segmentation of objects using REs in the video domain, although this provides the more natural setup compared to the image domain. Humans use referring expressions to identify objects for others in a moving world, better represented by videos than by still images. <ref type="bibr" target="#b6">Khoreva et al. (2018)</ref> were the first to transfer the referring expression segmentation task from images to videos by collecting referring expressions for the <ref type="bibr">DAVIS-2017</ref><ref type="bibr" target="#b9">(Pont-Tuset et al., 2017</ref> dataset. Later <ref type="bibr" target="#b2">Gavrilyuk et al. (2018)</ref> provided natural language descriptions as guidance for actor segmentation in A2D <ref type="bibr" target="#b15">(Xu et al., 2015)</ref> and J-HMDB <ref type="bibr" target="#b4">(Jhuang et al., 2013)</ref>, two datasets used for action and human pose recognition and segmentation. Finally, the first large-scale benchmark for referring video object segmentation, Refer-YouTube-VOS, was built by <ref type="bibr" target="#b13">Seo et al. (2020)</ref> on top of YouTube-VOS <ref type="bibr" target="#b16">(Xu et al., 2018)</ref>, a benchmark for video object segmentation.</p><p>As an alternative to collecting REs from annotators, we propose generating synthetic referring expressions for an image, using only the ground truth annotations of objects and their predicted visual attributes from an off-the-shelf deep learning model. We apply this method to build a large-scale dataset with synthetic referring expressions for video object segmentation, based on an existing benchmark dataset for video instance segmentation. We use our synthetic dataset for pretraining a deep neural network for the task of referring video object segmentation and evaluate our method on two benchmark datasets used in language-guided video object segmentation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method and Dataset</head><p>Our synthetic referring expressions are based on the ground-truth annotations of YouTube-VIS  dataset, described in the supplementary material. Specifically, we use the classes and bounding boxes of the target and other objects in a video frame, to determine a set of cues from which we heuristically generate a referring expression that is close to a natural language expression. We call our approach SynthRef, illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. We use the following four cues for generating referring expressions for a target object: 1. Object class In trivial cases where a single object of a known class is present, using the object class is enough to generate a referring expression. However, most cases involve multiple objects of the same class, thus other cues are necessary in order to disambiguate between instances. 2. Relative size If the total area of the bounding box of the referent is twice as big/small than the area(s) of the respective bounding boxes of the other object(s) of the same class, a characterization of "bigger/smaller" or "the biggest/smallest" is added to the synthetic referring expression, e.g. , "the smallest dog". 3. Relative location In scenarios where two or three objects of the same class are present in a video frame, relative location between these objects may suffice to disambiguate between them. If the bounding boxes of the objects are fully separable, or partially above a certain threshold, then we assume that relative location of the referent with respect to the other object(s) of the same class can be used in order to generate a non-ambiguous referring phrase. In this case, the steps for determining relative location are the following:</p><p>1. The axis which is the most separative for the bounding boxes of the two objects is determined. 2. According to the axis found and the position of the bounding boxes, a relative location description is given out of 4 options: {"on the right", "on the left", "in the back", "in the front"}. 3. If there are two other objects of the same class, steps 1 &amp; 2 are computed between the referent and each of the two other objects, and the results are combined, e.g. , "in the middle", "in the back right", etc. . referent, as long as its IoU is over 50%. <ref type="figure" target="#fig_0">Figure 1</ref> shows the full pipeline: <ref type="bibr">Tang et al.'s (2020)</ref> model can detect a total of 201 attributes, which we group to color-like and not color-like attributes, where the latter can be both adjectives (e.g. , "large", "spotted") or verbs (e.g. , "walking", "surfing").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Attributes</head><p>The ones with the highest prediction score, if above a certain threshold, are selected for the two subsets, while combinations of two colors are also possible if their scores are very close (e.g. , "a yellow and green parrot"). We add an attribute to the referring expression only if no other objects belonging to the same class share the same attribute, so that the expression is able to disambiguate between instances.</p><p>Finally, we combine the aforementioned components in a natural order and add a proper article to the sentence, ending up with a synthetic referring expression. There might be cases where the generated synthetic language expression may be ambiguous, especially in cases of many similar objects of the same class, although in most cases the generated expression uniquely identifies the referent. SynthRef treats each video frame separately, so we do not force any temporal coherence of the REs. Actually, since an object may change its location or appearance throughout the video, we generate one or more synthetic referring expressions for each frame of the video. In this way, a model can be trained with different referring expressions for the same video or frame increasing its ability to generalize.</p><p>Basic statistics of our SynthRef-YouTube-VIS dataset and a comparison with other relevant ones are presented in <ref type="table" target="#tab_0">Table 1</ref>. The comparison shows that our dataset, despite not being the largest one in terms of number of annotated objects and their categories, it still has the highest average number of unique referring expressions per annotated object (4.2) without involving any human annotation cost. The average number of words in our referring expressions is 4.4, which is smaller than those of the other datasets as our goal is to generate simple and efficient synthetic referring expressions. We point out three limitations of our dataset/method: (a) the predicted attributes may be wrong or not disambiguating, (b) the relative location is not applied for more than three objects of the same class, and (c) when none of our rules can be applied, SynthRef uses just the object class (e.g. "a dog"), even if there are more instances of that class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We show the benefits of our synthetic dataset SynthRef-YouTube-VIS by using it in extending the training dataset of RefVOS <ref type="bibr" target="#b0">(Bellver et al., 2020)</ref>, a state of the art model for referring video object segmentation. The first experiments focus on <ref type="bibr">DAVIS-2017</ref><ref type="bibr" target="#b9">(Pont-Tuset et al., 2017</ref>, and the latter on Refer-YouTube-VOS <ref type="bibr" target="#b13">(Seo et al., 2020)</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DAVIS-2017</head><p>We report the gains of adding the synthetic dataset when evaluating on the standard validation partition of DAVIS-2017 (30 videos), but also on the combined training and validation partitions (90 videos), to obtain more statistically significant results. The results in <ref type="table" target="#tab_2">Table 2</ref> show a significant improvement in segmentation accuracy when adding our synthetic REs to RefCOCO. Model SynthRef J&amp;F? <ref type="bibr" target="#b6">(Khoreva et al., 2018)</ref> 39.3 <ref type="bibr" target="#b13">(Seo et al., 2020)</ref> 44.1 <ref type="bibr" target="#b0">(Bellver et al., 2020)</ref> 45.1 <ref type="bibr" target="#b0">(Bellver et al., 2020)</ref> 45.3  <ref type="table" target="#tab_2">Table 2.</ref> results for this scenario, where the improvement of the segmentation masks is cleary visible. The gain is minor when RefVOS is fine-tuned with training data from DAVIS-2017 and evaluated on the validation partition, as shown in <ref type="table" target="#tab_3">Table 3</ref>. This setup is the commonly adopted by the related work, allowing a comparison of our results with them.   <ref type="table" target="#tab_5">Table 4</ref>, indicate that, even though the model trained on human referring expressions outperforms the model trained on synthetic ones, the drop in accuracy is not that big to prevent the use of our synthetic data for training. On the contrary, the obtained numbers show that our synthetic expressions can be used interchangeably with the human ones when the latter are hard to acquire.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this work, we propose SynthRef, a novel method for generating synthetic referring expressions, which is used to create the first large-scale dataset of synthetic referring expressions for video object segmentation, namely SynthRef-YouTube-VIS. Our experiments show that pretraining a model using our synthetic referring expressions increases its capability to generalize on new data, which is very important in scenarios where training data are not available for a target dataset. Our method, that does not involve any human annotation cost, can be applied to other existing datasets and tasks (e.g. object detection or text-to-image retrieval). We invite the community to explore further possibilities and benefits out of it.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of our method for generating synthetic referring expressions. Top: Ground truth labels (object class + bounding boxes) are used to compute a target object's relative location and size. Bottom: A Faster R-CNN object detector with attribute head predicts visual attributes for the detected objects, which are filtered by ground truth annotations. The combined cues create a set of referring expressions that uniquely describe the target object.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FigureFigure 2 :</head><label>2</label><figDesc>Qualitative results onDAVIS-2017. Subfigure 2a (left)  shows results when the model is pretrained only on RefCOCO, while Subfigure 2b (right) when it is also trained on our synthetic dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of our dataset and comparison to existing ones. The last two columns represent the average number of unique referring expressions per object and the average number of words per referring expression respectively.</figDesc><table><row><cell>We pretrain Faster R-CNN (Ren</cell></row><row><cell>et al., 2015) on Visual Genome (Krishna et al.,</cell></row><row><cell>2017) for object and attribute detection (Tang</cell></row><row><cell>et al., 2020). This model analyzed the video</cell></row><row><cell>frames of YouTube-VIS (Yang et al., 2019) dataset</cell></row><row><cell>to obtain, for each frame of a video, a set</cell></row><row><cell>of detected objects (with their bounding box</cell></row></table><note>coordinates) and their predicted attributes. The detected bounding box with the highest overlap, in terms of Intersection-over-Union (IoU), with the ground truth bounding box of the target object is considered as the prediction corresponding to the</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: Segmentation accuracy obtained with RefVOS</cell></row><row><cell>model on two partitionns of DAVIS-2017: validation</cell></row><row><cell>(val) or training+validation (train+val). Adding our</cell></row><row><cell>SynthRef-YouTube-VIS data significantly increases</cell></row><row><cell>the performance at a zero-cost in annotation. The J&amp;F</cell></row><row><cell>metric is defined in the supplementary material.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Comparison with the state of the art in</cell></row><row><cell>DAVIS-2017 validation, with models pretrained on</cell></row><row><cell>RefCOCO and fine-tuned with DAVIS-2017 training</cell></row><row><cell>data. Adding our generated SynthRef-YouTube-VIS</cell></row><row><cell>dataset to the RefCOCO pretraining achieves state of</cell></row><row><cell>the art results. However the relative gain is smaller than</cell></row><row><cell>in the scenario without fine-tuning, reported in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparison of the performance on a subset of Refer-YouTube-VOS when training with synthetic and human referring expressions.</figDesc><table><row><cell>Refer-YouTube-VOS We further evaluate our</cell></row><row><cell>method using the subset of Refer-YouTube-VOS</cell></row><row><cell>that corresponds to our synthetic dataset,</cell></row><row><cell>SynthRef-YouTube-VIS. We train two instances of</cell></row><row><cell>RefVOS, one using the human-produced REs of</cell></row><row><cell>Refer-YouTube-VOS and one using the synthetic</cell></row><row><cell>REs of SynthRef-YouTube-VIS. The evaluation is</cell></row><row><cell>done on the test split of SynthRef-YouTube-VIS</cell></row><row><cell>but using the human REs of Refer-YouTube-VOS</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partially supported by the EU Erasmus+ programme, and the Spanish Ministry of Economy and Competitivity under grants TEC2016-75976-R (UPC) and RTI2018-095232-B-C22 (UOC). We gratefully acknowledge the support of NVIDIA Corporation with the donation of GPUs used in this work. The authors would like to thank Yannis Kalantidis, Konstantinos Karantzalos and the anonymous reviewers for their valuable feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Refvos: A closer look at referring expressions for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Bellver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carles</forename><surname>Ventura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carina</forename><surname>Silberer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier Giro-I</forename><surname>Nieto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.00263</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">See-through-text grouping for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding-Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Chen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwann-Tzong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyng-Luh</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7454" to="7463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Actor and action video segmentation from a sentence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirill</forename><surname>Gavrilyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5958" to="5966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Segmentation from natural language expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="108" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards understanding action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conf. on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3192" to="3199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ReferItGame: Referring to Objects in Photographs of Natural Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahar</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="787" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Video object segmentation with language referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="123" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recurrent multimodal interaction for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1271" to="1280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergi</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Referring expression comprehension: A survey of methods and datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A fast algorithm for the generation of referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<idno type="DOI">10.3115/992066.992105</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference on Computational Linguistics</title>
		<meeting>the 14th Conference on Computational Linguistics<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1992" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="232" to="238" />
		</imprint>
	</monogr>
	<note>COLING &apos;92</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Urvos: Unified referring video object segmentation network with a large-scale benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonguk</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unbiased scene graph generation from biased training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3716" to="3725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Can humans fly? Action understanding with multiple classes of actors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Youtube-vos: Sequence-to-sequence video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingcheng</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="585" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5188" to="5197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cross-modal self-attention network for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linwei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrigank</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10502" to="10511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mattnet: Modular attention network for referring expression comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1307" to="1315" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
