<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Few-Shot Learning via Embedding Adaptation with Set-to-Set Functions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han-Jia</forename><surname>Ye</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Hu</surname></persName>
							<email>hexiangh@usc.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-Chuan</forename><surname>Zhan</surname></persName>
							<email>zhandc@lamda.nju.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
							<email>fsha@google.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">USC</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Nanjing University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">USC &amp; Google</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Few-Shot Learning via Embedding Adaptation with Set-to-Set Functions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning with limited data is a key challenge for visual recognition. Many few-shot learning methods address this challenge by learning an instance embedding function from seen classes and apply the function to instances from unseen classes with limited labels. This style of transfer learning is task-agnostic: the embedding function is not learned optimally discriminative with respect to the unseen classes, where discerning among them leads to the target task. In this paper, we propose a novel approach to adapt the instance embeddings to the target classification task with a set-to-set function, yielding embeddings that are task-specific and are discriminative. We empirically investigated various instantiations of such set-to-set functions and observed the Transformer is most effective -as it naturally satisfies key properties of our desired model. We denote this model as FEAT (few-shot embedding adaptation w/ Transformer) and validate it on both the standard few-shot classification benchmark and four extended few-shot learning settings with essential use cases, i.e., cross-domain, transductive, generalized few-shot learning, and low-shot learning. It archived consistent improvements over baseline models as well as previous methods, and established the new stateof-the-art results on two benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Few-shot visual recognition <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b48">49]</ref> emerged as a promising direction in tackling the challenge of learning new visual concepts with limited annotations. Concretely, it distinguishes two sets of visual concepts: SEEN and UNSEEN ones. The target task is to construct visual classifiers to identify classes from the UNSEEN where each class has a very small number of exemplars ("few-shot"). The main idea is to discover transferable visual knowledge in the SEEN classes, which have ample labeled instances, and leverage it to construct the desired classifier. For example, state-of-the-art approaches for few-shot learn-* Work mostly done when the author was a visiting scholar at USC. ? On leave from USC ing <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b48">49]</ref> usually learn a discriminative instance embedding model on the SEEN categories, and apply it to visual data in UNSEEN categories. In this common embedding space, non-parametric classifiers (e.g., nearest neighbors) are then used to avoid learning complicated recognition models from a small number of examples.</p><p>Such approaches suffer from one important limitation. Assuming a common embedding space implies that the discovered knowledge -discriminative visual features -on the SEEN classes are equally effective for any classification tasks constructed for an arbitrary set of UNSEEN classes. In concrete words, suppose we have two different target tasks: discerning "cat" versus "dog" and discerning "cat" versus "tiger". Intuitively, each task uses a different set of discriminative features. Thus, the most desired embedding model first needs to be able to extract discerning features for either task at the same time. This could be a challenging aspect in its own right as the current approaches are agnostic to what those "downstream" target tasks are and could accidentally de-emphasize selecting features for future use. Secondly, even if both sets of discriminative features are extracted, they do not necessarily lead to the optimal performance for a specific target task. The most useful features for discerning "cat" versus "tiger" could be irrelevant and noise to the task of discerning "cat" versus "dog"! What is missing from the current few-shot learning approaches is an adaptation strategy that tailors the visual knowledge extracted from the SEEN classes to the UNSEEN ones in a target task. In other words, we desire separate embedding spaces where each one of them is customized such that the visual features are most discriminative for a given task. Towards this, we propose a few-shot model-based embedding adaptation method that adjusts the instance embedding models derived from the SEEN classes. Such modelbased embedding adaptation requires a set-to-set function: a function mapping that takes all instances from the few-shot support set and outputs the set of adapted support instance embeddings, with elements in the set co-adapting with each other. Such output embeddings are then assembled as the prototypes for each visual category and serve as the nearest neighbor classifiers. <ref type="figure">Figure 1</ref> qualitatively illustrates the embedding adaptation procedure (as results of our best model). These class prototypes spread out in the embedding space toward the samples cluster of each category, indicating the effectiveness of embedding adaptation.</p><p>In this paper, we implement the set-to-set transformation using a variety of function approximators, including bidirectional LSTM <ref type="bibr" target="#b15">[16]</ref> (Bi-LSTM), deep sets <ref type="bibr" target="#b55">[56]</ref>, graph convolutional network (GCN) <ref type="bibr" target="#b20">[21]</ref>, and Transformer <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b46">47]</ref>. Our experimental results (refer to ? 5.2.1) suggest that Transformer is the most parameter efficient choice that at the same time best implements the key properties of the desired set-to-set transformation, including contextualization, permutation invariance, interpolation and extrapolation capabilities (see ? 4.1). As a consequence, we choose the set-to-set function instantiated with Transformer to be our final model and denote it as FEAT (Few-shot Embedding Adaptation with Transformer). We further conduct comprehensive analysis on FEAT and evaluate it on many extended tasks, including few-shot domain generalization, transductive few-shot learning, and generalized few-shot learning. Our overall contribution is three-fold. ? We formulate the few-shot learning as a model-based embedding adaptation to make instance embeddings taskspecific, via using a set-to-set transformation.</p><p>? We instantiate such set-to-set transformation with various function approximators, validating and analyzing their few-shot learning ability, task interpolation ability, and extrapolation ability, etc. It concludes our model (FEAT) that uses the Transformer as the set-to-set function.</p><p>? We evaluate our FEAT model on a variety of extended few-shot learning tasks, where it achieves superior performances compared with strong baseline approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Methods specifically designed for few-shot learning fall broadly into two categories. The first is to control how a classifier for the target task should be constructed. One fruitful idea is the meta-learning framework where the classifiers are optimized in anticipation that a future update due to data from a new task performs well on that task <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b39">40]</ref>, or the classifier itself is directly meta-predicted by the new task data <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b52">53]</ref>.</p><p>Another line of approach has focused on learning generalizable instance embeddings <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b48">49]</ref> and uses those embeddings on simple classifiers such as nearest neighbor rules. The key assumption is that the embeddings capture all necessarily discriminative representations of data such that simple classifiers are sufficed, hence avoiding the danger of overfitting on a small number of labeled instances. Early work such as <ref type="bibr" target="#b21">[22]</ref> first validated the importance of embedding in one-shot learning, whilst <ref type="bibr" target="#b48">[49]</ref> proposes to learn the embedding with a soft nearest neighbor objective, following a meta-learning routine. Recent advances have leveraged different objective functions for learning such embedding models, e.g., considering the class prototypes <ref type="bibr" target="#b42">[43]</ref>, decision ranking <ref type="bibr" target="#b45">[46]</ref>, and similarity comparison <ref type="bibr" target="#b44">[45]</ref>. Most recently, <ref type="bibr" target="#b40">[41]</ref> utilizes the graph convolution network <ref type="bibr" target="#b20">[21]</ref> to unify the embedding learning.</p><p>Our work follows the second school of thoughts. The main difference is that we do not assume the embeddings learned on SEEN classes, being agnostic to the target tasks, are necessarily discriminative for those tasks. In contrast, we propose to adapt those embeddings for each target task with a set-to-set function so that the transformed embeddings are better aligned with the discrimination needed in those tasks. We show empirically that such task-specific embeddings perform better than task-agnostic ones. MetaOptNet <ref type="bibr" target="#b24">[25]</ref> and CTM <ref type="bibr" target="#b27">[28]</ref> follow the same spirit of learning task-specific embedding (or classifiers) via either explicitly optimization of target task or using concentrator and projector to make distance metric task-specific. <ref type="figure">Figure 2</ref>: Illustration of the proposed Few-Shot Embedding Adaptation Transformer (FEAT). Existing methods usually use the same embedding function E for all tasks. We propose to adapt the embeddings to each target few-shot learning task with a set-to-set function such as Transformer, BiLSTM, DeepSets, and GCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Learning Embedding for Task-agnostic FSL</head><p>In the standard formulation of few-shot learning (FSL) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b48">49]</ref>, a task is represented as a M -shot N -way classification problem with N classes sampled from a set of visual concepts U and M (training/support) examples per class. We denote the training set (also referred as support sets in the literature) as</p><formula xml:id="formula_0">D train = {x i , y i } N M i=1</formula><p>, with the instance x i ? R D and the one-hot labeling vector y i ? {0, 1} N . We will use "support set" and "training set" interchangeably in the paper. In FSL, M is often small (e.g., M = 1 or M = 5). The goal is to find a function f that classifies a test instance x test b?</p><formula xml:id="formula_1">y test = f (x test ; D train ) ? {0, 1} N .</formula><p>Given a small number of training instances, it is challenging to construct complex classifiers f (?). To this end, the learning algorithm is also supplied with additional data consisting of ample labeled instances. These additional data are drawn from visual classes S, which does not overlap with U. We refer to the original task as the target task which discerns N UNSEEN classes U. To avoid confusion, we denote the data from the SEEN classes S as D S .</p><p>To learn f (?) using D S , we synthesize many M -shot Nway FSL tasks by sampling the data in the meta-learning manner <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b48">49]</ref>. Each sampling gives rise to a task to classify a test set instance x S test into one of the N SEEN classes by f (?), where the test instances set D S test is composed of the labeled instances with the same distribution as D S train . Formally, the function f (?) is learnt to minimize the averaged error over those sampled tasks</p><formula xml:id="formula_2">f * = arg min f (x S test ,y S test )?D S test (f (x S test ; D S train ), y S test )</formula><p>(1) where the loss (?) measures the discrepancy between the prediction and the true label. For simplicity, we have assumed we only synthesize one task with test set D S test . The optimal f * is then applied to the original target task.</p><p>We consider the approach based on learning embeddings for FSL <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b48">49]</ref> (see <ref type="figure">Figure 2</ref> (a) for an overview). In particular, the classifier f (?) is composed of two elements. The first is an embedding function ? x = E(x) ? R d that maps an instance x to a representation space. The second component applies the nearest neighbor classifiers in this space:</p><formula xml:id="formula_3">y test = f (? xtest ; {? x , ?(x, y) ? D train }) (2) ? exp sim(? xtest , ? x ) ? y, ?(x, y) ? D train</formula><p>Note that only the embedding function is learned by optimizing the loss in Eq. 1. For reasons to be made clear in below, we refer this embedding function as task-agnostic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Adapting Embedding for Task-specific FSL</head><p>In what follows, we describe our approach for few-shot learning (FSL). We start by describing the main idea ( ? 4.1, also illustrated in <ref type="figure">Figure 2</ref>), then introduce the set-to-set adaptation function ( ? 4.2). Last are learning ( ? 4.3) and implementations details ( ? 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Adapting to Task-Specific Embeddings</head><p>The key difference between our approach and traditional ones is to learn task-specific embeddings. We argue that the embedding ? x is not ideal. In particular, the embeddings do not necessarily highlight the most discriminative representation for a specific target task. To this end, we introduce an adaption step where the embedding function ? x (more precisely, its values on instances) is transformed. This transformation is a set-to-set function that contextualizes over the image instances of a set, to enable strong co-adaptation of each item. Instance functions fails to have such coadaptation property. Furthermore, the set-to-set-function receives instances as bags, or sets without orders, requiring the function to output the set of refined instance embeddings while being permutation-invariant. Concretely,</p><formula xml:id="formula_4">{? x ; ?x ? X train } = T ({? x ; ?x ? X train }) (3) = T (? {? x ; ?x ? X train }))</formula><p>where X train is a set of all the instances in the training set D train for the target task. ?(?) is a permutation operator over a set. Thus the set of adapted embedding will not change if we apply a permutation over the input embedding set. With adapted embedding ? x , the test instance x test can be classified by computing nearest neighbors w.r.t. D train :</p><formula xml:id="formula_5">y test = f (? xtest ; {? x , ?(x, y) ? D train })<label>(4)</label></formula><p>Our approach is generally applicable to different types of task-agnostic embedding function E and similarity measure sim(?, ?), e.g., the (normalized) cosine similarity <ref type="bibr" target="#b48">[49]</ref> or the negative distance <ref type="bibr" target="#b42">[43]</ref>. Both the embedding function E and the set transformation function T are optimized over synthesized FSL tasks sampled from D S , sketched in Alg. 1. Its key difference from conventional FSL is in the line 4 to line 8 where the embeddings are transformed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Embedding Adaptation via Set-to-set Functions</head><p>Next, we explain various choices as the instantiations of the set-to-set embedding adaptation function. Bidirectional LSTM (BILSTM) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b48">49]</ref> is one of the common choice to instantiate the set-to-set transformation, where the addition between the input and the hidden layer outputs of each BILSTM cell leads to the adapted embedding. It is notable that the output of the BILSTM suppose to depend on the order of the input set. Note that using BIL-STM as embedding adaptation model is similar but different from the fully conditional embedding <ref type="bibr" target="#b48">[49]</ref>, where the later one contextualizes both training and test instance embedding altogether, which results in a transductive setting. DeepSets <ref type="bibr" target="#b55">[56]</ref> is inherently a permutation-invariant transformation function. It is worth noting that DEEPSETS aggregates the instances in a set into a holistic set vector. We consider two components to implement such DeepSets transformation, an instance centric set vector combined with a set context vector. For x ? X train , we define its complementary set as x . Then we implement the DEEPSETS by:</p><formula xml:id="formula_6">? x = ? x + g([? x ; x i ?x h(? x i )])<label>(5)</label></formula><p>In Eq. 5, g and h are two-layer multi-layer perception (MLP) with ReLU activation which map the embedding into another space and increase the representation ability of the embedding. For each instance, embeddings in its complementary set is first combined into a set vector as the context, and then this vector is concatenated with the input embedding to obtain the residual component of adapted embedding. This conditioned embedding takes other instances in the set into consideration, and keeps the "set (permutation invariant)" property. In practice, we find using the maximum operator in Eq. 5 works better than the sum operator suggested in <ref type="bibr" target="#b55">[56]</ref>. Graph Convolutional Networks (GCN) <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b40">41]</ref> propagate the relationship between instances in the set. We first construct the degree matrix A to represent the similarity between instances in a set. If two instances come from the same class, then we set the corresponding element in A to 1, otherwise to 0. Based on A, we build the "normalized" adjacency matrix S for a given set with added self-loops</p><formula xml:id="formula_7">S = D ? 1 2 (A + I)D ? 1 2 .</formula><p>I is the identity matrix, and D is the diagonal matrix whose elements are equal to the sum of elements in the corresponding row of A + I.</p><p>Let ? 0 = {? x ; ?x ? X train }, the relationship between instances could be propagated based on S, i.e.,</p><formula xml:id="formula_8">? t+1 = ReLU(S? t W ) , t = 0, 1, . . . , T ? 1 (6)</formula><p>W is a projection matrix for feature transformation. In GCN, the embedding in the set is transformed based on Eq. 6 multiple times, and the final ? T gives rise to the {? x }. Transformer. <ref type="bibr" target="#b46">[47]</ref> We use the Transformer architecture <ref type="bibr" target="#b46">[47]</ref> to implement T. In particular, we employ selfattention mechanism <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b46">47]</ref> to transform each instance embedding with consideration to its contextual instances. Note that it naturally satisfies the desired properties of T because it outputs refined instance embeddings and is permutation invariant. We denote it as Few-Shot Embedding Adaptation with Transformer (FEAT).</p><p>Transformer is a store of triplets in the form of (query Q, key K, and value V). To compute proximity and return values, those points are first linearly mapped into some space K = W K ? x k ; ?x k ? K ? R d?|K| , which is also the same for Q and V with W Q and W V respectively. Transformer computes what is the right value for a query point -the query x q ? Q is first matched against a list of keys K where each key has a value V . The final value is then returned as the sum of all the values weighted by the proximity of the key to the query point,</p><formula xml:id="formula_9">i.e. ? xq = ? xq + k ? qk V :,k , where ? qk ? exp ? xq W Q ? K ? d</formula><p>and V :,k is the k-th column of V . In the standard FSL setup, we have Q = K = V = X train .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Contrastive Learning of Set-to-Set Functions</head><p>To facilitate the learning of embedding adaptation, we apply a contrastive objective in addition to the general one.</p><p>It is designed to make sure that instances embeddings after adaptation is similar to the same class neighbors and dissimilar to those from different classes. Specifically, the embedding adaptation function T is applied to instances of each n of the N class in D S train ? D S test , which gives rise to the transformed embedding ? x and class centers {c n } N n=1 . Then we apply the contrastive objective to make sure training instances are close to its own class center than other centers. The total objective function (together with Eq. 1) is shown as following:</p><formula xml:id="formula_10">L(? test , y test ) = (? test , y test )<label>(7)</label></formula><p>+? ? softmax sim(? xtest , c n ) , y test This contrastive learning makes the set transformation extract common characteristic for instances of the same category, so as to preserve the category-wise similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Implementation details</head><p>We consider three different types of convolutional networks as the backbone for instance embedding function E: 1) A 4-layer convolution network (ConvNet) <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b48">49]</ref> and 2) the 12-layer residual network (ResNet) used in <ref type="bibr" target="#b24">[25]</ref>, and 3) the Wide Residual Network (WideResNet) <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b54">55]</ref>. We apply an additional pre-training stage for the backbones over the SEEN classes, based on which our re-implemented methods are further optimized. To achieve more precise embedding, we average the same-class instances in the training set before the embedding adaptation with the set-toset transformation. Adam <ref type="bibr" target="#b19">[20]</ref> and SGD are used to optimize ConvNet and ResNet variants respectively. Moreover, we follow the most standard implementations for the four set-to-set functions -BiLSTM <ref type="bibr" target="#b15">[16]</ref>, DeepSets <ref type="bibr" target="#b55">[56]</ref>, Graph Convolutional Networks (GCN) <ref type="bibr" target="#b20">[21]</ref> and Transformer (FEAT) <ref type="bibr" target="#b46">[47]</ref>. We refer readers to supplementary material (SM) for complete details and ablation studies of each set-to-set functions. Our implementation is available at https://github.com/Sha-Lab/FEAT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we first evaluate a variety of models for embedding adaptation in ? 5.2 with standard FSL. It concludes that FEAT (with Transformer) is the most effective approach among different instantiations. Next, we perform ablation studies in ? 5.2.2 to analyze FEAT in details. Eventually, we evaluate FEAT on many extended few-shot learning tasks to study its general applicability ( ? 5.3). This study includes few-shot domain generalization, transductive few-shot learning, generalized few-shot learning, and large-scale low-shot learning (refer to SM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Setups</head><p>Datasets. MiniImageNet <ref type="bibr" target="#b48">[49]</ref> and TieredImageNet <ref type="bibr" target="#b37">[38]</ref> datasets are subsets of the ImageNet <ref type="bibr" target="#b38">[39]</ref>. MiniImageNet includes a total number of 100 classes and 600 examples per class. We follow the setup provided by <ref type="bibr" target="#b35">[36]</ref>, and use 64 classes as SEEN categories, 16 and 20 as two sets of UNSEEN categories for model validation and evaluation respectively. TieredImageNet is a large-scale dataset with more categories, which contains 351, 97, and 160 categories for model training, validation, and evaluation, respectively. In addition to these, we investigate the OfficeHome <ref type="bibr" target="#b47">[48]</ref> dataset to validate the generalization ability of FEAT across domains. There are four domains in OfficeHome, and two of them ("Clipart" and "Real World") are selected, which contains 8722 images. After randomly splitting all classes, 25 classes serve as the seen classes to train the model, and the remaining 15 and 25 classes are used as two UNSEEN for evaluation. Please refer to SM for more details. Evaluation protocols. Previous approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b45">46]</ref> usually follow the original setting of <ref type="bibr" target="#b48">[49]</ref> and evaluate the models on 600 sampled target tasks (15 test instances per class). In a later study <ref type="bibr" target="#b39">[40]</ref>, it was suggested that such an evaluation process could potentially introduce high variances. Therefore, we follow the new and more trustworthy evaluation setting to evaluate both baseline models and our approach on 10,000 sampled tasks. We report the mean accuracy (in %) as well as the 95% confidence interval. Baseline and embedding adaptation methods. We reimplement the prototypical network (ProtoNet) <ref type="bibr" target="#b42">[43]</ref> as a task-agnostic embedding baseline model. This is known as a very strong approach <ref type="bibr" target="#b7">[8]</ref> when the backbone architecture is deep, i.e., residual networks <ref type="bibr" target="#b14">[15]</ref>. As suggested by <ref type="bibr" target="#b32">[33]</ref>, we tune the scalar temperature carefully to scale the logits of both approaches in our re-implementation. As mentioned, we implement the embedding adaptation model with four different function approximators, and denote them as BILSTM, DEEPSETS, GCN, and FEAT (i.e. Transformer). The concrete details of each model are included in the SM. Backbone pre-training. Instead of optimizing from scratch, we apply an additional pre-training strategy as suggested in <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b39">40]</ref>. The backbone network, appended with a softmax layer, is trained to classify all SEEN classes with the cross-entropy loss (e.g., 64 classes in the MiniImageNet). The classification performance over the penultimate layer embeddings of sampled 1-shot tasks from the model validation split is evaluated to select the best pretrained model, whose weights are then used to initialize the embedding function E in the few-shot learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Standard Few-Shot Image Classification</head><p>We compare our proposed FEAT method with the instance embedding baselines as well as previous methods on the standard MiniImageNet <ref type="bibr" target="#b48">[49]</ref> and TieredImageNet <ref type="bibr" target="#b37">[38]</ref> benchmarks, and then perform detailed analysis on the ablated models. We include additional results with CUB <ref type="bibr" target="#b49">[50]</ref> dataset in SM, which shares a similar observation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Main Results</head><p>Comparison to previous State-of-the-arts. <ref type="table" target="#tab_1">Table 1</ref> and <ref type="table" target="#tab_2">Table 2</ref> show the results of our method and others on the MiniImageNet and TieredImageNet. First, we observe that the best embedding adaptation method (FEAT) outperforms the instance embedding baseline on both datasets, indicating the effectiveness of learning task-specific embedding space. Meanwhile, the FEAT model performs significantly better than the current state-of-the-art methods on MiniImageNet dataset. On the TieredImageNet, we observe that the ProtoNet baseline is already better than some previous state-of-the-arts based on the 12-layer ResNet backbone. This might due to the effectiveness of the pretraining stage on the TieredImageNet as it is larger than MiniImageNet and a fully converged model can be itself very effective. Based on this, all embedding adaptation approaches further improves over ProtoNet almost in all cases, with FEAT achieving the best performances among all approaches. Note that here our pre-training strategy is most similar to the one used in PFA <ref type="bibr" target="#b34">[35]</ref>, while we further finetune the backbone. Temperature scaling of the logits influences the performance a lot when fine-tuning over the pretrained weights. Additionally, we list some recent methods (SimpleShot <ref type="bibr" target="#b50">[51]</ref>, and CTM <ref type="bibr" target="#b27">[28]</ref>) using different backbone  <ref type="bibr" target="#b44">[45]</ref> 54.48 ? 0.93 71.32 ? 0.78 MetaOptNet <ref type="bibr" target="#b24">[25]</ref> 65.99 ? 0.72 81.56 ? 0.63 CTM <ref type="bibr" target="#b27">[28]</ref> 68.41 ? 0.39 84.28 ? 1.73 SimpleShot <ref type="bibr" target="#b50">[51]</ref> 69  Comparison among the embedding adaptation models.</p><p>Among the four embedding adaptation methods, BILSTM in most cases achieves the worst performances and sometimes even performs worse than ProtoNet. This is partially due to the fact that BILSTM can not easily implement the required permutation invariant property (also shown in <ref type="bibr" target="#b55">[56]</ref>), which confuses the learning process of embedding adaptation. Secondly, we find that DEEPSETS and GCN have the ability to adapt discriminative task-specific embeddings but do not achieve consistent performance improvement over the baseline ProtoNet especially on MiniImageNet with the ConvNet backbone. A potential explanation is that, such models when jointly learned with the backbone model, can make the optimization process more difficult, which leads to the varying final performances. In contrast, we observe that FEAT can consistently improve ProtoNet and other embedding adaptation approaches in all cases, without additional bells and whistles. It shows that the Transformer as a set-to-set function can implement rich interactions between instances, which provides its high expressiveness to model the embedding adaptation process.</p><p>Interpolation and extrapolation of classification ways. Next, we study different set-to-set functions on their capability of interpolating and extrapolating across the number of classification ways. To do so, we train each variant of em- Mean accuracy (in %) <ref type="bibr" target="#b51">52</ref>  bedding adaptation functions with both 1-shot 20-way and 1-shot 5-way tasks, and measure the performance change as a function to the number of categories in the test time. We report the mean accuracies evaluated on few-shot classification with N = {5, 10, 15, 20} classes, and show results in <ref type="figure" target="#fig_1">Figure 3</ref>. Surprisingly, we observe that FEAT achieves almost the same numerical performances in both extrapolation and interpolation scenarios, which further displays its strong capability of learning the set-to-set transformation. Meanwhile, we observe that DEEPSETS works well with interpolation but fails with extrapolation as its performance drops significantly with the larger N . In contrast, GCN achieves strong extrapolation performances but does not work as effectively in interpolation. BILSTM performs the worst in both cases, as it is by design not permutation invariant and may have fitted an arbitrary dependency between instances.</p><p>Parameter efficiency. <ref type="table" target="#tab_4">Table 3</ref> shows the number of additional parameters each set-to-set function has introduced. From this, we observe that with both ConvNet and ResNet backbones, FEAT has the smallest number of parameters compared with all other approaches while achieving best performances from various aspects (as results discussed above), which highlights its high parameter efficiency. All above, we conclude that: 1) learning embedding adaptation with a set-to-set model is very effective in modeling task-specific embeddings for few-shot learning 2) FEAT is the most parameter-efficient function approximater that achieves the best empirical performances, together with nice permutation invariant property and strong interpolation/extrapolation capability over the classification way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">Ablation Studies</head><p>We analyze FEAT and its ablated variants on the MiniImageNet dataset with ConvNet backbone.</p><p>How does the embedding adaptation looks like qualita-tively? We sample four few-shot learning tasks and learn a principal component analysis (PCA) model (that projects embeddings into 2-D space) using the instance embeddings of the test data. We then apply this learned PCA projection to both the support set's pre-adapted and post-adapted embeddings. The results are shown in <ref type="figure">Figure 1</ref> (the beginning of the paper). In three out of four examples, post-adaptation embeddings of FEAT improve over the pre-adaption embeddings. Interestingly, we found that the embedding adaptation step of FEAT has the tendency of pushing the support embeddings apart from the clutter, such that they can better fit the test data of its categories. In the negative example where post-adaptation degenerates the performances, we observe that the embedding adaptation step has pushed two support embeddings "Golden Retriever" and "Lion" too close to each other. It has qualitatively shown that the adaptation is crucial to obtain superior performances and helps to contrast against task-agnostic embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Extended Few-Shot Learning Tasks</head><p>In this section, we evaluate FEAT on 3 different few-shot learning tasks. Specifically, cross-domain FSL, transductive FSL <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b37">38]</ref>, and generalized FSL <ref type="bibr" target="#b6">[7]</ref>. We overview the setups briefly and please refer to SM for details. FS Domain Generalization assumes that examples in UN-SEEN support and test set can come from the different domains, e.g., sampled from different distributions <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b18">19]</ref>. The example of this task can be found in <ref type="figure" target="#fig_2">Figure 4</ref>. It requires a model to recognize the intrinsic property than texture of objects, and is de facto analogical recognition. Transductive FSL. The key difference between standard and transductive FSL is whether test instances arrive one at a time or all simultaneously. The latter setup allows the structure of unlabeled test instances to be utilized. Therefore, the prediction would depend on both the training (support) instances and all the available test instances in the target task from UNSEEN categories. Generalized FSL. Prior works assumed the test instances coming from unseen classes only. Different from them, the generalized FSL setting considers test instances from both SEEN and UNSEEN classes <ref type="bibr" target="#b36">[37]</ref>. In other words, during the model evaluation, while support instances all come from U, the test instances come from S ? U, and the classifier is required to predict on both SEEN and UNSEEN categories. .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1.">Few-Shot Domain Generalization</head><p>We show that FEAT learns to adapt the intrinsic structure of tasks, and generalizes across domains, i.e., predicting test instances even when the visual appearance is changed. Setups. We train the FSL model in the standard domain and evaluate with cross-domain tasks, where the N -categories are aligned but domains are different. In detail, a model is   trained on tasks from the "Clipart" domain of OfficeHome dataset <ref type="bibr" target="#b47">[48]</ref>, then the model is required to generalize to both "Clipart (C)" and"Real World (R)" test instances. In other words, we need to classify complex real images by seeing only a few sketches ( <ref type="figure" target="#fig_2">Figure 4</ref> gives an overview of data). Results. Here, the "supervised" denotes a model trained with the standard classification strategy and then its penultimate layer's output feature is used as the nearest neighbor classifier. We observe that ProtoNet can outperform this baseline on tasks when evaluating instances from "Clipart" but not ones from "real world". However, FEAT improves over "real world" few-shot classification even only seeing the support data from "Clipart".</p><formula xml:id="formula_11">C ? C C ? R<label>Supervised</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2.">Transductive Few-Shot Learning</head><p>We show that without additional efforts in modeling, FEAT outperforms existing methods in transductive FSL. Setups. We further study this semi-supervised learning setting to see how well FEAT can incorporate test instances into joint embedding adaptation. Specifically, we use the unlabeled test instances to augment the key and value sets of Transformer (refer to SM for details), so that the embedding adaptation takes relationship of all test instances into consideration. We evaluate this setting on the transductive protocol of MiniImageNet <ref type="bibr" target="#b37">[38]</ref>. With the adapted embedding, FEAT makes predictions based on Semi-ProtoNet <ref type="bibr" target="#b37">[38]</ref>. Results. We compare with two previous approaches, TPN <ref type="bibr" target="#b29">[30]</ref> and TEAM <ref type="bibr" target="#b33">[34]</ref>. The results are shown in Table 4 (b). We observe that FEAT improves its standard FSL performance (refer to <ref type="table" target="#tab_1">Table 1</ref>) and also outperforms previous semi-supervised approaches by a margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3.">Generalized Few-Shot Learning</head><p>We show that FEAT performs well on generalized fewshot classification of both SEEN and UNSEEN classes. Setups. In this scenario, we evaluate not only on classifying test instances from a N -way M -shot task from UN-SEEN set U, but also on all available SEEN classes from S. To do so, we hold out 150 instances from each of the 64 seen classes in MiniImageNet for validation and evaluation. Next, given a 1-shot 5-way training set D train , we consider three evaluation protocols based on different class sets <ref type="bibr" target="#b6">[7]</ref>: UNSEEN measures the mean accuracy on test instances only from U (5-Way few-shot classification); SEEN measures the mean accuracy on test instances only from S (64-Way classification); COMBINED measures the mean accuracy on test instances from S ? U (69-Way mixed classification). Results. The results can be found in <ref type="table" target="#tab_7">Table 4</ref> (c). We observe that again FEAT outperforms baseline ProtoNet. To calibrate the prediction score on SEEN and UNSEEN classes <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b51">52]</ref>, we select a constant seen/unseen class probability over the validation set, and subtract this calibration factor from seen classes' prediction score. Then we take the prediction with maximum score value after calibration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>A common embedding space fails to tailor discriminative visual knowledge for a target task especially when there are a few labeled training data. We propose to do embedding adaptation with a set-to-set function and instantiate it with transformer (FEAT), which customizes task-specific embedding spaces via a self-attention architecture. The adapted embedding space leverages the relationship between target task training instances, which leads to discriminative instance representations. FEAT achieves the state-of-the-art performance on benchmarks, and its superiority can generalize to tasks like cross-domain, transductive, and generalized few-shot classifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Details of Baseline Methods</head><p>In this section, we describe two important embedding learning baselines i.e., Matching Network (MatchNet) <ref type="bibr" target="#b48">[49]</ref> and Prototypical Network (ProtoNet) <ref type="bibr" target="#b42">[43]</ref>, to implement the prediction function f (x test ; D train ) in the few-shot learning framework.</p><p>MatchNet and ProtoNet. Both MatchNet and ProtoNet stress the learning of the embedding function E from the source task data D S with a meta-learning routine similar to Alg. 1 in the main text. We omit the super-script S since the prediction strategies can apply to tasks from both SEEN and UNSEEN sets.</p><p>Given the training data D train = {x i , y i } N M i=1 of an Mshot N -way classification task, we can obtain the embedding of each training instance based on the function E: 1</p><formula xml:id="formula_12">?(x i ) = E(x i ), ?x i ? X train<label>(8)</label></formula><p>To classify a test instance x test , we perform the nearest neighbor classification , i.e.,</p><formula xml:id="formula_13">y test ? exp ? ? sim(? xtest , ? xi ) ? y i (9) = exp ? ? sim(? xtest , ? xi ) x i ?X train exp ? ? sim(? xtest , ? x i ) ? y i = (xi,yi)?D train exp ? ? sim(? xtest , ? xi ) x i ?X train exp ? ? sim(? xtest , ? x i ) ? y i</formula><p>Here, MatchNet finds the most similar training instance to the test one, and assigns the label of the nearest neighbor to the test instance. Note that sim represents the cosine similarity, and ? &gt; 0 is the scalar temperature value over the similarity score, which is found important empirically <ref type="bibr" target="#b32">[33]</ref>. During the experiments, we tune this temperature value carefully, ranging from the reciprocal of {0.1, 1, 16, 32, 64, 128}. <ref type="bibr" target="#b1">2</ref> The ProtoNet has two key differences compared with the MatchNet. First, when M &gt; 1 in the target task, ProtoNet computes the mean of the same class embeddings as the class center (prototype) in advance and classifies a test instance by computing its similarity to the nearest class center (prototype). In addition, it uses the negative distance rather than the cosine similarity as the similarity metric:</p><formula xml:id="formula_14">c n = 1 M yi=n ?(x i ), ?n = 1, . . . , N<label>(10)</label></formula><formula xml:id="formula_15">y test ? exp ? ? ? xtest ? c n 2 2 ? y n = N n=1 exp ? ? ? xtest ? c n 2 2 N n =1 exp ? ? ? xtest ? c n 2 2 y n (11)</formula><p>Similar to the aforementioned scalar temperature for Match-Net, in Eq. 11 we also consider the scale ?. Here we abuse the notation by using y i = n to enumerate the instances with label n, and denote y n as the one-hot coding of the n-th class. Thus Eq. 11 outputs the probability to classify x test to the N classes.</p><p>In the experiments, we find ProtoNet incorporates better with FEAT. When there is more than one shot in each class, we average all instances per class in advance by Eq. 10 before inputting them to the set-to-set transformation. This pre-average manner makes more precise embedding for each class and facilitates the "downstream" embedding adaptation. We will validate this in the additional experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Details of the Set-to-Set Functions</head><p>In this section, we provide details about four implementations of the set-to-set embedding adaptation function T, i.e., the BILSTM, DEEPSETS, GCN, and the TRANS-FORMER. The last one is the key component in our Fewshot Embedding Adaptation with Transformer (FEAT) approach. Then we will introduce the configuration of the multi-layer/multi-head transformer, and the setup of the transformer for the transductive Few-Shot Learning (FSL).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. BiLSTM as the Set-to-Set Transformation</head><p>Bidirectional LSTM (BILSTM) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b48">49]</ref> is one of the common choice to instantiate the set-to-set transformation, where the addition between the input and the hidden layer outputs of each BILSTM cell leads to the adapted embedding. In detail, we have</p><formula xml:id="formula_16">{ ?(x), ?(x)} = BILSTM({?(x)}); ?x ? X train (12)</formula><p>Where ?(x) and ?(x) are the hidden layer outputs of the two LSTM models for each instance embedding in the input set. Then we get the adapted embedding as</p><formula xml:id="formula_17">?(x) = ?(x) + ?(x) + ?(x)<label>(13)</label></formula><p>It is notable that the output of the BILSTM suppose to depend on the order of the input set. Vinyals et al. <ref type="bibr" target="#b48">[49]</ref> propose to use the Fully Conditional Embedding to encode the context of both the test instance and the support set</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classification Scores</head><p>Embedding Adaptation  instances based on BILSTM and LSTM w/ Attention module. Different from <ref type="bibr" target="#b48">[49]</ref>, we apply the set-to-set embedding adaptation only over the support set, which leads to a fully inductive learning setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN CNN CNN CNN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Soft Nearest Neighbor</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Set-to-Set Function</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a) Embedding Adaptation (b) Transformer as the Set-to-Set Function (c) DeepSets as Set-to-Set Function</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. DeepSets as the Set-to-Set Transformation</head><p>Deep sets <ref type="bibr" target="#b55">[56]</ref> suggests a generic aggregation function over a set should be the transformed sum of all elements in this set. Therefore, a very simple set-to-set transformation baseline involves two components, an instance centric representation combined with a set context representation. For any instance x ? X train , we define its complementary set as x . Then we implement the set transformation by:</p><formula xml:id="formula_18">?(x) = ?(x) + g([?(x); x i ?x h(?(x i ))])<label>(14)</label></formula><p>In Eq. 14, g and h are transformations which map the embedding into another space and increase the representation ability of the embedding. Two-layer multi-layer perception (MLP) with ReLU activation is used to implement these two mappings. For each instance, embeddings in its complementary set are first combined into a vector as the context, and then this vector is concatenated with the input embedding to obtain the residual component of the adapted embedding. This conditioned embedding takes other instances in the set into consideration, and keeps the "set (permutation invariant)" property. Finally, we determine the label with the newly adapted embedding ? as Eq. 11. An illustration of the DeepSets notation in the embedding adaptation can be found in <ref type="figure" target="#fig_3">Figure 5</ref> (c). The summation operator in Eq. 14 could also be replaced as the maximum operator, and we find the maximum operator works better than summation operator in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. GCN as the Set-to-Set Transformation</head><p>Graph Convolutional Networks (GCN) <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b40">41]</ref> propagate the relationship between instances in the set. We first construct a degree matrix A ? R N K?N K to represent the similarity between instances in a set. If two instances x i and x j come from the same class, then we set the corresponding element A ij in A to 1, otherwise we have A ij = 0. Based on A, we build the "normalized" adjacency matrix S for a given set with added self-loops S = D ? 1 2 (A + I)D ? 1 2 . I ? R N K?N K is the identity matrix, and D is the diagonal matrix whose elements are equal to the sum of elements in the corresponding row of A + I, i.e., D ii = j A ij + 1 and D ij = 0 if i = j. Let ? 0 = {? x ; ?x ? X train } be the concatenation of all the instance embeddings in the training set X train . We use the super-script to denote the generation of the instance embedding matrix. The relationship between instances could be propagated based on S, i.e., ? t+1 = ReLU(S? t W ) , t = 0, 1, . . . , T ? 1 <ref type="bibr" target="#b14">(15)</ref> W is a learned a projection matrix for feature transformation. In GCN, the embedding in the set is transformed based on Eq. 15 multiple times (we propagate the embedding set two times during the experiments), and the final propagated embedding set ? T gives rise to the ? x .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Transformer as the Set-to-Set Transformation</head><p>In this section, we describe in details about our Few-Shot Embedding Adaptation w/ Transformer (FEAT) approach, specifically how to use the transformer architecture <ref type="bibr" target="#b46">[47]</ref> to implement the set-to-set function T, where self-attention mechanism facilitates the instance embedding adaptation with consideration of the contextual embeddings.</p><p>As mentioned before, the transformer is a store of triplets in the form of (query, key, and value). Elements in the query set are the ones we want to do the transformation. The transformer first matches a query point with each of the keys by computing the "query" -"key" similarities. Then the proximity of the key to the query point is used to weight the corresponding values of each key. The transformed input acts as a residual value which will be added to the input.</p><p>Basic Transformer. Following the definitions in <ref type="bibr" target="#b46">[47]</ref>, we use Q, K, and V to denote the set of the query, keys, and values, respectively. All these sets are implemented by different combinations of task instances.</p><p>To increase the flexibility of the transformer, three sets of linear projections (W Q ? R d?d , W K ? R d?d , and W V ? R d?d ) are defined, one for each set. <ref type="bibr" target="#b2">3</ref> The points in sets are first projected by the corresponding projections</p><formula xml:id="formula_19">Q = W Q ? xq ; ?x q ? Q ? R d ?|Q| K = W K ? x k ; ?x k ? K ? R d ?|K| V = W V ? xv ; ?x v ? V ? R d ?|V| (16)</formula><p>|Q|, |K|, and |V| are the number of elements in the sets Q, K, and V respectively. Since there is a one-to-one correspondence between elements in K and V we have |K| = |V|.</p><p>The similarity between a query point x q ? Q and the list of keys K is then computed as "attention":</p><formula xml:id="formula_20">? qk ? exp ? xq W Q ? K ? d ; ?x k ? K (17) ? q,: = softmax ? xq W Q ? K ? d ? R |K|<label>(18)</label></formula><p>The k-th element ? qk in the vector ? q,: reveals the particular proximity between x k and x q . The computed attention values are then used as weights for the final embedding x q :</p><formula xml:id="formula_21">? xq = k ? qk V :,k<label>(19)</label></formula><formula xml:id="formula_22">? xq = ? ? xq + W FC?xq<label>(20)</label></formula><p>V :,k is the k-th column of V . W FC ? R d ?d is the projection weights of a fully connected layer. ? completes a further transformation, which is implemented by the dropout <ref type="bibr" target="#b43">[44]</ref> and layer normalization <ref type="bibr" target="#b3">[4]</ref>. The whole flow of transformer in our FEAT approach can be found in <ref type="figure" target="#fig_3">Figure 5 (b)</ref>. With the help of transformer, the embeddings of all training set instances are adapted (we denote this approach as FEAT).</p><p>Multi-Head Multi-Layer Transformer. Following <ref type="bibr" target="#b46">[47]</ref>, an extended version of the transformer can be built with multiple parallel attention heads and stacked layers. Assume there are totally H heads, the transformer concatenates multiple attention-transformed embeddings, and then uses a linear mapping to project the embedding to the original embedding space (with the original dimensionality). Besides, we can take the transformer as a feature encoder of the input query instance. Therefore, it can be applied</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5. Extension to transductive FSL</head><p>Facilitated by the flexible set-to-set transformer in Eq. 20, our adaptation approach can naturally be extended to the transductive FSL setting.</p><p>When classifying test instance x test in the transdutive scenario, other test instances X test from the N categories would also be available. Therefore, we enrich the transformer's query and key/value sets</p><formula xml:id="formula_23">Q = K = V = X train ? X test<label>(21)</label></formula><p>In this manner, the embedding adaptation procedure would also consider the structure among unlabeled test instances. When the number of shots K &gt; 1, we average the embedding of labeled instances in each class first before combining them with the test set embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Details</head><p>Backbone architecture. We consider three backbones, as suggested in the literature, as the instance embedding function E for the purpose of fair comparisons. We resize the input image to 84 ? 84 ? 3 before using the backbones.</p><p>? ConvNet. The 4-layer convolution network <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b48">49]</ref> contains 4 repeated blocks. In each block, there is a convolutional layer with 3 ? 3 kernel, a Batch Normalization layer <ref type="bibr" target="#b17">[18]</ref>, a ReLU, and a Max pooling with size 2. We set the number of convolutional channels in each block as 64. A bit different from the literature, we add a global max pooling layer at last to reduce the dimension of the embedding. Based on the empirical observations, this will not influence the results, but reduces the computation burden of later transformations a lot.</p><p>? ResNet. We use the 12-layer residual network in <ref type="bibr" target="#b24">[25]</ref>. <ref type="bibr" target="#b3">4</ref> The DropBlock <ref type="bibr" target="#b10">[11]</ref> is used in this ResNet architecture to avoid over-fitting. A bit different from the ResNet-12 in <ref type="bibr" target="#b24">[25]</ref>, we apply a global average pooling after the final layer, which leads to 640 dimensional embeddings. <ref type="bibr" target="#b4">5</ref> ? WRN. We also consider the Wide residual network <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b54">55]</ref>. We use the WRN-28-10 structure as in <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b39">40]</ref>, which sets the depth to 28 and width to 10. After a global average pooling in the last layer of the backbone, we get a 640 dimensional embedding for further prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets. Four datasets,</head><p>MiniImageNet <ref type="bibr" target="#b48">[49]</ref>, TieredImageNet <ref type="bibr" target="#b37">[38]</ref>, Caltech-UCSD Birds (CUB) 200-2011 <ref type="bibr" target="#b49">[50]</ref>, and OfficeHome <ref type="bibr" target="#b47">[48]</ref> are investigated in this paper. Each dataset is split into three parts based on different non-overlapping sets of classes, for model training (a.k.a. meta-training in the literature), model validation (a.k.a. meta-val in the literature), and model evaluation (a.k.a. meta-test in the literature). The CUB dataset is initially designed for fine-grained classification. It contains in total 11,788 images of birds over 200 species. On CUB, we randomly sampled 100 species as SEEN classes, another two 50 species are used as two UNSEEN sets for model validation and evaluation <ref type="bibr" target="#b45">[46]</ref>. For all images in the CUB dataset, we use the provided bounding box to crop the images as a pre-processing <ref type="bibr" target="#b45">[46]</ref>. Before input into the backbone network, all images in the dataset are resized based on the requirement of the network.</p><p>Pre-training strategy. As mentioned before, we apply an additional pre-training strategy as suggested in <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b39">40]</ref>. The backbone network, appended with a softmax layer, is trained to classify all classes in the SEEN class split (e.g., 64 classes in the MiniImageNet) with the cross-entropy loss. In this stage, we apply image augmentations like random crop, color jittering, and random flip to increase the generalization ability of the model. After each epoch, we validate the performance of the pre-trained weights based on its few-shot classification performance on the model validation split. Specifically, we randomly sample 200 1-shot N -way few-shot learning tasks (N equals the number of classes in the validation split, e.g., 16 in the MiniImageNet), which contains 1 instance per class in the support set and 15 instances per class for evaluation. Based on the penultimate layer instance embeddings of the pre-trained weights, we utilize the nearest neighbor classifiers over the few-shot tasks and evaluate the quality of the backbone. We select the pre-trained weights with the best few-shot classification accuracy on the validation set. The pre-trained weights are used to initialize the embedding backbone E, and the weights of the whole model are then optimized together during the model training.</p><p>Transformer Hyper-parameters. We follow the architecture as presented in <ref type="bibr" target="#b46">[47]</ref> to build our FEAT model. The hidden dimension d for the linear transformation in our FEAT model is set to 64 for ConvNet and 640 for ResNet/WRN. The dropout rate in transformer is set as 0.5. We empirically observed that the shallow transformer (with one set of projection and one stacked layer) gives the best overall performance (also studied in ? D.2).</p><p>Optimization. Following the literature, different optimizers are used for the backbones during the model training. For the ConvNet backbone, stochastic gradient descent with Adam <ref type="bibr" target="#b19">[20]</ref> optimizer is employed, with the initial learning rate set to be 0.002. For the ResNet and WRN backbones, vanilla stochastic gradient descent with Nesterov acceleration is used with an initial rate of 0.001. We fix the weight decay in SGD as 5e-4 and momentum as 0.9. The schedule of the optimizers is tuned over the validation part of the dataset. As the backbone network is initialized with the pre-trained weights, we scale the learning rate for those parameters by 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Additional Experimental Results</head><p>In this section, we will show more experimental results over the MiniImageNet/CUB dataset, the ablation studies, and the extended few-shot learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Main Results</head><p>The full results of all methods on the MiniImageNet can be found in <ref type="table" target="#tab_10">Table 5</ref>. The results of MAML <ref type="bibr" target="#b9">[10]</ref> optimized over the pre-trained embedding network are also included. We re-implement the ConvNet backbone of MAML and cite the MAML results over the ResNet backbone from <ref type="bibr" target="#b39">[40]</ref>. It is also noteworthy that the FEAT gets the best performance among all popular methods and baselines.</p><p>We also investigate the Wide ResNet (WRN) backbone over MiniImageNet, which is also the popular one used in <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b39">40]</ref>. SimpleShot <ref type="bibr" target="#b50">[51]</ref> is a recent proposed embedding-based few-shot learning approach that takes full advantage of the pre-trained embeddings. We cite the results of PFA <ref type="bibr" target="#b34">[35]</ref>, LEO <ref type="bibr" target="#b39">[40]</ref>, and SimpleShot <ref type="bibr" target="#b50">[51]</ref> from their papers. The results can be found in <ref type="table" target="#tab_11">Table 6</ref>. We reimplement ProtoNet and our FEAT approach with WRN. It is notable that in this case, our FEAT achieves much higher promising results than the current state-of-the-art approaches. <ref type="table" target="#tab_12">Table 7</ref> shows the classification results with WRN on the TieredImageNet data set, where our FEAT still keeps its superiority when dealing with 1-shot tasks. <ref type="table" target="#tab_13">Table 8</ref> shows the 5-way 1-shot and 5-shot classification results on the CUB dataset based on the ConvNet backbone. The results on CUB are consistent with the trend on the MiniImageNet dataset. Embedding adaptation indeed assists the embedding encoder for the few-shot classification tasks. Facilitated by the set function property, the DEEPSETS works better than the BILSTM counterpart. Among all the results, the transformer based FEAT gets the top tier results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Ablation Studies</head><p>In this section, we perform further analyses for our proposed FEAT and its ablated variants classifying in the Pro-   toNet manner, on the MiniImageNet dataset, using the Con-vNet as the backbone network.</p><p>Do the adapted embeddings improve the pre-adapted embeddings? We report few-shot classification results by  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BILSTM DeepSets FEAT</head><p>(b) Task Extrapolation <ref type="figure">Figure 6</ref>: Interpolation and Extrapolation of few-shot tasks. We train different embedding adaptation models on 5-shot 20-way or 5-way classification tasks and evaluate models on unseen tasks with different number of classes (N ={5, <ref type="bibr">10, 15, 20})</ref>. It verifies both the interpolation and extrapolation ability of FEAT on a varying number of ways in few-shot classification. <ref type="table" target="#tab_1">Table 10</ref>: Ablation studies on the position to average the sameclass embeddings when there are multiple shots per class in FEAT (tested on the 5-Way tasks with different numbers of shots). "Pre-Avg" and "Post-Avg" means we get the embedding center for each class before or after the set-to-set transformation, respectively.   using the pre-adapted embeddings of support data (i.e., the embedding before adaptation), against those using adapted embeddings, for constructing classifiers. <ref type="table" target="#tab_14">Table 9</ref> shows that task-specific embeddings after adaptation improves over task-agnostic embeddings in few-shot classifications.</p><p>Can FEAT possesses the characteristic of the set function? We test three set-to-set transformation implementations, namely the BILSTM, the DEEPSETS, and the Transformer (FEAT), w.r.t. two important properties of the set function, i.e., task interpolation and task extrapolation. In particular, the few-shot learning model is first trained with 5-shot 20-way tasks. Then the learned model is required to evaluate different 5-shot tasks with N = {5, 10, 15, 20} (Extrapolation). Similarly, for interpolation, the model is trained with 5-shot 20-way tasks in advance and then evaluated on the previous multi-way tasks. The classification change results can be found in <ref type="figure">Figure 6</ref> (a) and (b). BILSTM cannot deal with the size change of the set, especially in the task extrapolation. In both cases, FEAT still gets improvements in all configurations of N .</p><p>When to average the same-class embeddings? When there is more than one instance per class, i.e. M &gt; 1, we average the instances in the same class and use the class center to make predictions as in Eq. 10. There are two positions to construct the prototypes in FEAT -before the set-to-set transformation (Pre-Avg) and after the set-to-set transformation (Post-Avg). In Pre-Avg, we adapt the embeddings of the centers, and a test instance is predicted based on its distance to the nearest adapted center; while in Post-Avg, the instance embeddings are adapted by the set-to-set function first, and the class centers are computed based on the adapted instance embeddings. We investigate the two choices in <ref type="table" target="#tab_1">Table 10</ref>, where we fix the number of ways to 5 (N = 5) and change the number of shots (M ) among {5, 15, 30}. The results demonstrate the Pre-Avg version performs better than the Post-Avg in all cases, which shows a more precise input of the set-to-set function by averaging the instances in the same class leads to better results. So we use the Pre-Avg strategy as a default option in our experiments.</p><p>Will deeper and multi-head transformer help? In our current implementation of the set-to-set transformation function, we make use of a shallow and simple transformer, i.e., one layer and one head (set of projection). From <ref type="bibr" target="#b46">[47]</ref>, the transformer can be equipped with complex components using multiple heads and deeper stacked layers. We evaluate this augmented structure, with the number of attention heads increases to 2, 4, 8, as well as with the number of layers increases to 2 and 3. As in <ref type="table" target="#tab_1">Table 11</ref> and <ref type="table" target="#tab_1">Table 12</ref>, we empirically observe that more complicated structures do not result in improved performance. We find that with more layers of transformer stacked, the difficulty of optimization increases and it becomes harder to train models until their convergence. Whilst for models with more heads, the models seem to over-fit heavily on the training data, even with the usage of auxiliary loss term (like the contrastive loss in  The effectiveness of contrastive loss. <ref type="table" target="#tab_1">Table 13</ref> show the few-shot classification results with different weight values (?) of the contrastive loss term for FEAT. From the results, we can find that the balance of the contrastive term in the learning objective can influence the final results. Empirically, we set ? = 0.1 in our experiments.</p><p>The influence of the prediction strategy. We investigate two embedding-based prediction ways for the few-shot classification, i.e., based on the cosine similarity and the negative euclidean distance to measure the relationship between objects, respectively. We compare these two choices in <ref type="table" target="#tab_1">Table 14</ref>. Two strategies in <ref type="table" target="#tab_1">Table 14</ref> only differ in their similarity measures. In other words, with more than one shot per class in the task training set, we average the same class embeddings first, and then make classification by computing the cosine similarity or the negative euclidean distance between a test instance and a class prototype. During the optimization, we tune the logits scale temperature for both these methods. We find that using the euclidean distance usually requires small temperatures (e.g., ? = 1 64 ) while a large temperature (e.g., ? = 1) works well with the normalized cosine similarity. The former choice achieves a slightly better performance than the latter one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Few-Shot Domain Generalization</head><p>We show that FEAT learns to adapt the intrinsic structure of tasks, and generalize across domains, i.e., predicting test instances even when the visual appearance is changed.  Setups. We train a few-shot learning model in the standard domain and evaluate it with cross-domain tasks, where the N -categories are aligned but domains are different. In detail, a model is trained on tasks from the "Clipart" domain of OfficeHome dataset <ref type="bibr" target="#b47">[48]</ref>, then the model is required to generalize to both "Clipart (C)" and "Real World (R)" instances. In other words, we need to classify complex real images by seeing only a few sketches, or even based on the instances in the "Real World (R)" domain.</p><p>Results. <ref type="table" target="#tab_1">Table 15</ref> gives the quantitative results. Here, the "supervised" refers to a model trained with standard classification and then is used for the nearest neighbor classifier with its penultimate layer's output feature. We observe that ProtoNet can outperform this baseline on tasks when evaluating instances from "Clipart" but not ones from "real world". However, FEAT can improve over "real world" fewshot classification even only seeing the support data from "Clipart". Besides, when the support set and the test set of the target task are sampled from the same but new domains, e.g., the training and test instances both come from "real world", FEAT also improves the classification accuracy w.r.t. the baseline methods. It verifies the domain generalization ability of the FEAT approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4. Additional Discussions on Transductive FSL</head><p>We list the results of the transductive few-shot classification in <ref type="table" target="#tab_1">Table 16</ref>, where the unlabeled test instances arrive simultaneously, so that the common structure among the unlabeled test instances could be captured. We compare with three approaches, Semi-ProtoNet <ref type="bibr" target="#b37">[38]</ref>, TPN <ref type="bibr" target="#b29">[30]</ref>, and TEAM <ref type="bibr" target="#b33">[34]</ref>. Semi-ProtoNet utilizes the unlabeled instances to facilitate the computation of the class center and makes predictions similar to the prototypical network; TPN meta learns a label propagation way to take the unlabeled instances relationship into consideration; TEAM explores the pairwise constraints in each task, and formulates the embedding adaptation into a semi-definite programming form. We cite the results of Semi-ProtoNet from <ref type="bibr" target="#b37">[38]</ref>, and cite the results of TPN and TEAM from <ref type="bibr" target="#b33">[34]</ref>. We also reimplement Semi-ProtoNet with our pre-trained backbone (the same pre-trained ConvNet weights as the standard fewshot learning setting) for a fair comparison.</p><p>In this setting, our model leverages the unlabeled test instances to augment the transformer as discussed in ? B.4 and the embedding adaptation takes the relationship of all test instances into consideration. Based on the adapted embedding by the joint set of labeled training instances and unlabeled test instances, we can make predictions with two strategies. First, we still compute the center of the labeled instances, while such adapted embeddings are influenced by the unlabeled instances (we denote this approach as FEAT ? , which works the same way as standard FEAT except the augmented input of the embedding transformation function); Second, we consider to take advantage of the unlabeled instances and use their adapted embeddings to construct a better class prototype as in Semi-ProtoNet (we denote this approach as FEAT ? ).</p><p>By using more unlabeled test instances in the transductive environment, FEAT ? achieves further performance improvement compared with the standard FEAT, which verifies the unlabeled instances could assist the embedding adaptation of the labeled ones. With more accurate class center estimation, FEAT ? gets a further improvement. The performance gain induced by the transductive FEAT is more significant in the one-shot learning setting compared with the five-shot scenario, since the helpfulness of unlabeled instance decreases when there are more labeled instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5. More Generalized FSL Results</head><p>Here we show the full results of FEAT in the generalized few-shot learning setting in <ref type="table" target="#tab_1">Table 17</ref>, which includes both the 1-shot and 5-shot performance. All methods are evaluated on instances composed by SEEN classes, UNSEEN classes, and both of them (COMBINED), respectively. In the 5-shot scenario, the performance improvement mainly comes from the improvement of over the UNSEEN tasks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.6. Large-Scale Low-Shot Learning</head><p>Similar to the generalized few-shot learning, the largescale low-shot learning <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b51">52]</ref> considers the few-shot classification ability on both SEEN and UNSEEN classes on the full ImageNet <ref type="bibr" target="#b38">[39]</ref> dataset. There are in total 389 SEEN classes and 611 UNSEEN classes <ref type="bibr" target="#b13">[14]</ref>. We follow the setting (including the splits) of the prior work <ref type="bibr" target="#b13">[14]</ref> and use features extracted based on the pre-trained ResNet-50 <ref type="bibr" target="#b14">[15]</ref>. Three evaluation protocols are evaluated, namely the top-5 fewshot accuracy on the UNSEEN classes, on the combined set of both SEEN and UNSEEN classes, and the calibrated accuracy on weighted by selected set prior on the combined set of both SEEN and UNSEEN classes. The results are listed in <ref type="table" target="#tab_1">Table 18</ref>. We observe that FEAT achieves better results than others, which further validates FEAT's superiority in generalized classification setup, a large scale learning setup.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Interpolation and Extrapolation of few-shot tasks from the "way" perspective. First, We train various embedding adaptation models on 1-shot 20-way (a) or 5-way (b) classification tasks and evaluate models on unseen tasks with different number of classes (N ={5, 10, 15, 20}). It shows that FEAT is superior in terms of way interpolation and extrapolation ability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative results of few-shot domain-generalization for FEAT. Correctly classified examples are shown in red boxes and incorrectly ones are shown in blue boxes. We visualize one task that FEAT succeeds (top) and one that fails (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Illustration of two embedding adpatation methods considered in the paper. (a) shows the main flow of Few-Shot Embedding Adaptation, while (b) and (c) demonstrate the workflow of Transformer and DeepSets respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 Training strategy of embedding adaptationRequire: Seen class set S 1: for all iteration = 1,...,MaxIteration do</figDesc><table><row><cell>2:</cell><cell cols="2">Sample N -way M -shot (D S train , D S test ) from S</cell></row><row><cell>3:</cell><cell cols="2">Compute ? x = E(x), for x ? X S train ? X S test</cell></row><row><cell>4:</cell><cell>for all (x S test , y S test ) ? D S test do</cell><cell></cell></row><row><cell>5:</cell><cell cols="2">Compute {? x ; ?x ? X S train } with T via Eq. 3</cell></row><row><cell>6:</cell><cell>Predict? S test with {? x } as Eq. 4</cell><cell></cell></row><row><cell>7:</cell><cell cols="2">Compute (? S test , y S test ) with Eq. 1</cell></row><row><cell>8:</cell><cell>end for</cell><cell></cell></row><row><cell>9:</cell><cell>Compute ? E,T (x S test ,y S test )?D S test</cell><cell>(? S test , y S test )</cell></row><row><cell>10:</cell><cell cols="2">Update E and T with ? E,T use SGD</cell></row></table><note>11: end for 12: return Embedding function E and set function T.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Few-shot classification accuracy on MiniImageNet. CTM<ref type="bibr" target="#b27">[28]</ref> and SimpleShot<ref type="bibr" target="#b50">[51]</ref> utilize the ResNet-18. (see SM for the full table with confidence intervals and WRN results.).</figDesc><table><row><cell>Setups ?</cell><cell cols="2">1-Shot 5-Way</cell><cell cols="2">5-Shot 5-Way</cell></row><row><cell>Backbone ?</cell><cell cols="4">ConvNet ResNet ConvNet ResNet</cell></row><row><cell>MatchNet [49]</cell><cell>43.40</cell><cell>-</cell><cell>51.09</cell><cell>-</cell></row><row><cell>MAML [10]</cell><cell>48.70</cell><cell>-</cell><cell>63.11</cell><cell>-</cell></row><row><cell>ProtoNet [43]</cell><cell>49.42</cell><cell>-</cell><cell>68.20</cell><cell>-</cell></row><row><cell>RelationNet [45]</cell><cell>51.38</cell><cell>-</cell><cell>67.07</cell><cell>-</cell></row><row><cell>PFA [35]</cell><cell>54.53</cell><cell>59.60</cell><cell>67.87</cell><cell>73.74</cell></row><row><cell>TADAM [33]</cell><cell>-</cell><cell>58.50</cell><cell>-</cell><cell>76.70</cell></row><row><cell>MetaOptNet [25]</cell><cell>-</cell><cell>62.64</cell><cell>-</cell><cell>78.63</cell></row><row><cell>CTM [28]</cell><cell>-</cell><cell>64.12</cell><cell>-</cell><cell>80.51</cell></row><row><cell>SimpleShot [51]</cell><cell>49.69</cell><cell>62.85</cell><cell>66.92</cell><cell>80.02</cell></row><row><cell cols="2">Instance embedding</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ProtoNet</cell><cell>52.61</cell><cell>62.39</cell><cell>71.33</cell><cell>80.53</cell></row><row><cell cols="2">Embedding adaptation</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BILSTM</cell><cell>52.13</cell><cell>63.90</cell><cell>69.15</cell><cell>80.62</cell></row><row><cell>DEEPSETS</cell><cell>54.41</cell><cell>64.14</cell><cell>70.96</cell><cell>80.93</cell></row><row><cell>GCN</cell><cell>53.25</cell><cell>64.50</cell><cell>70.59</cell><cell>81.65</cell></row><row><cell>FEAT</cell><cell>55.15</cell><cell>66.78</cell><cell>71.61</cell><cell>82.05</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Few-shot classification accuracy and 95% confidence interval on TieredImageNet with the ResNet backbone.</figDesc><table><row><cell>Setups ?</cell><cell cols="2">1-Shot 5-Way 5-Shot 5-Way</cell></row><row><cell>ProtoNet [43]</cell><cell>53.31 ? 0.89</cell><cell>72.69 ? 0.74</cell></row><row><cell>RelationNet</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>.09 ? 0.22 84.58 ? 0.16</figDesc><table><row><cell>Instance embedding</cell><cell></cell><cell></cell></row><row><cell>ProtoNet</cell><cell>68.23 ? 0.23</cell><cell>84.03 ? 0.16</cell></row><row><cell cols="2">Embedding adaptation</cell><cell></cell></row><row><cell>BILSTM</cell><cell>68.14 ? 0.23</cell><cell>84.23 ? 0.16</cell></row><row><cell>DEEPSETS</cell><cell>68.59 ? 0.24</cell><cell>84.36 ? 0.16</cell></row><row><cell>GCN</cell><cell>68.20 ? 0.23</cell><cell>84.64 ? 0.16</cell></row><row><cell>FEAT</cell><cell>70.80 ? 0.23</cell><cell>84.79 ? 0.16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Number of parameters introduced by each set-to-set function in additional to the backbone's parameters.</figDesc><table><row><cell></cell><cell cols="2">BILSTM DEEPSETS</cell><cell>GCN</cell><cell>FEAT</cell></row><row><cell>ConvNet</cell><cell>25K</cell><cell>82K</cell><cell>33K</cell><cell>16K</cell></row><row><cell>ResNet</cell><cell>2.5M</cell><cell>8.2M</cell><cell cols="2">3.3M 1.6M</cell></row><row><cell cols="4">architectures such as ResNet-18 for reference.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>34.38?0.16 29.49?0.16 ProtoNet 35.51?0.16 29.47?0.16 FEAT 36.83?0.17 30.89?0.17 57.04 ? 0.20 72.89 ? 0.16 SEEN UNSEEN COMBINED Random 1.56 ?0.00 20.00?0.00 1.45?0.00 ProtoNet 41.73?0.03 48.64?0.20 35.69?0.03 FEAT 43.94?0.03 49.72?0.20 40.50?0.03</figDesc><table><row><cell></cell><cell></cell><cell>1-Shot</cell><cell>5-Shot</cell></row><row><cell></cell><cell>TPN [30]</cell><cell>55.51</cell><cell>69.86</cell></row><row><cell></cell><cell>TEAM [34]</cell><cell>56.57</cell><cell>72.04</cell></row><row><cell>(a) Few-shot domain generalization</cell><cell cols="3">(b) Transductive few-shot learning</cell><cell>(c) Generalized few-shot learning</cell></row></table><note>FEAT</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>We evaluate our model on three additional few-shot learning tasks: (a) Few-shot domain generalization, (b) Transductive few-shot learning, and (c) Generalized few-shot learning. We observe that FEAT consistently outperform all previous methods or baselines.</figDesc><table><row><cell>Drill</cell><cell>Bed</cell><cell>TV</cell><cell>Flower</cell><cell>Screwdriver</cell></row><row><cell>Train Set</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>from</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>"Clipart"</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Classify</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Test Set</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>from</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>"Real World"</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Bed</cell><cell>Curtains</cell><cell>Refrigerator</cell><cell>Sneakers</cell><cell>Drill</cell></row><row><cell>Train Set</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>from</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>"Clipart"</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Classify</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Test Set</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>from</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>"Real World"</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4</head><label>4</label><figDesc></figDesc><table /><note>(a) gives the quantitative results and Fig- ure 4 qualitatively examines it.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Few-shot classification accuracy? 95% confidence interval on MiniImageNet with ConvNet and ResNet backbones. Our implementation methods are measured over 10,000 test trials.</figDesc><table><row><cell>Setups ?</cell><cell cols="2">1-Shot 5-Way</cell><cell cols="2">5-Shot 5-Way</cell></row><row><cell cols="2">Backbone Network ? ConvNet</cell><cell>ResNet</cell><cell>ConvNet</cell><cell>ResNet</cell></row><row><cell>MatchNet [49]</cell><cell>43.40? 0.78</cell><cell>-</cell><cell>51.09? 0.71</cell><cell>-</cell></row><row><cell>MAML [10]</cell><cell>48.70? 1.84</cell><cell>-</cell><cell>63.11? 0.92</cell><cell>-</cell></row><row><cell>ProtoNet [43]</cell><cell>49.42? 0.78</cell><cell>-</cell><cell>68.20? 0.66</cell><cell>-</cell></row><row><cell>RelationNet [45]</cell><cell>51.38? 0.82</cell><cell>-</cell><cell>67.07? 0.69</cell><cell>-</cell></row><row><cell>PFA [35]</cell><cell>54.53? 0.40</cell><cell>-</cell><cell>67.87? 0.20</cell><cell>-</cell></row><row><cell>TADAM [33]</cell><cell>-</cell><cell>58.50? 0.30</cell><cell>-</cell><cell>76.70? 0.30</cell></row><row><cell>MetaOptNet [25]</cell><cell>-</cell><cell>62.64? 0.61</cell><cell>-</cell><cell>78.63? 0.46</cell></row><row><cell>Baselines</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MAML</cell><cell cols="4">49.24? 0.21 58.05? 0.10 67.92? 0.17 72.41? 0.20</cell></row><row><cell>MatchNet</cell><cell cols="4">52.87? 0.20 65.64? 0.20 67.49? 0.17 78.72? 0.15</cell></row><row><cell>ProtoNet</cell><cell cols="4">52.61? 0.20 62.39? 0.21 71.33? 0.16 80.53? 0.14</cell></row><row><cell cols="2">Embedding Adaptation</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BILSTM</cell><cell cols="4">52.13? 0.20 63.90? 0.21 69.15? 0.16 80.63? 0.14</cell></row><row><cell>DEEPSETS</cell><cell cols="4">54.41? 0.20 64.14? 0.22 70.96? 0.16 80.93? 0.14</cell></row><row><cell>GCN</cell><cell cols="4">53.25? 0.20 64.50? 0.20 70.59? 0.16 81.65? 0.14</cell></row><row><cell>Ours: FEAT</cell><cell cols="4">55.15? 0.20 66.78? 0.20 71.61? 0.16 82.05? 0.14</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Few-shot classification performance with Wide ResNet (WRN)-28-10 backbone on MiniImageNet dataset (mean accuracy?95% confidence interval). Our implementation methods are measured over 10,000 test trials.</figDesc><table><row><cell>Setups ?</cell><cell>1-Shot 5-Way</cell><cell>5-Shot 5-Way</cell></row><row><cell>PFA [35]</cell><cell>59.60? 0.41</cell><cell>73.74? 0.19</cell></row><row><cell>LEO [40]</cell><cell>61.76? 0.08</cell><cell>77.59? 0.12</cell></row><row><cell>SimpleShot [51]</cell><cell>63.50? 0.20</cell><cell>80.33? 0.14</cell></row><row><cell>ProtoNet (Ours)</cell><cell>62.60? 0.20</cell><cell>79.97? 0.14</cell></row><row><cell>Ours: FEAT</cell><cell>65.10 ? 0.20</cell><cell>81.11 ? 0.14</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Few-shot classification performance with</figDesc><table><row><cell>Wide ResNet</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Few-shot classification performance with ConvNet backbone on CUB dataset (mean accuracy?95% confidence interval). Our implementation methods are measured over 10,000 test trials.</figDesc><table><row><cell>Setups ?</cell><cell>1-Shot 5-Way</cell><cell>5-Shot 5-Way</cell></row><row><cell>MatchNet [49]</cell><cell>61.16 ? 0.89</cell><cell>72.86 ? 0.70</cell></row><row><cell>MAML [10]</cell><cell>55.92 ? 0.95</cell><cell>72.09 ? 0.76</cell></row><row><cell>ProtoNet [43]</cell><cell>51.31 ? 0.91</cell><cell>70.77 ? 0.69</cell></row><row><cell>RelationNet [45]</cell><cell>62.45 ? 0.98</cell><cell>76.11 ? 0.69</cell></row><row><cell>Instance Embedding</cell><cell></cell><cell></cell></row><row><cell>MatchNet</cell><cell>67.73 ? 0.23</cell><cell>79.00 ? 0.16</cell></row><row><cell>ProtoNet</cell><cell>63.72 ? 0.22</cell><cell>81.50 ? 0.15</cell></row><row><cell cols="2">Embedding Adaptation</cell><cell></cell></row><row><cell>BILSTM</cell><cell>62.05 ? 0.23</cell><cell>73.51 ? 0.19</cell></row><row><cell>DEEPSETS</cell><cell>67.22 ? 0.23</cell><cell>79.65 ? 0.16</cell></row><row><cell>GCN</cell><cell>67.83 ? 0.23</cell><cell>80.26 ? 0.15</cell></row><row><cell>Ours: FEAT</cell><cell>68.87 ? 0.22</cell><cell>82.90 ? 0.15</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Ablation studies on whether the embedding adaptation improves the discerning quality of the embeddings.</figDesc><table><row><cell>After embed-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 11 :</head><label>11</label><figDesc>Ablation studies on the number of heads in the Transformer of FEAT (with number of layers fixes to one).</figDesc><table><row><cell cols="3">Setups ? 1-Shot 5-Way 5-Shot 5-Way</cell></row><row><cell>1</cell><cell>55.15? 0.20</cell><cell>71.57? 0.16</cell></row><row><cell>2</cell><cell>54.91? 0.20</cell><cell>71.44? 0.16</cell></row><row><cell>4</cell><cell>55.05? 0.20</cell><cell>71.63? 0.16</cell></row><row><cell>8</cell><cell>55.22? 0.20</cell><cell>71.39? 0.16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 12 :</head><label>12</label><figDesc>Ablation studies on the number of layers in the Transformer of FEAT (with number of heads fixes to one).</figDesc><table><row><cell cols="3">Setups ? 1-Shot 5-Way 5-Shot 5-Way</cell></row><row><cell>1</cell><cell>55.15? 0.20</cell><cell>71.57? 0.16</cell></row><row><cell>2</cell><cell>55.42? 0.20</cell><cell>71.44? 0.16</cell></row><row><cell>3</cell><cell>54.96? 0.20</cell><cell>71.63? 0.16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 13 :</head><label>13</label><figDesc>Ablation studies on effects of the contrastive learning of the set-to-set function on FEAT.</figDesc><table><row><cell cols="3">Setups ? 1-Shot 5-Way 5-Shot 5-Way</cell></row><row><cell>? = 10</cell><cell>53.92 ? 0.20</cell><cell>70.41 ? 0.16</cell></row><row><cell>? = 1</cell><cell>54.84 ? 0.20</cell><cell>71.00 ? 0.16</cell></row><row><cell>? = 0.1</cell><cell>55.15 ? 0.20</cell><cell>71.61 ? 0.16</cell></row><row><cell>? = 0.01</cell><cell>54.67 ? 0.20</cell><cell>71.26 ? 0.16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 14 :</head><label>14</label><figDesc>Ablation studies on the prediction strategy (with cosine similarity or euclidean distance) of FEAT.</figDesc><table><row><cell>Setups ?</cell><cell cols="2">1-Shot 5-Way</cell><cell cols="2">5-Shot 5-Way</cell></row><row><cell cols="2">Backbone ? ConvNet</cell><cell>ResNet</cell><cell>ConvNet</cell><cell>ResNet</cell></row><row><cell cols="3">Cosine Similarity-based Prediction</cell><cell></cell></row><row><cell>FEAT</cell><cell cols="4">54.64? 0.20 66.26? 0.20 71.72? 0.16 81.83? 0.15</cell></row><row><cell cols="3">Euclidean Distance-based Prediction</cell><cell></cell></row><row><cell>FEAT</cell><cell cols="4">55.15? 0.20 66.78? 0.20 71.61? 0.16 82.05? 0.14</cell></row><row><cell cols="5">our approach). It might require some careful regularizations</cell></row><row><cell cols="5">to prevent over-fitting, which we leave for future work.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 15 :</head><label>15</label><figDesc>Cross-Domain 1-shot 5-way classification results of the FEAT approach. Supervised 34.38?0.16 29.49?0.16 37.43?0.16 ProtoNet 35.51?0.16 29.47?0.16 37.24?0.16 FEAT 36.83?0.17 30.89?0.17 38.49?0.16</figDesc><table><row><cell>C ? C</cell><cell>C ? R</cell><cell>R ? R</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 16 :</head><label>16</label><figDesc>Results of models for transductive FSL with ConvNet backbone on MiniImageNet. We cite the results of Semi-ProtoNet and TPN from<ref type="bibr" target="#b37">[38]</ref> and<ref type="bibr" target="#b33">[34]</ref> respectively. For TEAM<ref type="bibr" target="#b33">[34]</ref>, the authors do not report the confidence intervals, so we set them to 0.00 in the table. FEAT ? and FEAT ? adapt embeddings with the joint set of labeled training and unlabeled test instances, while make prediction via ProtoNet and Semi-ProtoNet respectively.</figDesc><table><row><cell cols="2">Setups ?</cell><cell cols="2">1-Shot 5-Way 5-Shot 5-Way</cell></row><row><cell cols="2">Standard</cell><cell></cell><cell></cell></row><row><cell cols="2">ProtoNet</cell><cell>52.61 ? 0.20</cell><cell>71.33 ? 0.16</cell></row><row><cell>FEAT</cell><cell></cell><cell>55.15 ? 0.20</cell><cell>71.61 ? 0.16</cell></row><row><cell cols="2">Transductive</cell><cell></cell><cell></cell></row><row><cell cols="2">Semi-ProtoNet [38]</cell><cell>50.41 ? 0.31</cell><cell>64.39 ? 0.24</cell></row><row><cell cols="2">TPN [30]</cell><cell>55.51 ? 0.84</cell><cell>69.86 ? 0.67</cell></row><row><cell cols="2">TEAM [34]</cell><cell>56.57 ? 0.00</cell><cell>72.04 ? 0.00</cell></row><row><cell cols="2">Semi-ProtoNet (Ours)</cell><cell>55.50 ? 0.10</cell><cell>71.76 ? 0.08</cell></row><row><cell>FEAT</cell><cell>?</cell><cell>56.49 ? 0.16</cell><cell>72.65 ? 0.20</cell></row><row><cell>FEAT</cell><cell>?</cell><cell>57.04 ? 0.16</cell><cell>72.89 ? 0.20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 17 :</head><label>17</label><figDesc>Results of generalized FEAT with ConvNet backbone onMiniImageNet. All methods are evaluated on instances composed by SEEN classes, UNSEEN classes, and both of them (COMBINED), respectively.</figDesc><table><row><cell>Measures ?</cell><cell>SEEN</cell><cell>UNSEEN</cell><cell>COMBINED</cell></row><row><cell>1-shot learning</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ProtoNet</cell><cell cols="2">41.73?0.03 48.64?0.20</cell><cell>35.69?0.03</cell></row><row><cell>FEAT</cell><cell cols="2">43.94?0.03 49.72?0.20</cell><cell>40.50?0.03</cell></row><row><cell>5-shot learning</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ProtoNet</cell><cell cols="2">41.06?0.03 64.94?0.17</cell><cell>38.04?0.02</cell></row><row><cell>FEAT</cell><cell cols="2">44.94?0.03 65.33?0.16</cell><cell>41.68?0.03</cell></row><row><cell>Random Chance</cell><cell>1.56</cell><cell>20.00</cell><cell>1.45</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 18 :</head><label>18</label><figDesc>The top-5 low-shot learning accuracy over all classes on the large scale ImageNet<ref type="bibr" target="#b38">[39]</ref> dataset (w/ ResNet-50).</figDesc><table><row><cell>UNSEEN</cell><cell cols="5">1-Shot 2-Shot 5-Shot 10-Shot 20-Shot</cell></row><row><cell>ProtoNet [43]</cell><cell>49.6</cell><cell>64.0</cell><cell>74.4</cell><cell>78.1</cell><cell>80.0</cell></row><row><cell>PMN [52]</cell><cell>53.3</cell><cell>65.2</cell><cell>75.9</cell><cell>80.1</cell><cell>82.6</cell></row><row><cell>FEAT</cell><cell>53.8</cell><cell>65.4</cell><cell>76.0</cell><cell>81.2</cell><cell>83.6</cell></row><row><cell>All</cell><cell cols="5">1-Shot 2-Shot 5-Shot 10-Shot 20-Shot</cell></row><row><cell>ProtoNet [43]</cell><cell>61.4</cell><cell>71.4</cell><cell>78.0</cell><cell>80.0</cell><cell>81.1</cell></row><row><cell>PMN [52]</cell><cell>64.8</cell><cell>72.1</cell><cell>78.8</cell><cell>81.7</cell><cell>83.3</cell></row><row><cell>FEAT</cell><cell>65.1</cell><cell>72.5</cell><cell>79.3</cell><cell>82.1</cell><cell>83.9</cell></row><row><cell cols="6">All w/ Prior 1-Shot 2-Shot 5-Shot 10-Shot 20-Shot</cell></row><row><cell>ProtoNet [43]</cell><cell>62.9</cell><cell>70.5</cell><cell>77.1</cell><cell>79.5</cell><cell>80.8</cell></row><row><cell>PMN [52]</cell><cell>63.4</cell><cell>70.8</cell><cell>77.9</cell><cell>80.9</cell><cell>82.7</cell></row><row><cell>FEAT</cell><cell>63.8</cell><cell>71.2</cell><cell>78.1</cell><cell>81.3</cell><cell>83.4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In the following, we use ?(x i ) and ?x i exchangeably to represent the embedding of an instance x i based on the mapping ?.<ref type="bibr" target="#b1">2</ref> In experiments, we find the temperature scale over logits influences the model training a lot when we optimize based on pre-trained weights.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">For notation simplicity, we omit the bias in the linear projection here. over the input query multiple times (with different sets of parameters), which gives rise to the multi-layer transformer. We discuss the empirical performances with respect to the change number of heads and layers in ? D.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The source code of the ResNet is publicly available on https:// github.com/kjunelee/MetaOptNet<ref type="bibr" target="#b4">5</ref> We use the ResNet backbone with input image size 80 ? 80 ? 3 from<ref type="bibr" target="#b34">[35]</ref> in the old version of our paper<ref type="bibr" target="#b53">[54]</ref>, whose source code of ResNet is publicly available on https://github.com/ joe-siyuan-qiao/FewShot-CVPR.Empirically we find the ResNet-12<ref type="bibr" target="#b24">[25]</ref> works better than our old ResNet architecture.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Labelembedding for attribute-based classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="819" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to learn by gradient descent by gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3981" to="3989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">How to train your MAML</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Layer normalization. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Synthesized classifiers for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5327" to="5336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Predicting visual exemplars of unseen classes for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3496" to="3505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An empirical study and analysis of generalized zero-shot learning for object recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="52" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Domain adaption in one-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML PKDD</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="573" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Model-agnostic metalearning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dropblock: A regularization method for convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y.</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10750" to="10760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Few-shot human motion prediction via meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M F</forename><surname>Moura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="441" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Low-shot visual recognition by shrinking and hallucinating features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised learning via meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Transferable meta learning across domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="177" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Tenenbaum. One shot learning of simple visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CogSci</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Humanlevel concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Metalearning with differentiable convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gradient-based meta-learning with learned layerwise metric and subspace</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2933" to="2942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="594" to="611" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Finding task-relevant features for few-shot learning by category traversal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to propagate labels: Transductive propagation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning unsupervised learning rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<idno>abs/1804.00222</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">On first-order metalearning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<idno>abs/1803.02999</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">TADAM: task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">N</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Transductive episodic-wise adaptive metric for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Few-shot image recognition by predicting parameters from activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Optimization as a model for fewshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Incremental few-shot learning with attention attractor networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5276" to="5286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Meta-learning for semi-supervised few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Meta-learning with latent embedding optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Satorras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Estrach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Adapted deep embeddings: A synthesis of methods for k-shot inductive transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ridgeway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Mozer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="76" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Few-shot learning through an information retrieval lens</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Simpleshot: Revisiting nearest-neighbor classification for few-shot learning. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1911" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Low-shot learning from imaginary data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Piecewise classifier mappings: Learning fine-grained learners for novel categories with few examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="6116" to="6125" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Learning embedding adaptation for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-C</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<idno>abs/1812.03664</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>P?czos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
