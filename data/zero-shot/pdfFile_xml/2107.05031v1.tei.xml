<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semi-Supervised Object Detection with Adaptive Class-Rebalancing Self-Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyuan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Software School of Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianxiang</forename><surname>Pan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Software School of Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Software School of Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Semi-Supervised Object Detection with Adaptive Class-Rebalancing Self-Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This study delves into semi-supervised object detection (SSOD) to improve detector performance with additional unlabeled data. State-of-the-art SSOD performance has been achieved recently by self-training, in which training supervision consists of ground truths and pseudo-labels. In current studies, we observe that class imbalance in SSOD severely impedes the effectiveness of self-training. To address the class imbalance, we propose adaptive class-rebalancing selftraining (ACRST) with a novel memory module called Crop-Bank. ACRST adaptively rebalances the training data with foreground instances extracted from the CropBank, thereby alleviating the class imbalance. Owing to the high complexity of detection tasks, we observe that both self-training and datarebalancing suffer from noisy pseudo-labels in SSOD. Therefore, we propose a novel two-stage filtering algorithm to generate accurate pseudo-labels. Our method achieves satisfactory improvements on MS-COCO and VOC benchmarks. When using only 1% labeled data in MS-COCO, our method achieves 17.02 mAP improvement over supervised baselines, and 5.32 mAP improvement compared with state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Object detection to classify and localize objects in the image is one of the most important research topics in computer vision. In recent years, significant progress has been witnessed in deep-learning-based object detection. The majority of the existing studies follow a fully supervised setting and heavily rely on large datasets with bounding-box annotations. However, creating fully annotated detection datasets costs thousands of hours <ref type="bibr" target="#b27">(Russakovsky, Li, and Fei-Fei 2015;</ref><ref type="bibr" target="#b8">Dollar et al. 2012)</ref>, thereby hindering the practicability of current studies. Therefore, a surge of attention has been dedicated to semi-supervised object detection <ref type="bibr">(SSOD)</ref>. Although SSOD has made immense progress, the current SSOD methods and their fully supervised counterparts continue to have a significant performance gap.</p><p>State-of-the-art SSOD performance has been achieved recently by the self-training paradigm, in which pseudo-labels of unlabeled data are generated to train detectors. However, the majority of advanced self-training algorithms <ref type="bibr" target="#b31">(Tarvainen and Valpola 2017;</ref><ref type="bibr" target="#b33">Xie et al. 2020b;</ref><ref type="bibr" target="#b17">Laine and Aila 2017)</ref> are designed specifically for classification. In the experiments, we observe that using them directly is suboptimal when class imbalance in SSOD considerably hinders the use of self-training.</p><p>Class imbalance is a longstanding challenge in object detection that mainly includes foreground-background imbalance <ref type="bibr" target="#b19">(Lin et al. 2017;</ref><ref type="bibr" target="#b26">Ren et al. 2015;</ref><ref type="bibr" target="#b5">Chen et al. 2019;</ref><ref type="bibr" target="#b4">Cao et al. 2020)</ref> and foreground-foreground imbalance <ref type="bibr" target="#b24">(Peng et al. 2020;</ref><ref type="bibr" target="#b21">Oksuz et al. 2020)</ref>. As presented in <ref type="figure" target="#fig_0">Figure 1</ref>(a), background instances are predominant in the training targets (background instances account for 90% of all training instances) when the foreground-background imbalance exists in detection data. This problem is compounded in pseudolabels (foreground instances only account for 5% of all training instances).</p><p>Apart from foreground-background imbalance, we find a severe foreground-foreground imbalance problem in SSOD. As shown in <ref type="figure" target="#fig_0">Figure 1(b)</ref>, there are some neglected classes in pseudo-labels in 1% COCO-standard. The cause of this balance is two folds. First, pseudo-labels in SSOD are inaccurate due to the high complexity of the detection task. Moreover, the model trained on foreground-background imbalanced labeled data is prone to generate biased predictions.</p><p>The above two-type imbalance yields biased pseudolabels in self-training-based SSOD. Subsequent training on biased pseudo-labels further intensifies the class imbalance, thereby aggravating the performance of the final model.</p><p>To address the preceding issues, an intuitive idea is to utilize data-rebalancing algorithms in classification tasks <ref type="bibr" target="#b23">(Pang et al. 2019;</ref><ref type="bibr" target="#b22">Ouyang et al. 2016;</ref><ref type="bibr" target="#b26">Ren et al. 2015)</ref>. However, this idea is impeded by entanglements of foregroundbackground and foreground-foreground instances in detection data. To decouple these entanglements, we introduce a novel memory module called CropBank to store ground truths/pseudo labels of foreground instances in labeled/unlabeled data. With the CropBank, we propose two detection-specific data-rebalancing algorithms: foregroundbackground rebalancing (FBR) and adaptive foregroundforeground rebalancing (AFFR). We extend the original selftraining paradigm to adaptive class-rebalancing self-training (ACRST) based on FBR and AFFR.</p><p>We first propose FBR to address the foregroundbackground imbalance in SSOD. FBR first extracts foreground instances from entire datasets according to ground truths/pseudo labels stored in the CropBank. Thereafter, foreground instances are augmented and pasted to random arXiv:2107.05031v1 [cs.CV] 11 Jul 2021 locations in training images. With synthetic data, FBR can increase the proportion of foreground instances in training targets and alleviate the foreground-background imbalance.</p><p>For foreground-foreground imbalance, we propose adaptive foreground-foreground rebalancing(AFFR) based on FBR. In particular, we design a novel criterion called pseudo recall to judge whether a class is neglected or over-focused in SSOD. Thereafter, pseudo-labels of the neglected classes are sampled more frequently because of higher negative confidence. Consequently, the entire dataset is foregroundforeground rebalanced, thereby leading to a minimally biased detector for online pseudo-labeling in the subsequent self-training.</p><p>However, as presented in <ref type="figure" target="#fig_1">Figure 2</ref>, the accuracy of pseudo-labels is undesirable. We observe that FBR and AFFR suffer from noisy pseudo-labels in the mutualtraining stage. Therefore, we exploit additional high-level semantics to filter noisy pseudo-labels. In particular, we propose a semi-supervised multi-label classification module to generate image-level pseudo-labels for unlabeled data. Thereafter, we design a two-stage filter mechanism to filter out pseudo-labels that activates negative in classification confidences or image-level pseudo-labels.</p><p>Our method outperforms previous state-of-the-art methods on MS-COCO and VOC benchmarks by significant margins. When using only 1% labeled COCO-standard <ref type="bibr" target="#b20">(Lin et al. 2014</ref>), our method obtains 5.32 mAP improvement over state-of-the-arts. When using VOC07 <ref type="bibr" target="#b10">(Everingham et al. 2010)</ref> as labeled data, our method outperforms stateof-the-arts by 1.43 mAP improvement.</p><p>We summarize our contributions as follows:</p><p>? We design a novel memory module called CropBank to disentangle fore/background and fore/foreground instances in detection data. With the CropBank, we further propose adaptive class-rebalancing self-training (ACRST) to address the foreground-background and foreground-foreground imbalance in SSOD.</p><p>? We propose a semi-supervised multi-label classification module to mine high-level semantics from unlabeled data. Thereafter, we propose a two-stage pseudo-label filtering mechanism with classification confidence and high-level semantics. This mechanism is effective in pseudo-label denoising, thereby further facilitating FBR and AFFR.</p><p>? The proposed data-rebalancing and pseudo-label filtering algorithms are plug-and-play for any self-training-based SSOD framework. Moreover, the CropBank provides an effective detection-specific data augmentation algorithm.   <ref type="bibr" target="#b0">Arazo et al. 2019;</ref><ref type="bibr" target="#b14">Iscen et al. 2019</ref>) exploits high-quality pseudo-labels of unlabeled data to refine the model pre-trained on few labeled data.SSL has made remarkable success in image classification. However, foreground-background imbalance and foreground-foreground in SSOD heavily impede the effectiveness of the current SSL approaches. To address these limitations, we propose a pseudo-labeling-based method to alleviate the class imbalance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semi-supervised Object Detection</head><p>Object detection is a fundamental task in computer vision. Existing object detection frameworks include two-and onestage detectors. Two-stage detectors <ref type="bibr" target="#b26">(Ren et al. 2015;</ref><ref type="bibr" target="#b13">He et al. 2017;</ref><ref type="bibr" target="#b12">Girshick et al. 2014;</ref><ref type="bibr" target="#b11">Girshick 2015)</ref> first generate regions of interest (RoIs) and perform refinement on RoIs thereafter for the final bounding-boxes classification and regression. For one-stage detectors <ref type="bibr" target="#b25">(Redmon et al. 2016;</ref><ref type="bibr" target="#b19">Lin et al. 2017;</ref><ref type="bibr" target="#b18">Law and Deng 2018;</ref><ref type="bibr" target="#b9">Duan et al. 2019)</ref>, their predictions are performed on dense grids directly. Although existing studies have made remarkable progress in the past years, they have primarily focused on detectors in a fully supervised setting. SSOD algorithms, which train detectors with a combination of labeled and unlabeled data following standard SSL settings, have increasing attention recently. CSD <ref type="bibr" target="#b15">(Jeong et al. 2019</ref>) utilizes a consistency-based mechanism, which enforces the model predictions consistency of different flipped versions of the same image for generalized feature learning. Similar to CSD, ISD <ref type="bibr" target="#b16">(Jeong et al. 2020)</ref> imposes consistency-regularization on input images and their mixed versions. STAC <ref type="bibr" target="#b29">(Sohn et al. 2020)</ref> introduces a pseudo-labeling-based method, which first pretrains a detector on available labeled data and generates pseudo-labels on unlabeled data thereafter to re-train the detector. Instant Teaching <ref type="bibr" target="#b36">(Zhou et al. 2021</ref>) develops an SSOD framework with MixUp <ref type="bibr" target="#b35">(Zhang et al. 2018)</ref> and <ref type="bibr">Mosaic (Bochkovskiy, Wang, and Liao 2020)</ref>. Although these studies have improved the performance against the model in a supervised setting, they lack considerations into serious data imbalance issues in SSOD. Recently, Unbiased-Teacher <ref type="bibr" target="#b21">(Liu et al. 2021</ref>) has been proposed recently to utilize focal loss <ref type="bibr" target="#b19">(Lin et al. 2017)</ref> to alleviate class imbalance. However, the effectiveness of focal loss on unlabeled data is impeded by noisy pseudo-labels. Moreover, Unbiased-Teacher does not facilitate an increase in data diversity. To address the preceding issues and enhance the performance of SSOD, we propose FBR and AFFR to alleviate foregroundbackground and foreground-foreground imbalance simultaneously. We also devise a two-stage pseudo-label filtering algorithm with classification confidence and high-level semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>This section describes our solutions in detail.Section defines the problem, while Section introduces the baseline framework Mean Teacher. Section defines the CropBank, and Section introduces the proposed adaptive class self-training rebalancing (ACRST) algorithm. Section illustrates the twostage pseudo-label filtering method. Lastly, Section introduces the selective supervision mechanism. The overview of the entire training process is shown in <ref type="figure">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem Definition</head><p>Semi-supervised object detection aims to train detectors in a semi-supervised setting, where a small labeled dataset</p><formula xml:id="formula_0">D s = {x s i , y s i } Ns i=1 and a large unlabeled dataset D u = {x u i } Nu i=1</formula><p>are available. N s /N u presents the number of labeled/unlabeled data.y s i contains bounding-box annotations including object locations, sizes, and categories in ith labeled image x s i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean Teacher for Semi-supervised Object Detection</head><p>This study takes the Mean Teacher <ref type="bibr" target="#b31">(Tarvainen and Valpola 2017)</ref> as the SSOD baseline. Mean Teacher consists of a teacher and a student model, in which the entire framework is optimized via a mutual learning mechanism. The training pipeline of Mean Teacher consists of two stages.</p><p>Pre-training. The student model is pre-trained with a small amount of labeled data D s via gradient backpropagation in a supervised manner. Thereafter, we initialize the teacher model with pre-trained model weights of the student model, which produce noisy-less pseudo-labels, thereby facilitating the subsequent training.</p><p>Teacher-Student Mutual Learning. In the mutual learning stage, we train the student model with supervision signals consisting of ground truths and pseudo-labels. Once the student model is updated via the gradient back-propagation, the learned knowledge is feedback to the teacher model in an exponential moving average (EMA) mechanism,</p><formula xml:id="formula_1">? s ? ? s + ?L ?? s ,<label>(1)</label></formula><formula xml:id="formula_2">? t ? ?? t + (1 ? ?)? s ,<label>(2)</label></formula><p>where ? s /? t represents the model parameters of the student/teacher model, and L represents the total SSOD losses. By using Faster-RCNN <ref type="bibr" target="#b26">(Ren et al. 2015)</ref> as the detection module, the loss function of SSOD can be summarized as a combination of losses on labeled data L sup and unlabeled data L unsup .</p><formula xml:id="formula_3">L = L sup + ? unsup L unsup ,<label>(3)</label></formula><formula xml:id="formula_4">L sup = ? i L rpn cls (x s i , y s i ) + L rpn reg (x s i , y s i ) +L roi cls (x s i , y s i ) + L roi reg (x s i , y s i ),<label>(4)</label></formula><formula xml:id="formula_5">L unsup = ? i L rpn cls (x u i , y u i ) + L roi cls (x u i , y u i ),<label>(5)</label></formula><p>where L rpn cls is the RPN classification loss,L rpn reg is the RPN regression loss, L roi cls is the ROI classification loss, L roi reg is the ROI regression loss. y s i represents the annotation of the labeled image x s i , y u i represents the pseudo-labels of unlabeled image x u i , and ? unsup is used to balance the supervised and unsupervised losses. Note that regression losses are removed in L unsup in previous SSOD studies for denoising.</p><p>To succeed in SSOD, the teacher model must generate accurate pseudo-labels and maintain a reliable performance margin over the student model throughout the training. However, we observe that class imbalance in SSOD significantly hinders the performance of the teacher model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CropBank</head><p>Data-rebalancing algorithms have been proved to be the most simple and effective data-rebalancing method in classification tasks. However, their effectiveness is heavily : Overview of our methods. We take Mean Teacher as our SSOD baseline. In Mean Teacher, the teacher model generates pseudo labels from weakly augmented unlabeled data, and the student model is trained with a combination of ground-truths and pseudo labels. To alleviate the class imbalance in SSOD, we first design a memory module called CropBank which absorbs instance-level annotations including ground truths and pseudo labels. Thereafter, we utilize the CropBank to perform foreground-background rebalancing (FBR) and adaptive foreground-foreground rebalancing (AFFR) on strong augmented unlabeled data for adaptive class-rebalancing self-training (ACRST). To further improve ACRST, we introduce a two-stage pseudo-label filtering algorithm with classification confidence and high-level semantics.</p><p>impeded by strong interconnections on both foregroundbackground and foreground-foreground instances. To separate the entanglement, we propose a novel memory module called CropBank, which stores abundant instance-level annotations. The CropBank consists of two sub-banks, namely,</p><formula xml:id="formula_6">Labeled CropBank ? L = {y l i } N L i=1 and Pseudo Crop- Bank ? U = { y u i } N U i=1 , where N L /N U represent the size of Labeled/Pseudo CropBank, y l i / y u i represents ground truths/pseudo-labels of ith labeled/unlabeled image.</formula><p>In the implementation, the CropBank size is unlimited owing to the negligible memory consumption of instancelevel annotations. In the training stage, ? L is fixed once generated, while ? U is updated periodically with improved pseudo-labels in mutual training. We use CropBank as the basis to decouple instances and design adaptive classrebalancing self-training (ACRST) to address the class imbalance.  Adaptive Class-Rebalancing Self-Training</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instances in the CropBank</head><p>The self-training paradigm is an ideal solution to alleviate the lack of labeled data. However, its effectiveness is impeded by the inherent class imbalance in object detection tasks. Therefore, we propose ACRST to alleviate the class imbalance in SSOD. ACRST consists of foreground-background rebalancing (FBR) and adaptive foreground-foreground rebalancing (AFFR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Foreground-Background Rebalancing</head><p>Models trained on foreground-background imbalanced data tend to overfit excessive background instances. Foregroundbackground imbalance in object detection has been widely explored. Various solutions have been proposed to alleviate such an imbalance, including loss-reweighting <ref type="bibr" target="#b19">(Lin et al. 2017</ref>) and region refinement <ref type="bibr" target="#b26">(Ren et al. 2015)</ref>. Unfortunately, these methods rely on ground truths unavailable in SSOD to guide the rebalancing procedure. Hence, we utilize abundant instance-level annotations including ground truths and pseudo-labels in the CropBank to perform foregroundbackground rebalancing. Given a training mini-batch B = {x i , y i } N T i=1 , we fetch a set of foreground instances F = {c j , y j } N C j=1 from the Crop-Bank ? L and ? U for each image x i following a sampling distribution P, where c j is a foreground instance cropped from original images with annotation y j . Thereafter, a new training mini-batch</p><formula xml:id="formula_7">B mix = {x mix i , y mix i } N T i=1</formula><p>is generated as follows:</p><p>x</p><formula xml:id="formula_8">mix i = ?x i + (1 ? ?)c j ,<label>(6)</label></formula><p>y mix i = merge(y i , y j ).</p><p>(7) Where ? denotes a binary mask of pasted objects and y mix i denotes mixed annotations, in which fully occluded instances are removed from mixed image x mix i . In detail, c j is a rectangular region cropped from the image based on instance-level annotations in the CropBank. During training, c j is augmented and pasted thereafter to random locations of x i . This combined procedure increases the ratio of foreground instances in the training data for foregroundbackground rebalancing and also explores essential context semantics from a holistic perspective.</p><p>Once mixed images are ready, we take them to train the detector as the pipeline of Mean Teacher. The rebalancing process is shown in <ref type="figure" target="#fig_3">Figure 4</ref>. As discussed in Section , such a crop-and-paste operation enables higher model performance with selective supervision.</p><p>Adaptive Foreground-Foreground Rebalancing FBR adequately alleviates the foreground-background imbalance with considerable attention on foreground instances. However, a random sampling distribution P, such as uniform distribution, fails to correct the foreground-foreground imbalance. Hence, we propose an adaptive sampling probability distribution P for foreground-foreground rebalancing. In particular, samples in neglected classes are selected more frequently during the training.</p><p>To measure the neglected degree of a class, we propose a novel criterion pseudo recall (P R), which quantities the proportion of pseudo-labels to ground truths. In detail, we estimate the class distribution of unlabeled data from labeled data on account of distribution similarity between labeled and unlabeled data. Suppose there are K classes {1, 2, ..K} in datasets. We calculate pseudo recall for class k as</p><formula xml:id="formula_9">P R k = N u k rN l k ,<label>(8)</label></formula><p>where N u k and N l k denote the number of pseudo-labels and ground truths of class k, and r is the ratio of the unlabeled to labeled data.</p><p>Pseudo recall defines how neglected one class is under the SSOD setting. High P R k indicates that the detector is certain even overconfident on class k. Consequently, lower sampling probabilities should be allocated to samples in class k for overfitting alleviation. By contrast, low P R k implies that the detector lacks confidence for detecting instances of class k. Therefore, we should select these instances frequently. As a solution, we sort the classes in descending order according to pseudo recall and design the following adaptively sampling strategy:</p><formula xml:id="formula_10">? k = P R K?k+1 ? K i=1 P R i ? ,<label>(9)</label></formula><p>where ? k is the probability of selecting instances of class k, and ? is used to tune the sampling probability. This mechanism adaptively allocates higher/lower sampling rates to neglected/over-focused instances. Note that AFFR performs FBR simultaneously. There are numerous ways to rebalance class distribution, and we introduce an effective example. A potential problem with this mechanism is that the noise of pseudo-labels in the neglected classes is amplified. Therefore, we propose a two-stage pseudo-label filtering mechanism in Section .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Two-stage Pseudo-label Filtering</head><p>The proposed ACRST considerably alleviates the class imbalance in SSOD. However, its effectiveness is heavily affected by the quality of pseudo-labels. Once noise in the CropBank is selected improperly, it will be undesirably amplified in self-training. Consequently, we should filter noisy pseudo-labels from the teacher model predictions and store noisy-less pseudo-labels in the CropBank. In SSOD, the general filtering algorithm sets a threshold ? cls to filter predictions with low classification confidence out. However, such single-stage filtering without additional semantics constraints is prone to produce noisy pseudo-labels. As a solution, we propose a semi-supervised multi-label classification module to learn high-level semantics (i.e., image-level pseudo-labels). Thereafter, we design a two-stage filtering algorithm with classification confidences and high-level semantics to generate accurate pseudo-labels.</p><p>Semi-supervised Multi-label Classification.</p><p>The proposed semi-supervised multi-label classification module is devised based on Mean Teacher for the classification task. For each image x i , we particularly aim to predict its image-level pseudo-labels v i = {l k } K k=1 , l k ? {0, 1}, where K is the total category number and l k determines whether there are instances of class k in the image. In the training stage, predictions of the teacher model are converted to image-level pseudo-labels which supervise the student model. We utilize a focal-binary-cross-entropy loss to optimize the student model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Two-stage Pseudo-label Filtering.</head><p>For bounding-box prediction i with classification score s i of image x j with image-level pseudo-label y j , we perform a two-stage filtering to get bounding-box pseudo-labels with low-noise. In the first stage, we filter predictions out with scores s &lt; ? cls to remove predictions with low objectness or wrong class labels. In the second stage, predictions whose classes activate negative in y j (i.e., activation values are smaller than ? ml ) are removed. The second filtering stage utilizes high-level semantics to filter noisy predictions inconsistent with image-level pseudo-labels. With the twostage filtering, the consistency of low-and high-level semantics are achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selective Supervision</head><p>In previous SSOD research, bounding-box regression losses are removed during training to alleviate noise. By contrast, utilizing regression losses in our framework is beneficial to achieve high SSOD performance, which is attributed to the CropBank module.</p><p>The contribution is two-fold. First, the CropBank alleviates noise from partially detected instances, which take a large proportion in bias predictions. Learning blindly with these noisy pseudo-labels will heavily aggravate the model performance. However, when the partially detected bounding-boxes from the CropBank are cropped and pasted to training batches, they become independent and complete in the new background, thereby providing additional clean training supervisions. Second, the CropBank provides a detection-specific data augmentation method. The additional augmented data continuously improves the regression accuracy of pseudo-labels.</p><p>With selective supervision, loss function L unsup in Equation 5 can be represented as follows:</p><formula xml:id="formula_11">L unsup = ? i L rpn cls (x u i , y u i ) + L rpn reg (x u i , y ss i ) +L roi cls (x u i , y u i ) + L roi reg (x u i , y ss i ),<label>(10)</label></formula><p>where y ss i denotes instances from the CropBank in x u i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Datasets</head><p>We evaluate our method on three SSOD benchmarks from MS-COCO <ref type="bibr" target="#b20">(Lin et al. 2014</ref>) and PASCAL VOC <ref type="bibr" target="#b10">(Everingham et al. 2010</ref>).</p><p>1. COCO-standard: We sample 0.5/1/2/5/10% of the COCO2017-train set as the labeled dataset and take the remaining data as the unlabeled dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">COCO-additional:</head><p>We use the COCO2017-train set as the labeled dataset and the additional COCO2017-unlabeled set as the unlabeled dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">VOC07&amp;12:</head><p>We use the VOC07-trainval set as the labeled dataset and the VOC12-trainval set as the unlabeled dataset.</p><p>We evaluate the model performance on the COCO2017-val set for (1)(2) and VOC07-test set for (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>We use FPN-Faster-RCNN with ResNet-50 backbone as the detection module. ResNet-50 is initialized with Im-ageNet pre-trained weights. We set the hyper-parameters ? unsup = 2, ? = 2. For two-stage pseudo-label filtering, we use classification confidence threshold ? cls = 0.7 and multi-label confidence threshold ? ml = 0.2. We use AP 50:95 , i.e, mAP as the evaluation metric. We construct each training batch with 32 labeled and 32 unlabeled images for all the training settings. For the COCO-standard, the pre-training stage takes 2500/5000/10000/20000/40000 steps for 0.5/1/2/5/10% COCO-standard and 180000 steps for the whole training stage of COCO-standard. For COCOadditional, the pre-training stage takes 90000 steps in total 270000 training steps. For VOC07&amp;12, the pre-training stage takes 12000 steps and 36000 steps for the entire training stage. Strong augmentations in our research consist of random jittering, Gaussian noise, and random crop. Weak augmentations in our study consist of random resize and flip. Moreover, we set the above hyper-parameters without aggressively searching. Consequently, high model performance may be achieved with improved choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Comparisons</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>COCO-standard</head><p>We first evaluate the efficacy of our method on COCOstandard. As shown in <ref type="table" target="#tab_2">Table 1</ref>, when only 0.5% to 10% of the entire dataset are labeled, our model consistently performs better against all previous studies in CSD, STAC, Instant Teaching, and Unbiased Teacher. When trained on the 1% COCO-standard, our method achieves 5.32 mAP improvement compared Unbiased-Teacher. The mAP is even higher than CSD trained on 10% COCO-standard. When trained on 10% COCO-standard, our method achieves 10.42 mAP improvement compared with supervised baselines. We attribute the success of model performance to two factors. Class rebalanced data. Our method alleviates the class imbalance in SSOD with two rebalancing algorithms (i.e., FBR and AFFR). Foreground-background rebalanced data prevents the model from overfitting on background instances and helps mine beneficial information from enormous unlabeled data. Foreground-foreground rebalanced data benefits the model predictions with information from neglected classes and avoids biased predictions on overfocused classes.</p><p>Noise-less pseudo-labels. When using the self-training paradigm for SSOD, accurate and reliable pseudo-labels from pre-trained models should be generated. We propose a teacher-student mutual learning mechanism for progressive pseudo-labels refinement. In addition, we introduce a two-stage pseudo-label filtering algorithm to remove noisy predictions with classification confidence and high-level semantics. With accurate pseudo-labels, the student model is well optimized and gives beneficial feedback to the teacher model. We present an ablation study on two-stage pseudolabel filtering in Section .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>COCO-additional</head><p>In this section, we verify whether our method can further improve the model trained on a large-scale labeled dataset with additional unlabeled data. <ref type="table" target="#tab_3">Table 2</ref> shows that our model has a 0.41 mAP improvement compared with those of previous methods, and 1.51 mAP improvement compared with the supervised baseline. This result indicates that our method achieves satisfying improvement even on the well-trained model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VOC07&amp;12</head><p>We evaluate models on a less imbalanced dataset VOC07&amp;12 to demonstrate the generalization of our method. <ref type="table" target="#tab_4">Table 3</ref> provides the mAP results of CSD, STAC, Unbiased Teacher, and our method. Our method achieves 7.99 mAP improvement compared with the supervised setting, and 1.43 mAP improvement against previous stateof-the-art methods, even though Unbiased Teacher has witnessed performance saturation in VOC07&amp;12. We owe the success of our approach to the generalization ability of ACRST. Even if training data is foreground-foreground balanced, FBR can substantially alleviate the inevitable foreground-background imbalance in SSOD. Furthermore, the two-stage pseudo-label filtering mechanism benefits the model trained on VOC07&amp;12.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>Foreground-Background Rebalancing (FBR) We first verify the effect of FBR. <ref type="table" target="#tab_5">Table 4</ref> shows that applying FBR improves mAP in 1% labeled COCO from     20.75 to 23.32. To analyze the divergent results, we visualize the foreground-background distribution of the rebalanced pseudo-labels. <ref type="figure">Figure 5(a)</ref> shows that after rebalancing, the distribution of the foreground and background instances is rebalanced. The ratio of foreground instances in rebalanced pseudo-labels is even higher than that of ground truths. Therefore, training detectors with rebalanced training data alleviates data bias and produces high mAP.</p><p>Adaptive Foreground-Foreground Rebalancing (AFFR) <ref type="table" target="#tab_5">Table 4</ref> shows that AFFR improves mAP from 23.32 to 24.17 based on FBR. We verify the effectiveness of AFFR by analyzing the foreground class distribution. <ref type="figure">Figure 5</ref>(b) presents that AFFR alleviates foreground-foreground imbalance and reduces KL-divergence from 0.0982 to 0.084. This result confirms the effectiveness of AFFR in handling foreground-foreground imbalance issues in pseudo-labels. Using AFFR can generate an unbiased training data distribution and results in a higher mAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Two-stage Pseudo-label Filtering</head><p>We also verify the effectiveness of the two-stage pseudolabel filtering with classification confidences and high-level semantics. As presented in <ref type="table" target="#tab_5">Table 4</ref>, the model that filters pseudo-labels with high-level semantics can favorably improve model performance against the model with only classification confidence. The two-stage filtering mechanism utilizes an uncertainty mechanism where only predictions with high objectness and follow image-level constraints are regarded as accurate pseudo-labels. <ref type="figure">Figure 6(a)</ref> shows that the two-stage filtering mechanism has a continuous improvement on the accuracy of pseudo-labels. This confirms the effectiveness of the two-stage filtering mechanism in removing noisy predictions in SSOD. Moreover, a two-stage filtering mechanism is necessary to build a clean Pseudo Crop-Bank and further improve the performances of ACRST. Table 4 indicates that applying the two-stage filtering mechanism improves the mAP from 20.75 to 23.48. Moreover, applying the mechanism further improves the mAP of the model trained with ACRST from 24.17 to 25.56. This result confirms that the two-stage filtering mechanism is effective in handling the noisy pseudo labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selective Supervision (SS)</head><p>Lastly, we examine the effectiveness of selective supervision in SSOD. As presented in <ref type="table" target="#tab_5">Table 4</ref>, the selective supervision improves the mAP from 25.56 to 26.12 in 1% COCO-standard. We owe the improvement to the crop-and-paste operation in ACRST, in which incomplete instances are pasted to a new background in the training data. As a result, utilizing these instances as targets of regression optimization produces less noise compared to regress them directly in originating images. We further analyze the distribution of regression accuracy of pseudo-labels. <ref type="figure">Figure 6(b)</ref> shows that selective supervision improves the mIoU of pseudo-labels. Accordingly, transferring these incomplete predictions to complete objects in a new background alleviates noise in the regression targets and improves the model performance. Selective supervision is still under exploration. For example, the current strategy fails to handle noise in which objects are overlapped with each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussions CropBank</head><p>Data augmentations that generate different views of the same image are necessary to pseudo-labeling-based and consistency-regularization-based semi-supervised learning. Conventional data augmentations consist of color jittering <ref type="bibr" target="#b2">(Berthelot et al. 2019a</ref>), rotation <ref type="bibr" target="#b15">(Jeong et al. 2019)</ref>, and Gaussian Noise . Although these methods are effective in vision tasks, they fail to change the image semantics. MixUp <ref type="bibr" target="#b35">(Zhang et al. 2018)</ref> and <ref type="bibr">Mosaic (Bochkovskiy, Wang, and Liao 2020)</ref> are proposed to change the image semantics by mixing images.However, they fail to decouple instances semantics in detection data.</p><p>The CropBank provides a detection-specific data augmentation that effectively decouples entangled semantics in images. The idea of the CropBank is inspired by the Cut-Mix <ref type="bibr" target="#b34">(Yun et al. 2019)</ref>.</p><p>The difference between the CropBank and the CutMix is two-fold. First, the CropBank decouples instances in detection data and creates training new detection datasets with complex disentangled semantics, while the CutMix is classification-specific and unable to decouple semantics. Second, the CropBank adaptively injects semantics from the entire dataset to training images, but the CutMix only exchanges image-to-image semantics. <ref type="table" target="#tab_6">Table 5</ref> provides the model performance with different data augmentations in SSOD. The CropBank improves AP 50:95 from 16.00 to 16.85 compared to MixUp and Mosaic in <ref type="bibr" target="#b36">(Zhou et al. 2021)</ref>.</p><p>Moreover, the CropBank can be easily embedded in semantic and instance segmentation with few modifications. We will release related researches in future studies. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image-level Pseudo-labels</head><p>There is an unexplored question in our study: Why we utilize high-level semantics (i.e., image-level pseudo-labels) to fil- ter noisy pseudo-labels instead of mining neglected predictions? To answer this question, we first evaluate the model trained in three different settings.</p><p>(1) One-stage: Predictions with low classification confidence are filtered.</p><p>(2) Twostage filtering: Predictions with low classification confidence or low activation in image-level pseudo-labels are filtered.</p><p>(3) Two-stage Mining: Predictions with high classification confidence or high activation in image-level pseudolabels are reserved. Thereafter, we calculate the AP 50:95 , accuracy, and recall of pseudo-labels. <ref type="table" target="#tab_7">Table 6</ref> indicates that the two-stage mining mechanism improves recall from 0.299 to 0.376 and reduces accuracy from 0.735 to 0.722, compared to the one-stage strategy. The two-stage filtering mechanism improves the accuracy from 0.735 to 0.792 and reduces the recall from 0.299 to 0.292 compared to the one-stage filtering. Although the recall improvement of the two-stage mining is higher than the accuracy improvement of the two-stage filtering, the AP 50:95 improvement of the latter is 2.73, which is higher compared to the former.</p><p>This result indicates that a higher accuracy of pseudolabels is considerably more important than a higher recall. Hence, future studies should carefully consider the balance between accuracy and recall. We will further analyze this problem in future studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>This study proposes ACRST based on a novel memory module called CropBank to address the class imbalance in SSOD. ACRST considerably alleviates foregroundbackground and foreground-foreground imbalance with proposed FBR and AFFR. To further improve FBR and AFFR, we design a two-stage pseudo-label filtering algorithm with classification confidence and high-level semantics. Over iterations on rebalanced training data, SSOD detectors become unbiased and ameliorate the model performance progressively. Extensive experiments on benchmarks demonstrate the effectiveness of our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Class imbalance in pseudo-labels in 1% COCO-standard. (a) Foreground-background imbalance. In Faster-RCNN, background instances are predominant in the training targets of pseudo-labels. (b) Foreground-foreground imbalance is compounded in pseudo-labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Accuracy and Recall of pseudo-labels for each class in 1% COCO-standard.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 3: Overview of our methods. We take Mean Teacher as our SSOD baseline. In Mean Teacher, the teacher model generates pseudo labels from weakly augmented unlabeled data, and the student model is trained with a combination of ground-truths and pseudo labels. To alleviate the class imbalance in SSOD, we first design a memory module called CropBank which absorbs instance-level annotations including ground truths and pseudo labels. Thereafter, we utilize the CropBank to perform foreground-background rebalancing (FBR) and adaptive foreground-foreground rebalancing (AFFR) on strong augmented unlabeled data for adaptive class-rebalancing self-training (ACRST). To further improve ACRST, we introduce a two-stage pseudo-label filtering algorithm with classification confidence and high-level semantics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Data-rebalancing with instances in the Crop-Bank.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Ablation study on the FBR (a) and AFFR (b). (a) FBR alleviates foreground-background imbalance in pseudo-labels in 1% COCO-standard. (b) AFFR reduces the KL-Divergence(KLD) between the pseudo-labels distribution and the ground truths distribution from 0.0982 to 0.084 in 1% COCO-standard. Pseudo-labels improvement on Box Accuracy and Box mIoU in 1% COCO-standard. (a) Box accuracy of pseudolabels with/without two-stage pseudo-label filtering. (b) Box mIoU between pseudo-labels and ground truths with/without selective supervision (SS).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Experimental results on COCO-standard comparing with CSD, STAC and Unbiased Teacher. ? 0.16 12.70 ? 0.15 18.47 ? 0.22 23.86 ? 0.81 CSD(Jeong et al. 2019) 7.41 ? 0.21 10.51 ? 0.06 13.93 ? 0.12 18.63 ? 0.07 22.46 ? 0.08 STAC(Sohn et al. 2020) 9.78 ? 0.53 13.97 ? 0.35 18.25 ? 0.25 24.38 ? 0.12 28.64 ? 0.21 Instant Teaching(Zhou et al. 2021) -18.05 ? 0.15 22.45 ? 0.15 26.75 ? 0.05 30.40 ? 0.05 Unbiased Teacher(Liu et al. 2021) 16.94 ? 0.23 20.75 ? 0.12 24.30 ? 0.07 28.27 ? 0.11 31.5 ? 0.10 Ours 19.62 ? 0.37 26.07 ? 0.46 28.69 ? 0.17 31.35 ? 0.13 34.92 ? 0.22</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>COCO-standard (AP 50:95 )</cell><cell></cell></row><row><cell></cell><cell>0.5%</cell><cell>1%</cell><cell>2%</cell><cell>5%</cell><cell>10%</cell></row><row><cell>Supervised</cell><cell>6.83 ? 0.15</cell><cell>9.05</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Experimental results on COCO-additional with CSD, STAC, and Unbiased Teacher.</figDesc><table><row><cell>Note that N?represents N?90K</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Experimental results on VOC07&amp;12 comparing with CSD, STAC and Unbiased Teacher.</figDesc><table><row><cell></cell><cell cols="4">Labeled Unlabeled AP 50 AP 50:95</cell></row><row><cell>Supervised</cell><cell>VOC07</cell><cell>None</cell><cell>72.63</cell><cell>42.13</cell></row><row><cell>CSD(Jeong et al. 2019)</cell><cell>VOC07</cell><cell>VOC12</cell><cell>74.70</cell><cell>-</cell></row><row><cell>STAC(Sohn et al. 2020)</cell><cell>VOC07</cell><cell>VOC12</cell><cell>77.45</cell><cell>44.64</cell></row><row><cell cols="2">Unbiased Teacher(Liu et al. 2021) VOC07</cell><cell>VOC12</cell><cell>77.37</cell><cell>48.69</cell></row><row><cell>Ours</cell><cell>VOC07</cell><cell>VOC12</cell><cell>78.16</cell><cell>50.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation study in 1% COCO-standard.</figDesc><table><row><cell>FBR AFFR Two-Stage SS AP 50:95</cell></row><row><cell>20.75</cell></row><row><cell>23.48</cell></row><row><cell>23.32</cell></row><row><cell>24.17</cell></row><row><cell>25.56</cell></row><row><cell>26.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>STAC<ref type="bibr" target="#b29">(Sohn et al. 2020)</ref> performance in 1% COCOstandard under different data augmentations.</figDesc><table><row><cell>Augmentations</cell><cell>AP 50:95</cell></row><row><cell>MixUp and Mosaic (Zhou et al. 2021)</cell><cell>16.00</cell></row><row><cell>CropBank</cell><cell>16.85</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Model Performance, Accuracy and Recall of pseudo-labels in 1% COCO-standard in three settings.</figDesc><table><row><cell>Setting</cell><cell cols="3">Accuracy Recall AP 50:95</cell></row><row><cell>One-stage</cell><cell>0.735</cell><cell>0.299</cell><cell>20.75</cell></row><row><cell>Two-stage Filtering</cell><cell>0.792</cell><cell>0.292</cell><cell>23.48</cell></row><row><cell>Two-stage Mining</cell><cell>0.722</cell><cell>0.376</cell><cell>21.56</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Pseudo-Labeling and Confirmation Bias in Deep Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
		<idno>abs/1908.02983</idno>
		<ptr target="http://arxiv.org/abs/1908.02983" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning with Pseudo-Ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Alsharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2014/hash/66be31e4c40d676991f2405aaecc6934-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-08" />
			<biblScope unit="page" from="3365" to="3373" />
		</imprint>
	</monogr>
	<note>Ghahramani</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">ReMixMatch: Semi-Supervised Learning with Distribution Alignment and Augmentation Anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<idno>abs/1911.09785</idno>
		<ptr target="http://arxiv.org/abs/1911.09785" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">MixMatch: A Holistic Approach to Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">; H M</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>; D&amp;apos;alch?-Buc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Liao</surname></persName>
		</author>
		<ptr target="CoRRabs/2004.10934.URLhttps://arxiv.org/abs/2004.10934" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="5050" to="5060" />
		</imprint>
	</monogr>
	<note>YOLOv4: Optimal Speed and Accuracy of Object Detection</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Prime Sample Attention in Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR42600.2020.01160</idno>
		<ptr target="https://doi.org/10.1109/CVPR42600.2020.01160" />
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06-13" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="11580" to="11588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards Accurate One-Stage Object Detection With AP-Loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00526</idno>
		<ptr target="http://openaccess.thecvf.com/contentCVPR2019/html/" />
	</analytic>
	<monogr>
		<title level="m">Chen Towards Accurate One-Stage Object Detection With AP-Loss CVPR</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-16" />
			<biblScope unit="page" from="5119" to="5127" />
		</imprint>
	</monogr>
	<note>IEEE Conference on Computer Vision and Pattern Recognition. 2019 paper.html</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Simple Framework for Contrastive Learning of Visual Representations</title>
		<ptr target="http://proceedings.mlr.press/v119/chen20j.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pedestrian Detection: An Evaluation of the State of the Art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2011.155</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="743" to="761" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">CenterNet: Keypoint Triplets for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00667</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2019.00667" />
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019</title>
		<meeting><address><addrLine>Seoul, Korea (South; Oc</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-11-27" />
			<biblScope unit="page" from="6568" to="6577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Pascal Visual Object Classes (VOC) Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.169</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2015.169" />
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision, ICCV 2015</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2014.81</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2014.81" />
	</analytic>
	<monogr>
		<title level="j">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="page" from="580" to="587" />
			<date type="published" when="2014-06-23" />
			<publisher>IEEE Computer Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.322</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2017.322" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Label Propagation for Deep Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Iscen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00521</idno>
		<ptr target="http://openaccess.thecvf.com/contentCVPR2019/html/" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-16" />
			<biblScope unit="page" from="5070" to="5079" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE. Iscen Label Propagation for Deep Semi-Supervised Learning CVPR 2019 paper.html</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Consistency-based Semi-supervised Learning for Object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">; H M</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>; D&amp;apos;alch?-Buc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/hash/d0f4dae80c3d0277922f8371d5827292-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="10758" to="10767" />
		</imprint>
	</monogr>
	<note>Wallach,</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Interpolation-based semi-supervised learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hyun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>ArXiv abs/2006.02158</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Temporal Ensembling for Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aila</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJ6oOfqge" />
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">CornerNet: Detecting Objects as Paired Keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<idno>ArXiv abs/1808.01244</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Focal Loss for Dense Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.324</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2017.324" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="2999" to="3007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>978-3-319-10602-1</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014</title>
		<editor>Tuytelaars, T.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unbiased Teacher for Semi-Supervised Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Oksuz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Cam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kalkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Akbas</surname></persName>
		</author>
		<idno>ArXiv abs/2102.09480</idno>
	</analytic>
	<monogr>
		<title level="m">Imbalance Problems in Object Detection: A Review. Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Factors in Finetuning Deep Model for Object Detection with Long-Tail Distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.100</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.100" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="864" to="873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Libra R-CNN: Towards Balanced Learning for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00091</idno>
		<ptr target="http://openaccess.thecvf.com/contentCVPR2019/html/PangLibraR-CNNTowards" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-16" />
			<biblScope unit="page" from="821" to="830" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE. Balanced Learning for Object Detection CVPR 2019 paper.html</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Large-Scale Object Detection in the Wild From Imbalanced Multi-Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR42600.2020.00973</idno>
		<ptr target="https://doi.org/10.1109/CVPR42600.2020.00973" />
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06-13" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="9706" to="9715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">You Only Look Once: Unified, Real-Time Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.91</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.91" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2015/hash/14bfa6bb14875e45bba028a21ed38046-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Best of both worlds: Human-machine collaboration for object annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno>doi:10.1109/ CVPR.2015.7298824</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2121" to="2131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Regularization With Stochastic Transformations and Perturbations for Deep Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Javanmardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">; D D</forename><surname>Tasdizen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2016/hash/30ef30b64204a3088a26bc2e6ecf7602-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05" />
			<biblScope unit="page" from="1163" to="1171" />
		</imprint>
	</monogr>
	<note>Lee</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A Simple Semi-Supervised Learning Framework for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04757</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Takeru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shin-Ichi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Masanori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/hash/68053af2923e00204c3ca7c6a3150cf7-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised Data Augmentation for Consistency Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/44feb0096faa8326192570788b38c1d1-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Self-Training With Noisy Student Improves ImageNet Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR42600.2020.01070</idno>
		<idno>10684-10695. IEEE. doi:10.1109/ CVPR42600.2020.01070</idno>
		<ptr target="https://doi.org/10.1109/CVPR42600.2020.01070" />
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-06-13" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">CutMix: Regularization Strategy to Train Strong Classifiers With Localizable Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00612</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2019.00612" />
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10-27" />
			<biblScope unit="page" from="6022" to="6031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">mixup: Beyond Empirical Risk Minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ciss?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1Ddp1-Rb" />
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Instant-Teaching: An End-to-End Semi-Supervised Object Detection Framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/2103.11402</idno>
		<ptr target="https://arxiv.org/abs/2103.11402" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
