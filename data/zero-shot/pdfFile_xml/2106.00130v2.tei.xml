<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bringing Structure into Summaries: a Faceted Summarization Dataset for Long Scientific Documents</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Meng</surname></persName>
							<email>rui.meng@pitt.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing and Information</orgName>
								<orgName type="institution">University of Pittsburgh</orgName>
								<address>
									<settlement>Mila</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">McGill University ? Microsoft Research</orgName>
								<address>
									<settlement>Montr?al</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khushboo</forename><surname>Thaker</surname></persName>
							<email>k.thaker@pitt.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing and Information</orgName>
								<orgName type="institution">University of Pittsburgh</orgName>
								<address>
									<settlement>Mila</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">McGill University ? Microsoft Research</orgName>
								<address>
									<settlement>Montr?al</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing and Information</orgName>
								<orgName type="institution">University of Pittsburgh</orgName>
								<address>
									<settlement>Mila</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">McGill University ? Microsoft Research</orgName>
								<address>
									<settlement>Montr?al</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Yue</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">?</forename><surname>Xingdi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><forename type="middle">?</forename><surname>Tong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>He ?</roleName><forename type="first">Wang</forename><forename type="middle">?</forename><surname>Daqing</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing and Information</orgName>
								<orgName type="institution">University of Pittsburgh</orgName>
								<address>
									<settlement>Mila</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">McGill University ? Microsoft Research</orgName>
								<address>
									<settlement>Montr?al</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bringing Structure into Summaries: a Faceted Summarization Dataset for Long Scientific Documents</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Faceted summarization provides briefings of a document from different perspectives. Readers can quickly comprehend the main points of a long document with the help of a structured outline. However, little research has been conducted on this subject, partially due to the lack of large-scale faceted summarization datasets. In this study, we present FacetSum, a faceted summarization benchmark built on Emerald journal articles, covering a diverse range of domains. Different from traditional documentsummary pairs, FacetSum provides multiple summaries, each targeted at specific sections of a long document, including the purpose, method, findings, and value. Analyses and empirical results on our dataset reveal the importance of bringing structure into summaries. We believe FacetSum will spur further advances in summarization research and foster the development of NLP systems that can leverage the structured information in both long texts and summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Title</head><p>Emotion in enterprise social media systems</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Purpose</head><p>The purpose of this paper is to investigate enterprise social media systems and quantified gender and status influences on emotional content presented in these systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Internal social media messages were collected from a global software company running an enterprise social media system. An indirect observatory test using Berlo's "source-messagechannel-receiver" model served as a framework to evaluate sender, message, channel and receiver for each text. These texts were categorized by gender and status using text analytics with SAP SA to produce sentiment indications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Findings</head><p>Results reveal women use positive language 2.1 times more than men. Senior managers express positive language 1.7 times more than non-managers, and feeling rules affect all genders and statuses, but not necessarily as predicted by theory. Other findings show that public messages contained less emotional content, and women expressed more positivity to lower status colleagues. Men expressed more positivity to those in higher positions. Many gender and status stereotypes found in face-toface studies are also present in digital enterprise social networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Value</head><p>This study offers a behavioral measurement approach free from validity issues found in self-reported surveys, direct observations and interviews. The collected data offered new perspectives on existing social theories within a new environment of computerized, enterprise social media.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text summarization is the task of condensing a long piece of text into a short summary without losing salient information. Research has shown that a well-structured summary can effectively facilitate comprehension <ref type="bibr">(Hartley et al., 1996;</ref><ref type="bibr">Hartley and Sydes, 1997)</ref>. A case in point is the structured abstract, which consists of multiple segments, each focusing on a specific facet of a scientific publication <ref type="bibr">(Hartley, 2014)</ref>, such as background, method, conclusions, etc. The structure therein can provide much additional clarity for improved comprehension and has long been adopted by databases and publishers such as MEDLINE and Emerald.</p><p>Despite these evident benefits of structure, summaries are often framed as a linear, structure-less sequence of sentences in the flourishing array of summarization studies <ref type="bibr" target="#b11">See</ref>   <ref type="bibr" target="#b8">Paulus et al., 2018;</ref><ref type="bibr">Grusky et al., 2018;</ref><ref type="bibr" target="#b6">Narayan et al., 2018;</ref><ref type="bibr" target="#b12">Sharma et al., 2019;</ref><ref type="bibr" target="#b3">Lu et al., 2020;</ref><ref type="bibr">Cachola et al., 2020)</ref>. We postulate that a primary reason for this absence of structure lies in the lack of a high-quality, large-scale dataset with structured summaries. In fact, existing studies in faceted summarization <ref type="bibr">(Huang et al., 2020;</ref><ref type="bibr" target="#b15">Tauchmann et al., 2018;</ref><ref type="bibr">Jaidka et al., 2016;</ref><ref type="bibr">Contractor et al., 2012;</ref><ref type="bibr" target="#b1">Kim et al., 2011;</ref><ref type="bibr" target="#b0">Jaidka et al., 2018;</ref><ref type="bibr" target="#b13">Stead et al., 2019)</ref> are often conducted with rather limited amount of data that are grossly insufficient to meet today's ever-growing model capacity.</p><p>We aim to address this issue by proposing the FacetSum dataset. It consists of 60,024 scientific articles collected from Emerald journals, each associated with a structured abstract that summarizes the article from distinct aspects including purpose, method, findings, and value. Scale-wise, we empirically show that the dataset is sufficient for training large-scale neural generation models such as BART <ref type="bibr" target="#b2">(Lewis et al., 2020)</ref> for adequate generalization. In terms of quality, each structured abstract in FacetSum is provided by the original author(s) of the article, who are arguably in the best position to summarize their own work. We also provide  quantitative analyses and baseline performances on the dataset with mainstream models in Sections 2 and 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">FacetSum for Faceted Summarization</head><p>The FacetSum dataset is sourced from journal articles published by Emerald Publishing 1 <ref type="figure" target="#fig_0">(Figure 1)</ref>. Unlike many publishers, Emerald imposes explicit requirements that authors summarize their work from multiple aspects (Emerald, 2021): Purpose describes the motivation, objective, and relevance of the research; Method enumerates specific measures taken to reach the objective, such as experiment design, tools, methods, protocols, and datasets used in the study; Findings present major results such as answers to the research questions and confirmation of hypotheses; and Value highlights the work's value and originality 2 . Together, these facets give rise to a comprehensive and informative structure in the abstracts of the Emerald articles, and by extension, to FacetSum's unique ability to support faceted summarization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">General Statistics</head><p>We collect 60,532 publications from Emerald Publishing spanning 25 domains. <ref type="table" target="#tab_1">Table 1</ref> lists some descriptive statistics of the dataset. Since FacetSum is sourced from journal articles, texts therein are naturally expected to be longer compared to other formats of scientific publications. In addition, although each facet is more succinct than the traditional, structure-less abstracts, a full length abstract containing all facets can be considerably longer.</p><p>Empirically, we compare the source and the target lengths with some existing summarization datasets in similar domains including CLPubSum <ref type="bibr">(Collins et al., 2017)</ref>, <ref type="bibr">PubMed (Cohan et al., 2018</ref><ref type="bibr">), ArXiv (Cohan et al., 2018</ref>, SciSummNet <ref type="bibr" target="#b17">(Yasunaga et al., 2019), and</ref><ref type="bibr">SciTldr (Cachola et al., 2020)</ref>. On average, the source length in FacetSum is 58.9% longer <ref type="bibr">(6,827 vs 4,297)</ref>, and the target length is 37.0% longer (290.4 vs 212.0). From a summarization perspective, these differences imply that FacetSum may pose significantly increased modeling and computation challenges due to the increased lengths in both the source and the target. Moreover, the wide range of research domains <ref type="figure" target="#fig_1">(Figure 3</ref>, Appendix D) may also introduce much linguistic diversity w.r.t. vocabulary, style, and discourse. Therefore, compared to existing scientific publication datasets that only focus on specific academic disciplines <ref type="bibr">(Cohan et al., 2018;</ref><ref type="bibr">Cachola et al., 2020)</ref>, FacetSum can also be used to assess a model's robustness in domain shift and systematic generalization.</p><p>To facilitate assessment of generalization, we reserve a dev and a test set each consisting of 6,000 randomly sampled data points; the remaining data are intended as the training set. We ensure that the domain distribution is consistent across all three subsets. Besides, we intentionally leave out Open-Access papers as another test set, to facilitate researchers who do not have full Emerald access 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Structural Alignment</head><p>In this section, we focus our analysis on one of the defining features of FacetSum -its potential to support faceted summarization. Specifically, we investigate how the abstract structure (i.e., facets) aligns with the article structure. Given an abstract facet A and its corresponding article S, we quantify this alignment by:</p><formula xml:id="formula_0">SA = {arg max s i ?S (Rouge-1(si, aj)) : aj ? A} (1)</formula><p>Semantically, S A consists of sentence indices in S that best align with each sentence in A. Sentence-level Alignment We first plot the tuples {(s i , i/|S|) : i ? S A }, where s i is the i-th sentence in S, and |S| is the number of sentences in S. Intuitively, the plot density around position i/|S| entails the degree of alignment between the facet <ref type="figure">Figure 2</ref>: Oracle sentence distribution over a paper. X-axis: 10,000 papers sampled from FacetSum, sorted by full text length from long to short; y-axis: normalized position in a paper. We provide each sub-figure's density histogram on their right.  A and the article S at that position 4 . With 10,000 articles randomly sampled from FacetSum, Figure 2 exhibits distinct differences in the density distribution among the facets in FacetSum. For example, with A = Purpose, resemblance is clearly skewed towards the beginning of the articles, while Findings are mostly positioned towards the end; the Method distribution is noticeably more uniform than the others. These patterns align well with intuition, and are further exemplified by the accompanying density histograms. Section-level Alignment We now demonstrate how different abstract facets align with different sections in an article. Following conventional structure of scientific publications <ref type="bibr" target="#b14">(Suppe, 1998;</ref><ref type="bibr" target="#b10">Rosenfeldt et al., 2000)</ref>, we first classify sections into Introduction, Method, Result and Conclusion using keyword matching in the section titles. <ref type="bibr">5</ref> Given a section S i ? S and an abstract A j ? A, we define the section-level alignment g(</p><formula xml:id="formula_1">S i , A j ) as Rouge-1(cat(S i A j ), cat(A j )), where cat(?)</formula><p>denotes sentences concatenation, and S i A j is defined by Equation <ref type="formula">(1)</ref>. <ref type="table" target="#tab_3">Table 2</ref> is populated by varying A j and S i across the rows and columns, respectively. Full denotes the full paper or abstract (concatenation of all facets). We also include the concatenation of introduction and conclusion (denoted I+C) as a possible value for S i , due to its demonstrated effectiveness as summaries in prior work <ref type="bibr">(Cachola et al., 2020)</ref>.</p><p>The larger numbers on the diagonal (in red) empirically confirm a strong alignment between FacetSum facets and their sectional counterparts in articles. We also observe a significant performance gap between using I+C and the full paper as S i . One possible reason is that the summaries in FacetSum (particularly Method and Findings) may contain more detailed information beyond introduction and conclusion. This suggests that for some facets in FacetSum, simple tricks to condense full articles do not always work; models need to instead comprehend and retrieve relevant texts from full articles in a more sophisticated manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>We use FacetSum to benchmark a variety of summarization models from state-of-the-art supervised models to unsupervised and heuristics-based models. We also provide the scores of a sentence-level extractive oracle system . We report Rouge-L in this section and include Rouge-1/2 results in Appendix E. Unsupervised Models vs Heuristics We report performances of unsupervised and heuristics summarization methods (see <ref type="table" target="#tab_5">Table 3</ref>). Tailoring to the unique task of generating summaries for a specific facet, we only use the section (defined in Section 2.2) corresponding to a facet as model input. Evaluation is also performed on the concatenation  of all facets (column Full), which resembles the traditional research abstract. Lead-K/Tail-K are two heuristic-based models that extract the first/last k sentences from the source text. We observe that heuristic models do not perform well on Full, where the unsupervised models can achieve decent performance. Nevertheless, all models perform poorly on summarizing individual facets, and unsupervised models fail to perform better than simple heuristics consistently. The inductive biases of those models may not be good indicators of summary sentences on specific facets. A possible reason is that they are good at locating overall important sentences of a document, but they cannot differentiate sentences of each facet, even we try to alleviate this by using the corresponding section as input. Supervised Models As for the supervised baseline, we adopt the BART model <ref type="bibr" target="#b2">(Lewis et al., 2020)</ref>, which has recently achieved SOTA performance on abstractive summarization tasks with scientific articles <ref type="bibr">(Cachola et al., 2020)</ref>. We propose two training strategies for the BART model, adapting it to handle the unique challenge of faceted summarization in FacetSum. In BART, we train the model to generate the concatenation of all facets, joined by special tokens that indicate the start of a specific facet (e.g., |PURPOSE| to indicate the start of Purpose summary). During evaluation, the generated text is split into multiple facets based on the special tokens, and each facet is compared against the corresponding ground-truth summary.</p><p>In BART-Facet, we train the model to generate one specific facet given the source text and an indicator specifies which facet to generate. Inspired by CATTS (Cachola et al., 2020), we prepend section tags at the beginning of each training input to generate summaries for a particular facet (see implementation details in Appendix C).</p><p>Empirically, supervised models outperform unsupervised baselines by a large margin (Table 3). Comparing between the two training strategies, BART-Facet outperforms BART significantly. While BART performs comparably on Purpose, performance decreases drastically for subsequent facets, possibly due to current models' inadequacy with long targets. Thus it can perform decently at the beginning of generation (?40 on Purpose), where the dependency is relatively easy-to-handle. However, the output quality degrades quickly towards the end (?5 on Value).</p><p>With I+C as source text, both training strategies exhibit much better results than using full paper. This is opposite to the observation in <ref type="table" target="#tab_3">Table 2</ref>, potentially due to the limitation of the current NLG systems, i.e., the length of source text has crucial impacts to the model performance. With the much extended positional embeddings in our models (10,000 tokens), we suspect some other issues such as long term dependencies may lead to this discrepancy, which warrants further investigation.</p><p>We acknowledge several previous efforts towards faceted summarization. Prior to our study, generating structured abstracts for scientific publications was also discussed in <ref type="table">(Gidiotis and Tsoumakas</ref> <ref type="bibr">, 2019, 2020)</ref>. The authors built a structured abstract dataset PMC-SA, consisting of 712,911 biomedical articles from PubMed Central, and they proposed to summarize a paper from a specific facet by taking corresponding sections as inputs. Compared with their works, FacetSum covers a wider range of academic fields and we provide in-depth discussions on the structured abstract to justify its value as a novel NLP challenge. Our research shares some resemblance to studies on abstract sentence classification, whose goal is to classify abstract sentences into several facets, instead of summarizing the full text. MEDLINE is commonly used for this task <ref type="bibr" target="#b1">(Kim et al., 2011)</ref>, so as the Emerald data <ref type="bibr" target="#b13">(Stead et al., 2019)</ref>. A recent study <ref type="bibr">(Huang et al., 2020)</ref> introduced a new dataset CODA, in which 10,966 abstracts are split into subsentences and labelled into five categories by third-party annotators. However, we think scientific documents are generally difficult to comprehend for people without specific training, thus original authors are in the best position to summarize their own work. Faceted summarization was also involved in CL-SciSumm 2016 Shared Task <ref type="bibr" target="#b0">(Jaidka et al., 2018)</ref>, where the faceted summary of a paper is defined as the citation sentences in its successor studies, since new studies typically describe previous work from different perspectives. However, this idea may not easily scale up in the real world since many papers do not have enough number of citations, especially for newly published ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion &amp; Future Work</head><p>We introduce FacetSum to support the research of faceted summarization, which targets summarizing scientific documents from multiple facets. We provide extensive analyses and results to investigate the characteristics of FacetSum. Our observations call for the development of models capable of handling very long documents and outputting controlled text. Specifically, we will consider exploring the following topics in future work:</p><p>(1) incorporating methods for long-document processing, such as reducing input length by extracting key sentences <ref type="bibr" target="#b9">(Pilault et al., 2020)</ref> or segments <ref type="bibr" target="#b18">(Zhao et al., 2020)</ref>; (2) examining the possi-bility of building a benchmark for systematic generalization <ref type="bibr">(Bahdanau et al., 2018)</ref>    <ref type="table">Table 5</ref>: Top five frequent verbs/nouns/adjectives in each facet of structured abstract. We preprocess the text with lowercasing, stemming and stopword removal and extract part-of-speech tags using Spacy <ref type="bibr">(Honnibal et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Implementation Details</head><p>To make BART take full text as input, we extend the positional embedding to 10,000 tokens. This was required to leverage long text of papers in FacetSum with average length of 6000 words. Experiments of unsupervised baselines are implemented with Sumy (Belica, 2021) and official code of HipoRank. We tune the hyperparameters of HipoRank with the validation set. The BART experiments are finetuned using Fairseq <ref type="bibr" target="#b7">(Ott et al., 2019)</ref>, with learning rate of 3e ?5 , batch size of 1, max tokens per batch of 10,000 and update frequency of 4. We finetune all models for 20,000 steps with single NVIDIA Tesla V100 16GB and we report the results of the last checkpoint. The small batch size is the consequence of the large input size. For inference, we use beam size of 4 and maximum length of 500/200 tokens for BART/BART-Facet respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Domains Covered by FacetSum</head><p>In <ref type="figure" target="#fig_1">Figure 3</ref>, we show the distribution of domain categories in FacetSum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Full Results</head><p>In this section, we provide additional experiment results. In <ref type="table">Table 6</ref>, we show the full results of the extractive oracle system (first row in <ref type="table" target="#tab_5">Table 3</ref>). In <ref type="table">Table 7</ref>, we provide full results of all other models (heuristic models, unsupervised models, and supervised models in <ref type="table" target="#tab_5">Table 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Example of Outputs by BART and BART-Facet</head><p>In <ref type="table" target="#tab_8">Table 8</ref>, we show an example of the generated faceted summaries by BART and BART-Facet of the same paper, compared against the ground-truth faceted abstract. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Facet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth BART BART-Facet</head><p>Purpose The purpose of this paper is to advance the information society research by examining and better understanding the impact of the adoption information and communication technologies (ICT) within households on improving sustainability.</p><p>The purpose of this paper is to answer the question: Does the adoption of information and communication technologies (ICT) within households contribute to sustainability? For this purpose, the study focuses on addressing the following specific questions: What criteria clearly and deeply describe the ICT adoption within households?</p><p>What are the criteria that clearly describe sustainability within households, and are there statistically significant correlations between ICT and sustainability?</p><p>The purpose of this paper is to answer the question: Does the adoption of information and communication technologies (ICT) within households contribute to sustainability?</p><p>Method The survey questionnaire was used and data collected from 679 Polish households were statistically analyzed to understand the phenomena of ICT adoption and sustainability as well as identify correlations between them.</p><p>A survey was carried out in Poland.</p><p>The data were collected through a structured questionnaire and analyzed using structural equation modeling (SEM).</p><p>The study is based on a quantitative research design. The data were collected through a questionnaire survey among the households in Poland. The survey was carried out in two phases. In the first phase, the ICT adoption within households was measured using a structured questionnaire, and in the second phase the data were analyzed using structural equation modeling (SEM) to test the hypothesized relationships.</p><p>Findings The research findings reveal that the ICT adoption is well described by the ICT outlay, information culture, ICT management and ICT quality, whereas sustainability is composed of ecological, economic, socio-cultural and political sustainability. Furthermore, the ICT quality, ICT management and information culture have a significant impact on sustainability, whereas the ICT outlay does not have such an impact.</p><p>Results show that ICT is a key enabler of sustainability in households. The results also show that there are statistically significant correlation between the IIT adoption within the households and sustainability.</p><p>The results show that the adoption of ICT within households is positively related to sustainability.</p><p>The results also show that there are statistically significant correlations between the ICT adoption within households and sustainability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Value</head><p>The paper provides and verifies a new theoretical model of sustainable information society to depict various dimensions shaping the ICT adoption and their impact on different types of sustainability in the context of households.</p><p>This study is the first to empirically investigate the impact of ICT on sustainability. The findings of this study will be complementary with findings concerning the contribution of IIT to sustainability in enterprises and allow for the advancement in the sustainable information society (SIS) research.</p><p>This study contributes to the literature by providing a deeper understanding of the ICT adoption within households and the contribution of ICT to sustainability in transition economies, i.e. the former European Eastern Bloc countries. BART-Facet provides more information of Method and less errors than BART (e.g. "IIT" is a typo of "ICT"). However both models tend to directly copy text from the source, for example both outputs of Purpose can be found in the introduction of the paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example of the proposed FacetSum dataset. Each facet of the structured abstract summarizes different sections of the paper. et al., 2017;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Data distribution of domain categories, sorted in descending order.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the FacetSum dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Scores of sentence aligning in Rouge-L.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Model performance on FacetSum (Rouge-L). SeeTable 6and 7 in Appendix E for full results. Bold text indicates the best scores on FacetSum test split in each column.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>with the domain categories; (3) automatically structuring traditional abstracts(Huang et al., 2020)  with FacetSum.</figDesc><table><row><cell>Alexios Gidiotis and Grigorios Tsoumakas. 2019.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Structured summarization of academic publications.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>In Joint European Conference on Machine Learning</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>and Knowledge Discovery in Databases, pages 636-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>645. Springer.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Alexios Gidiotis and Grigorios Tsoumakas. 2020. A Introduction intro, purpose</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3040. Method divide-and-conquer approach to the summarization design, method, approach Result result, find, discuss, analy of long documents. IEEE/ACM Transactions on Au-dio, Speech, and Language Processing, 28:3029-Conclusion conclu, future</cell><cell cols="5">References Dzmitry Bahdanau, Shikhar Murty, Michael and Aaron Courville. 2018. Systematic generaliza-Noukhovitch, Thien Huu Nguyen, Harm de Vries,</cell></row><row><cell>Yihong Gong and Xin Liu. 2001. Generic text summa-rization using relevance measure and latent semantic analysis. In Proceedings of the 24th annual interna-</cell><cell cols="5">tion: What is required and can it be learned? In International Conference on Learning Representa-tions.</cell></row><row><cell>tional ACM SIGIR conference on Research and de-velopment in information retrieval, pages 19-25.</cell><cell cols="5">Mi?o Belica. 2021. sumy: Automatic text summarizer. https://github.com/miso-belica/</cell></row><row><cell>Max Grusky, Mor Naaman, and Yoav Artzi. 2018.</cell><cell>sumy.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-man Language Technologies, Volume 1 (Long Pa-pers), pages 708-719.</cell><cell cols="5">Isabel Cachola, Kyle Lo, Arman Cohan, and Daniel S Weld. 2020. Tldr: Extreme summarization of sci-entific documents. In Proceedings of the 2020 Con-ference on Empirical Methods in Natural Language Processing: Findings, pages 4766-4777.</cell></row><row><cell>James Hartley. 2014. Current findings from research on structured abstracts: an update. Journal of the Medical Library Association: JMLA, 102(3):146.</cell><cell cols="5">Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian. 2018. A discourse-aware attention model for abstractive summarization of long documents. In</cell></row><row><cell>James Hartley and Matthew Sydes. 1997. Are struc-tured abstracts easier to read than traditional ones? Journal of Research in Reading, 20(2):122-136.</cell><cell cols="5">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computa-tional Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 615-621.</cell></row><row><cell>James Hartley, Matthew Sydes, and Anthony Blurton. 1996. Obtaining information accurately and quickly: are structured abstracts more efficient? Journal of information science, 22(5):349-356.</cell><cell cols="5">Ed Collins, Isabelle Augenstein, and Sebastian Riedel. 2017. A supervised approach to extractive sum-marisation of scientific papers. In Proceedings of the 21st Conference on Computational Natural</cell></row><row><cell>Matthew Honnibal, Ines Montani, Sofie Van Lan-deghem, and Adriane Boyd. 2020. spaCy: Industrial-strength Natural Language Processing in</cell><cell cols="5">Language Learning (CoNLL 2017), pages 195-205, Vancouver, Canada. Association for Computational Linguistics.</cell></row><row><cell>Python.</cell><cell cols="5">Danish Contractor, Yufan Guo, and Anna Korhonen.</cell></row><row><cell>Ting-Hao Huang, Chieh-Yang Huang, Chien-Kuang Cornelia Ding, Yen-Chia Hsu, and C Lee Giles. 2020. Coda-19: Using a non-expert crowd to</cell><cell cols="5">2012. Using argumentative zones for extractive sum-marization of scientific articles. In Proceedings of COLING 2012, pages 663-678.</cell></row><row><cell>annotate research aspects on 10,000+ abstracts in</cell><cell cols="5">Yue Dong, Andrei Romascanu, and Jackie CK Che-</cell></row><row><cell>the covid-19 open research dataset. In Proceedings</cell><cell cols="5">ung. 2020. Hiporank: Incorporating hierarchical</cell></row><row><cell>of the 1st Workshop on NLP for COVID-19 at ACL</cell><cell cols="5">and positional information into graph-based unsu-</cell></row><row><cell>2020.</cell><cell cols="5">pervised long document extractive summarization.</cell></row><row><cell></cell><cell cols="3">arXiv preprint arXiv:2005.00513.</cell><cell></cell><cell></cell></row><row><cell>Kokil Jaidka, Muthu Kumar Chandrasekaran, Sajal</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Rustagi, and Min-Yen Kan. 2016. Overview of the</cell><cell>Emerald.</cell><cell>2021.</cell><cell>Writing</cell><cell>an</cell><cell>arti-</cell></row><row><cell>cl-scisumm 2016 shared task. In Proceedings of the</cell><cell>cle</cell><cell>abstract.</cell><cell cols="3">https://www.</cell></row><row><cell>joint workshop on bibliometric-enhanced informa-</cell><cell cols="5">emeraldgrouppublishing.com/how-to/</cell></row><row><cell>tion retrieval and natural language processing for</cell><cell cols="4">authoring-editing-reviewing/</cell><cell></cell></row><row><cell>digital libraries (BIRNDL), pages 93-102.</cell><cell cols="3">write-article-abstract.</cell><cell cols="2">[Online; ac-</cell></row><row><cell></cell><cell cols="2">cessed 26-January-2021].</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">G?nes Erkan and Dragomir R Radev. 2004. Lexrank:</cell></row><row><cell></cell><cell cols="5">Graph-based lexical centrality as salience in text</cell></row><row><cell></cell><cell cols="5">summarization. Journal of artificial intelligence re-</cell></row><row><cell></cell><cell cols="2">search, 22:457-479.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="4">: Keywords for identifying paper sections used</cell></row><row><cell cols="2">in Section 2.2.</cell><cell></cell><cell></cell></row><row><cell cols="4">B Most Frequent Words in Each</cell></row><row><cell cols="2">Abstract Facet</cell><cell></cell><cell></cell></row><row><cell>Facet</cell><cell>Verb</cell><cell>Noun</cell><cell>Adjective</cell></row><row><cell cols="2">Purpose aim</cell><cell>paper</cell><cell>social</cell></row><row><cell></cell><cell>examin</cell><cell>purpos</cell><cell>new</cell></row><row><cell></cell><cell>investig</cell><cell>studi</cell><cell>organiz</cell></row><row><cell></cell><cell>explor</cell><cell>manag</cell><cell>differ</cell></row><row><cell></cell><cell>develop</cell><cell>research</cell><cell>public</cell></row><row><cell cols="2">Method base</cell><cell>studi</cell><cell>structur</cell></row><row><cell></cell><cell>conduct</cell><cell>data</cell><cell>qualit</cell></row><row><cell></cell><cell>collect</cell><cell>analysi</cell><cell>differ</cell></row><row><cell></cell><cell>test</cell><cell>model</cell><cell>empir</cell></row><row><cell></cell><cell>develop</cell><cell>paper</cell><cell>social</cell></row><row><cell cols="2">Findings found</cell><cell>result</cell><cell>signific</cell></row><row><cell></cell><cell>indic</cell><cell>studi</cell><cell>posit</cell></row><row><cell></cell><cell>suggest</cell><cell>manag</cell><cell>social</cell></row><row><cell></cell><cell>provid</cell><cell>effect</cell><cell>differ</cell></row><row><cell></cell><cell>identifi</cell><cell cols="2">relationship higher</cell></row><row><cell>Value</cell><cell>provid</cell><cell>studi</cell><cell>new</cell></row><row><cell></cell><cell>contribut</cell><cell>paper</cell><cell>social</cell></row><row><cell></cell><cell>develop</cell><cell>research</cell><cell>differ</cell></row><row><cell></cell><cell>base</cell><cell>manag</cell><cell>empir</cell></row><row><cell></cell><cell>examin</cell><cell>literatur</cell><cell>import</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Outputs by BART and BART-Facet on different facets. Both models are able to generate reasonable summaries given the specified facet.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The data has been licensed to researchers at subscribing institutions to use (including data mining) for noncommercial purposes. See detailed policies at https:// www.emerald.com/ 2 There are three optional facets (about research, practical and social implications) that are missing from a large number of articles and hence omitted in this study.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Both the split information of FacetSum and the code for scraping and parsing the data are available at https: //github.com/hfthair/emerald_crawler</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We use the relative position i/|S| so that all positions are commensurate across multiple documents.5  To ensure close-to-perfect precision, we choose keywords that are as specific and prototypical to each section as possible (listed in Appendix A). The resulting recall is around 0.7, i.e. about 70% of sections can be correctly retrieved with the titlekeyword matching method. And we find 2,751 (out of 6,000) test samples that all four sections are matched successfully. Though far from perfect, we believe this size is sufficient for the significance of subsequent analyses.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Insights from cl-scisumm 2016: the faceted scientific document summarization shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kokil</forename><surname>Jaidka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muthu</forename><surname>Kumar Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sajal</forename><surname>Rustagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Digital Libraries</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="163" to="171" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic classification of sentences to support evidence based medicine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Cavedon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yencken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMC bioinformatics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bart: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multixscience: A large-scale dataset for extreme multidocument summarization of scientific articles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8068" to="8074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">TextRank: Bringing order into text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tarau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2004 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="404" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Summarunner: a recurrent neural network based sequence model for extractive summarization of documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3075" to="3081" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Don&apos;t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1797" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT 2019: Demonstrations</title>
		<meeting>NAACL-HLT 2019: Demonstrations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A deep reinforced model for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On extractive and abstractive neural document summarization with transformer language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Pilault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Pal</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.748</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9308" to="9319" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">How to write a paper for publication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">T</forename><surname>Rosenfeldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salvatore</forename><surname>Dowling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meryl</forename><forename type="middle">J</forename><surname>Pepe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fullerton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Heart, Lung and Circulation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="82" to="87" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointergenerator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1073" to="1083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bigpatent: A large-scale dataset for abstractive and coherent summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2204" to="2213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Emerald 110k: a multidisciplinary dataset for abstract sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connor</forename><surname>Stead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Busch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Savanid</forename><surname>Vatanasakdakul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the The 17th Annual Workshop of the Australasian Language Technology Association</title>
		<meeting>the The 17th Annual Workshop of the Australasian Language Technology Association</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="120" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The structure of a scientific paper</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Suppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophy of Science</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="381" to="405" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Beyond generic summarization: A multi-faceted hierarchical summarization corpus of large heterogeneous data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Tauchmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Hanselowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Christian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margot</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mieskes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<publisher>LREC</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Beyond sumbasic: Taskfocused summarization with sentence simplification and lexical expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisami</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1606" to="1618" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scisummnet: A large annotated corpus and content-impact models for scientific paper summarization with citation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungo</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">R</forename><surname>Fabbri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dragomir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Radev</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33017386</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7386" to="7393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Seal: Segment-wise extractive-abstractive long-form text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10213</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<idno>55.26/25.26/50.58 47.76/18.88/38.94 40.53/13.84/32.83 51.81/25.81/44.73 46.14/19.66/38.10</idno>
		<title level="m">Conclu body</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Full results (Rouge-1/2/L) of the extractive oracle system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nallapati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Table</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>on FacetSum. Bold text indicates the best scores in the lower four rows in each column</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Full results (Rouge-1/2/L) of different models on FacetSum. Bold text indicates the best scores on</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Marketing Human Resource Management Information and Knowledge Management Industry and Public Sector Management Education Managing Quality Economics Learning and Development Operations and Logistics Management Library and Information Science Built Environment Health Care Management / Healthcare Accounting and Finance Enterprise and Innovation Tourism and Hospitality Organization Studies Strategy Business Ethics and Law International Business Performance Management and Measurement Environmental Management / Environment Health and Social Care Management Science</title>
	</analytic>
	<monogr>
		<title level="j">Management Studies Mechanical and Materials Engineering</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
