<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Group-wise Inhibition based Feature Regularization for Robust Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhe</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqian</forename><surname>Wu</surname></persName>
							<email>wuhaoqian2019@email.szu.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weicheng</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Shen</surname></persName>
							<email>llshen@szu.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Vision Institute</orgName>
								<orgName type="department" key="dep2">College of Computer Science and Software Engineering</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">SZU Branch</orgName>
								<orgName type="department" key="dep2">Shenzhen Institute of Artificial Intelligence and Robotics for Society</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">National Engineering Laboratory for Big Data System Computing Technology 4 Guangdong Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<postCode>518060</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Group-wise Inhibition based Feature Regularization for Robust Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The convolutional neural network (CNN) is vulnerable to degraded images with even very small variations (e.g. corrupted and adversarial samples). One of the possible reasons is that CNN pays more attention to the most discriminative regions, but ignores the auxiliary features when learning, leading to the lack of feature diversity for final judgment. In our method, we propose to dynamically suppress significant activation values of CNN by group-wise inhibition, but not fixedly or randomly handle them when training. The feature maps with different activation distribution are then processed separately to take the feature independence into account. CNN is finally guided to learn richer discriminative features hierarchically for robust classification according to the proposed regularization. Our method is comprehensively evaluated under multiple settings, including classification against corruptions, adversarial attacks and low data regime. Extensive experimental results show that the proposed method can achieve significant improvements in terms of both robustness and generalization performances, when compared with the state-of-theart methods. Code is available at https://github. com/LinusWu/TENET_Training.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>tacked samples into the training data, as shown in <ref type="figure" target="#fig_0">Fig. 1 (b)</ref>.</p><p>Since adversarial training may impair the generalization performance, there is often an inherent trade-off between classification accuracy and adversarial robustness <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>. In order to improve the robustness and generalization simultaneously, data augmentation and regularization methods (e.g. Random Erasing <ref type="bibr" target="#b32">[33]</ref>, Augmix <ref type="bibr" target="#b13">[14]</ref>, Cutout <ref type="bibr" target="#b6">[7]</ref>,  <ref type="figure">Figure 2</ref>. The heatmap visualization of feature maps encoded with ResNet-50, based on Grad-CAM <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b33">34]</ref> with or without the proposed method. Our method locates more diverse discriminative regions (in red boxes) for both single-instance (a) and multipleinstance (b) samples.</p><p>Dropout <ref type="bibr" target="#b14">[15]</ref> and DeepAugmentation <ref type="bibr" target="#b11">[12]</ref>) are proposed. As shown in <ref type="figure" target="#fig_0">Fig. 1 (c)</ref>, these algorithms address data augmentation by randomly generating new samples obeying the same distribution as the training data. Generally, data regularization methods are state-agnostic, which can not be dynamically adjusted during CNN training. Thus, these regularization techniques of CNNs <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref> failed to learn features with sufficient diversity. As shown in the first row of <ref type="figure">Fig. 2</ref>, CNNs can locate the most discriminative regions <ref type="bibr" target="#b33">[34]</ref> for both single-instance and multi-instance samples with the regularization method, while neglecting other auxiliary features that are critical for the recognition. The lack of auxiliary features may lead to insufficient feature diversity, which consequently results in a feature space with low-dimension for classification and limits the robustness. Meanwhile, current adversarial training and regularization methods concentrate on the global image information by expanding the training set, while the independence of local features is not fully explored. These limitations motivate us to improve the diversity of extracted features by CNNs and devise a non-image-wise regularization strategy to enhance network robustness. In this paper, we propose a group-wise inhibition based regularization method for improving feature diversity and network robustness, denoted as TENET Training. <ref type="figure" target="#fig_0">Fig.1</ref> (d1), (d2) and (d3) show the motivation of the proposed method, where the increase of feature dimension and diversity is beneficial for classification robustness against input variations and adversarial attacks. To increase feature representation space, group-wise feature regularization is proposed to leverage the independence among group-wise features. To improve feature diversity, the proposed algorithm regularizes group-wise features dynamically in each training step. Specifically, based on the grouping of feature maps and their importance evaluation, the group-wise reversed map is proposed to suppress the activation values corresponding to the most significant discriminative regions, and guide the network to learn more auxiliary information in less significant regions. As shown in the second row of <ref type="figure">Fig. 2</ref>, the suppression of most significant discriminative regions is beneficial for exploring more diverse features in CNNs. Experimental results show that the proposed method can improve the top-1 error rate of adversarial training from 36.37% to 31.75%, and outperforms regularization methods significantly in terms of classification accuracy based on small sample. In summary,</p><p>? A group-wise inhibition based regularization method is proposed to explore auxiliary features and promote feature diversity.</p><p>? Feature maps with different activation distribution are processed separately to learn richer discriminative features hierarchically to better represent images.</p><p>? Our proposed method achieves competitive performances in terms of adversarial robustness and generalization compared with related variants and the state of the arts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Robustness against Corruption and Adversarial Attack</head><p>The human vision system is robust in ways that CNN based computer vision systems are not <ref type="bibr" target="#b12">[13]</ref>. Particularly, a large mount of studies <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17]</ref> show that CNNs can be easily fooled by small variations in query images, including common corruption <ref type="bibr" target="#b12">[13]</ref> and adversarial perturbation <ref type="bibr" target="#b9">[10]</ref>. In order to improve the robustness against these variations, studies have been proposed based on various strategies, such as structure modification, adversarial training and regularization. Xie et al. <ref type="bibr" target="#b29">[30]</ref> proposed a non-local feature denoising block to suppress the disturbation caused by the malicious perturbation. A Discrete Wavelet Transform (DWT) layer is proposed by Li et al. <ref type="bibr" target="#b20">[21]</ref>, which disentangles the low-and high-frequency components to yield the noise-robust classification. Different from structure based methods, adversarial training and regularization methods can improve the robustness without the modification of network structure. Adversarial training proposed by Goodfellow et al. <ref type="bibr" target="#b9">[10]</ref>, in which a network is trained on adversarial examples, is reported to be able to withstand strong attacks <ref type="bibr" target="#b23">[24]</ref>. However, there is a trade-off between classification accuracy (generalization) and adversarial robustness. Hence, more and more studies are resorted to the regularization solutions <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b32">33]</ref> to simultaneously improve generalization and robustness against variations, i.e. common corruption and adversarial attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Regularization for CNNs</head><p>Regularization <ref type="bibr">[7, 12, 14-16, 25, 28, 33]</ref> has been widely employed in the training of CNNs, where image-wise and feature-wise regularization methods were proposed to improve generalization or robustness. Data augmentation is a typical image-wise solution to regularize the data distribution <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b32">33]</ref>. Devries et al. <ref type="bibr" target="#b6">[7]</ref> proposed a regularization technique to randomly mask out square regions of input during training. Random Erasing proposed by Zhong et al. <ref type="bibr" target="#b32">[33]</ref> randomizes the values of pixels in a random rectangle region. Hendrycks et al. <ref type="bibr" target="#b13">[14]</ref> proposed Augmix to coordinate simple augmentation operations with a consistency loss. In a nutshell, these image-wise regularization solutions generate images by random operations (e.g. cutout, erasing and mixing), which concentrate on the global information without fully exploring the independence of local features. Meanwhile, the random operations are not dynamically adapted during the training, which limit the feature diversity. These studies motivate us to enhance the feature diversity to improve network robustness and generalization performances.</p><p>To explore local information during regularization, feature-wise regularization techniques, including attention based dropout <ref type="bibr" target="#b4">[5]</ref>, self-erasing <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b27">28]</ref> and group orthogonal training <ref type="bibr" target="#b3">[4]</ref>, are proposed. Attention based dropout proposed by Choe et al. <ref type="bibr" target="#b4">[5]</ref> utilizes the self-attention mechanism to regularize the feature maps. Self-erasing <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b27">28]</ref> is an extension method of popular class activation map (CAM) <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b33">34]</ref>, which erases the most discriminative part of CAM, and guides the CNNs to learn classification features from auxiliary regions and activations <ref type="bibr" target="#b26">[27]</ref>. However, these methods are proposed for semantic segmentation rather than the classification task. Meanwhile, the steep gradients introduced by the binary mask limit the performances of dropout and erasing operation for classification task. From another aspect, the erasing operation and dropout are global regularizers, which do not fully explore the independence of feature semantics, i.e. different feature groups contain different semantics and should be processed specifically. Group orthogonal training proposed by Chen et al. <ref type="bibr" target="#b3">[4]</ref> provides a solution for this problem, which guides CNNs to learn discriminative features from foreground and background separately. Although this group orthogonalization strategy brings improvement of classification performance by enhancing feature diversity, the relied large annotation limits its applicability for general tasks.</p><p>In this paper, a regularization method based on groupwise inhibition, namely TENET Training, is proposed to improve network robustness and generalization, which is free of extra annotations. Particularly, a Channel-wise Feature Grouping (CFG) module is proposed to model the channel-wise features in groups. Subsequently, the features in different groups are processed specifically by Group-wise Map Weighting (GMW) module to quantify the importance of each group. Meanwhile, in order to avoid the steep gradients caused by binary mask, a Rectified Reverse Function (RRF) is proposed to smooth group-wise reversed maps. Finally, these reversed maps are used to suppress the activation values to regularize the learned features. Extensive experiments clearly show the significant improvements in terms of robustness and generalization performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>The overview of the proposed TENET Training is shown in <ref type="figure">Fig. 3</ref>., where CNN is dynamically regularized according to the training step, and significant activation values are suppressed to guide network to explore different features hierarchically. Since the feature maps with the similar activation distribution are prone to contain redundant information, we firstly group the channel-wise feature maps using the proposed CFG module in Section 3.1. In order to further quantify the contribution of each group, the GMW module is introduced in Section 3.2 to evaluate the group importance. Considering the feature groups with negative importance score should contribute less to the classification performance, Rectified Reverse Function (RRF) is proposed to smooth the reversed map of the filtered groups. Following RRF, the group-wise inhibition is devised to suppress the most significant features and explores the less significant auxiliary features, which is introduced in Section 3.3. Finally, we conclude the pipeline of the proposed TENET Training together with the loss design in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Channel-wise Feature Grouping Module</head><p>According to the pipeline shown in <ref type="figure">Fig. 3</ref>, a feature extraction module F (?) is firstly applied to encode the features set A = {a 1 , ..., a j , ..., a Nc } of the input sample x, where a j is the jth feature map. Since A is prone to contain redundant features, a Channel-wise Feature Grouping module, denoted as CFG module, is introduced to group A to reduce the complexity of feature-wise operation. Given N c features as input, the corresponding N G centers are obtained to form the set A c , which are initialized as a random subset of A. The distance from each feature map of A to the corresponding center is calculated as follows</p><formula xml:id="formula_0">Dist(a j , A c [l]) = 1 H a ? W a Ha Wa (a j ? A c [l]) 2 (1) where l ? [1, N G ] is the index of the center and (H a , W a )</formula><p>is the size of a j . Based on Eq. <ref type="formula">(1)</ref>, the centers are updated as similar as k-means clustering. N G groups are then obtained by grouping the feature maps to the corresponding center. In order to alleviate the influence caused by the random selection, the center searching process is carried out repeatedly in the CFG module. Based on the grouping procedure, the centers are updated according to Center Point Search Function, i.e. CF(?) as follows</p><formula xml:id="formula_1">CF(IDS) = {arg min aj ?A dist(a j , 1 n l IDi=l a i ) l ? [1,N G ]} (2) ? First Inference Group-wise Hadamard Product Obtain More Discriminative Features ? , ? ( ) 1 2 ?1 ? Channel-wise Feature Grouping Module (CFG) Feature Extraction Module F(?) Classifier D(?) Classifier D(?) ? ? Group-wise Reversed Maps Importance Scores &amp; Feature Maps ( , ) with Group-wise Feature Maps Input Rectified Reverse Function (RRF)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Second Inference</head><p>Group-wise Map Weighting Module (GMW) <ref type="figure">Figure 3</ref>. The pipeline of the proposed regularization method (TENET Training). Notice that CNNs consist of the feature extraction module F (?) and the classifier D(?). In the first inference, feature maps A encoded with F (?) are divided into NG groups by the CF G module, and loss L d is calculated based on D(?). Reversed maps RM are then derived using GM W module and RRF . In the second inference, the Hadamard Product of A (with IDS) and RM is fed to D(?) to calculate the loss L total .</p><p>where the set IDS = {ID 1 , ..., ID j , ..., ID Nc } stands for the set of feature map indices corresponding to each group. ID j refers to the group index of a j . n l is the number of feature maps in the lth group. Based on Eq. <ref type="formula">(2)</ref>, A c can be refined iteratively until CF(?) is stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Group-wise Map Weighting Module</head><p>Following feature grouping module, the feature maps are processed in the group-wise mode. To differ the contribution of each group, a Group-wise Map Weighting module, namely GMW module, is proposed to calculate the weight w j of each a j as follows</p><formula xml:id="formula_2">w j = 1 H a ? W a Ha Wa ?L d (A) ?a j L d (A) = D(A) ? One-Hot(D(A))<label>(3)</label></formula><p>where D(?) is a classifier, which maps A to the class score. L d (A) is the product of prediction and the corresponding one-hot vector of D(A).</p><formula xml:id="formula_3">Since ?L d (A)</formula><p>?aj is applied to quantify the importance of a j to the prediction, the group-wise importance scores, i.e. IS = {I 1 , ..., I l , ..., I N G } can be obtained by averaging w j of each group (ID j =l) as follows</p><formula xml:id="formula_4">I l = 1 N l IDj =l w j<label>(4)</label></formula><p>Similar to IS, the group-wise feature maps, i.e. M = {m 1 , ..., m l , ..., m N G } can be obtained by averaging the weighted feature maps as follows</p><formula xml:id="formula_5">m l = 1 N l IDj =l w j ? a j<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Group-wise Inhibition using Rectified Reverse Function</head><p>Based on the importance scores, group-wise feature maps are applied to obtain the reversed map set, i.e. RM = {rm 1 , ..., rm l , ..., rm N G }. Since the steep gradients introduced by the binary mask may limit the classification performance, the reversed maps are further smoothed. Meanwhile, considering the feature groups with negative importance scores should contribute less to the update of the reversed mask, we therefore propose a Rectified Reverse Function, i.e. RRF(?), to obtain the reversed maps as follows</p><formula xml:id="formula_6">rm l = RRF(m l , I l ) = sgn(I l &gt; 0) ? 1 1 + e m l<label>(6)</label></formula><p>where sgn(?) is the sign function. Due to the negative correlation between m l and rm l , the computation of RM is deemed as a reversed map. Based on RM , the group-wise inhibition is formulated as follow?</p><formula xml:id="formula_7">y = D(RM ? A)<label>(7)</label></formula><p>where D(?) is a classifier with the input of A and? refers to the predicted label of the group-wise inhibition. ? refers to the group-wise Hadamard product.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss Design of TENET Training</head><p>While? is obtained by group-wise inhibition, F (?) and D(?) can be directly learned based on the loss L c (y,?), i.e. the cross entropy for single-label classification or binary cross entropy for multi-label classification. The groupwise inhibition reduces the variation between groups, while it may introduce invalid activation units in F (?) or D(?). To regularize these activation units, an orthogonal loss L o (A) is adopted, which is formulated as follows</p><formula xml:id="formula_8">L o (A) = Ng l=1 ( Nc j=1 (sgn(ID j = l) ? a j ))<label>(8)</label></formula><p>From another aspect, by mapping rm l into the region of [0, 1], the magnitude of back-propagation gradients is suppressed for F (?) and D(?). To alleviate vanishing gradient problem, a general classification loss, i.e. L c (y i , D(A)), is employed. Finally, the total loss is formulated as follows</p><formula xml:id="formula_9">L total = L c (y i , D(A)) + ?L c (y i ,?) + ?L o (A)<label>(9)</label></formula><p>where ? and ? are the hyper parameters. For clarity, TENET Training is summarized in Algo. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 TENET Training Input:</head><p>Training Sample:</p><p>x Initialization of F (?) and D(?) Output:</p><p>Trained CNNs: F (?) and D(?) 1: for all training steps do 2:</p><p>Extract A from F (x);</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>Obtain IDS of A using CFG Module according to Eqs. (1) and (2); <ref type="bibr">4:</ref> Derive (IS, M ) with GMW Module according to Eqs. (3), (4) and (5); <ref type="bibr">5:</ref> Employ RRF to obtain RM according to Eq. (6); <ref type="bibr">6:</ref> Obtain? according to Eq. (7); <ref type="bibr">7:</ref> Calculate L total according to Eqs. (8) and (9); <ref type="bibr">8:</ref> Update F (?) based on ?L total ?F and update D(?) based on ?L total ?D ; 9: end for 10: Return F (?) and D(?).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results and Analysis</head><p>As listed in <ref type="table" target="#tab_1">Table 1</ref>, to evaluate the performance of the proposed method, extensive experiments are carried on publicly-available data sets, including PASCAL VOC 2012 Robustness against CIFAR-10/100 <ref type="bibr" target="#b17">[18]</ref> A. T. <ref type="bibr" target="#b23">[24]</ref> 5.75% Adversarial Attack- <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">24]</ref> Augmix <ref type="bibr" target="#b13">[14]</ref> 15.56% * Robustness against CIFAR-10/100-C <ref type="bibr" target="#b12">[13]</ref> Augmix <ref type="bibr" target="#b13">[14]</ref> 1.77% Common Corruption- <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21]</ref> ImageNet-C <ref type="bibr" target="#b12">[13]</ref> 2.8% ? Generalization- <ref type="bibr" target="#b1">[2]</ref> CUB-200 <ref type="bibr" target="#b25">[26]</ref> GLICO <ref type="bibr" target="#b1">[2]</ref> 2.75% * The gain is obtained in CIFAR-10 against FGSM (8/255). ? The gain is obtained by following 90-epoch Protocol <ref type="bibr" target="#b20">[21]</ref>. <ref type="bibr" target="#b8">[9]</ref>, CIFAR-10/100 <ref type="bibr" target="#b17">[18]</ref>, ImageNet-C <ref type="bibr" target="#b12">[13]</ref> and CUB-200 <ref type="bibr" target="#b25">[26]</ref>. We firstly introduce the employed data sets and the corresponding implementation details. The performance of the proposed method on standard image classification task is evaluated, and the encoded feature maps are visualized for the algorithm analysis. Finally, both the robustness and generalization performances of the proposed method are evaluated based on the comparison with the state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data Sets and Implementation Details</head><p>We evaluate the performance of TENET Training from three aspects, i.e. standard classification, robustness and generalization (see <ref type="table" target="#tab_1">Table 1</ref>).</p><p>Standard Classification. In this case, ResNet-18 <ref type="bibr" target="#b10">[11]</ref> is selected as the backbone in our TENET Training. PAS-CAL VOC 2012 <ref type="bibr" target="#b8">[9]</ref> is used for the evaluation, while 5,717 and 5,823 images are used for the training and validation, respectively. The protocol in <ref type="bibr" target="#b3">[4]</ref> is adopted. The CNNs for evaluation are pretrained on the ImageNet <ref type="bibr" target="#b5">[6]</ref>, and finetuned on PASCAL VOC 2012 training set. In the training stage, the shorter side of image is resized to a random value within <ref type="bibr">[256,</ref><ref type="bibr">480]</ref> for the scale augmentation. The resized image is then randomly cropped to the size of 224 ? 224 for the training based on the batch size of 256. In the testing stage, ten-crop testing is used to evaluate the performance.</p><p>Robustness. In this case, the robustness of the proposed algorithm against both adversarial attack and image corruption is evaluated on CIFAR 10/100 <ref type="bibr" target="#b17">[18]</ref>, CIFAR 10/100-C <ref type="bibr" target="#b11">[12]</ref> and ImageNet-C <ref type="bibr" target="#b11">[12]</ref>. ResNeXt-29 <ref type="bibr" target="#b30">[31]</ref> and ResNet-50 <ref type="bibr" target="#b10">[11]</ref> are chosen as the backbones. To test the robustness of the proposed method against adversarial attacks, two popular attacks, FGSM <ref type="bibr" target="#b9">[10]</ref> and PGD <ref type="bibr" target="#b0">[1]</ref>, are employed. The performance is then evaluated according to the protocol in <ref type="bibr" target="#b7">[8]</ref>. The perturbation budget ( ) is set to 8/255 or 4/255 under l ? norm distance for the two attacks. PGD-K stands for K-step attack with a step size of 2/255. Meanwhile, adversarial training is used to defense powerful iterative attacks of PGD. To make the results more convincing, an efficient adversarial training method (free-AT) <ref type="bibr" target="#b23">[24]</ref> is adopted, where the hop step of free-AT, i.e. m, is set to 4.</p><p>Against image corruption, 15 different kinds of corruptions, such as noise, blur, weather and digital corruptions, are performed on CIFAR 10/100-C and ImageNet-C for the  evaluation, and each kind of corrupted data has five different severity levels <ref type="bibr" target="#b11">[12]</ref>. We follow the training protocols and evaluation metrics used in Augmix <ref type="bibr" target="#b13">[14]</ref> and WRes-Net50 <ref type="bibr" target="#b20">[21]</ref>. The Clean Error is the regular classification error on the original (uncorrupted) test or validation dataset, and mCE (Mean Corruption Error) for CIFAR-10/100-C is the mean over all 15 corruptions. Meanwhile, the mCE for ImageNet-C is normalized by the corruption error of AlexNet <ref type="bibr" target="#b18">[19]</ref>. Due to the computational efficiency, Augmix without Jensen-Shannon divergence (JSD) loss is implemented.</p><p>Generalization. Since CUB-200 <ref type="bibr" target="#b25">[26]</ref> contains only 30 images for each of the 200 species of birds, it is used as a popular benchmark to test the generalization of CNNs. We follow the protocol in <ref type="bibr" target="#b1">[2]</ref>, and evaluate the generalization with three numbers of samples per class (SPC) for training, i.e. 10, 20 and 30. For a fair comparison, the same ResNet-50 <ref type="bibr" target="#b10">[11]</ref> in the protocol <ref type="bibr" target="#b1">[2]</ref> is adopted as the backbone. To train the CNNs, the smaller side of the images from CUB-200 is resized to 256, the scaled images are then randomly cropped to the size of 224 ? 224. In the testing stage, the prediction is based on the center cropping with the size of 224 ? 224.</p><p>TENET Training. For the hyper parameter setting, the cluster number N G is set to 6, while ? and ? are set as 0.1 and 0.1, respectively.</p><p>The public platform pytorch <ref type="bibr" target="#b21">[22]</ref> is used for the implementation of all the experiments on a work station with CPU of 2.8GHz, RAM of 512GB and GPU of NVIDIA Tesla V100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Effectiveness Analysis of the Proposed Method</head><p>Ablation Study. To quantify the contribution of each module in TENET Training, we test the discriminative performance of the variant with or without this module. <ref type="table" target="#tab_2">Table 2</ref> shows the results carried out for standard classification. Since GMW is based on CFG module, these two modules denoted as Channel-wise Inhibition and Group-wise Inhibition are evaluated integratedly. <ref type="table" target="#tab_2">Table 2</ref> shows that the performance of the baseline in the first row can be improved by both channel-wise inhibition and group-wise inhibition. Specifically, an improvement of 4.1% in terms of mAP is achieved by channel-wise inhibition. To study the performance of GMW and CFG modules, <ref type="table" target="#tab_2">Table 2</ref> shows that the group-wise inhibition further improves the performance using L o (A). The most significant improvement of TENET Training happens when all the proposed modules are employed, i.e. the proposed method achieves a mAP of 82.3%, which largely outperforms the baseline with a mAP of 77.1%.</p><p>Visualization of TENET Training. To study the diversity of the learned features with the proposed TENET Training, we visualize the discriminative regions of the input samples from CUB-200 using Grad-CAM <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b33">34]</ref> in <ref type="figure" target="#fig_1">Fig. 4</ref>. Compared with the baseline, the CNN using TENET Training derives more discriminative regions, such as wings, heads and tails, for classification.</p><p>To study the distribution of the extracted features, we  <ref type="bibr" target="#b10">[11]</ref> reported in <ref type="bibr" target="#b13">[14]</ref> 23 <ref type="table" target="#tab_1">.8  79  80  82  82  90  84  80  86  81 75 65  79  91  77  80 80.6  Cutout [7]</ref> 90-epoch Protocol <ref type="bibr" target="#b20">[21]</ref> 23 <ref type="table" target="#tab_1">.2  79  81  80  77  90  80  81  80  78 70 61  74  87  74</ref> 75 77.7 WResNet50 (Haar) <ref type="bibr" target="#b20">[21]</ref> 23 <ref type="table" target="#tab_1">.1  77  79  79  71  86  77  77  80  75 66 57  71  84  75  77 75.3  Augmix [14]</ref> 23 <ref type="table" target="#tab_1">.0  71  71  71  72  88  72  72  78  78 67 60  72  86  75  76 73.9  TENET  23.1  73  78  75  74  87  76  80  79  78 67 63  73  84  72  71</ref>   further visualize the group-wise maps with different importance scores of the input image in <ref type="figure" target="#fig_2">Fig. 5</ref>, where feature maps are clustered into six groups. The confidence score of each group corresponds to the variant with or without the selected group. <ref type="figure" target="#fig_2">Fig. 5 (b)</ref> shows that, the importance score (orange line) calculated by GMW module is similar with confidence score (green line) in tendency, which illustrates the effectiveness of the GMW module. Meanwhile, <ref type="figure" target="#fig_2">Fig. 5</ref> (b) also shows the large variations among the activation distributions of the group-wise features, which indicates the reasonability of group-independent processing. As a contrast, the instance-wise operation involved in traditional methods can not regularize the most important features but only the features with the largest group size ( i.e. group-1 in <ref type="figure" target="#fig_2">Fig. 5</ref>), based on the average of activation maps or annotations. Thus, the proposed group-independent processing can facilitate our TENET training to achieve better performance than other regularization methods. <ref type="figure" target="#fig_2">Fig. 5 (c)</ref> shows that group-3 and group-4 out of six groups are the most important for CNN, which can improve the confidence score output by the CNN from 0 to 99.8%. Group-1 is relatively less important than group-3 and group-4 but can increase confidence score, while the impacts of group-2, 5 and 6 on the classification performance is very limited. More precisely, when these three groups are not used, the confidence score has dropped by only 0.09%. This observation indicates that the inhibition of the important groups can help improve the efficiency without losing accuracy. Hence, in the proposed method, we only regularize the groups with higher importance scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with Related Methods</head><p>Comparison in Standard Classification. To study the classification performance of the proposed method, we compare it with the group orthogonal training <ref type="bibr" target="#b3">[4]</ref> denoted as GoCNN in <ref type="table" target="#tab_3">Table 3</ref>. In additional, we include TENET (Binary Mask) and TENET (Instance-wise Inhibition) for the comparison. TENET (Binary Mask) refers to the proposed method that suppresses the activation value using binary masks rather than the smoothed reversed maps. In TENET (Instance-wise Inhibition), CFG module and GMW module are replaced by Grad-CAM <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b33">34]</ref>, which process features by instance-wise operation. <ref type="table" target="#tab_3">Table 3</ref> shows that TENET Training outperforms the competing methods significantly. The proposed method achieves a mAP of 82.3%, exceeding group orthogonal training by 2.9% absolutely. This indicates group-wise inhibition using the smoothed reversed maps is suitable for classification. Meanwhile, the proposed method uses less information than group orthogonal training, i.e. large-scale dense annotations, e.g. segmentation or localization labels, are not demanded. While state-agnostic inhibition used in group orthogonal training regularizes features in a coarse way, it limits both the accuracy and efficiency. However, based on the proposed group-wise inhibition, our method can consistently improve the classification performance, and does not demand any extra annotations.</p><p>Comparison in Robustness. We compare the proposed method with two state-of-the-art regularization methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14]</ref>, a wavelet integrated method <ref type="bibr" target="#b20">[21]</ref> and an adversarial training one <ref type="bibr" target="#b23">[24]</ref>, for robustness evaluation against image corruption and adversarial attacks in Tables 4, 5 and 6. One can observe that TENET Training outperforms the competing methods in each case. For the recognition against image corruption, the best performance is achieved with the combination of TENET Training and Augmix (denoted as TENET(Aug.)), which achieves 69.6%, 12.31% and 35.73% error rates on ImageNet-C, CIFAR-10-C and CIFAR-100-C, respectively. Augmix <ref type="bibr" target="#b13">[14]</ref> with JSD loss can achieve a mCE of 68.4% on ImageNet-C, while it requires three times the GPU memory and runtime cost compared with the proposed method. For robustness against adversarial attacks, two attack paradigms, namely FGSM and PGD, are employed to test the trained CNNs with different regularization methods. Tables 5 and 6 show that the CNNs using the proposed method outperform those with other regularization methods by a large margin. When FGSM is considered, our method can achieve an error rate of 60.47%, exceeding other regularization methods by around 10% absolutely. Meanwhile, our method is complementary to the Adversarial Training (denoted as A.T.). Typically, the proposed method achieves the error rates of 37.07% and 63.13% against PGD-100 on CIFAR-10/100, which outperforms Adversarial Training clearly, i.e. 37.07% vs. 42.82% and 63.13% vs. 65.17%. Comparison in Generalization. To further study the generalization performance achieved by TENET Training, we compare the proposed method with regularization methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b32">33]</ref>, data augmentation method <ref type="bibr" target="#b1">[2]</ref> and adversarial training <ref type="bibr" target="#b23">[24]</ref> in <ref type="table" target="#tab_8">Table 7</ref>. <ref type="table" target="#tab_8">Table 7</ref> shows the evident improvements of TENET Training over other methods in every case. Typically, when 20 samples per class are used for training, the proposed method can achieve 76.91% in terms of Top-1 accuracy. As a comparison, adversarial training <ref type="bibr" target="#b23">[24]</ref> achieves the Top-1 accuracy of only 57.91% in this case. It seems that adversarial training can improve the robustness, while it may also largely impair the generalization performance. Hence, <ref type="table" target="#tab_8">Table 7</ref> illustrates that the proposed method can better maintain the generalization performance compared with other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed a group-wise inhibition based feature regularization method to improve the robustness and generalization of CNNs. In the proposed algorithm, CNN is regularized dynamically when learning, where the most discriminative regions with significant activation values are suppressed to enable the network to explore more diverse features. Richer features then help to better represent images even with malicious variations. The effectiveness of the proposed method was verified in terms of standard classification, adversarial robustness and generalization performance based on small number of training samples.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Some solutions to improve the robustness of CNN. Unlike with the regular training (a), adversarial training (b) widely utilizes adversarial samples to train a robust CNN. Data augmentation and regularization based method (c) improves the robustness performance by filling up new samples surrounding the decision boundary. The proposed regularization method (d) enables network to increase the representation space (e.g. red auxiliary axis in d1) of the features learned by the CNN, and achieves better robustness against corrupted and adversarial samples, with various projections on new planes (e.g. d2 and d3). Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>The visualization of the discriminative regions for image classification of CUB-200 using Grad-CAM<ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b33">34]</ref>. The 1st-3rd rows show the input samples, the discriminative regions extracted by ResNet-50 and the results based on TENET Training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>The visualization and quantification of the feature maps extracted by the 3rd residual block of ResNet-50 using TENET Training. (a) An input image with the label of European Goldfinch. (b) The activation distribution, the corresponding importance and confidence scores of each group clustered by CFG module. (c) The example feature maps selected from the 3rd and 4th groups.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:2103.02152v3 [cs.CV] 17 Aug 2021</figDesc><table><row><cell>Baseline</cell><cell>Baseline</cell></row><row><cell>Ours</cell><cell>Ours</cell></row><row><cell>(a) Single Instance</cell><cell>(b) Multiple Instances</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Summary of Experiment Configurations and TENET Training Gains.</figDesc><table><row><cell>Task-[protocol]</cell><cell>Dataset</cell><cell>Previous SOTA</cell><cell>Gain</cell></row><row><cell>Standard Classification-[4]</cell><cell>PASCAL VOC 2012[9]</cell><cell>Group Orthogonal Training [4]</cell><cell>2.9%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>The Ablation Study of the Proposed Method on the Validation Dataset of Pascal VOC 2012 in terms of Average Precision (%). areo bike bird boat bottle bus car cat chair cow table dog horse mbk prsn plant sheep sofa train tv mean ? ? ? ? 94.8 83.8 91.5 79.4 56.6 88.2 78.9 90.8 64.8 61.5 57.9 90.9 73.7 83.8 96.0 51.6 77.1 58.2 89.8 77.1 77.1 ? ? ? ? 94.2 82.8 92.9 83.3 62.2 90.8 81.0 92.8 71.1 74.1 63.0 88.2 83.9 88.5 93.5 58.4 85.2 64.7 93.1 80.6 81.2 ? ? ? ? 93.9 81.7 92.5 83.7 63.8 90.9 82.7 91.5 69.5 76.4 64.6 89.6 85.9 89.3 96.5 58.1 84.6 64.5 93.2 83.7 81.8 ? ? ? ? 95.6 84.3 91.1 83.1 61.3 91.4 83.2 91.6 72.8 77.4 65.9 91.3 84.4 89.2 96.3 57.4 83.9 67.6 94.5 83.1 82.3</figDesc><table><row><cell>Baseline</cell><cell>Channel-wise Inhibition</cell><cell>Group-wise Inhibition</cell><cell>L o</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Performance Comparison between the Proposed Method and the State of the Arts on the Validation Dataset of Pascal VOC 2012 in terms of Average Precision (%). Model areo bike bird boat bottle bus car cat chair cow table dog horse mbk prsn plant sheep sofa train tv mean ResNet18[11] reported in [4] 95.2 79.3 90.2 82.8 52.6 90.9 78.5 90.2 62.3 64.9 64.5 84.2 81.1 82.0 91.4 50.0 78.0 61.1 92.7 77.5 77.5 ResNet18 trained in this paper 94.8 83.8 91.5 79.4 56.6 88.2 78.9 90.8 64.8 61.5 57.9 90.9 73.7 83.8 96.0 51.6 77.1 58.2 89.8 77.Instance-wise Inhibition) 93.1 82.7 92.6 82.9 61.1 90.9 81.8 91.6 70.6 73.7 63.3 91.5 85.6 88.5 96.4 56.8 85.1 61.8 93.2 82.3 81.3 TENET 95.6 84.3 91.1 83.1 61.3 91.4 83.2 91.6 72.8 77.4 65.9 91.3 84.4 89.2 96.3 57.4 83.9 67.6 94.5 83.1 82.3</figDesc><table><row><cell>1 77.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Top-1 error rates (%) on ImageNet and Top-1 mCE rates (%) on ImageNet-C with ResNet-50. Aug. stands for Augmix. Shot Impulse Defocus Glass Motion Zoom Snow Frost Fog Bright Contrast Elastic Pixel JPEG mCE Baseline</figDesc><table><row><cell>Protocol</cell><cell>Clean Error Gauss.</cell><cell>Noise</cell><cell>Blur</cell><cell>Weather</cell><cell>Digital</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Top-1 error rates (%) on CIFAR-10 and Top-1 mCE rates (%) on CIFAR-10-C trained with various methods based on ResNeXt-29. A.T. stands for Adversarial Training. The brackets following the adversarial attack method show the perturbation budget ( ).</figDesc><table><row><cell></cell><cell cols="2">Clean mCE</cell><cell>FGSM (8/255)</cell><cell>PGD-7 (4/255)</cell><cell>PGD-100 (8/255)</cell></row><row><cell cols="4">Baseline [31] 5.72 29.88 72.81</cell><cell>94.15</cell><cell>-</cell></row><row><cell>Cutout [7]</cell><cell cols="3">3.97 29.20 71.07</cell><cell>97.19</cell><cell>-</cell></row><row><cell cols="4">Augmix [14] 3.95 13.32 76.03</cell><cell>93.67</cell><cell>-</cell></row><row><cell>TENET</cell><cell cols="3">3.89 26.46 61.05</cell><cell>91.28</cell><cell>-</cell></row><row><cell>TENET (Aug.)</cell><cell cols="3">3.50 12.31 60.47</cell><cell>90.45</cell><cell>-</cell></row><row><cell>A.T. [24]</cell><cell>-</cell><cell>-</cell><cell>36.37</cell><cell>22.61</cell><cell>42.82</cell></row><row><cell>TENET (A.T.)</cell><cell>-</cell><cell>-</cell><cell>31.75</cell><cell>20.07</cell><cell>37.07</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Top-1 error rates (%) on CIFAR-100 and Top-1 mCE rates (%) on CIFAR-100-C trained with various methods based on ResNeXt-29.</figDesc><table><row><cell></cell><cell cols="2">Clean mCE</cell><cell>FGSM (8/255)</cell><cell>PGD-7 (4/255)</cell><cell>PGD-100 (8/255)</cell></row><row><cell cols="4">Baseline [31] 23.33 53.40 85.93</cell><cell>95.96</cell><cell>-</cell></row><row><cell>Cutout [7]</cell><cell cols="3">20.73 54.60 87.03</cell><cell>98.13</cell><cell>-</cell></row><row><cell cols="4">Augmix [14] 21.83 37.50 84.65</cell><cell>95.32</cell><cell>-</cell></row><row><cell>TENET</cell><cell cols="3">20.56 51.21 78.71</cell><cell>94.62</cell><cell>-</cell></row><row><cell>TENET (Aug.)</cell><cell cols="3">19.46 35.73 75.28</cell><cell>93.54</cell><cell>-</cell></row><row><cell>A.T. [24]</cell><cell>-</cell><cell>-</cell><cell>60.13</cell><cell>47.99</cell><cell>65.17</cell></row><row><cell>TENET (A.T.)</cell><cell>-</cell><cell>-</cell><cell>58.60</cell><cell>46.17</cell><cell>63.13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 .</head><label>7</label><figDesc>Comparison of TOP-1 Accuracy (%) for CUB-200 based on ResNet-50 with Different Numbers of Training Samples Per Class (SPC).</figDesc><table><row><cell>Methods</cell><cell cols="3">SPC = 10 SPC = 20 SPC = 30</cell></row><row><cell>MixMatch [3]</cell><cell>36.02</cell><cell>60.57</cell><cell>70.41</cell></row><row><cell>Random Erase [33]</cell><cell>63.72</cell><cell>66.14</cell><cell>73.74</cell></row><row><cell>Cutout [7]</cell><cell>64.33</cell><cell>68.47</cell><cell>74.97</cell></row><row><cell>GLICO [2]</cell><cell>65.13</cell><cell>74.16</cell><cell>77.75</cell></row><row><cell>A.T. [24]</cell><cell>44.53</cell><cell>57.91</cell><cell>63.67</cell></row><row><cell>TENET</cell><cell>66.07</cell><cell>76.91</cell><cell>80.34</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>The work is partially supported by the National Natural Science Foundation of China under grants no. 62076163, 91959108, 61602315 and U1713214,the Science and Technology Project of Guangdong Province under grant no. 2020A1515010707, the Shenzhen Fundamental Research fund JCYJ20190808163401646, JCYJ20180305125822769 and JCYJ20190808165203670, and Tencent "Rhinoceros Birds"-Scientific Research Foundation for Young Teachers of Shenzhen University.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.00420</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning from small data through sampling an implicit conditional generative latent optimization model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Azuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphna</forename><surname>Weinshall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR, 2021. 5</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5049" to="5059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Training group orthogonal neural networks with privileged information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Attention-based dropout layer for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjung</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Benchmarking adversarial robustness on image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi-An</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="321" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The many faces of robustness: A critical analysis of out-of-distribution generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Dorundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samyak</forename><surname>Parajuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16241</idno>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Augmix: A simple data processing method to improve robustness and uncertainty. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lakshminarayanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan R</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Self-erasing network for integral object attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengtao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Robust assessment of real-world adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brett</forename><surname>Jefferson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos Ortiz</forename><surname>Marrero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="792" to="793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Wavelet integrated cnns for noise-robust image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiufu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adversarial training for free! In NIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Shafahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Amin Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Dickerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Larry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gavin</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goldstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="3358" to="3369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Self-supervised equivariant attention mechanism for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yude</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meina</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Object region mining with adversarial erasing: A simple classification to semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adversarial examples improve image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="819" to="828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Feature denoising for improving adversarial robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="501" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficient adversarial training with transferable adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juncheng</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atul</forename><surname>Prakash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="1181" to="1190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="13001" to="13008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
