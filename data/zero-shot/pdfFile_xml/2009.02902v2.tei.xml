<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TransModality: An End2End Fusion Method with Transformer for Multimodal Sentiment Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-07">2016. July 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Wang</surname></persName>
							<email>wangzilong@pku.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohong</forename><surname>Wan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
							<email>wanxiaojun@pku.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Wangxuan Institute of Computer Technology</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">MOE Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TransModality: An End2End Fusion Method with Transformer for Multimodal Sentiment Analysis</title>
					</analytic>
					<monogr>
						<title level="m">ACM Reference format: Zilong Wang, Zhaohong Wan, and Xiaojun Wan</title>
						<meeting> <address><addrLine>Washington, DC, USA</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="volume">7</biblScope>
							<date type="published" when="2016-07">2016. July 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/nnnnnnn.nnnnnnn</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS ?Information systems ? Data mining</term>
					<term>?Computing method- ologies ? Natural language processing</term>
					<term>Arti cial intelligence</term>
					<term>KEYWORDS sentiment analysis, multimodal, neural network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multimodal sentiment analysis is an important research area that predicts speaker's sentiment tendency through features extracted from textual, visual and acoustic modalities. e central challenge is the fusion method of the multimodal information. A variety of fusion methods have been proposed, but few of them adopt endto-end translation models to mine the subtle correlation between modalities. Enlightened by recent success of Transformer in the area of machine translation, we propose a new fusion method, TransModality, to address the task of multimodal sentiment analysis. We assume that translation between modalities contributes to a be er joint representation of speaker's u erance. With Transformer, the learned features embody the information both from the source modality and the target modality. We validate our model on multiple multimodal datasets: CMU-MOSI, MELD, IEMOCAP. e experiments show that our proposed method achieves the state-ofthe-art performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Semantic analysis has been a hot research topic for many years. Most traditional methods are based on texts <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18]</ref>, since textural materials are easy to get. E orts have been made to obtain materials from other modalities, such as collecting video scenes from daily TV series <ref type="bibr" target="#b21">[22]</ref> and asking the professionals to perform improvisations or scripted scenarios <ref type="bibr" target="#b1">[2]</ref>. Besides, emerging social Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. erefore, multiple datasets are available, not only from the actor's performance <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22]</ref>, but also from the social media videos <ref type="bibr" target="#b24">[25]</ref>. anks to the datasets, multimodal sentiment analysis has a racted more and more a ention these years.</p><p>All of these resources, TV series, actors' performance and social media videos, consist of information from not only the textual modality but the acoustic and visual modality as well. Intuitively, information from acoustic or visual modalities will certainly contribute to a be er prediction in sentiment analysis because it can be er deal with language ambiguity. It is a non-trivial task to distinguish ambiguity only from textual information. If visual or acoustic information is considered, it can be much easier. An example from CMU-MOSI, one of datasets used, is provided below <ref type="bibr" target="#b0">1</ref> . e speaker in the video is talking about an interesting horror movie and giving her opinion on the movie. e u erance is ambiguous and will probably be predicted as negative only through the textual information. But given smiling face and happy tone, we know the u erance is positive and describes something interesting or surprising but not frightening in the movie plot. With multimodal information, speaker's sentiment and emotion can be analyzed more precisely and properly. Although modalities are useful, their distinct properties make it non-trivial to fully utilize them. It must be noted that not all modalities play equal roles in sentiment analysis. Since sentiment analysis with textual features has long been studied, textual features serve as key features for prediction. By contrast, visual and acoustic features cannot predict sentiment tendency very well. So, to prevent the interference from the visual and acoustic features, they are used as auxiliary features to improve textual sentiment analysis <ref type="bibr" target="#b2">[3]</ref>. How to blend features of di erent modalities and improve the performance is a major challenge for the multimodal sentiment analysis task.</p><p>In this paper, we propose a novel method, TransModality, to fuse multimodal features with end-to-end translation models for multimodal sentiment analysis. We select Transformer <ref type="bibr" target="#b22">[23]</ref> for translation. Transformer uses a ention mechanism to model the relation between source and target language. In our proposed method, features of di erent modalities are treated as source and target of Transformer models, respectively. We hypothesize that translation between modalities helps modality fusion. e feature of one modality is encoded rst by Transformer. And the feature of another modality is decoded from the encoded feature as result. So the encoded feature embodies information from both source modality and target modality. To improve the performance of translation and robustness of our model, we adopt the parallel translation, which means we fuse textual features with acoustic features and fuse textual features with visual features independently. We also train our model with Forward Translation and Backward Translation, which means translating one modality to another and backward. A joint loss considering both classi cation and translation is used to train our model.</p><p>We conduct experiments on three multimodal sentiment analysis datasets -CMU-MOSI <ref type="bibr" target="#b24">[25]</ref>, MELD <ref type="bibr" target="#b21">[22]</ref> and IEMOCAP <ref type="bibr" target="#b1">[2]</ref>. ese datasets are widely used as benchmark of multimodal sentiment analysis. Experimental results show the e cacy of our proposed method TransModality which outperforms several strong baseline models. We also analyze the learning behavior of translation models in our method.</p><p>Our contributions are summarized as follows:</p><p>? We propose a novel method TransModality to address the challenging task of multimodal sentiment analysis, by translation between modalities with end-to-end translation model -Transformer. ? We adopt Forward and Backward Translation, which conduct translation from one modality to another and backward, to be er fuse multimodal features and we prove the e ectiveness through experiments. ? Our proposed method achieves the state-of-the-art performance on three multimodal datasets: CMU-MOSI, MELD, IEMOCAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Sentiment analysis has been studied as the basic task in the area of natural language processing. Most approaches focused on textual materials and used CNN <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> or RNN <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b14">15</ref>] to deal with the task. ese approaches achieve some progress in this area and provide experience and inspiration for later research. Over the last few years, multimodal sentiment analysis gained many interests. Researchers focus on how to utilize multimodal features to improve text-based approaches <ref type="bibr" target="#b12">[13]</ref>.</p><p>Many fusion methods have been proposed. Poria et al. <ref type="bibr" target="#b20">[21]</ref> simply feeds the concatenation of unimodal features into an LSTM.</p><p>is early work demonstrates the out-performance of multimodal features and pushes some progress towards this direction. Some advanced approaches are proposed to deal with the fusion challenge as well. Zadeh et al. <ref type="bibr" target="#b23">[24]</ref> explicitly models the unimodal, bimodal and trimodal interactions using a 3-fold Cartesian product from modality embeddings to create a new joint feature. e new feature is considered as be er organized representation of the original features. Liu et al. <ref type="bibr" target="#b15">[16]</ref> proposes an e cient low-rank weight decomposition method to obtain joint representation in reduced computational complexity.</p><p>Recently, various neural network fusion methods have been proposed. Ghosal et al. <ref type="bibr" target="#b6">[7]</ref> uses RNN to extract contextual information and applies a ention mechanism on multimodal features to obtain be er u erance representation. Chen et al. <ref type="bibr" target="#b2">[3]</ref> proposes a key idea that some multimodal features may be redundant and interfere with other features. So they adopt reinforcement learning to train a gate to lter out noisy modalities and re ne the joint representation. <ref type="bibr">Pham et al. [19,</ref><ref type="bibr" target="#b19">20]</ref> extends the usage of seq2seq model to the realm of multimodal learning. ey assume that the intermediate representation of this model is close to the joint representation.</p><p>Most works mentioned above conducts simple or direct mathematical calculation between unimodal features. e calculation may confuse the unimodal features and harm sentiment prediction. By contrast, Pham et al. <ref type="bibr" target="#b19">[20]</ref> and <ref type="bibr">Pham et al. [19]</ref> solve the problem through indirect fusion method with seq2seq models. Enlightened by previous work, we follow the idea and propose a new end-to-end fusion method with Transformer for multimodal sentiment analysis. e main di erence between our method and the existing work is that our framework not only abandons the simple and direct fusion methods, embraces the new idea of indirect fusion, but also leverages the state-of-the-art translation model Transformer and discovers its potential in multi-modality fusion. Compared with the sequential translation method in <ref type="bibr">Pham et al. [19,</ref><ref type="bibr" target="#b19">20]</ref>, i.e. from textual to acoustic and then to visual modality, our model conducts the translation in parallel way. We design two independent modality fusion cells to blend textual features with acoustic features, and blend textual features with visual features, respectively. is will help to eliminate the interference between di erent modalities. We also add extra components to the architecture, including contextual information extraction and joint feature concatenation, and propose the Forward and Backward Translation, i.e. translating features of one modality to another and backward, to be er fuse the multimodal information. To the best of our knowledge, our current work is the very rst of its kind that focus on modality fusion through translation with Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED MODEL</head><p>In our proposed model, we aim to combine multimodal features for be er sentiment prediction. We believe that the sentiment tendency depends not only on the features from each modality itself, but also on the interrelated relationship between them. So we utilize Transformer to translate between di erent modalities and learn the joint representation for u erances. In this section, we will introduce the architecture of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem De nition and Notation</head><p>A multimodal dataset consists of several videos, and each video is separated into u erance-level segments. So a video V is denoted as V = (X 1 , X 2 , ..., X N ) where N is the number of u erances in the video, and X i is one of the u erance-level segments. For each u erance X i (1 ? i ? N ), it has features of multiple modalities, de ned as X i = (X t i , X i , X a i ) for the textual, visual, acoustic modalities respectively. e features of di erent modalities are u erance-level feature vectors extracted from the textual, visual, acoustic modalities respectively, which will be described in detail for each dataset in Section 4.1. e dimensions of features are denoted as d t , d , d a , respectively. e corresponding sentiment label for this u erance is denoted as i .</p><p>So our task is to build a model to predict the y = ( 1 , 2 , ..., N ) through the multimodal features of a video V = (X 1 , X 2 , ..., X N ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Modality Fusion Cell</head><p>Modality Fusion Cell is the key component of our model to perform modality fusion. It leverages the translation model -Transformer and learns the joint features of two modalities involved.</p><p>We denote the two modalities involved as ? and ?, where ?, ? ? {t, , a}, and the features of these two modalities for the N u erances in a video as (X ? 1 , X ? 2 , ..., X ? N ) and (X ? 1 , X ? 2 , ..., X ? N ). To enhance the fusion performance of our method, a Forward and a Backward Translation are conducted, which means we rst translate modality ? into modality ? (forward) and then translate modality ? into modality ? (backward). So two Transformer models are used in one Modality Fusion Cell. We also use bi-directional Gated Recurrent Units (GRU) to extract contextual features before translation step.</p><p>Contextual Information Extraction. We conduct bi-directional GRUs and fully connected dense layers on features of modalities ? and ?, separately. In this way, we extract the contextual information in the hidden states. en we project the hidden states into a hyperspace of xed dimension.</p><formula xml:id="formula_0">H ? i = BiGRU ? (X ? i ) (1) D ? i = tanh(W ? H ? i + b ? )<label>(2)</label></formula><p>where ? ? {?, ? }, W ? is the weight matrix, b ? is the bias, and H ? i is the hidden state.</p><p>Forward Translation. Forward Translation aims to blend modality ? with modality ? by translating ). e , from X ? i and X ? i . ese features will be used later in this section to predict the sentiment tendency.</p><formula xml:id="formula_1">(D ? 1 ...D ? N ) into (X ? 1 ...X ? N ) with</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">TransModality</head><p>Multimodal features are di erent in abilities to predict speaker's sentiment tendency. We adopt text as the main modality, visual and acoustic as auxiliary modalities. We build our model, TransModality, with two Modality Fusion Cells for the text-visual and text-acoustic modality fusion, and a fully connected dense layer for classi cation. e two Modality Fusion Cells are built in parallel way, rather than sequential way, to eliminate the interference between modalities.</p><p>Modality Fusion. We input (textual, visual) and (textual, acoustic) features into the two separate Modality Fusion Cells. Each Modality Fusion Cell enables the textual features to blend with the information from another modality. Since there are two Modality Fusion Cells in our model, we get four encoded features for u erance X i ,</p><formula xml:id="formula_2">E t ? i , E ?t i , E t ?a i , E a?t i</formula><p>, which will be used for classi cation. e four decoded features for u erance</p><formula xml:id="formula_3">X i , D t ? i , D ?t i , D t ?a i , D a?t i</formula><p>, will be used to compare with corresponding target features to improve the translation performance.</p><p>Translation Evaluation. e decoded features serves as the result of translation. Because the decoded features may be of di erent dimension from the target features, fully connected dense layers are used to project decoded features to the dimension of target features.</p><p>Four separate fully connected dense layers are used and their outputs are denoted asX ? ?? i where (?, ?) ? {(t, ), ( , t), (t, a), (a, t)}. e mean absolute error is used to train the translation model.</p><formula xml:id="formula_4">L ? ?? i = 1 d ? d ? j=1 X ? ?? i j ? X ? i j<label>(3)</label></formula><p>where d ? is the dimension of X ? i . For each u erance X i , we get four MAE losses to evaluate the translation performance:</p><formula xml:id="formula_5">L t ? i , L ?t i , L t ?a i , L a?t i .</formula><p>Classi cation Layer. e encoded features from the Modality Fusion Cells are considered as the joint features of the two modalities involved. Motivated by the residual skip connection network <ref type="bibr" target="#b7">[8]</ref>, all encoded features are concatenated to obtain the nal joint features F i which embodies information from all modalities available.</p><formula xml:id="formula_6">F i = [E t ? i , E ?t i , E t ?a i , E a?t i , D t i , D i , D a i ]<label>(4)</label></formula><p>Finally, F i is passed to a fully connected dense layer to calculate the probability P i and we use the cross entropy loss function.</p><formula xml:id="formula_7">P i = so max(W F i + b) ? R d (5) i = argmax j (P i [j])<label>(6)</label></formula><formula xml:id="formula_8">L c i = ? log(P i [ i ])<label>(7)</label></formula><p>whereW is the weight matrix, b is the bias. d is the category number. P i [j] refers to the possibility for label j of u erance X i .? i is the predicted label for u erance X i . i is the true label for u erance X i . e joint loss of each u erance is a weighted summation of translation losses and classi cation loss</p><formula xml:id="formula_9">(L t ? i , L ?t i , L t ?a i , L a?t i , L c i )</formula><p>. e nal loss of the whole model is the average of each u erance's joint loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Bi-TransModality</head><p>For a dataset with only two modalities, we propose Bi-TransModality, which is similar to TransModality but takes only two modality features as input, denoted as ? and ?. Only one Modality Fusion Cell is used. e nal joint features are calculated as</p><formula xml:id="formula_10">F i = [E ? ?? i , E ? ?? i , D ? i , D ? i ]<label>(8)</label></formula><p>We also calculate the weighted joint loss to train the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT SETTING 4.1 Datasets</head><p>We evaluate our proposed model on three multimodal datasets: CMU-MOSI, MELD and IEMOCAP. In each dataset, videos are separated into several u erance-level segments, and each u erance is annotated with a label showing its sentiment tendency or emotion tendency. <ref type="table" target="#tab_1">Table 1</ref> shows the distribution of train, validation and test samples in the datasets. CMU-MOSI <ref type="bibr" target="#b24">[25]</ref> dataset contains opinion videos from online sharing websites such as YouTube. Each u erance is annotated as either positive or negative. We use u erancelevel features provided in Poria et al. <ref type="bibr" target="#b20">[21]</ref> for fair comparison with MMMU-BA <ref type="bibr" target="#b6">[7]</ref>. MELD <ref type="bibr" target="#b21">[22]</ref> dataset is a new multimodal multiparty conversational dataset which collects actor's lines from Friends, a famous American TV series. e dataset provides features of two modalities, textual and acoustic. Each u erance has been annotated with two labels. One shows its sentiment tendency among positive, neutral or negative. e other one shows its emotion tendency among anger, disgust, fear, joy, neutral, sadness and surprise (7 categories). We denote it as MELD (Sentiment) and MELD (Emotion). IEMOCAP <ref type="bibr" target="#b1">[2]</ref> dataset contains conversation videos. Each video contains a single dialogue and is segmented into u erances. e u erances are annotated with one of 6 emotion labels: happy, sad, neutral, angry, excited and frustrated. ese features are pre-trained through CNN, 3D-CNN and openS-MILE <ref type="bibr" target="#b5">[6]</ref> for textual, visual and acoustic features, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>To prove the e ectiveness of our proposed method, we compare our TransModality model with the following strong baseline models:</p><p>bc-LSTM <ref type="bibr" target="#b20">[21]</ref> use LSTM and dense layers to extract the contextual features and use simple concatenation as fusion method.</p><p>CHFusion <ref type="bibr" target="#b16">[17]</ref> (current state-of-the-art on IEMOCAP) fuses multimodal features through a hierarchical network. It rst fuses each two modalities and then fuses all three modalities.</p><p>MMMU-BA <ref type="bibr" target="#b6">[7]</ref> (current state-of-the-art on CMU-MOSI) uses a complex a ention mechanism as fusion method. GME-LSTM(A) <ref type="bibr" target="#b2">[3]</ref> uses Gated Embedding to alleviate the interference of noisy modalities, and the LSTM with Temporal A ention to fuse input modalities.</p><p>Seq2Seq2Sent <ref type="bibr" target="#b19">[20]</ref> adopts hierarchical seq2seq model to get joint multimodal representation.</p><p>MCTN [19] uses seq2seq model to fuse modalities. It builds a sequential fusion pipeline, which fuses textual and visual features at rst, and then fuses the joint feature with acoustic features.</p><p>MELD-base <ref type="bibr" target="#b21">[22]</ref> is baseline in Poria et al. <ref type="bibr" target="#b21">[22]</ref>. It extracts contextual information through GRU and fuses the multimodal features by concatenation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS AND DISCUSSION</head><p>We compare our proposed model with all baseline models in the multimodal datasets under di erent modality se ings. Weighted accuracy score is used as evaluation metric. We also use sign test <ref type="bibr" target="#b4">[5]</ref> to compare our model with the existing state-of-the-art models. e results are shown in <ref type="table" target="#tab_2">Table 2</ref>, 3. All the experiments are conducted under the same se ings, so the comparison is fair and trustworthy.</p><p>Our proposed fusion method for bi-modal se ings is Bi-TransModality, and TransModality is for tri-modal se ing.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Comparison with Baselines</head><p>As evidenced by <ref type="table" target="#tab_2">Table 2</ref> and <ref type="table" target="#tab_3">Table 3</ref>, with more modalities involved, an improvement in prediction performance can be witnessed. In our proposed method, there is big accuracy improvement with all three modalities considered, compared to two-modality version. is suggests that the multimodality fusion can make extra contribution to the prediction performance.</p><p>We can also observe that our model surpasses the existing stateof-the-art methods on the three datasets in most se ings, including bi-modality se ings and tri-modality se ings. On CMU-MOSI dataset, our proposed method achieves about 1.6 points accuracy improvement. On MELD dataset, our proposed method achieves 0.33 points and 1.69 points accuracy improvement for sentiment and emotion analysis, respectively. On IEMOCAP dataset, our proposed method achieves about 1.4 accuracy improvement. e comparison on all three di erent datasets and even for di erent tasks (sentiment analysis and emotion analysis) well demonstrates TransModality can be widely adopted to be er human sentiment and emotion analysis. We think the enhancement is caused by the new fusion methodology used by our model.</p><p>We further perform sign test when comparing the performance of our method and baseline methods with all modalities, i.e., on the CMU-MOSi and IEMOCAP datasets, we perform sign test on the results with three modalities, and on the MELD dataset, we perform sign test on the results with two modalities. In <ref type="table" target="#tab_2">Table 2</ref> and <ref type="table" target="#tab_3">Table 3</ref>, we use * to indicate that the p-value for sign-test between the results of the baseline method and our method is less than 0.05, showing a statistically signi cant di erence. We can see that our method can signi cantly outperform all the baseline methods on the CMU-MOSI dataset and the IEMOCAP dataset, and signi cantly outperform most baseline methods on the MELD dataset.</p><p>Bc-LSTM, MMMU-BA and MELD-base utilize concatenation or a ention mechanism to directly blend multimodal features, but features may interfere with each other when making predictions. GME-LSTM(A) and MCTN focus on the interference and rene the modality fusion, but information from di erent modalities cannot be blended well. e LSTM with Temporal A ention in GME-LSTM(A) is limited in the ability to fuse multimodal features together. e sequential fusion pipeline in MCTN also increases the risk of interference between modalities.</p><p>In our proposed model TransModality, fusion between modalities is conducted by end-to-end translation with Transformer. e encoded features serve as the joint features of the two modalities involved. Our joint features are encoded from the source modality features by Multi-Head A ention and Feed Forward Layers, so they contain most information of the source modality. e target modality features can be decoded from the joint features as the result of Transformer, so we assume that the information from target modality has also been well blended into our joint features.</p><p>Compared with the baselines, our model conducts no direct calculation on the multimodal features and the pair-wise fusion is conducted in parallel way, so it can be er prevent the interference. Our method also ensures the modality fusion performance through Forward and Backward Translation.</p><p>Our new idea on the fusion methodology improves the performance and contributes to a be er prediction result, and the e ectiveness of our proposed method under di erent circumstances has been proven clearly from the comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Analysis of Translation</head><p>e end-to-end translation is the core idea of our model. We select Transformer as the major component, use parallel translation to eliminate modality interference, and adopt Forward and Backward Translation to improve the performance.</p><p>We conduct head-to-head experiment to compare with Pham et al. <ref type="bibr">[19,</ref><ref type="bibr" target="#b19">20]</ref>    in our proposed method, such as the parallel translation, also re ne the architecture to predict sentiment tendency more accurately.</p><p>Backward Translation. We further analyze our proposed architecture with and without Backward Translation. In the architecture for comparison, only one-way translation (i.e. only Forward Translation) is conducted. From <ref type="table" target="#tab_6">Table 5</ref>, we observe that using Backward Translation indeed helps improve the performance of both Trans-Modality and Bi-TransModality. ere is approximately 1.5 points accuracy improvement on each dataset in di erent se ings.</p><p>e comparative experiments suggests that Backward Translation assists Forward Translation and improves the performance. Forward and Backward Translation is important components in our proposed model. A er the careful analysis of translation performance and Backward Translation, our hypothesis has been veri ed that TransModality is a reliable method to conduct modality fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we propose a new end-to-end translation based multimodal fusion method for u erance-level sentiment analysis. Our method utilizes Transformer to translate between modalities and blends multimodal information into the encoded features. With Transformer, the encoded features can embody the information from both the source modality and the target modality. We also adopt Forward and Backward Translations to be er model the correlation between multimodal features and improve the translation performance.</p><p>rough the validation results on multiple multimodal datasets (CMU-MOSI, MELD, IEMOCAP), we demonstration the reliability of Transformer in multimodality fusion and the e ectiveness of Forward and Backward Translations.</p><p>In future work, we will apply our model to other multimodal classi cation tasks, such as multimodal sarcasm detection and stance detection, to further test the robustness of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ACKNOWLEDGMENTS</head><p>is work was supported by National Natural Science Foundation of China (61772036) and Key Laboratory of Science, Technology and Standard in Press Industry (Key Laboratory of Intelligent Press Media Technology). We appreciate the anonymous reviewers for their helpful comments. Xiaojun Wan is the corresponding author.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Conference'17, Washington, DC, USA ? 2016 ACM. 978-x-xxxx-xxxx-x/YY/MM. . . $15.00 DOI: 10.1145/nnnnnnn.nnnnnnn media, such as YouTube, o er a new resource of multimodal materials, and the new multimodal materials are closer to real human.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>An example from CMU-MOSI dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Modality Fusion Cell: X ? 1 ...X ? N and X ? 1 ...X ? N are the features of modalities involved.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 : 1 .? ?? 1 .</head><label>311</label><figDesc>e architecture of TransModality decoder takes (D ? 1 ...D ? N ), (E ? ?? Backward Translation. Backward Translation ensures the robustness of E ? ?? i by translating between modalities backwards (i.e. from modality ? to ?). e Backward Transformer is similar to the Forward Transformer, but the backward one translates from (D ? ?? 1 ...D ? ?? N ) into (X ? 1 ...X ? N ). e encoded features and the decoded features in Backward Transformer are denoted as (E For each u erance X i , the Modality Fusion Cell produces four new features, E</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Data distribution</figDesc><table><row><cell>Dataset</cell><cell>Modality</cell><cell>Partition</cell><cell cols="2">Videos U erances</cell></row><row><cell></cell><cell></cell><cell>Train &amp; Valid</cell><cell>62</cell><cell>1447</cell></row><row><cell>CMU-MOSI</cell><cell>t,v,a</cell><cell>Test</cell><cell>31</cell><cell>752</cell></row><row><cell></cell><cell></cell><cell>Train &amp; Valid</cell><cell>1152</cell><cell>11098</cell></row><row><cell>MELD</cell><cell>t,a</cell><cell>Test</cell><cell>280</cell><cell>2610</cell></row><row><cell></cell><cell></cell><cell>Train &amp; Valid</cell><cell>120</cell><cell>5810</cell></row><row><cell>IEMOCAP</cell><cell>t,v,a</cell><cell>Test</cell><cell>31</cell><cell>1623</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on CMU-MOSI &amp; MELD (Sentiment) * indicates p-value ? 0.05 for sign test when compared with our method</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CMU-MOSI</cell><cell></cell><cell></cell><cell cols="3">MELD (Sentiment)</cell></row><row><cell></cell><cell></cell><cell>Uni</cell><cell></cell><cell></cell><cell>Bi</cell><cell></cell><cell>Tri</cell><cell>Uni</cell><cell></cell><cell>Bi</cell></row><row><cell></cell><cell>t</cell><cell>v</cell><cell>a</cell><cell>v,a</cell><cell>t,v</cell><cell>t,a</cell><cell>t,v,a</cell><cell>t</cell><cell>a</cell><cell>t,a</cell></row><row><cell>MELD-base</cell><cell cols="4">77.79 55.19 55.85 54.79</cell><cell>76.60</cell><cell cols="5">76.99 79.19* 66.33 46.43 66.68*</cell></row><row><cell>bc-LSTM</cell><cell cols="4">79.12 55.98 57.31 56.52</cell><cell>78.59</cell><cell cols="5">78.86 79.26* 65.85 54.39 66.09*</cell></row><row><cell>CHFusion</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>54.49</cell><cell>74.77</cell><cell cols="2">78.54 76.51*</cell><cell>-</cell><cell>-</cell><cell>65.85*</cell></row><row><cell>MMMU-BA</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">57.45 80.85</cell><cell cols="2">79.92 81.25*</cell><cell>-</cell><cell>-</cell><cell>65.56*</cell></row><row><cell cols="5">GME-LSTM(A) 71.30 52.30 55.40 52.90</cell><cell>74.30</cell><cell cols="5">73.50 76.50* 65.52 52.03 66.46</cell></row><row><cell>seq2seq2sent</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>58.00</cell><cell>67.00</cell><cell cols="2">66.00 70.00*</cell><cell>-</cell><cell>-</cell><cell>63.84*</cell></row><row><cell>MCTN</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>53.10</cell><cell>76.80</cell><cell cols="2">76.40 79.30*</cell><cell>-</cell><cell>-</cell><cell>66.27</cell></row><row><cell>Ours</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>59.97</cell><cell cols="3">80.58 81.25 82.71</cell><cell>-</cell><cell>-</cell><cell>67.04</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="6">Results on IEMOCAP &amp; MELD (Emotion)</cell><cell></cell></row><row><cell cols="10">* indicates p-value ? 0.05 for sign test when compared with our method</cell></row><row><cell></cell><cell cols="3">MELD (Emotion)</cell><cell></cell><cell></cell><cell></cell><cell cols="2">IEMOCAP</cell><cell></cell></row><row><cell></cell><cell>Uni</cell><cell></cell><cell>Bi</cell><cell></cell><cell>Uni</cell><cell></cell><cell></cell><cell>Bi</cell><cell></cell><cell>Tri</cell></row><row><cell></cell><cell>t</cell><cell>a</cell><cell>t,a</cell><cell>t</cell><cell>v</cell><cell>a</cell><cell>v,a</cell><cell>t,v</cell><cell>t,a</cell><cell>t,v,a</cell></row><row><cell>MELD-base</cell><cell cols="7">56.75 39.74 57.85* 55.51 39.06 47.57 48.31</cell><cell>56.62</cell><cell cols="2">57.12 58.29*</cell></row><row><cell>bc-LSTM</cell><cell cols="8">59.96 49.46 60.19 56.81 38.51 46.15 47.38 57.42</cell><cell cols="2">57.55 58.23*</cell></row><row><cell>CHFusion</cell><cell>-</cell><cell>-</cell><cell>58.35*</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>41.84</cell><cell>56.83</cell><cell cols="2">57.30 58.90*</cell></row><row><cell>MMMU-BA</cell><cell>-</cell><cell>-</cell><cell>60.26*</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>49.66</cell><cell>57.30</cell><cell cols="2">57.18 58.78*</cell></row><row><cell cols="8">GME-LSTM(A) 59.57 49.59 60.01* 56.69 39.86 48.98 48.55</cell><cell>56.44</cell><cell cols="2">56.93 57.98*</cell></row><row><cell>seq2seq2sent</cell><cell>-</cell><cell>-</cell><cell>56.42*</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>46.81</cell><cell>53.81</cell><cell cols="2">52.56 54.75*</cell></row><row><cell>MCTN</cell><cell>-</cell><cell>-</cell><cell>59.96*</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>49.88</cell><cell>54.81</cell><cell cols="2">55.13 57.38*</cell></row><row><cell>Ours</cell><cell>-</cell><cell>-</cell><cell>61.95</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>50.15</cell><cell cols="3">56.93 58.84 60.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>to show the contribution of Transformer and other components in our model. en we also demonstrate the e ectiveness of Forward and Backward Translation through comparative experiments between di erent versions of our model. Comparison with Seq2seq Translation. We replace the seq2seq in MCTN [19] with Transformer, and build a model called Trans-MCTN. is model is the same as MCTN except for the use of Transformer. We compare the result of seq2seq2sent, MCTN, Trans-MCTN and TransModality on all three datasets to show the improvement from Transformer. e results are listed in Table 4. e</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Results of Seq2seq-based or Transformer-based Translation indicates p-value ? 0.05 (compared with Trans-MCTN) * indicates p-value ? 0.05 (compared with TransModailty)</figDesc><table><row><cell></cell><cell>CMU-MOSI</cell><cell>MELD(Sentiment)</cell></row><row><cell></cell><cell>t,v,a</cell><cell>t,a</cell></row><row><cell>seq2seq2sent</cell><cell>70.00*</cell><cell>63.84*</cell></row><row><cell>MCTN</cell><cell>79.30*</cell><cell>66.27</cell></row><row><cell>Trans-MCTN</cell><cell>81.67</cell><cell>66.16*</cell></row><row><cell>TransModality</cell><cell>82.71</cell><cell>67.04</cell></row><row><cell></cell><cell>MELD(Emotion)</cell><cell>IEMOCAP</cell></row><row><cell></cell><cell>t,a</cell><cell>t,v,a</cell></row><row><cell>seq2seq2sent</cell><cell>56.42*</cell><cell>54.75*</cell></row><row><cell>MCTN</cell><cell>59.96*</cell><cell>57.38*</cell></row><row><cell>Trans-MCTN</cell><cell>60.24*</cell><cell>58.72*</cell></row><row><cell>TransModality</cell><cell>61.95</cell><cell>60.81</cell></row><row><cell cols="3">models in the rst two lines use seq2seq to translate. e models in</cell></row><row><cell cols="3">the next two lines use Transformer to translate. With the p-values</cell></row><row><cell cols="3">from sign test, the stable increment is observed in the compari-</cell></row><row><cell cols="3">son. On all three datasets, Trans-MCTN achieves an improvement</cell></row><row><cell cols="3">compared to MCTN, and with other innovations in our proposed</cell></row><row><cell cols="3">method, TransModality achieves an improvement compared to</cell></row><row><cell>Trans-MCTN.</cell><cell></cell><cell></cell></row><row><cell cols="3">e experiment demonstrates that Transformer contributes to</cell></row><row><cell cols="3">be er performance than simple seq2seq method. Other innovations</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Results of using or not using Backward Translation</figDesc><table><row><cell></cell><cell cols="2">CMU-MOSI</cell><cell cols="2">MELD (Sentiment)</cell></row><row><cell>Modal</cell><cell cols="3">with without with</cell><cell>without</cell></row><row><cell>v, a</cell><cell>59.97</cell><cell>58.78</cell><cell>-</cell><cell>-</cell></row><row><cell>t, v</cell><cell>80.58</cell><cell>79.12</cell><cell>-</cell><cell>-</cell></row><row><cell>t, a</cell><cell>81.25</cell><cell>79.78</cell><cell>67.04</cell><cell>66.02</cell></row><row><cell cols="2">t, v, a 82.71</cell><cell>80.18</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">MELD (Emotion)</cell><cell cols="2">IEMOCAP</cell></row><row><cell>Modal</cell><cell cols="3">with without with</cell><cell>without</cell></row><row><cell>v, a</cell><cell>-</cell><cell>-</cell><cell>50.15</cell><cell>48.80</cell></row><row><cell>t, v</cell><cell>-</cell><cell>-</cell><cell>56.93</cell><cell>56.07</cell></row><row><cell>t, a</cell><cell>61.95</cell><cell>60.58</cell><cell>58.84</cell><cell>57.98</cell></row><row><cell>t, v, a</cell><cell>-</cell><cell>-</cell><cell>60.81</cell><cell>59.21</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Emonet: Fine-grained emotion detection with gated recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Abdul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Mageed</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle</forename><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="718" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">IEMOCAP: Interactive emotional dyadic motion capture database. Language resources and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murtaza</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Chun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Mower</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeanne</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungbok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrikanth S</forename><surname>Narayanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page">335</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multimodal sentiment analysis with word-level fusion and reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Baltru?aitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM International Conference on Multimodal Interaction</title>
		<meeting>the 19th ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="163" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">e statistical sign test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wilfrid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="557" to="566" />
			<date type="published" when="1946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Opensmile: the munich versatile and fast open-source audio feature extractor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>W?llmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM international conference on Multimedia</title>
		<meeting>the 18th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1459" to="1462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Contextual Inter-modal A ention for Multi-modal Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepanway</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shad</forename><surname>Md</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3454" to="3466" />
		</imprint>
	</monogr>
	<note>Asif Ekbal, and Pushpak Bha acharyya</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pa ern recognition</title>
		<meeting>the IEEE conference on computer vision and pa ern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<title level="m">Convolutional neural networks for sentence classi cation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet classi cation with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geo Rey E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Investigation of Multimodal Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhua</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.06225</idno>
	</analytic>
	<monogr>
		<title level="m">Classi ers and Fusion Methods for Emotion Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sentiment analysis and opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis lectures on human language technologies</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="167" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Recurrent neural network for text classi cation with multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.05101</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">E cient low-rank multimodal fusion with modality-speci c factors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Bharadhwaj Lakshminarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00064</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Multimodal sentiment analysis using hierarchical fusion with context modeling. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">161</biblScope>
			<biblScope unit="page" from="124" to="133" />
		</imprint>
	</monogr>
	<note>Erik Cambria, and Soujanya Poria</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">NRC-Canada: Building the state-of-the-art in sentiment analysis of tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.6242</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnab?s</forename><surname>P?czos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.07809</idno>
		<title level="m">Found in Translation: Learning Robust Joint Representations by Cyclic Translations Between Modalities</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Seq2seq2sentiment: Multimodal sequence to sequence models for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Manzini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poczos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03915</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Context-dependent sentiment analysis in user-generated videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="873" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02508</idno>
		<title level="m">MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A ention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Tensor fusion network for multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07250</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">MOSI: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Pincus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06259</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
