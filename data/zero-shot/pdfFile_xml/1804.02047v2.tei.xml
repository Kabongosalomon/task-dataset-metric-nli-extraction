<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pedestrian-Synthesis-GAN: Generating Pedestrian Data in Real Scene and Beyond</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Ouyang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Machine Learning Department</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Pedestrian-Synthesis-GAN: Generating Pedestrian Data in Real Scene and Beyond</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>( * denotes equal contribution)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Generative Adversarial Network</term>
					<term>Pedestrian Detection</term>
					<term>Spa- tial Pyramid Pooling</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>State-of-the-art pedestrian detection models have achieved great success in many benchmarks. However, these models require lots of annotation information and the labeling process usually takes much time and efforts. In this paper, we propose a method to generate labeled pedestrian data and adopt them to support the training of pedestrian detectors. The proposed framework is built on the Generative Adversarial Network (GAN) with multiple discriminators, trying to synthesize realistic pedestrians and learn the background context simultaneously. To handle the pedestrians of different sizes, we adopt the Spatial Pyramid Pooling (SPP) layer in the discriminator. We conduct experiments on two benchmarks. The results show that our framework can smoothly synthesize pedestrians on background images of variations and different levels of details. To quantitatively evaluate our approach, we add the generated samples into training data of the baseline pedestrian detectors and show the synthetic images are able to improve the detectors' performance. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pedestrian detection is a crucial task in computer vision with a wide range of applications, including autopilot, surveillance and robotics <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. Recently, pedestrian detectors based on convolutional neural networks (CNNs), such as Faster R-CNN <ref type="bibr" target="#b4">[5]</ref> and YOLO9000 <ref type="bibr" target="#b5">[6]</ref>, have been applied to various of benchmarks. Built on tremendous amount of training examples, these models can achieve significant performance improvement over previous baselines.</p><p>However, labeling ground-truth bounding boxes for pedestrian locations requires time consuming and considerable human effort. Meanwhile, the performance of CNN-based pedestrian detectors heavily depends on the quality and the diversity of annotations in the training datasets. In other words, those methods expect the training data set to cover the same scenes or similar background environment as the testing data, such as camera configurations, lighting conditions D Noise <ref type="figure">Fig. 1</ref>: The PS-GAN model learns to smoothly synthesize pedestrians in background images through the multiple discriminators (D b and D p ) network. and backgrounds. This becomes an issue when one applies these methods to a new unannotated video or video with limited supervision. Therefore, it is very important to design approaches that only rely on limited supervision and can be extended to new unannotated datasets smoothly.</p><p>One way to solve this problem is to develop methods to automatically generate labeled datasets. There exists some efforts that use simulation techniques to generate pedestrian appearance and its location in the image <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. But these methods apply in strict environment like fixed cameras. One model proposed to be used for the moving camera <ref type="bibr" target="#b8">[9]</ref>, can combine real-world background information in a scene with synthetically generated pedestrians. Nevertheless, since they generate pedestrians through rendering of 3D human models, the synthetic images look unrealistic and unnatural. Motivated by recent promising success of generative adversarial networks (GANs) <ref type="bibr" target="#b9">[10]</ref> in several applications <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>, we propose to build a GAN-based model to generate realistic pedestrian images in real scene and utilize them as the augmented data to train the CNN-based pedestrian detector. Compared with adopting the regular GAN as a powerful tool for generating images, the goal of our model is different and more challenging due to: 1) generating pedestrians to fit the background scene well; 2) providing the corresponding locations of those synthetic pedestrians as the ground truths for the CNN-based detectors. We denominate it as Pedestrian-Synthesis-GAN (PS-GAN).</p><p>PS-GAN adopts the adversarial learning recipe and contains multiple discriminators (D b for background context learning and D p for pedestrian classifying), as shown in <ref type="figure">Figure 1</ref>. We replace the pedestrians in the bounding boxes with random noise and train the generator G to synthesize new pedestrians within that noise region. The discriminator D b , learns to discriminate between real and synthesized pair. Meanwhile, the discriminator D p learns to judge whether the synthetic pedestrian cropped from the bounding boxes is real or fake. D b aims to force G to learn the background information like the road, light condition in noise boxes. It leads to smooth connection between the background and the synthetic pedestrian. D p makes G to generate real pedestrians with more realistic shape and details. Moreover, due to the varied sizes of cropped synthetic pedestrians, we utilize the Spatial Pyramid Pooling (SPP) layer <ref type="bibr" target="#b13">[14]</ref> in D p to avoid the effect of resizing. After training, the generator G can learn to generate photo-realistic pedestrians in the noise box regions and the locations of noise boxes are taken as the ground truths for detectors.</p><p>To the best of our knowledge, PS-GAN is the first work that utilizes GAN to generate data for pedestrian/object detection task. We evaluate it on two large-scale datasets: Cityscapes <ref type="bibr" target="#b14">[15]</ref> and Tsinghua-Daimler Cyclist Benchmark <ref type="bibr" target="#b15">[16]</ref>. We use the model to generate results on these two datasets, and also train the Faster R-CNNs <ref type="bibr" target="#b4">[5]</ref> with real and synthetic data to prove the effectiveness of data augmentation. We show that: -Our proposed model can generate sharp and photo-realistic pedestrian images and fit the background well in real scene/image; -The data generated from PS-GAN can be used with some real samples to train CNN-based detectors. This data augmentation step can improve both detection performance and stability over original model; -On cross-dataset experiments, i.e., model is trained on one dataset and tested on the other, PS-GAN is also able to generate good samples and improve the performances of CNN-based detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Pedestrian Detection Pedestrian detection attracts great interest due to its wild applications including driving systems, surveillance and robotics <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. Built upon parameterized CNN models, recent works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref> can achieve good detection performances in several benchmarks. However, these models require a large amount of training samples, which is quite time-consuming and takes many human efforts.</p><p>To handle this issue, researchers have proposed different solutions, one of which is to develop data augmentation techniques. Existing data augmentation methods are generally limited to certain tasks or conditions: <ref type="bibr" target="#b6">[7]</ref> focuses exclusively on crowd behavior, <ref type="bibr" target="#b7">[8]</ref> works only when the camera is stationary. The paper <ref type="bibr" target="#b8">[9]</ref> provides an automatic and relatively robust model STD-PD, which selects possible locations to place synthetic agents. Using a 3D model for pedestrian rendering, its generated pictures are not realistic. Realizing that it is difficult to model the complex distribution of pedestrians in real scene by using hand-crafted rules only, we decide to adopt data-driven approach like GANs to perform the task.</p><p>Generative Adversarial Network The original GAN was proposed by <ref type="bibr" target="#b9">[10]</ref>, and there are plenty of following works to improve the training stability and visual quality of the generation <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>. GANs also have been employed in many other applications, for example, super-resolution <ref type="bibr" target="#b12">[13]</ref>, image in-painting <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref>, image translation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>. <ref type="bibr" target="#b20">[21]</ref> proposed DCGAN and adopted it to augment training data for person re-identification, which focuses on verifying the effectiveness of the label smoothing regularization, instead of the quality of the generated pictures. <ref type="bibr" target="#b31">[32]</ref> proposed PGGAN to synthesize persons in arbitrary poses in the cropped person images.</p><p>The most related work to ours is the P ix2P ix GAN <ref type="bibr" target="#b10">[11]</ref>, which have solid and robust results when paired training samples are available. <ref type="bibr" target="#b30">[31]</ref> add Cycle Consistency Loss to the original version, enabling the model to conduct translations without paired training examples and task-specific designed functions. To synthesize the pedestrians in the noise boxes and the locations of which can be taken as the bounding box labels, we adopt the paired training in P ix2P ix GAN but a different architecture with multi-discriminators.</p><p>Compared with the image in-painting work <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref>, which aims to fill the randomly removed monochromatic patches in original image, our framework fills the missing area with noise rather than monochromatic blocks to generate patches with diverse shapes/colors. We only need to learn the background information based on the context provided by surrounding parts of the image when synthesizing pedestrians in noise boxes. The work in <ref type="bibr" target="#b27">[28]</ref> exploits a similar two discriminators GAN for image in-painting to learn more context information of surrounding pixel. Different from that, we pass the pedestrian patch cropped from the generated output into the discriminator to encourage the model to generate person in diverse shapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Pedestrian-Synthesis-GAN</head><p>Generative Adversarial Networks <ref type="bibr" target="#b9">[10]</ref> consist of a generator G and a discriminator D that compete in a two-player minimax game. In this paper, we adopt the adversarial learning idea and propose PS-GAN with multiple discriminators, which has ability to synthesize photo-realistic pedestrians with the corresponding bounding boxes information. Unlike the regular GAN, our method leverages an adversarial process between the generator G and both two discriminators: D b for background context learning and D p for discriminating pedestrian.</p><p>Our framework is inspired by the conditional GAN work <ref type="bibr" target="#b32">[33]</ref>. While training, we replace the pedestrian region in original image with random noise and push it to the generator G. Suppose the noise image is x while the original image with pedestrian is y. G is trying to generate fake image from x as similar as possible to y to fool the two discriminators D b and D p . Therefore, when generating new data, we can place noise boxes on the certain area where the pedestrians are expected and use the generator G to synthesize pedestrian within the noise boxes. In this section, we first introduce the model architecture, then the detailed formulation of the overall objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model architecture</head><p>U-Net for Generator G The generator G learns a mapping function G : x ? y, where x is the input noisy image and y is the ground truth image. In this work, we adopt the enhanced encoder-decoder network (U-Net) <ref type="bibr" target="#b10">[11]</ref> for G. It follows the main structure of the encode-decoder architecture, where the input image x is passed through a series of convolutional layers as downsampling layers until a bottleneck layer. Then the bottleneck layer feeds the encoded information of original inputs to the deconvolutional layers to be upsampled. U-Net uses the skip connections to connect the downsampling and upsampling layers in a symmetric position with respect to the bottleneck layer, which can preserve richer local information.</p><p>D p to Discriminate fake/real Pedestrians For this discriminator D p , we crop the synthetic pedestrian from the generated image as the negative example, while the real pedestrian y p from the original image y as the positive example. Therefore, D p is used to classify whether the pedestrian is real or fake in the noise box. It forces G to learn the mapping from the noise z to the real pedestrian y p , wher z is the noise region in the noise image x.</p><p>The overall structure of D p is shown in <ref type="figure" target="#fig_0">Figure 2</ref>. We apply a 5-layer convolutional network with LeakyRelu and BatchNorm layers. Normally the discriminator net accepts a fixed-size input. However, the input for our D p is the cropped pedestrian from the generated image or the ground truth image, which have various sizes. To address this issue, we adopt the Spatial Pyramid Pooling (SPP) layer <ref type="bibr" target="#b13">[14]</ref> in D p and the detail of SPP-layer is also shown in <ref type="figure" target="#fig_0">Figure 2</ref>. In our experiments, for each cropped pedestrian, we use a 3-level spatial pyramid (1 ? 1, 2 ? 2, 4 ? 4, totally 21 bins) to pool the features. After that, we concatenate all those 3-level features to an entire feature vector and apply the P atch GAN loss <ref type="bibr" target="#b10">[11]</ref> here.</p><formula xml:id="formula_0">D b to Learn Background Context</formula><p>The goal of our model is to not only synthesize a realistic pedestrian but also smoothly fill the synthetic pedestrian into the background. Thus it requires our model to learn context information like light conditions, surrounding backgrounds, etc. Following the pair-training recipe from P ix2P ix GAN <ref type="bibr" target="#b10">[11]</ref>, D b is used to classify between real and synthesized pairs. The real pair concatenates the noise image x and ground truth image y while the synthesized pair concatenates the noise image x and the generated image. The overall framework training D b is shown in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>The main structure of D b follows the design of DCGAN <ref type="bibr" target="#b20">[21]</ref> with the following modifications: 1) we make the first convolutional layer accept the 6-channel input of the stacked pair of images; 2) we use the P atchGAN in this discriminator as in <ref type="bibr" target="#b10">[11]</ref>, which means D b tries to classify if each N ? N (in our experiment, N is set to 70) patch in an image is real or fake; 3) we adopt the loss function of LSGAN <ref type="bibr" target="#b21">[22]</ref> in D b . To fit the P atchGAN setting, we calculate the mean squares between the N ? N output and corresponding all-ones or all-zeros matrix as the loss function for D b .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Loss Function</head><p>As illustrated in <ref type="figure">Figure 1</ref>, our model consists of two adversarial learning procedures G ? D b and G ? D p . The adversarial learning between G and D b can be formulated as: <ref type="formula">(1)</ref> where x is the image with the noise box and y is the ground truth image. We use LSGAN here to replace the original GAN loss by a least square loss.</p><formula xml:id="formula_1">L LSGAN (G, D b ) = E y?pgt?image(y) [(D b (y) ? 1) 2 ] + E x,z?pnoise?image(x,z) [(D b (G(x, z))) 2 ],</formula><p>To encourage G to generate realistic pedestrians within the noise box z in the input image x, we conduce the other adversarial procedure between G and D p : <ref type="formula">(2)</ref> where z is the noise box in x and y p is the cropped pedestrian in the ground truth image y. We use the negative log likelihood objective to update the parameters of G and D p .</p><formula xml:id="formula_2">L GAN (G, D p ) = E yp?p pedestrian (yp) [log D p (y p )] + E z?pnoise(z) [log(1 ? D p (G(z)))],</formula><p>The training of GAN can be benefited from the traditional loss <ref type="bibr" target="#b10">[11]</ref>. In this paper, we apply 1 loss to control the differences between the generated image and ground image y:</p><formula xml:id="formula_3">L 1 (G) = E x,z?p noise?image (x,z),y?p gt?image (y) [ y ? G(x, z) 1 ],<label>(3)</label></formula><p>Finally, combining the losses previously defined results in the final loss function as:</p><formula xml:id="formula_4">L(G, D b , Dp) = LLSGAN (G, D b ) + LGAN (G, Dp) + ?L 1 (G).<label>(4)</label></formula><p>where ? controls the relative importance of the 1 loss. Empirically, we found that ? = 100 is a good setting and fixed it for the all experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>We test PS-GAN model on Cityscapes <ref type="bibr" target="#b14">[15]</ref> and show the quality of the synthesized images. To analyze the effect of the data augmentation, we combine the real and synthesized data to train the Faster R-CNN <ref type="bibr" target="#b4">[5]</ref> detectors and evaluate the performance. Moreover, to evaluate the ability to generate training example on the new video with limited supervision, we test PS-GAN model trained using Cityscapes on Tsinghua-Daimler Cyclist Benchmark <ref type="bibr" target="#b15">[16]</ref>. All those experiments are based on PyTorch 2 and run on Titan X GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Cityscapes</head><p>The Cityscapes dataset is a large-scale dataset for semantic urban scene understanding that contains a diverse set of stereo video recordings from 50 cities <ref type="bibr" target="#b14">[15]</ref>. Compared to other benchmarks like Caltech Pedestrian <ref type="bibr" target="#b33">[34]</ref> and KITTI <ref type="bibr" target="#b34">[35]</ref>, Cityscapes has higher resolution pictures and contains more pedestrians with rich variety, which is more suitable to train GANs.</p><p>Qualitative Result We generate the bounding boxes for all pedestrians based on the pixel-wise labels. There are some labeled pedestrians which are too small or partially blocked by cars or walls. So we filter out all the bounding boxes with the height smaller than 70 pixels and width smaller than 25 pixels. After that, we obtain 2326 images containing totally 9708 labeled pedestrians and randomly select 500 images of them as the testing dataset. We do not feed the original images (1024 ? 2048) into PS-GAN directly. Instead, we crop the 256 ? 256 patches around the chosen pedestrians from the original 1024 ? 2048 images. Moreover, we select 1200 pedestrian patches from the 1826 training images which display intact body shapes. Those 1200 patches will be covered with noise boxes in the pedestrian positions, then those noise images are taken as the training data for PS-GAN.</p><p>To show the pedestrians generated by PS-GAN, we conduct two experiments: 1) generating pedestrians on the real pedestrian positions, and 2) generating pedestrians only on the background images without pedestrians. For the first setting, we crop the 256 ? 256 patches around the pedestrians from the original 1024 ? 2048 images among the 500 test examples and fill the noise boxes to cover the real pedestrians in those patches. Our pre-trained generator synthesizes pedestrians within those noise boxes and we compare the synthetic and real pedestrians as shown in <ref type="figure" target="#fig_1">Figure 3</ref>. For the second setting, we randomly crop the 256 ? 256 patches from the blank scene images without any labeled pedestrians. Considering that the pedestrians can not appear in unreasonable positions like in the wall or within a car, we remove those wrong images and add the noise boxes in the remaining image patches. The results are in <ref type="figure">Figure 4</ref>.</p><p>We list the synthesized samples of all baseline models, trained on the same training set for 200 epochs. Compared with the baseline P ix2P ix GAN, PS-GAN can generate better quality of images both on <ref type="figure" target="#fig_1">Figure 3</ref> and <ref type="figure">Figure 4</ref>. Most of the results of P ix2P ix GAN only have murky person shapes while PS-GAN gets very clear shape of pedestrians. It proves that our discriminator D p can effectively guide generator G to learn more realistic shape information and details of pedestrians. To evaluate the effect of the SPP layer in D p , we compare the results of PS-GAN with the model A, which does not have SPP layer in D p . As shown in <ref type="figure" target="#fig_1">Figure 3</ref> and <ref type="figure">Figure 4</ref>, the model with SPP layer can learn more detailed information of pedestrians. For instance, in the first row of both <ref type="figure" target="#fig_1">Figure  3</ref> and <ref type="figure">Figure 4</ref>, the legs of the person from PS-GAN can clearly be seen while they are blurry in the one from model A.</p><p>In our experiments, we find that using LSGAN <ref type="bibr" target="#b21">[22]</ref> for D b is helpful to learn the background context. PS-GAN can obtain the best picture quality when applying the least square loss for the adversarial learning G ? D b and keeping the regular GAN loss for G ? D p . We design the model B that adopts the LSGAN loss in both adversarial learning procedures, but the results are not competitive with PS-GAN as shown in both <ref type="figure" target="#fig_1">Figure 3</ref> and <ref type="figure">Figure 4</ref>. Model B performs only slightly better than the P ix2P ix GAN. We also study model C, which uses the regular loss on both adversarial learning procedures. Actually, the model C can generate pedestrians with nice human-body shape. In the last row of <ref type="figure" target="#fig_1">Figure 3</ref>, it even generates a pedestrian with better shape. However, this model can not learn the adequate background context information to fit surrounding pixels.</p><p>We analyze the reason why the two discriminators D p and D b have different optimal GAN losses in our work: 1) for D p , as we apply the P atchGAN trick, LSGAN with least square loss will get larger error than the regular GAN loss. It makes the model to be more sensitive to every pixel in images than the regular GAN. Thus the generator G may be forced to learn too much detailed information of pedestrians instead of capturing the global distribution; 2) however, our discriminator D b can take benefit from the least square loss when learning the background context information. We expect the generator to strictly learn the background information from the surrounding pixels.  We crop the pedestrians from the generated images, and demonstrate that PS-GAN can generate pedestrians with sharp body shapes and detailed information as illustrated in <ref type="figure" target="#fig_3">Figure 5</ref>. Compared with the work in <ref type="bibr" target="#b35">[36]</ref>, which uses 12,936 images to train the GAN for the person re-identification task, we only use 1200 images to train PS-GAN and get sharper and more photo-realistic results.</p><p>Quantitative Analysis In this section, we combine the data generated by PS-GAN with some real data to train the Faster R-CNN detector <ref type="bibr" target="#b4">[5]</ref> to analyze the effects of data augmentation. In the experiment, we follow the setting on the above qualitative result section and randomly put noise boxes to generate pedestrians on the 256 ? 256 patches from the images on Cityscapes. After that, we fill those patches with generated pedestrians into the original images. Some examples are shown in <ref type="figure" target="#fig_4">Figure 6</ref>. Many synthetic pedestrians by PS-GAN look hallucinating real in the original images, which is only trained on the 1826 training images. It is notable that all the patches are add into the original 1826 training images which means we do not involve any new images with synthetic pedestrians. To demonstrate how the augmented synthetic images can help boost the performance of the Faster R-CNN model, we train three Faster R-CNN detectors <ref type="bibr" target="#b4">[5]</ref> (VGG-16 <ref type="bibr" target="#b36">[37]</ref> based models). The baseline detector is trained on the original 1826 training images, and two detectors are trained on those images adding synthetic pedestrians from P ix2P ix GAN and PS-GAN separately. All the detectors are tested on the 500 testing images and the average precisions (AP) are from the best performance when all the models converge. We also add different amounts of synthetic pedestrians into the 1826 training image and present the results on <ref type="table" target="#tab_1">Table 1</ref>. Although the Faster RCNN detector has been trained well (60.11%) on 1856 images, adding synthetic pedestrians on the original images for training the detector is still beneficial. With 5000 synthetic pedestrians from PS-GAN, we improve the the detector performance from 60.11% to 61.79%. On the contrary, adding 8000 synthetic pedestrians from P ix2P ix GAN downgrades the performance to be 58.41% since adding too many examples from P ix2P ix GAN destroys the normal data distribution. This experimental result matches the terrible visual quality of the P ix2P ix GAN.</p><p>To attain deeper insight into the effect of the augmented synthetic images, we conduct more experiments as shown in <ref type="table" target="#tab_1">Table 1</ref>. We train a baseline Faster R-CNN detectors <ref type="bibr" target="#b4">[5]</ref> (VGG-16 <ref type="bibr" target="#b36">[37]</ref> based models) of using 300 real image and also adopt the detectors <ref type="bibr" target="#b4">[5]</ref> pretrained on Pascal VOC <ref type="bibr">[38]</ref>. Also, all the detectors are tested on the 500 testing images. Moreover, to avoid the GAN model to see more data than the Faster R-CNN, all the Pix2Pix GAN model and PS-GAN models are retrained on the same image set for training the Faster R-CNN. In other words, we retrain those GAN models on the 300 images for fair comparison. The synthetic pedestrians are also adding into the original images without adding any new image into training. As shown in <ref type="table" target="#tab_1">Table 1</ref>, the detectors pretrained on Pascal VOC 2007 dataset and 2007 &amp; 2012 datasets can achieve 34.13% AP and 36.85% respectively. This observation indicates that pretrained model on different background can not perform well. The baseline detector using 300 real images with 1173 pedestrians in Cityscapes, can achieve 47.08% of the average precision (AP) for pedestrian detection. By adding the synthetic images, the AP rate can be improved. We get the best performance when adding 1000 synthetic pedestrians. It outperforms the baseline to 1.71% while adding 2000 synthetic pedestrians can improve by only 1.04%. In both cases, we compare the results with image synthesized from P ix2P ix GAN. It slightly downgrades the performance in all the experiments. We also train another baseline Faster R-CNN detector using 1000 real images, with 4368 pedestrians annotated in total. Meanwhile, the GAN models are retrained on the 1000 real images. The motivation here is to see how the augmented synthetic images can help boost the performance when the Faster R-CNN model gets different amounts of real training data. We add 2000 and 4000 synthetic pedestrians to the original 1000 real ones and retrain the Faster R-CNN detector. We can see that, even Faster R-CNN is trained in a more saturated state, the model with data augmentation can achieves 56.19% AP, outperforming the baseline 3.47%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Tsinghua-Daimler Cyclist Benchmark</head><p>Tsinghua-Daimler Cyclist Benchmark <ref type="bibr" target="#b15">[16]</ref> is a dataset for cyclist detection, which contains 4 subsets: train, validation, test and "NonVRU" set. The train set contains 9741 images with annotations as "cyclist". There are 1019 images in validation set and 2914 images in test set, which contain the annotations as "pedestrian", "cyclist", "motorcyclist", "tricyclist", "wheelchairuser", and "mopedrider". The "NonVRU" set contains 1000 images with background image only (no pedestrian).</p><p>To explore the generalization ability of PS-GAN, we perform the cross-dataset test. The goal of this experiment is to simulate the situation that applying our GAN model on the new unannotated video or video with limited supervision. It is useful to improve the performance when the training set contains the similar scenes in testing set. If the PS-GAN has great generalization ability in the new data, it could be very helpful when we face a new task with limited annotated information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original Image</head><p>Noise Image Generated Image (a) Synthesize pedestrians in background images.</p><p>Original Image Synthetic Image (b) Synthetic images for data augmentation <ref type="figure">Fig. 7</ref>: The results of generating pedestrians on Tsinghua-Daimler Cyclist Benchmark. All those images are generated by PS-GAN pretrained on Cityscapes without any data from Tsinghua-Daimler Cyclist Benchmark.</p><p>Firstly, we directly apply the PS-GAN model pretrained on Cityscapes (using 1826 images) to generate pedestrians on the empty background images from "NonVRU" set. Since some images in "NonVRU" set are not suitable (e.g. no road, too dark or light, etc.) to synthesize pedestrians, we get 650 images after removing those images. Similar to what we did in Cityscapes, we cropped 256?256 patches from those images and put noise boxes to synthesize pedestrians.</p><p>The generated examples are shown in <ref type="figure">Figure 7</ref>(a). Without adding any data from Tsinghua-Daimler Cyclist Benchmark, PS-GAN can still generate high-quality and realistic images on this dataset. Note that there are many differences between these two datasets, such as the background, lighting conditions and pedestrian styles. We can expect the generated image quality has a slight drop compared to the results in Cityscapes. Specifically, the region around pedestrian does not match the background well, and the body of pedestrian loses some details in some cases. Nevertheless, the generated images still look natural with satisfactory qualities.</p><p>Also, we conduct the comparison between using the real data on Cityscapes and adding synthetic data to train the Faster R-CNN. For the test, all the 2914 test images of Tsinghua-Daimler Cyclist Benchmark with the bounding boxes annotated as "pedestrian" and "cyclist" are directly used. The results are presented in <ref type="table" target="#tab_2">Table 2</ref>, where adding the 650 synthetic images gain a huge improvement (2.64%) than the baseline with the real data on Cityscapes. Different from the setting in Cityscapes, we add new background images when adding the synthetic pedestrians. To illustrate the effect of adding new images, we also compare with the detector trained on the real data on Cityscapes and the 650 empty background images. Adding background images can bring a slight improvement, about 0.29%. In this case, the result with image synthesized from P ix2P ix GAN can slight improve the AP rate but the improvement is much poor compared with the PS-GAN by 2.3%.</p><p>Meanwhile, we execute the detection experiments with different amounts of training data for the Faster-RCNN. We report the results of using 300 and 1000 real images, and also adding synthetic images and background images separately on <ref type="table" target="#tab_2">Table 2</ref>. Also, we use the GAN models retrained on 300 and 1000 images as we did in section 4.1. The performances get improved in both cases. Adding background images can bring limited improvement, 0.91% and 0.6%, respectively. Adding some synthetic data here shows significant help here, boosting the performance by 2.62% and 2.52%, respectively. Especially when adding 650 synthetic images into 1000 real images, the AP rate get better from 42.42% to 44.94% which even significantly outperforms the AP rate 43.77% of using 1826 real images to train the detector. Moreover, in all cases, we compare the results with image synthesized from P ix2P ix GAN. It can only achieve similar AP rate as the baseline detectors and has not done better than PS-GAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation with Pretrained Detectors</head><p>Finally, we use the detectors pretrained on real images to detect the synthetic samples (using 500 samples) and report the AP rate. Two Faster RCNN detectors <ref type="bibr" target="#b4">[5]</ref> trained on Pascal VOC and Cityscapes (300 samples) are utilized. We also compare PS-GAN with P ix2P ix GAN on this task. The results are list in <ref type="table" target="#tab_3">Table  3</ref>. We can see that the AP rate of the detectors on the samples generated with PS-GAN are much higher than that with P ix2P ix GAN, showing the generation power of PS-GAN in another prospective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose PS-GAN to synthesize pedestrian within the certain bounding boxes in real scenes. The experimental results show that our model can generate high  quality pedestrian images, and the synthetic images can effectively improve the ability of the CNN based detectors. In the cross dataset test, our PS-GAN model trained on Cityscapes can do pretty good generation in the other new dataset as well as help boost the detection, which demonstrates the ability of generalization and transferring knowledge. This is helpful when we face a new task with limited annotated information. Currently PS-GAN pedestrians vary in a mild range of scales (can not be too small or large), which restricts it to generate more diverse and natural data. Making PS-GAN to handle the extreme case is challenging. Besides that, how to control PS-GAN to generate pedestrians in reasonable locations (e.g., pedestrian should not be on the tree or in the water) is also interesting.</p><p>In the meantime, applying PS-GAN to other detection tasks is definitely one of our future works. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Supplementary Experiments</head><p>In this supplemental, We provide more results to help understand our proposed approach described in the paper. Firstly, we show more generation results from PS-GAN on Cityscapges and Tsinghua-Daimler Cyclist Benchmark. Secondly, we investigate the effects of data augmentation used to boost the detection performance on both dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Generated images comparison</head><p>We show more results of the work in Section 4.1.1 in <ref type="figure">Figure 8</ref>. PS-GAN still gets the best performance. We also compare the synthetic images of all models on Tsinghua-Daimler Cyclist. Here we further evaluated different approaches in the cross-dataset setting. As shown in <ref type="figure">Figure 9</ref>, PS-GAN, even trained on Cityscapes, can generate the best quality of images and fit the background well. on the contrary, the synthesized pedestrians using P ix2P ix GAN are not good. Some of them are very blur. We also see the results of other variant ions of PS-GAN and have similar observation of them as in Cityscapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original Image</head><p>Noise Image Pix2Pix A B C P S -G A N <ref type="figure">Fig. 8</ref>: More results of synthesizing pedestrians on cityscapes with different models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Visualization of detection results</head><p>To future investigate how the synthesized can help improving the performance of Faster R-CNN, we put some visualized detection results on for bot Cityscapes Original Image Noise Image Pix2Pix A B C P S -G A N <ref type="figure">Fig. 9</ref>: Results of synthesizing pedestrians on Tsinghua-Daimler Cyclist Benchmark with different models.</p><p>and Tsinghua-Daimler Cyclist. The experimental details are described in Section 4.1.2 and 4.2. We first show some examples of Cityscapes in <ref type="figure" target="#fig_5">Figures 10 and 11</ref>. For the 300 real image setting, it is clear that the data augmentation step can increase true positives while reduce some false positives. Even for the 1000 real setting, it can also gain more real detections than the original model.</p><p>For Tsinghua-Daimler Cyclist, the results are demonstrated in <ref type="figure" target="#fig_0">Figures 12 and  13</ref>. In both settings, adding the generated data is able to boost the performance of detector by getting more real detections.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>The discriminator D b is applied to classify between real and synthesized pair to learn the background context in the noise box. The discriminator D p learns to classify the real and synthesized pedestrian with the noise box. We adopt a 3-level SPP layer (a = 1, b = 2, c = 4, totally 21 bins) before the final feature representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>We compare PS-GAN with four different models. The baseline is P ix2P ix GAN<ref type="bibr" target="#b10">[11]</ref> in the third column, which only contain one discriminator to classify the real and synthesized pair. The following columns (A-C) show the ablation test of the proposed PS-GAN. Model A: the main structure is same as PS-GAN but the SPP layer is removed; Model B: The difference with our final model is that this model adopt the LSGAN loss on both the two adversarial learning G ? D b and G ? D p ; Model C: The regular GAN loss is kept in both two adversarial learning procedures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Comparison of generated and real pedestrians.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>The examples of synthesizing pedestrians in the original scenes. It shows the original images in the left and corresponding synthesized images in the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 10 :</head><label>10</label><figDesc>Visualization of the detection result on Cityscapes. The left column contains the results of the Faster R-CNN trained on 300 real images, while the right are the results for adding 1000 synthetic pedestrians from PS-GAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 11 :</head><label>11</label><figDesc>Visualization of the detection result on Cityscapes. The left column contains the results of the Faster R-CNN trained on 1000 real images, while the right are the results for adding 4000 synthetic pedestrians from PS-GAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 12 :</head><label>12</label><figDesc>Visualization of the detection result on Tsinghua-Daimler Cyclist Benchmark. The left column contains the results of the Faster R-CNN trained on 300 real images from Cityscapes, while the right are the results for adding 300 synthetic images from PS-GAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 13 :</head><label>13</label><figDesc>Visualization of the detection result on Tsinghua-Daimler Cyclist Benchmark. The left column contains the results of the Faster R-CNN trained on 1000 real images from Cityscapes, while the right are the results for adding 650 synthetic images from PS-GAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The performance comparison of using different settings to train the Faster R-CNN, including adding different amounts of synthetic data from P ix2P ix GAN and PS-GAN, separately.</figDesc><table><row><cell>Data</cell><cell cols="2">Pix2Pix GAN PS-GAN</cell></row><row><cell>1826 real images (7729 labels)</cell><cell>60.11%</cell><cell></cell></row><row><cell>+ 3000 synthetic pedestrians</cell><cell>59.95%</cell><cell>61.02%</cell></row><row><cell>+ 5000 synthetic pedestrians</cell><cell>60.23%</cell><cell>61.79%</cell></row><row><cell>+ 8000 synthetic pedestrians</cell><cell>58.41%</cell><cell>61.59%</cell></row><row><cell>Pascal VOC 2007</cell><cell>34.13%</cell><cell></cell></row><row><cell>Pascal VOC 2007 &amp; 2012</cell><cell>36.85%</cell><cell></cell></row><row><cell>300 real images (1173 labels)</cell><cell>47.08%</cell><cell></cell></row><row><cell>+ 500 synthetic pedestrians</cell><cell>46.97%</cell><cell>47.36%</cell></row><row><cell>+ 1000 synthetic pedestrians</cell><cell>46.71%</cell><cell>48.79%</cell></row><row><cell>+ 2000 synthetic pedestrians</cell><cell>46.12%</cell><cell>48.11%</cell></row><row><cell>1000 real images (4368 labels)</cell><cell>52.72%</cell><cell></cell></row><row><cell>+ 2000 synthetic pedestrians</cell><cell>52.07%</cell><cell>54.41%</cell></row><row><cell>+ 4000 synthetic pedestrians</cell><cell>51.68%</cell><cell>56.19%</cell></row><row><cell>+ 5000 synthetic pedestrians</cell><cell>51.24%</cell><cell>55.96%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The comparison of adding different amounts of synthetic data to train the Faster R-CNN. The number of label means the number of real pedestrians in the real images or generated pedestrians in the synthetic images.</figDesc><table><row><cell>Data</cell><cell cols="2">Pix2Pix GAN PS-GAN</cell></row><row><cell>1826 real images from Cityscapes (7729 labels)</cell><cell>43.77%</cell><cell></cell></row><row><cell>+ 650 background images (no pedestrian)</cell><cell>44.06%</cell><cell></cell></row><row><cell>+ 650 synthetic images (4500 pedestrians)</cell><cell>44.11%</cell><cell>46.41%</cell></row><row><cell>Pascal VOC 2007</cell><cell>23.24%</cell><cell></cell></row><row><cell>Pascal VOC 2007 &amp; 2012</cell><cell>26.50%</cell><cell></cell></row><row><cell>300 real images from Cityscapes (1173 labels)</cell><cell>32.15%</cell><cell></cell></row><row><cell>+ 300 background images (no pedestrian)</cell><cell>33.06%</cell><cell></cell></row><row><cell>+ 300 synthetic images (2000 pedestrians)</cell><cell>32.64%</cell><cell>34.77%</cell></row><row><cell>1000 real images from Cityscapes (4368 labels)</cell><cell>42.42%</cell><cell></cell></row><row><cell>+ 650 background images (no pedestrian)</cell><cell>43.02%</cell><cell></cell></row><row><cell>+ 650 synthetic images (4500 pedestrians)</cell><cell>42.70%</cell><cell>44.94%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>The AP rate comparison of different pretrained detectors. The two sets, Cityscapes and Tsinghua-Daimler Cyclist, are used as the background.</figDesc><table><row><cell>Generator</cell><cell cols="2">Pretrained Detector Cityscapes Tsinghua Background</cell></row><row><cell>PS-GAN</cell><cell>Pascal VOC 84.55%</cell><cell>88.85%</cell></row><row><cell></cell><cell>Cityscapes 90.11%</cell><cell>90.46%</cell></row><row><cell cols="2">P ix2P ix GAN Pascal VOC 52.46%</cell><cell>69.42%</cell></row><row><cell></cell><cell>Cityscapes 58.82%</cell><cell>71.68%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>38. Everingham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A.: The PAS-CAL Visual Object Classes Challenge 2007 (VOC2007) Results. http://www.pascalnetwork.org/challenges/VOC/voc2007/workshop/index.html</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://pytorch.org/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Monocular pedestrian detection: Survey and experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2179" to="2195" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Pedestrian detection: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CVPR, IEEE</publisher>
			<biblScope unit="page" from="304" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="743" to="761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">How far are we from solving pedestrian detection?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1259" to="1267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08242</idno>
		<title level="m">Yolo9000: better, faster, stronger</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Lcrowdv: Generating labeled videos for simulation-based crowd behavior learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="709" to="727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning scene-specific pedestrian detectors without real data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hattori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Naresh Boddeti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3819" to="3827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">STD-PD: Generating synthetic training data for pedestrian detection in unannotated videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09100</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07004</idno>
		<title level="m">Image-to-image translation with conditional adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04802</idno>
		<title level="m">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Marius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Markus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Uwe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stefan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bernt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A new benchmark for vision-based cyclist detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Flohr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="1028" to="1033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint deep learning for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2056" to="2063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A unified multi-scale deep convolutional neural network for fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="354" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Is faster r-cnn doing well for pedestrian detection?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="443" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Generative adversarial networks as variational training of energy based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/1611.01799</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xudong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1486" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Wasserstein gan. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mmd gan: Towards deeper understanding of moment matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>P?czos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2200" to="2210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00028</idno>
		<title level="m">Improved training of wasserstein gans</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semantic image inpainting with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5485" to="5493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06430</idno>
		<title level="m">Semi-supervised learning with context-conditional generative adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Unsupervised cross-domain image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02200</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00848</idno>
		<title level="m">Unsupervised image-to-image translation networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10593</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pose guided person image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="405" to="415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="743" to="761" />
			<date type="published" when="2012-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07717</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
