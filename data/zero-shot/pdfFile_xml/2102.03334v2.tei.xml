<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjae</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bokyung</forename><surname>Son</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ildoo</forename><surname>Kim</surname></persName>
						</author>
						<title level="a" type="main">ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Vision-and-Language Pre-training (VLP) has improved performance on various joint vision-andlanguage downstream tasks. Current approaches to VLP heavily rely on image feature extraction processes, most of which involve region supervision (e.g., object detection) and the convolutional architecture (e.g., ResNet). Although disregarded in the literature, we find it problematic in terms of both (1) efficiency/speed, that simply extracting input features requires much more computation than the multimodal interaction steps; and (2) expressive power, as it is upper bounded to the expressive power of the visual embedder and its predefined visual vocabulary. In this paper, we present a minimal VLP model, Vision-and-Language Transformer (ViLT), monolithic in the sense that the processing of visual inputs is drastically simplified to just the same convolution-free manner that we process textual inputs. We show that ViLT is up to tens of times faster than previous VLP models, yet with competitive or better downstream task performance. Our code and pre-trained weights are available at https://github.com/dandelin/vilt.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The pre-train-and-fine-tune scheme has been expanded to a joint domain of vision and language, giving birth to the category of Vision-and-Language Pre-training (VLP) models <ref type="bibr" target="#b9">Chen et al., 2019;</ref><ref type="bibr" target="#b47">Su et al., 2019;</ref><ref type="bibr" target="#b31">Li et al., 2019;</ref><ref type="bibr" target="#b49">Tan &amp; Bansal, 2019;</ref><ref type="bibr" target="#b30">Li et al., 2020a;</ref><ref type="bibr" target="#b36">Lu et al., 2020;</ref><ref type="bibr" target="#b10">Cho et al., 2020;</ref><ref type="bibr" target="#b41">Qi et al., 2020;</ref><ref type="bibr" target="#b59">Zhou et al., 2020;</ref>  Region Feature <ref type="bibr">(ViLBERT, UNITER, ...)</ref> Grid Feature <ref type="bibr">(Pixel-BERT)</ref> Patch Projection (Ours) Image Image Image Text <ref type="figure">Figure 1</ref>. Visual comparison of conventional VLP architectures and our proposed ViLT. We have entirely removed convolutional neural networks from the VLP pipeline without hurting performance on downstream tasks. ViLT is the first VLP model of which the modal-specific components require less computation than the transformer component for multimodal interactions. <ref type="bibr" target="#b32">Li et al., 2020b;</ref><ref type="bibr" target="#b16">Gan et al., 2020;</ref><ref type="bibr" target="#b54">Yu et al., 2020;</ref><ref type="bibr" target="#b58">Zhang et al., 2021)</ref>. These models are pre-trained with image text matching and masked language modeling objectives 1 on images and their aligned descriptions, and are fine-tuned on vision-and-language downstream tasks where the inputs involve two modalities.</p><p>To be fed into VLP models, image pixels need to be initially embedded in a dense form alongside language tokens. Since the seminal work of <ref type="bibr" target="#b27">Krizhevsky et al. (2012)</ref>, deep convolutional networks have been regarded as essential for this visual embedding step. Most VLP models employ an object detector pre-trained on the Visual Genome dataset <ref type="bibr" target="#b26">(Krishna et al., 2017)</ref> annotated with 1,600 object classes and 400 attribute classes as in <ref type="bibr" target="#b0">Anderson et al. (2018)</ref>  BERT <ref type="bibr" target="#b22">(Huang et al., 2020)</ref> is one exception of this trend, as it uses ResNet variants <ref type="bibr" target="#b52">Xie et al., 2017)</ref> pre-trained on ImageNet classification <ref type="bibr" target="#b44">(Russakovsky et al., 2015)</ref> embedding pixels in place of object detection modules.</p><p>To this date, most VLP studies have focused on improving performance by increasing the power of visual embedders. The shortcomings of having a heavy visual embedder are often disregarded in academic experiments because region features are commonly cached in advance at training time to ease the burden of feature extraction. However, the limitations are still evident in real-world applications as the queries in the wild have to undergo a slow extraction process.</p><p>To this end, we shift our attention to the lightweight and fast embedding of visual inputs. Recent work <ref type="bibr" target="#b14">(Dosovitskiy et al., 2020;</ref><ref type="bibr" target="#b50">Touvron et al., 2020)</ref> demonstrated that using a simple linear projection of a patch is effective enough to embed pixels before feeding them into transformers. Whereas being the solid mainstream for text <ref type="bibr" target="#b13">(Devlin et al., 2019)</ref>, it is only recently that transformers <ref type="bibr" target="#b51">(Vaswani et al., 2017)</ref> are used for images as well. We presume that the transformer moduleused for modality interaction in VLP models-can also manage to process visual features in place of a convolutional visual embedder, just as it processes textual features.</p><p>This paper proposes the Vision-and-Language Transformer (ViLT) that handles two modalities in a single unified manner. It mainly differs from previous VLP models in its shallow, convolution-free embedding of pixel-level inputs.</p><p>Removing deep embedders solely dedicated to visual inputs significantly cuts down the model size and running time by design. <ref type="figure">Figure 1</ref> shows that our parameter-efficient model is tens of times faster than VLP models with region features and at least four times faster than those with grid features while exhibiting similar or even better performance on vision-and-language downstream tasks.</p><p>Our key contributions can be summarized as follows:</p><p>? ViLT is the simplest architecture by far for a visionand-language model as it commissions the transformer module to extract and process visual features in place of a separate deep visual embedder. This design inherently leads to significant runtime and parameter efficiency.</p><p>? For the first time, we achieve competent performance on vision-and-language tasks without using region features or deep convolutional visual embedders in general.</p><p>? Also, for the first time, we empirically show that whole word masking and image augmentations that were unprecedented in VLP training schemes further drive downstream performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Taxonomy of Vision-and-Language Models</head><p>We propose a taxonomy of vision-and-language models based on two points: (1) whether the two modalities have an even level of expressiveness in terms of dedicated parameters and/or computation; and (2) whether the two modalities interact in a deep network. A combination of these points leads to four archetypes in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>The visual semantic embedding (VSE) models such as VSE++ <ref type="bibr" target="#b15">(Faghri et al., 2017)</ref> and SCAN <ref type="bibr" target="#b29">(Lee et al., 2018)</ref> belong to <ref type="figure" target="#fig_0">Figure 2a</ref>. They use separate embedders for image and text, with the former being much heavier. Then, they represent the similarity of the embedded features from the two modalities with simple dot products or shallow attention layers.</p><p>CLIP <ref type="bibr" target="#b42">(Radford et al., 2021)</ref> belongs to <ref type="figure" target="#fig_0">Figure 2b</ref> as it uses separate but equally expensive transformer embedders for each modality. Interaction between the pooled image vector and text vector is still shallow (dot product). Despite CLIP's remarkable zero-shot performance on image-to-text retrieval, we could not observe the same level of performance on other vision-and-language downstream tasks. For instance, fine-tuning the MLP head on NLVR2 <ref type="bibr" target="#b48">(Suhr et al., 2018)</ref> with the dot product of pooled visual and textual vectors from CLIP as the multimodal representation gives a low dev accuracy of 50.99 ? 0.38 (ran with three different seeds); as chance level accuracy is 0.5, we conclude that the representations are incapable of learning this task. It also matches the findings of <ref type="bibr" target="#b48">Suhr et al. (2018)</ref> that all models with simply fused multimodal representation failed to learn NLVR2.</p><p>This result backs up our speculation that simple fusion of outputs even from high-performing unimodal embedders may not be sufficient to learn complex vision-and-language tasks, bolstering the need for a more rigorous inter-modal interaction scheme.</p><p>Unlike models with shallow interaction, the more recent VLP models that fall under <ref type="figure" target="#fig_0">Figure 2c</ref> use a deep transformer to model the interaction of image and text features. Aside from the interaction module, however, convolutional networks are still involved in extracting and embedding image features, which accounts for most of the computation as depicted in <ref type="figure">Figure 1</ref>. Modulation-based vision-and-language models <ref type="bibr" target="#b39">(Perez et al., 2018;</ref><ref type="bibr" target="#b37">Nguyen et al., 2020)</ref> also fall under <ref type="figure" target="#fig_0">Figure 2c</ref>, with their visual CNN stems corresponding to visual embedder, RNNs producing the modulation parameters to textual embedder, and modulated CNNs to modality interaction.</p><p>Our proposed ViLT is the first model of type <ref type="figure" target="#fig_0">Figure 2d</ref> where the embedding layers of raw pixels are shallow and computationally light as of text tokens. This architecture thereby concentrates most of the computation on modeling modality interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Modality Interaction Schema</head><p>At the very core of contemporary VLP models lie transformers. They get visual and textual embedding sequences as input, model inter-modal and optionally intra-modal interactions throughout layers, then output a contextualized feature sequence. <ref type="bibr" target="#b3">Bugliarello et al. (2020)</ref> classifies interaction schema into two categories: (1) single-stream approaches (e.g., Visual-BERT , UNITER <ref type="bibr" target="#b9">(Chen et al., 2019)</ref>) where layers collectively operate on a concatenation of image and text inputs; and (2) dual-stream approaches (e.g., ViLBERT , LXMERT <ref type="bibr" target="#b49">(Tan &amp; Bansal, 2019)</ref>) where the two modalities are not concatenated at the input level. We follow the single-stream approach for our interaction transformer module because the dual-stream approach introduces additional parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Visual Embedding Schema</head><p>Whereas all performant VLP models share the same textual embedder-tokenizer from pre-trained BERT, word and position embeddings resembling those of BERT-they differ on visual embedders. Still, in most (if not all) cases, visual embedding is the bottleneck of existing VLP models. We focus on cutting corners on this step by introducing patch projection instead of using region or grid features for which heavy extraction modules are used.</p><p>Region Feature. VLP models dominantly utilize region features, also known as bottom-up features <ref type="bibr" target="#b0">(Anderson et al., 2018)</ref>. They are obtained from an off-the-shelf object detector like Faster R-CNN .</p><p>The general pipeline of generating region features is as follows. First, a region proposal network (RPN) proposes regions of interest (RoI) based on the grid features pooled from the CNN backbone. Non-maximum suppression (NMS) then reduces the number of RoIs to a few thousand. After being pooled by operations such as RoI Align , the RoIs go through RoI heads and become region features. NMS is again applied to every class, finally reducing the number of features under a hundred.</p><p>The above process involves several factors that affect the performance and runtime: the backbone, the style of NMS, the RoI heads. Previous works were lenient with controlling these factors, making varying choices from each other as listed in <ref type="table">Table 7</ref>. 2</p><p>? Backbone: ResNet-101 <ref type="bibr" target="#b49">Tan &amp; Bansal, 2019;</ref><ref type="bibr" target="#b47">Su et al., 2019)</ref> and ResNext-152 <ref type="bibr" target="#b58">Zhang et al., 2021)</ref> are two commonly used backbones.</p><p>? NMS: NMS is typically done in a per-class fashion. Applying NMS to each and every class becomes a major runtime bottleneck with a large number of classes, e.g. 1.6K in the VG dataset <ref type="bibr" target="#b23">(Jiang et al., 2020)</ref>. Classagnostic NMS was recently introduced to tackle this issue <ref type="bibr" target="#b58">(Zhang et al., 2021)</ref>.</p><p>? RoI head: C4 heads were initially used <ref type="bibr" target="#b0">(Anderson et al., 2018)</ref>. FPN-MLP heads were introduced later <ref type="bibr" target="#b24">(Jiang et al., 2018)</ref>. As heads operate for each and every RoI, they pose a substantial runtime burden.</p><p>However lightweight, object detectors are less likely to be faster than the backbone or a single-layer convolution.</p><p>Freezing the visual backbone and caching the region features in advance only helps at training time and not during  inference, not to mention that it could hold performance back.</p><formula xml:id="formula_0">Pooler MLP FC OT z D | t z D | v z D | t z D | v Word Patch Alignment</formula><p>Grid Feature. Besides detector heads, the output feature grid of convolutional neural networks such as ResNets can also be used as visual features for vision-and-language pretraining. Direct use of grid features was first proposed by VQA-specific models <ref type="bibr" target="#b23">(Jiang et al., 2020;</ref><ref type="bibr" target="#b37">Nguyen et al., 2020)</ref>, mainly to avoid using severely slow region selection operations.</p><p>X-LXMERT <ref type="bibr" target="#b10">(Cho et al., 2020)</ref> revisited grid features by fixing the region proposals to grids instead of those from the region proposal networks. However, their caching of features excluded further tuning of the backbone.</p><p>Pixel-BERT is the only VLP model that replaces the VGpre-trained object detector with a ResNet variant backbone pre-trained with ImageNet classification. Unlike frozen detectors in region-feature-based VLP models, the backbone of Pixel-BERT is tuned during vision-and-language pretraining. The downstream performance of Pixel-BERT with ResNet-50 falls below region-feature-based VLP models, but it matches that of other competitors with the use of a much heavier ResNeXt-152.</p><p>We claim that grid features are not the go-to option, however, since deep CNNs are still expensive that they account for a large portion of the whole computation as in <ref type="figure">Figure 1</ref>.</p><p>Patch Projection. To minimize overhead, we adopt the simplest visual embedding scheme: linear projection that operates on image patches. The patch projection embedding was introduced by ViT <ref type="bibr" target="#b14">(Dosovitskiy et al., 2020)</ref> for image classification tasks. Patch projection drastically simplifies the visual embedding step to the level of textual embedding, which also consists of simple projection (lookup) operations.</p><p>We use a 32 ? 32 patch projection which only requires 2.4M parameters. This is in sharp contrast to complex ResNe(X)t backbones 3 and detection components. Its running time is also ignorable as shown in <ref type="figure">Figure 1</ref>. We make a detailed runtime analysis in Section 4.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Vision-and-Language Transformer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model Overview</head><p>ViLT has a succinct architecture as a VLP model with a minimal visual embedding pipeline and following the singlestream approach.</p><p>We deviate from the literature that we initialize the interaction transformer weights from pre-trained ViT instead of BERT. Such initialization exploits the power of the interaction layers to process visual features while lacking a separate deep visual embedder. 4</p><formula xml:id="formula_1">t = [t class ; t 1 T ; ? ? ? ; t L T ] + T pos (1) v = [v class ; v 1 V ; ? ? ? ; v N V ] + V pos (2) z 0 = [t + t type ;v + v type ]<label>(3)</label></formula><formula xml:id="formula_2">z d = MSA(LN(z d?1 )) + z d?1 , d = 1 . . . D (4) z d = MLP(LN(? d )) +? d , d = 1 . . . D (5) p = tanh(z D 0 W pool )<label>(6)</label></formula><p>ViT consists of stacked blocks that include a multiheaded self-attention (MSA) layer and an MLP layer. The position of layer normalization (LN) in ViT is the only difference from BERT: LN comes after MSA and MLP in BERT ("post-norm") and before in ViT ("pre-norm"). The input text t ? R L?|V | is embedded tot ? R L?H with a word embedding matrix T ? R |V |?H and a position embedding matrix T pos ? R (L+1)?H .</p><p>The input image I ? R C?H?W is sliced into patches and flattened to v ? R N ?(P 2 ?C) where (P, P ) is the patch resolution and N = HW/P 2 . Followed by linear projection V ? R (P 2 ?C)?H and position embedding</p><formula xml:id="formula_3">V pos ? R (N +1)?H , v is embedded intov ? R N ?H .</formula><p>The text and image embeddings are summed with their corresponding modal-type embedding vectors t type , v type ? R H , then are concatenated into a combined sequence z 0 . The contextualized vector z is iteratively updated through Ddepth transformer layers up until the final contextualized sequence z D . p is a pooled representation of the whole multimodal input, and is obtained by applying linear projection W pool ? R H?H and hyperbolic tangent upon the first index of sequence z D .</p><p>For all experiments, we use weights from ViT-B/32 pretrained on ImageNet, hence the name ViLT-B/32. 5 Hidden size H is 768, layer depth D is 12, patch size P is 32, MLP size is 3,072, and the number of attention heads is 12.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pre-training Objectives</head><p>We train ViLT with two objectives commonly used to train VLP models: image text matching (ITM) and masked language modeling (MLM).</p><p>Image Text Matching. We randomly replace the aligned image with a different image with the probability of 0.5. A single linear layer ITM head projects the pooled output feature p to logits over binary class, and we compute negative log-likelihood loss as our ITM loss.</p><p>Plus, inspired by the word region alignment objective in <ref type="bibr" target="#b9">Chen et al. (2019)</ref>, we design word patch alignment (WPA) that computes the alignment score between two subsets of z D : z D | t (textual subset) and z D | v (visual subset), using the inexact proximal point method for optimal transports (IPOT) . We set the hyperparameters of IPOT following Chen et al. (2019) (? = 0.5, N = 50), and add the approximate wasserstein distance multiplied by 0.1 to the ITM loss.</p><p>Masked Language Modeling. This objective is to predict the ground truth labels of masked text tokens t masked from its contextualized vector z D masked | t . Following the heuristics of <ref type="bibr" target="#b13">Devlin et al. (2019)</ref>, we randomly mask t with the probability of 0.15.</p><p>We use a two-layer MLP MLM head that inputs z D masked | t and outputs logits over vocabulary, just as the MLM objective of BERT. The MLM loss is then computed as the negative log-likelihood loss for the masked tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Whole Word Masking</head><p>Whole word masking is a masking technique that masks all consecutive subword tokens that compose a whole word. It is shown to be effective on downstream tasks when applied to original and Chinese BERT .</p><p>We hypothesize that whole word masking is particularly crucial for VLP in order to make full use of information from the other modality. For example, the word "giraffe" is tokenized into three wordpiece tokens ["gi", "##raf", "##fe"] with the pre-trained bert-base-uncased tokenizer. If not all tokens are masked, say, ["gi", "[MASK]", "##fe"], the model may solely rely on the nearby two language tokens ["gi", "##fe"] to predict the masked "##raf" rather than using the information from the image.</p><p>We mask whole words with a mask probability of 0.15 during pre-training. We discuss its impact in Section 4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Image Augmentation</head><p>Image augmentation reportedly improves the generalization power of vision models <ref type="bibr" target="#b46">(Shorten &amp; Khoshgoftaar, 2019)</ref>. DeiT <ref type="bibr" target="#b50">(Touvron et al., 2020)</ref> that builds on ViT experimented with various augmentation techniques <ref type="bibr" target="#b57">(Zhang et al., 2017;</ref><ref type="bibr" target="#b56">Yun et al., 2019;</ref><ref type="bibr" target="#b2">Berman et al., 2019;</ref><ref type="bibr" target="#b21">Hoffer et al., 2020;</ref><ref type="bibr" target="#b11">Cubuk et al., 2020)</ref>, and found them beneficial for ViT training. However, the effects of image augmentation have not been explored within VLP models. Caching visual features restrains region-feature-based VLP models from using image augmentation. Notwithstanding its applicability, neither did Pixel-BERT study its effects.</p><p>To this end, we apply RandAugment <ref type="bibr" target="#b11">(Cubuk et al., 2020)</ref> during fine-tuning. We use all the original policies except two: color inversion, because texts often contain color information as well, and cutout, as it may clear out small but important objects dispersed throughout the whole image. We use N = 2, M = 9 as the hyperparameters. We discuss its impact in Section 4.5 and Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Overview</head><p>We use four datasets for pre-training: Microsoft COCO (MSCOCO) <ref type="bibr" target="#b33">(Lin et al., 2014)</ref>, Visual Genome (VG) <ref type="bibr" target="#b26">(Krishna et al., 2017)</ref>, SBU Captions (SBU) <ref type="bibr" target="#b38">(Ordonez et al., 2011)</ref>, and Google Conceptual Captions (GCC) (Sharma We evaluate ViLT on two widely explored types of visionand-language downstream tasks: for classification, we use VQAv2 <ref type="bibr" target="#b17">(Goyal et al., 2017)</ref> and NLVR2 <ref type="bibr" target="#b48">(Suhr et al., 2018)</ref>, and for retrieval, we use MSCOCO and Flickr30K (F30K) <ref type="bibr" target="#b40">(Plummer et al., 2015)</ref> re-splited by <ref type="bibr" target="#b25">Karpathy &amp; Fei-Fei (2015)</ref>. For the classification tasks, we fine-tune three times with different initialization seeds for the head and data ordering and report the mean scores. We report the standard deviation in <ref type="table" target="#tab_6">Table 5</ref> along with ablation studies. For the retrieval tasks, we only fine-tune once.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>For all experiments, we use AdamW optimizer <ref type="bibr" target="#b34">(Loshchilov &amp; Hutter, 2018)</ref> with base learning rate of 10 ?4 and weight decay of 10 ?2 . The learning rate was warmed up for 10% of the total training steps and was decayed linearly to zero for the rest of the training. Note that downstream performance may be further improved if we customize the hyperparameters to each task.</p><p>We resize the shorter edge of input images to 384 and limit the longer edge to under 640 while preserving the aspect ratio. This resizing scheme is also used during object detection in other VLP models, but with a larger size of the shorter edge (800). Patch projection of ViLT-B/32 yields 12 ? 20 = 240 patches for an image with a resolution of 384 ? 640. As this is a rarely reached upper limit, we sample 200 patches at maximum during pre-training. We interpolate V pos of ViT-B/32 to fit the size of each image and pad the patches for batch training. Note that the resulting image resolution is four times smaller than 800 ? 1,333, which is the size that all other VLP models use for inputs to their visual embedders.</p><p>We use the bert-base-uncased tokenizer to tokenize text inputs. Instead of fine-tuning from pre-trained BERT, we learn the textual embedding-related parameters t class , T , and T pos from scratch. Although beneficial prima facie, employing a pre-trained text-only BERT does not guarantee performance gain for vision and language downstream tasks. Counterevidence has already been reported by Tan &amp; Bansal (2019), where initializing with pre-trained BERT parameters led to weaker performance than pre-training from scratch.</p><p>We pre-train ViLT-B/32 for 100K or 200K steps on 64 NVIDIA V100 GPUs with a batch size of 4,096. For all downstream tasks, we train for ten epochs with a batch size of 256 for VQAv2/retrieval tasks and 128 for NLVR2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Classification Tasks</head><p>We evaluate ViLT-B/32 on two commonly used datasets: VQAv2 and NLVR2. We use a two-layer MLP of hidden size 1,536 as the fine-tuned downstream head.</p><p>Visual Question Answering. The VQAv2 task asks for answers given pairs of an image and a question in natural language. The annotated answers are originally in free-form natural language, but it is a common practice to convert the task to a classification task with 3,129 answer classes. Following this practice, we fine-tune ViLT-B/32 on the VQAv2 train and validation sets while reserving 1,000 validation images and their related questions for internal validation.</p><p>We report the test-dev score results 6 from the submission to the evaluation server. ViLT falls short of VQA score compared to other VLP models with a heavy visual embedder. We suspect a detached object representation generated by the object detector eases the training of VQA since questions in VQA typically ask about objects. Natural Language for Visual Reasoning. The NLVR2 task is a binary classification task given triplets of two images and a question in natural language. As there are two input images unlike the pre-training setup, multiple strategies exist 7 . Following OSCAR <ref type="bibr" target="#b32">(Li et al., 2020b)</ref> and VinVL <ref type="bibr" target="#b58">(Zhang et al., 2021)</ref>, we use the pair method. Here, the triplet input is reformulated into two pairs (question, im-age1) and (question, image2), and each pair goes through the ViLT. The head takes the concatenation of two pooled representations (p) as input and outputs the binary prediction. <ref type="table" target="#tab_4">Table 2</ref> shows the results. ViLT-B/32 maintains competitive performance on both datasets considering its remarkable inference speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Retrieval Tasks</head><p>We fine-tune ViLT-B/32 on the <ref type="bibr" target="#b25">Karpathy &amp; Fei-Fei (2015)</ref> split of MSCOCO and F30K. For image-to-text and text-toimage retrieval, we measure both zero-shot and fine-tuned performance 8 . We initialize the similarity score head from 7 UNITER proposed three downstream head setups: pair, triplet, and pair-biattn. 8 R@K corresponds to whether the ground truth is included among top K results from the validation set. the pre-trained ITM head, particularly the part that computes the true-pair logits. We sample 15 random texts as negative samples and tune the model with cross-entropy loss that maximizes the scores on positive pairs. We report the zero shot retrieval results in <ref type="table" target="#tab_5">Table 3</ref> and finetuned results in <ref type="table">Table 4</ref>. At zero-shot retrieval, ViLT-B/32 performs better in general than ImageBERT despite Image-BERT's pre-training on a larger (14M) dataset. At fine-tuned retrieval, recalls for ViLT-B/32 are higher by a large margin than the second fastest model (Pixel-BERT-R50).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Study</head><p>In <ref type="table" target="#tab_6">Table 5</ref>, we perform various ablations. More training steps, whole word masking, and image augmentation come to be beneficial, whereas an additional training objective does not help.</p><p>It has been reported that the number of training iterations affects the performance of self-supervised models <ref type="bibr" target="#b13">(Devlin et al., 2019;</ref><ref type="bibr" target="#b7">Chen et al., 2020a;</ref>. As VLP is also a form of self-supervised training, we examine the effects of training durations. As expected, the performance constantly increases as we train the model for longer training steps (rows 1~3). Masking whole words for the MLM objective (rows 3~4) and fine-tuning with augmentation (row 6) also An additional masked region modeling (MRM) objective has been the key for performance boost in VLP models such as <ref type="bibr" target="#b9">Chen et al. (2019)</ref>. We experiment with masked patch prediction (MPP) <ref type="bibr" target="#b14">(Dosovitskiy et al., 2020)</ref> which mimics the effect of MRM in a form compatible with patch projections. The patch v is masked with the probability of 0.15, and the model predicts the mean RGB value of the masked patch from its contextualized vector z D masked | v . However, MPP turns out not to be contributing to downstream performance (rows 4~5). This result is in sharp contrast to the MRM objective on supervision signals from object detection. <ref type="table">Table 7</ref>. VLP model components. "PC" is for per-class manner NMS and "CA" is for class-agnostic. Following <ref type="bibr" target="#b49">Tan &amp; Bansal (2019)</ref>, one single-modality layer is counted as 0.5 multi-modality layer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Complexity Analysis of VLP Models</head><p>We analyze the complexity of VLP models in various terms.</p><p>In <ref type="table">Table 6</ref>, we report the number of parameters, the number of floating-point operations (FLOPs), and the inference latency of the visual embedder and transformer. We exclude the textual embedder because it is shared by all VLP models 9 . The latency is averaged over 10K times on a Xeon E5-2650 CPU and an NVIDIA P40 GPU.</p><p>The input size in terms of image resolution and the length of concatenated multimodal input sequence affects the number of FLOPs. We co-note the sequence lengths. The image resolution is 800 ? 1,333 for region-based VLP models and Pixel-BERT-R50, 600 ? 1,000 for Pixel-BERT-X152, and 384 ? 640 for ViLT-B/32.</p><p>In Pixel-BERT and ViLT, visual tokens are sampled during pre-training and used in full during fine-tuning. We report the maximum number of visual tokens.</p><p>We observe that the runtime of BERT-base-like transformers varies only by &lt; 1 ms for input sequences of length under 300. Since patch projection of ViLT-B/32 generates at most flowers wall cottages cloudy a display of flowers growing out and over the retaining wall in front of cottages on a cloudy day. rug chair painting plant a room with a rug, a chair, a painting, and a plant. 240 image tokens, our model can still be efficient even though it receives a combination of image and text tokens. <ref type="figure" target="#fig_2">Figure 4</ref> is an example of a cross-modal alignment. The transportation plan of WPA expresses a heatmap for a text token highlighted in pink color. Each square tile represents a patch, and its opacity indicates how much mass is transported from the highlighted word token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Visualization</head><p>More IPOT iterations-more than over 50 as in the training phase-help the visualization heatmap converge; empirically, 1,000 iterations are sufficient to get a clearly identifiable heatmap. We z-normalize the plan for each token and clamp the values to [1.0, 3.0].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>In this paper, we present a minimal VLP architecture, Vision-and-Langauge Transformer (ViLT). ViLT is competent to competitors which are heavily equipped with convolutional visual embedding networks (e.g., Faster R-CNN and ResNets). We ask for future work on VLP to focus more on the modality interactions inside the transformer module rather than engaging in an arms race that merely powers up unimodal embedders.</p><p>Although remarkable as it is, ViLT-B/32 is more of a proof of concept that efficient VLP models free of convolution and region supervision can still be competent. We wrap up by pointing out a few factors that may add to the ViLT family.</p><p>Scalability. As shown in papers on large-scale transformers <ref type="bibr" target="#b13">(Devlin et al., 2019;</ref><ref type="bibr" target="#b14">Dosovitskiy et al., 2020)</ref>, the per-formance of pre-trained transformers scale well given an appropriate amount of data. This observation paves the way for even better performing ViLT variants (e.g., ViLT-L (large) and ViLT-H (huge)). We leave training larger models for future work because aligned vision-and-language datasets are yet scarce.</p><p>Masked Modeling for Visual Inputs. Considering the success of MRM, we speculate that the masked modeling objective for the visual modality helps by preserving the information up until the last layer of the transformer. However, as observed in <ref type="table" target="#tab_6">Table 5</ref>, a naive variant of MRM on image patches (MPP) fails. <ref type="bibr" target="#b10">Cho et al. (2020)</ref> proposed to train their grid RoIs on masked object classification (MOC) tasks. However, the visual vocabulary cluster in this work was fixed during the vision and language pre-training together with the visual backbone. For trainable visual embedders, one-time clustering is not a viable option. We believe that alternating clustering <ref type="bibr" target="#b4">(Caron et al., 2018;</ref> or simultaneous clustering <ref type="bibr" target="#b1">(Asano et al., 2019;</ref><ref type="bibr" target="#b6">Caron et al., 2020)</ref> methods studied in visual unsupervised learning research could be applied.</p><p>We encourage future work that does not use region supervision to devise a more sophisticated masking objective for the visual modality.</p><p>Augmentation Strategies. Previous work on contrastive visual representation learning <ref type="bibr" target="#b7">(Chen et al., 2020a;</ref> showed that gaussian blur, not employed by RandAugment, brings noticeable gains to downstream performance compared with a simpler augmentation strategy <ref type="bibr" target="#b20">(He et al., 2020)</ref>. Exploration of appropriate augmentation strategies for textual and visual inputs would be a valuable addition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Four categories of vision-and-language models. The height of each rectangle denotes its relative computational size. VE, TE, and MI are short for visual embedder, textual embedder, and modality interaction, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Model overview. Illustration inspired by<ref type="bibr" target="#b14">Dosovitskiy et al. (2020)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Visualizations of transportation plan of word patch alignment. Best viewed zoomed in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Equal contribution ? Current affiliation: NAVER AI Lab, Seongnam, Gyeonggi, Republic of Korea. 1 Kakao Enterprise, Seongnam, Gyeonggi, Republic of Korea 2 Kakao Brain, Seongnam, Gyeonggi, Republic of Korea. Correspondence to: Wonjae Kim &lt;wonjae.kim@navercorp.com&gt;. Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s).</figDesc><table><row><cell></cell><cell cols="2">Visual Embedding Schema</cell></row><row><cell></cell><cell></cell><cell>CNN</cell><cell>Region</cell></row><row><cell></cell><cell></cell><cell>Backbone</cell><cell>Operations</cell></row><row><cell></cell><cell></cell><cell>CNN</cell></row><row><cell></cell><cell></cell><cell>Backbone</cell></row><row><cell></cell><cell></cell><cell>Modality</cell></row><row><cell></cell><cell></cell><cell>Linear</cell><cell>Interaction</cell></row><row><cell></cell><cell></cell><cell>Embedding</cell></row><row><cell></cell><cell></cell><cell>Linear</cell></row><row><cell></cell><cell></cell><cell>Embedding</cell></row><row><cell></cell><cell></cell><cell>Running Time</cell></row><row><cell cols="3">(Performances : NLVR2 test-P Acc. / F30K TR R@1 / F30K IR R@1)</cell></row><row><cell>UNITER-Base (75.8 / 85.9 / 72.5)</cell><cell cols="2">5 ms (R101)~8 (RPNs, RoI Align, NMS, and RoI Heads) 10 ms 00 ms</cell></row><row><cell>Pixel-BERT-R50 (72.4 / 75.7 / 53.4)</cell><cell cols="2">45 ms (R50)~9 60 ms 5 ms</cell></row><row><cell></cell><cell></cell><cell>(BERT-base-like)</cell></row><row><cell cols="2">ViLT-B/32 (Ours) (76.1 / 83.5 / 64.4)~7 15 ms~0</cell><cell>.4 ms</cell></row><row><cell></cell><cell cols="2">(Linear Embedding)~1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>. Pixel-</figDesc><table><row><cell cols="2">Modality Interaction</cell><cell cols="2">Modality Interaction</cell><cell cols="2">Modality Interaction</cell><cell cols="2">Modality Interaction</cell></row><row><cell></cell><cell>Visual</cell><cell>Textual</cell><cell>Visual</cell><cell></cell><cell>Visual</cell><cell></cell><cell></cell></row><row><cell>Textual</cell><cell>Embed</cell><cell>Embed</cell><cell>Embed</cell><cell>Textual</cell><cell>Embed</cell><cell>Textual</cell><cell>Visual</cell></row><row><cell>Embed</cell><cell></cell><cell></cell><cell></cell><cell>Embed</cell><cell></cell><cell>Embed</cell><cell>Embed</cell></row><row><cell>Text</cell><cell>Image</cell><cell>Text</cell><cell>Image</cell><cell>Text</cell><cell>Image</cell><cell>Text</cell><cell>Image</cell></row><row><cell cols="2">(a) VE &gt; TE &gt; MI</cell><cell cols="2">(b) VE = TE &gt; MI</cell><cell cols="2">(c) VE &gt; MI &gt; TE</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Pre-training dataset statistics. Caption length is the length of tokens from pre-trained bert-base-uncased tokenizer. ? GCC and SBU provide only image urls, so we collect the images from urls which were still accessible.</figDesc><table><row><cell>Dataset</cell><cell cols="3"># Images # Captions Caption Length</cell></row><row><cell>MSCOCO</cell><cell>113K</cell><cell>567K</cell><cell>11.81 ? 2.81</cell></row><row><cell>VG</cell><cell>108K</cell><cell>5.41M</cell><cell>5.53 ? 1.76</cell></row><row><cell>GCC  ?</cell><cell>3.01M</cell><cell>3.01M</cell><cell>10.66 ? 4.93</cell></row><row><cell>SBU  ?</cell><cell>867K</cell><cell>867K</cell><cell>15.0 ? 7.74</cell></row><row><cell cols="4">et al., 2018). Table 1 reports the dataset statistics.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Comparison of ViLT-B/32 with other models on downstream classification tasks. We use MCAN and MaxEnt<ref type="bibr" target="#b48">(Suhr et al., 2018)</ref> for VQAv2 and NLVR2 w/o VLP SOTA results. ? additionally used GQA, VQAv2, VG-QA for pre-training. ? made additional use of the Open Images<ref type="bibr" target="#b28">(Kuznetsova et al., 2020)</ref> dataset. a indicates RandAugment is applied during fine-tuning. + indicates model trained for a longer 200K pre-training steps.</figDesc><table><row><cell>Visual Embed</cell><cell>Model</cell><cell>Time (ms)</cell><cell>VQAv2 test-dev</cell><cell cols="2">NLVR2 dev test-P</cell></row><row><cell></cell><cell cols="2">w/o VLP SOTA~900</cell><cell>70.63</cell><cell cols="2">54.80 53.50</cell></row><row><cell></cell><cell cols="2">ViLBERT~920</cell><cell>70.55</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">VisualBERT~925</cell><cell>70.80</cell><cell cols="2">67.40 67.00</cell></row><row><cell>Region</cell><cell cols="2">LXMERT~900</cell><cell>72.42</cell><cell cols="2">74.90 74.50</cell></row><row><cell></cell><cell cols="2">UNITER-Base~900</cell><cell>72.70</cell><cell cols="2">75.85 75.80</cell></row><row><cell></cell><cell cols="2">OSCAR-Base  ?~9 00</cell><cell>73.16</cell><cell cols="2">78.07 78.36</cell></row><row><cell></cell><cell cols="2">VinVL-Base  ? ?~6 50</cell><cell>75.95</cell><cell cols="2">82.05 83.08</cell></row><row><cell>Grid</cell><cell cols="2">Pixel-BERT-X152~160 Pixel-BERT-R50~60</cell><cell>74.45 71.35</cell><cell cols="2">76.50 77.20 71.70 72.40</cell></row><row><cell></cell><cell cols="2">ViLT-B/32~15</cell><cell>70.33</cell><cell cols="2">74.41 74.57</cell></row><row><cell>Linear</cell><cell>ViLT-B/32 a</cell><cell>~15</cell><cell>70.85</cell><cell cols="2">74.91 75.57</cell></row><row><cell></cell><cell>ViLT-B/32 a +</cell><cell>~15</cell><cell>71.26</cell><cell cols="2">75.70 76.13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Comparison of ViLT-B/32 with other VLP models on downstream zero-shot retrieval tasks. We exclude the models of which zero-shot retrieval performances were not reported in their original papers. ? is pre-trained with a 10M proprietary vision-and-language dataset in addition to the 4M dataset of GCC+SBU. + indicates model trained for a longer 200K pre-training steps.</figDesc><table><row><cell>Visual Embed</cell><cell>Model</cell><cell>Time (ms)</cell><cell cols="12">Zero-Shot Text Retrieval Flickr30k (1K) MSCOCO (5K) R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 Zero-Shot Image Retrieval Flickr30k (1K) MSCOCO (5K)</cell></row><row><cell></cell><cell cols="2">ViLBERT~900</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>31.9</cell><cell>61.1</cell><cell>72.8</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Region</cell><cell cols="2">Unicoder-VL~925 UNITER-Base~900</cell><cell>64.3 80.7</cell><cell>85.8 95.7</cell><cell>92.3 98.0</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>48.4 66.2</cell><cell>76.0 88.4</cell><cell>85.2 92.9</cell><cell>--</cell><cell>--</cell><cell>--</cell></row><row><cell></cell><cell cols="2">ImageBERT  ?~9 25</cell><cell>70.7</cell><cell>90.2</cell><cell>94.0</cell><cell>44.0</cell><cell>71.2</cell><cell>80.4</cell><cell>54.3</cell><cell>79.6</cell><cell>87.5</cell><cell>32.3</cell><cell>59.0</cell><cell>70.2</cell></row><row><cell>Linear</cell><cell cols="2">ViLT-B/32~15 ViLT-B/32 + ~15</cell><cell>69.7 73.2</cell><cell>91.0 93.6</cell><cell>96.0 96.5</cell><cell>53.4 56.5</cell><cell>80.7 82.6</cell><cell>88.8 89.6</cell><cell>51.3 55.0</cell><cell>79.9 82.5</cell><cell>87.9 89.8</cell><cell>37.3 40.4</cell><cell>67.4 70.0</cell><cell>79.0 81.1</cell></row><row><cell cols="15">Table 4. Comparison of ViLT-B/32 with other models on downstream retrieval tasks. We use SCAN for w/o VLP SOTA results.  ?</cell></row><row><cell cols="15">additionally used GQA, VQAv2, VG-QA for pre-training.  ? additionally used the Open Images dataset. a indicates RandAugment is</cell></row><row><cell cols="10">applied during fine-tuning. + indicates model trained for a longer 200K pre-training steps.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Visual Embed</cell><cell>Model</cell><cell>Time (ms)</cell><cell cols="6">Text Retrieval Flickr30k (1K) MSCOCO (5K) R@1 R@5 R@10 R@1 R@5 R@10</cell><cell cols="6">Image Retrieval Flickr30k (1K) MSCOCO (5K) R@1 R@5 R@10 R@1 R@5 R@10</cell></row><row><cell></cell><cell cols="2">w/o VLP SOTA~900</cell><cell>67.4</cell><cell>90.3</cell><cell>95.8</cell><cell>50.4</cell><cell>82.2</cell><cell>90.0</cell><cell>48.6</cell><cell>77.7</cell><cell>85.2</cell><cell>38.6</cell><cell>69.3</cell><cell>80.4</cell></row><row><cell></cell><cell cols="2">ViLBERT-Base~920</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>58.2</cell><cell>84.9</cell><cell>91.5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Region</cell><cell cols="2">Unicoder-VL~925 UNITER-Base~900</cell><cell>86.2 85.9</cell><cell>96.3 97.1</cell><cell>99.0 98.8</cell><cell>62.3 64.4</cell><cell>87.1 87.4</cell><cell>92.8 93.1</cell><cell>71.5 72.5</cell><cell>91.2 92.4</cell><cell>95.2 96.1</cell><cell>48.4 50.3</cell><cell>76.7 78.5</cell><cell>85.9 87.2</cell></row><row><cell></cell><cell cols="2">OSCAR-Base  ?~9 00</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>70.0</cell><cell>91.1</cell><cell>95.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>54.0</cell><cell>80.8</cell><cell>88.5</cell></row><row><cell></cell><cell cols="2">VinVL-Base  ? ?~6 50</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>74.6</cell><cell>92.6</cell><cell>96.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>58.1</cell><cell>83.2</cell><cell>90.1</cell></row><row><cell>Grid</cell><cell cols="2">Pixel-BERT-X152~160 Pixel-BERT-R50~60</cell><cell>87.0 75.7</cell><cell>98.9 94.7</cell><cell>99.5 97.1</cell><cell>63.6 59.8</cell><cell>87.5 85.5</cell><cell>93.6 91.6</cell><cell>71.5 53.4</cell><cell>92.1 80.4</cell><cell>95.8 88.5</cell><cell>50.1 41.1</cell><cell>77.6 69.7</cell><cell>86.2 80.5</cell></row><row><cell></cell><cell cols="2">ViLT-B/32~15</cell><cell>81.4</cell><cell>95.6</cell><cell>97.6</cell><cell>61.8</cell><cell>86.2</cell><cell>92.6</cell><cell>61.9</cell><cell>86.8</cell><cell>92.8</cell><cell>41.3</cell><cell>72.0</cell><cell>82.5</cell></row><row><cell>Linear</cell><cell>ViLT-B/32 a</cell><cell>~15</cell><cell>83.7</cell><cell>97.2</cell><cell>98.1</cell><cell>62.9</cell><cell>87.1</cell><cell>92.7</cell><cell>62.2</cell><cell>87.6</cell><cell>93.2</cell><cell>42.6</cell><cell>72.8</cell><cell>83.4</cell></row><row><cell></cell><cell>ViLT-B/32 a +</cell><cell>~15</cell><cell>83.5</cell><cell>96.7</cell><cell>98.6</cell><cell>61.5</cell><cell>86.3</cell><cell>92.7</cell><cell>64.4</cell><cell>88.7</cell><cell>93.8</cell><cell>42.7</cell><cell>72.9</cell><cell>83.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Ablation study of ViLT-B/32. w denotes whether whole word masking is used for pre-training. m denotes whether MPP objective is used for pre-training. a denotes whether RandAugment is used during fine-tuning. ? 0.07 70.83 ? 0.19 70.83 ? 0.23 75.39 (45.12) 52.52 (31.80) 53.72 (31.55) 34.88 (21.</figDesc><table><row><cell>Training</cell><cell cols="2">Ablation</cell><cell></cell><cell cols="2">VQAv2</cell><cell cols="2">NLVR2</cell><cell cols="2">Flickr30k R@1 (1K)</cell><cell cols="2">MSCOCO R@1 (5K)</cell></row><row><cell>Steps</cell><cell>w</cell><cell>m</cell><cell>a</cell><cell cols="2">test-dev</cell><cell>dev</cell><cell>test-P</cell><cell>TR (ZS)</cell><cell>IR (ZS)</cell><cell>TR (ZS)</cell><cell>IR (ZS)</cell></row><row><cell>25K</cell><cell>X</cell><cell>X</cell><cell>X</cell><cell cols="8">68.96 58)</cell></row><row><cell>50K</cell><cell>X</cell><cell>X</cell><cell>X</cell><cell cols="8">69.80 ? 0.01 71.93 ? 0.27 72.92 ? 0.82 78.13 (55.57) 57.36 (40.94) 57.00 (39.56) 37.47 (27.51)</cell></row><row><cell>100K</cell><cell>X</cell><cell>X</cell><cell>X</cell><cell cols="8">70.16 ? 0.01 73.54 ? 0.02 74.15 ? 0.27 79.39 (66.99) 60.50 (47.62) 60.15 (51.25) 40.45 (34.59)</cell></row><row><cell>100K</cell><cell>O</cell><cell>X</cell><cell>X</cell><cell cols="8">70.33 ? 0.01 74.41 ? 0.21 74.57 ? 0.09 81.35 (69.73) 61.86 (51.28) 61.79 (53.40) 41.25 (37.26)</cell></row><row><cell>100K</cell><cell>O</cell><cell>O</cell><cell>X</cell><cell cols="8">70.21 ? 0.05 72.76 ? 0.50 73.54 ? 0.47 78.91 (63.67) 58.76 (46.96) 59.53 (47.75) 40.08 (32.28)</cell></row><row><cell>100K</cell><cell>O</cell><cell>X</cell><cell>O</cell><cell cols="8">70.85 ? 0.13 74.91 ? 0.29 75.57 ? 0.61 83.69 (69.73) 62.22 (51.28) 62.88 (53.40) 42.62 (37.26)</cell></row><row><cell>200K</cell><cell>O</cell><cell>X</cell><cell>O</cell><cell cols="8">71.26 ? 0.06 75.70 ? 0.32 76.13 ? 0.39 83.50 (73.24) 64.36 (54.96) 61.49 (56.51) 42.70 (40.42)</cell></row><row><cell cols="8">Table 6. Comparison of VLP models in terms of parameter size,</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">FLOPs, and inference latency. Since FLOPs are proportional to</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">input size, we denote the number of input tokens (image+text) as</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">superscripts ("?" when text length is unreported; we arbitrarily use</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">length 40). Although not captured in FLOPs count nor parameter</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">size (because it is not a tensor operation), note that per-class NMS</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">for 1,600 classes amounts to more than 500 ms in latency. NMS</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">latency varies a lot according to the number of detected classes.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Visual Embed</cell><cell>Model</cell><cell></cell><cell></cell><cell></cell><cell>#Params (M)</cell><cell>#FLOPs (G)</cell><cell>Time (ms)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">ViLBERT 36+36</cell><cell></cell><cell>274.3</cell><cell cols="2">958.1~900</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">VisualBERT 36+128</cell><cell></cell><cell>170.3</cell><cell cols="2">425.0~925</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">LXMERT 36+20</cell><cell></cell><cell>239.8</cell><cell cols="2">952.0~900</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Region</cell><cell cols="3">UNITER-Base 36+60 OSCAR-Base 50+35</cell><cell></cell><cell>154.7 154.7</cell><cell cols="2">949.9~900 956.4~900</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">VinVL-Base 50+35</cell><cell></cell><cell>157.3</cell><cell cols="2">1023.3~650</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Unicoder-VL 100+?</cell><cell></cell><cell>170.3</cell><cell cols="2">419.7~925</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">ImageBERT 100+44</cell><cell></cell><cell>170.3</cell><cell cols="2">420.6~925</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Grid</cell><cell cols="4">Pixel-BERT-X152 146+? Pixel-BERT-R50 260+?</cell><cell>144.3 94.9</cell><cell cols="2">185.8~160 136.8~60</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Linear</cell><cell cols="3">ViLT-B/32 200+40</cell><cell></cell><cell>87.4</cell><cell cols="2">55.9~15</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">drive performance. Further increase in training iterations</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">to 200K improved performance on VQAv2, NLVR2, and</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">zero-shot retrieval. We stop increasing the number of itera-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">tions over 200K as the fine-tuned text retrieval performance</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">decreases afterward.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">While some works employ additional objectives and data structures, these two objectives apply to almost every VLP model.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2"><ref type="bibr" target="#b3">Bugliarello et al. (2020)</ref> showed that a controlled setup bridges the performance gap of various region-feature-based VLP models.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Parameters for R50 is 25M, R101 is 44M, and X152 is 60M.4  We also experimented with initializing the layers from BERT weights and using the pre-trained patch projection from ViT, but it did not work.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">ViT-B/32 is pre-trained with ImageNet-21K and fine-tuned on ImageNet-1K for image classification. We expect that weights pre-trained on larger datasets (e.g., JFT-300M) would yield better performance.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">VQA score is calculated by comparing the inferred answer to 10 ground-truth answers: see https://visualqa.org/ evaluation.html for details.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">FLOPs and time are neglectable because the operation is an embedding lookup. The 30K embedding dictionary used by bert-base-uncased has 23.47 M parameters</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Self-labelling via simultaneous clustering and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Multigrain: a unified image embedding for classes and instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.05509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bugliarello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Okazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Elliott</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.15124</idno>
		<title level="m">Multimodal pretraining unmasked: Unifying the vision and language berts</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised pre-training of image features on non-curated data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joulin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2959" to="2968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joulin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09882</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uniter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11740</idno>
		<title level="m">Learning universal imagetext representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">X-lxmert: Paint, caption and answer questions with multi-modal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8785" to="8805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Pre-training with whole word masking for chinese bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08101</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vse++</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05612</idno>
		<title level="m">Improving visual-semantic embeddings with hard negatives</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Large-scale adversarial training for vision-and-language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06195</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Making the v in vqa matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6904" to="6913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mask rcnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Augment your batch: Improving generalization through instance repetition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Giladi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8129" to="8138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.00849</idno>
		<title level="m">Aligning image pixels with text by deep multi-modal transformers</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">In defense of grid features for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10267" to="10276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Pythia v0. 1: the winning entry to the vqa challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.09956</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The open images dataset v4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Stacked cross attention for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="201" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11336" to="11344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03557</idno>
		<title level="m">Visualbert: A simple and performant baseline for vision and language</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Objectsemantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="121" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pretraining task-agnostic visiolinguistic representations for visionand-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">12-in-1: Multi-task vision and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10437" to="10446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-K</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11883</idno>
		<title level="m">Revisiting modulated convolutions for visual counting and beyond</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Im2text: Describing images using 1 million captioned photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1143" to="1151" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2641" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Imagebert: Cross-modal pre-training with largescale weak-supervised image-text data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bharti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sacheti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07966</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2556" to="2565" />
		</imprint>
	</monogr>
	<note>ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A survey on image data augmentation for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shorten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="48" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vl-Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08530</idno>
		<title level="m">Pre-training of generic visual-linguistic representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">A corpus for reasoning about natural language grounded in photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00491</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lxmert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07490</idno>
		<title level="m">Learning cross-modality encoder representations from transformers</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A fast proximal point method for computing exact wasserstein distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="433" to="453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Ernie-vil: Knowledge enhanced visionlanguage representations through scene graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16934</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deep modular co-attention networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6281" to="6290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinvl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00529</idno>
		<title level="m">Making visual representations matter in vision-language models</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Unified vision-language pre-training for image captioning and vqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13041" to="13049" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
