<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Look, Listen and Learn</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovi?</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">VGG</orgName>
								<orgName type="department" key="dep2">Department of Engineering Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">VGG</orgName>
								<orgName type="department" key="dep2">Department of Engineering Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zisserman@google</forename><surname>Com</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">VGG</orgName>
								<orgName type="department" key="dep2">Department of Engineering Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Deepmind</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">VGG</orgName>
								<orgName type="department" key="dep2">Department of Engineering Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Look, Listen and Learn</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider the question: what can be learnt by looking at and listening to a large number of unlabelled videos? There is a valuable, but so far untapped, source of information contained in the video itself -the correspondence between the visual and the audio streams, and we introduce a novel "Audio-Visual Correspondence" learning task that makes use of this. Training visual and audio networks from scratch, without any additional supervision other than the raw unconstrained videos themselves, is shown to successfully solve this task, and, more interestingly, result in good visual and audio representations. These features set the new state-of-the-art on two sound classification benchmarks, and perform on par with the state-of-the-art selfsupervised approaches on ImageNet classification. We also demonstrate that the network is able to localize objects in both modalities, as well as perform fine-grained recognition tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual and audio events tend to occur together; not always but often: the movement of fingers and sound of the instrument when a piano, guitar or drum is played; lips moving and speech when talking; cars moving and engine noise when observing a street. The visual and audio events are concurrent in these cases because there is a common cause. In this paper we investigate whether we can use this simple observation to learn about the world both visually and aurally by simply watching and listening to videos.</p><p>We ask the question: what can be learnt by training visual and audio networks simultaneously to predict whether visual information (a video frame) corresponds or not to audio information (a sound snippet)? This is a looser requirement than that the visual and audio events occur in sync. It only requires that there is something in the image that correlates with something in the audio clip -a car present in the video frame, for instance, correlating with engine noise; or an exterior shot with the sound of wind.</p><p>Our motivation for this work is three fold: first, as in many recent self-supervision tasks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref>, it is interesting to learn from a virtually infinite source of free supervision (video with visual and audio modes in this case) rather than requiring strong supervision; second, this is a possible source of supervision that an infant could use as their visual and audio capabilities develop; third, we want to know what can be learnt, and how well the networks are trained, for example in the performance of the visual and audio networks for other tasks.</p><p>Of course, we are not the first to make the observation that visual and audio events co-occur, and to use their concurrence or correlation as supervision for training a network. In a series of recent and inspiring papers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>, the group at MIT has investigated precisely this. However, their goal is always to train a single network for one of the modes, for example, train a visual network to generate sounds in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>; or train an audio network to correlate with visual outputs in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11]</ref>, where the visual networks are pre-trained and fixed and act as a teacher. In earlier, pre deep-learning, approaches the observation was used to beautiful effect in <ref type="bibr" target="#b13">[14]</ref> showing "pixels that sound" (e.g. for a guitar) learnt using CCA. In contrast, we train both visual and audio networks and, somewhat surprisingly, show that this is beneficial -in that our performance improves substantially over that of <ref type="bibr" target="#b1">[2]</ref> when trained on the same data.</p><p>In summary: our goal is to design a system that is able to learn both visual and audio semantic information in a completely unsupervised manner by simply looking at and listening to a large number of unlabelled videos. To achieve this we introduce a novel Audio-Visual Correspondence (AVC) learning task that is used to train the two (visual and audio) networks from scratch. This task is described in section 2, together with the network architecture and training procedure. In section 3 we describe what semantic information has been learnt, and assess the performance of the audio and visual networks. We find, which we had not anticipated, that this task leads to quite fine grained visual and audio discrimination, e.g. into different instruments. In terms of quantitative performance, the audio network exceed those recently trained for audio recognition using visual super-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vision subnetwork</head><p>Audio subnetwork</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fusion layers</head><p>Correspond?</p><p>Audio-visual correspondence detector network Yes / No <ref type="figure">Figure 1</ref>. Audio-visual correspondence task (AVC). A network should learn to determine whether a pair of (video frame, short audio clip) correspond to each other or not. Positives are (frame, audio) extracted from the same time of one video, while negatives are a frame and audio extracted from different videos.</p><p>vision, and the visual network has similar performance to those trained for other, purely visual, self-supervision tasks. Furthermore, we show, as an added benefit, that we are able to localize the source of the audio event in the video frame (and also localize the corresponding regions of the sound source) using activation visualization.</p><p>In terms of prior work, the most closely related deep learning approach that we know of is 'SyncNet' in <ref type="bibr" target="#b4">[5]</ref>. However, <ref type="bibr" target="#b4">[5]</ref> is aimed at learning to synchronize lip-regions and speech for lip-reading, rather than the more general video and audio material considered here for learning semantic representations. More generally, the AVC task is a form of co-training <ref type="bibr" target="#b3">[4]</ref>, where there are two 'views' of the data, and each view provides complementary information. In our case the two views are visual and audio (and each can determine semantic information independently). A similar scenario arises when the two views are visual and language (text) as in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b30">31]</ref> where a common embedding is learnt. However, usually one (or both) of the networks (for images and text) are pre-trained, in contrast to the approach taken here where no supervision is required and both networks are trained from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Audio-visual correspondence learning</head><p>The core idea is to use a valuable but so far untapped source of information contained in the video itself -the correspondence between visual and audio streams available by virtue of them appearing together at the same time in the same video. By seeing and hearing many examples of a person playing a violin and examples of a dog barking, and never, or at least very infrequently, seeing a violin being played while hearing a dog bark and vice versa, it should be possible to conclude what a violin and a dog look and sound like, without ever being explicitly taught what is a violin or a dog.</p><p>We leverage this for learning by an audio-visual correspondence (AVC) task, illustrated in <ref type="figure">Figure 1</ref>. The AVC task is a simple binary classification task: given an example video frame and a short audio clip -decide whether they correspond to each other or not. The corresponding (positive) pairs are the ones that are taken at the same time from the same video, while mismatched (negative) pairs are extracted from different videos. The only way for a system to solve this task is if it learns to detect various semantic concepts in both the visual and the audio domain. Indeed, we demonstrate in Section 3.5 that our network automatically learns relevant semantic concepts in both modalities.</p><p>It should be noted that the task is very difficult. The network is made to learn visual and audio features and concepts from scratch without ever seeing a single label. Furthermore, the AVC task itself is quite hard when done on completely unconstrained videos -videos can be very noisy, the audio source is not necessarily visible in the video (e.g. camera operator speaking, person narrating the video, sound source out of view or occluded, etc.), and the audio and visual content can be completely unrelated (e.g. edited videos with added music, very low volume sound, ambient sound such as wind dominating the audio track despite other audio events being present, etc.). Nevertheless, the results in Section 3 show that our network is able to fairly successfully solve the AVC task, and in the process learn very good visual and audio representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Network architecture</head><p>To tackle the AVC task, we propose the network structure shown in <ref type="figure">Figure 2</ref>. It has three distinct parts: the vision and the audio subnetworks which extract visual and audio features, respectively, and the fusion network which takes these features into account to produce the final decision on whether the visual and audio signals correspond. Here we describe the three parts in more detail. Vision subnetwork. The input to the vision subnetwork is a 224 ? 224 colour image. We follow the VGG-network <ref type="bibr" target="#b29">[30]</ref> design style, with 3 ? 3 convolutional filters, and 2 ? 2 max-pooling layers with stride 2 and no padding. The network can be segmented into four blocks of conv+conv+pool layers such that inside each block the two conv layers have the same number of filters, while consecutive blocks have doubling filter numbers: 64, 128, 256 and 512. At the very end, max-pooling is performed across all spatial locations to produce a single 512-D feature vector. Each conv layer is followed by batch normalization <ref type="bibr" target="#b11">[12]</ref> and a ReLU nonlinearity. Audio subnetwork. The input to the audio subnetwork is a 1 second sound clip converted into a log-spectrogram (more details are provided later in this section), which is thereafter treated as a greyscale 257 ? 199 image. The architecture of the audio subnetwork is identical to the vision one with the exception that input pixels are 1-D intensities instead of 3-D colours and therefore the conv1 1 filter sizes are 3? smaller compared to the vision subnetwork. The final audio feature is also 512-D.  <ref type="figure">Figure 2</ref>. L 3 -Net architecture. Each blocks represents a single layer with text providing more information -first row: layer name and parameters, second row: output feature map size. Layers with a name prefix conv, pool, fc, concat, softmax are convolutional, max-pooling, fully connected, concatenation and softmax layers, respectively. The listed parameters are: conv -kernel size and number of channels, pooling -kernel size, fc -size of the weight matrix. The stride of pool layers is equal to the kernel size and there is no padding. Each convolutional layer is followed by batch normalization <ref type="bibr" target="#b11">[12]</ref> and a ReLU nonlinearity, and the first fully connected layer (fc1) is followed by ReLU.</p><p>Fusion network. The two 512-D visual and audio features are concatenated into a 1024-D vector which is passed through the fusion network to produce a 2-way classification output, namely, whether the vision and audio correspond or not. It consists of two fully connected layers, with ReLU in between them, and the intermediate feature size of 128-D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Implementation details</head><p>Training data sampling. A non-corresponding frameaudio pair is compiled by randomly sampling two different videos and picking a random frame from one and a random 1 second audio clip from the other. A corresponding frameaudio pair is created by sampling a random video, picking a random frame in that video, and then picking a random 1 second audio clip that overlaps in time with the sampled frame. This provides additional training samples compared to simply sampling the 1 second audio with the frame at its mid-point. We use standard data augmentation techniques for images: each training image is uniformly scaled such that the smallest dimension is equal to 256, followed by random cropping into 224 ? 224, random horizontal flipping, and brightness and saturation jittering. Audio is only augmented by changing the volume up to 10% randomly but consistently across the sample.</p><p>Log-spectrogram computation. The 1 second audio is resampled to 48 kHz, and a spectrogram is computed with window length of 0.01 seconds and a half-window overlap; this produces 199 windows with 257 frequency bands. The response map is passed through a logarithm before feeding it into the audio subnetwork. Training procedure. We use the Adam optimizer <ref type="bibr" target="#b14">[15]</ref>, weight decay 10 ?5 , and perform a grid search on the learning rate, although 10 ?4 usually works well. The network was trained on 16 GPUs in parallel with synchronous training implemented in TensorFlow, where each worker processed a 16-element batch, thus making the effective batch size of 256. For a training set of 400k 10 second videos, the network is trained for two days, during which it has seen 60M frame-audio pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results and discussion</head><p>Our "look, listen and learn" network (L 3 -Net) approach is evaluated and examined in multiple ways. First, the performance of the network on the audio-visual correspondence task itself is investigated, and compared to supervised baselines. Second, the quality of the learnt visual and audio features is tested in a transfer learning setting, on visual and audio classification tasks. Finally, we perform a qualitative analysis of what the network has learnt. We start by introducing the datasets used for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets</head><p>Two video datasets are used for training the networks: Flickr-SoundNet and Kinetics-Sounds. Flickr-SoundNet <ref type="bibr" target="#b1">[2]</ref>. This is a large unlabelled dataset of completely unconstrained videos from Flickr, compiled by searching for popular tags, but no tags or any sort of additional information apart from the videos themselves are used. It contains over 2 million videos but for practical reasons we use a random subset of 500k videos (400k training, 50k validation and 50k test) and only use the first 10 seconds of each video. This is the dataset that is used for training the L 3 -Net for the transfer learning experiments in Sections 3.3 and 3.4. Kinetics-Sounds. While our goal is to learn from completely unconstrained videos, having a labelled dataset is useful for quantitative evaluation. For this purpose we took a subset (much smaller than Flickr-SoundNet) of the Kinetics dataset <ref type="bibr">[13]</ref>, which contains YouTube videos manually annotated for human actions using Mechanical Turk, and cropped to 10 seconds around the action. The subset contains 19k 10 second video clips (15k training, 1.9k validation, 1.9k test) formed by filtering the Kinetics dataset for 34 human action classes, which have been chosen to be potentially manifested visually and aurally, such as playing various instruments (guitar, violin, xylophone, etc.), using tools (lawn mowing, shovelling snow, etc.), as well as performing miscellaneous actions (tap dancing, bowling, laughing, singing, blowing nose, etc.); the full list is given in appendix A. Although this dataset is fairly clean by construction, it still contains considerable noise, e.g. the bowling action is often accompanied by loud music at the bowling alley, human voices (camera operators or video narrations) often masks the sound of interest, and many videos contain sound tracks that are completely unrelated to the visual content (e.g. music montage for a snow shovelling video).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Audio-visual correspondence</head><p>First we evaluate the performance of our method on the task it was trained to solve -deciding whether a frame and a 1 second audio clip correspond (Section 2). For the Kinetics-Sounds dataset which contains labelled videos, we also evaluate two supervised baselines in order to gauge how well the AVC training compares to supervised training.</p><p>Supervised baselines. For both baselines we first train vision and audio networks independently on the action classification task, and then combine them in two different ways. The vision network has an identical feature extraction trunk as our vision subnetwork (Section 2.1), on top of which two fully connected layers are attached (sizes: 512?128 and 128?34) to perform classification into the 34 Kinetics-Sounds classes. The audio classification network is constructed analogously. The direct combination baseline computes the audio-video correspondence score as the similarity of class score distributions of the two networks, computed as the scalar product between the 34-D network softmax outputs, and decides that audio and video are in correspondence if the score is larger than a threshold. The motivation behind this baseline is that if the vision network believes the frame contains a dog while the audio network is confident it hears a violin, then the (frame, audio) pair is unlikely to be in correspondence. The supervised pretraining baseline takes the feature extraction trunks from the two trained networks, assembles them into our network architecture by concatenating the features and adding two fully connected layers (Section 2.1). The weights of the feature extractors are frozen and the fully connected layers are trained on the AVC task in the same manner as our</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Flickr-SoundNet Kinetics-Sounds Supervised direct -65% Supervised pretraining -74% L 3 -Net 78% 74% <ref type="table">Table 1</ref>. Audio-visual correspondence (AVC) results. Test set accuracy on the AVC task for the L 3 -Net, and the two supervised baselines on the labelled Kinetics-Sounds dataset. The number of positives and negatives is the same, so chance gets 50%. All methods are trained on the training set of the respective datasets.</p><p>network. This is the strongest baseline as it directly corresponds to our method, but with features learnt in a fully supervised manner. Results and discussion. <ref type="table">Table 1</ref> shows the results on the AVC task. The L 3 -Net achieves 74% and 78% on the two datasets, where chance is 50%. It should be noted that the task itself is quite hard due to the unconstrained nature of the videos (Section 2), as well as due to the very local input data which lacks context -even humans find it hard to judge whether an isolated frame and an isolated single second of audio correspond; informal human tests indicated that humans are only a few percent better than the L 3 -Net. Furthermore, the supervised baselines do not beat the L 3 -Net as "supervised pretraining" performs on par with it, while "supervised direct combination" works significantly worse as, unlike "supervised pretraining", it has not been trained for the AVC task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Audio features</head><p>In this section we evaluate the power of the audio representation that emerges from the L 3 -Net approach. Namely, the L 3 -Net audio subnetwork trained on Flickr-SoundNet is used to extract features from 1 second audio clips, and the effectiveness of these features is evaluated on two standard sound classification benchmarks: ESC-50 and DCASE. Environmental sound classification (ESC-50) <ref type="bibr" target="#b25">[26]</ref>. This dataset contains 2000 audio clips, 5 seconds each, equally balanced between 50 classes.</p><p>These include animal sounds, natural soundscapes, human non-speech sounds, interior/domestic sounds, and exterior/urban noises. The data is split into 5 predefined folds and performance is measured in terms of mean accuracy over 5 leave-one-fold-out evaluations. Detection and classification of acoustic scenes and events (DCASE) <ref type="bibr" target="#b31">[32]</ref>. We consider the scene classification task of the challenge which contains 10 classes (bus, busy street, office, open air market, park, quiet street, restaurant, supermarket, tube, tube station), with 10 training and 100 test clips per class, where each clip is 30 seconds long. Experimental procedure. To enable a fair direct comparison with the current state-of-the-art, Aytar et al. <ref type="bibr" target="#b1">[2]</ref>, we follow the same experimental setup. Multiple overlapping (a) ESC-50</p><formula xml:id="formula_0">(b) DCASE Method</formula><p>Accuracy SVM-MFCC <ref type="bibr" target="#b25">[26]</ref> 39.6% Autoencoder <ref type="bibr" target="#b1">[2]</ref> 39.9% Random Forest <ref type="bibr" target="#b25">[26]</ref> 44.3% Piczak ConvNet <ref type="bibr" target="#b24">[25]</ref> 64.5% SoundNet <ref type="bibr" target="#b1">[2]</ref> 74.2% Ours random 62.5% Ours 79.3% Human perf. <ref type="bibr" target="#b25">[26]</ref> 81.3%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Accuracy RG <ref type="bibr" target="#b26">[27]</ref> 69% LTT <ref type="bibr" target="#b18">[19]</ref> 72% RNH <ref type="bibr" target="#b27">[28]</ref> 77% Ensemble <ref type="bibr" target="#b31">[32]</ref> 78% SoundNet <ref type="bibr" target="#b1">[2]</ref> 88% Ours random 85% Ours 93% subclips are extracted from each recording and described using our features. For 5 second recordings from ESC-50 we extract 10 equally spaced 1 second subclips, while for the 6 times longer DCASE recordings, 60 subclips are extracted per clip. The audio features are obtained by maxpooling the last convolutional layer of the audio subnetwork (conv4 2), before the ReLU, into a 4 ? 3 ? 512 = 6144 dimensional representation (the conv4 2 outputs are originally 16 ? 12 ? 512). The features are preprocessed using z-score normalization, i.e. shifted and scaled to have a zero mean and unit variance. A multi-class one-vs-all linear SVM is trained, and at test time the class scores for a recording are computed as the mean over the class scores for its subclips.</p><p>Results and discussion. <ref type="table">Table 2</ref> shows the results on ESC-50 and DCASE. On both benchmarks we convincingly beat the previous state-of-the-art, SoundNet <ref type="bibr" target="#b1">[2]</ref>, by 5.1% and 5% absolute. For ESC-50 we reduce the gap between the previous best result and the human performance by 72% while for DCASE we reduce the error by 42%. The results are especially impressive as SoundNet uses two vision networks trained in a fully supervised manner on ImageNet and Places2 as teachers for the audio network, while we learn both the vision and the audio networks without any supervision whatsoever. Note that we train our networks with a random subset of the SoundNet videos for efficiency purposes, so it is possible that further gains can be achieved by using all the available training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Visual features</head><p>In this section we evaluate the power of the visual representation that emerges from the L 3 -Net approach. Namely, the L 3 -Net vision subnetwork trained on Flickr-SoundNet is used to extract features from images, and the effectiveness of these features is evaluated on the ImageNet large scale visual recognition challenge 2012 <ref type="bibr" target="#b28">[29]</ref>. Experimental procedure. We follow the experimental setup of Zhang et al. <ref type="bibr" target="#b35">[36]</ref> where features are extracted from 256 ? 256 images and used to perform linear classification on ImageNet. As in <ref type="bibr" target="#b35">[36]</ref>, we take conv4 2 features after ReLU and perform max-pooling with equal kernel and stride sizes until feature dimensionality is below 10k; in our case this results in 4 ? 4 ? 512 = 8192-D features. A single fully connected layer is added to perform linear classification into the 1000 ImageNet classes. All the weights are frozen to their L 3 -Net-trained values, apart from the final classification layer which is trained with cross-entropy loss on the ImageNet training set. The training procedure (data augmentation, learning rate schedule, label smoothing) is identical to <ref type="bibr" target="#b32">[33]</ref>, the only differences being that we use the Adam optimizer instead of RMSprop, and a 256?256 input image instead of 299 ? 299 as it fits our architecture better and to be consistent with <ref type="bibr" target="#b35">[36]</ref>. Results and discussion. Classification accuracy on the Im-ageNet validation set is shown in <ref type="table">Table 3</ref> and contrasted with other unsupervised and self-supervised methods. We also test the performance of random features, i.e. our L 3 -Net architecture without AVC training but with a trained classification layer.</p><p>Our L 3 -Net-trained features achieve 32.3% accuracy which is on par with other state-of-the-art self-supervised methods of <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b35">36]</ref>, while convincingly beating random initialization, data-dependent initialization <ref type="bibr" target="#b15">[16]</ref>, and Context Encoders <ref type="bibr" target="#b23">[24]</ref>. It should be noted that these methods use the AlexNet <ref type="bibr" target="#b16">[17]</ref> architecture which is different to ours, so the results are not fully comparable. On the one hand, our architecture when trained from scratch in its entirety achieves a higher performance (59.2% vs AlexNet's 51.0%). On the other hand, it is deeper which makes it harder to train as can be seen from the fact that our random features perform worse than theirs (12.9% vs AlexNet's 18.3%), and that all competing methods hit peak performance when they use earlier layers (e.g. <ref type="bibr" target="#b6">[7]</ref> drops from Fingerpicking Lawn mowing P. accordion P. bass guitar P. saxophone Typing Bowling P. clarinet P. organ <ref type="figure">Figure 3</ref>. Learnt visual concepts (Kinetics-Sounds). Each column shows five images that most activate a particular unit of the 512 in pool4 for the vision subnetwork. Note that these features do not take sound as input. Videos come from the Kinetics-Sounds test set and the network was trained on the Kinetics-Sounds train set. The top row shows the dominant action label for the unit ("P." stands for "playing").   <ref type="figure">Figure 3</ref>. A semantic heatmap is obtained as a slice of activations from conv4 2 of the vision subnetwork that corresponds to the same unit from pool4 as in <ref type="figure">Figure 3</ref>, i.e. the unit that responds highly to the class in question. <ref type="bibr" target="#b30">31</ref>.0% to 27.1% when going from conv3 to pool5). In fact, when measuring the improvement achieved due to AVC or self-supervised training versus the performance of the network with random initialization, our AVC training beats all competitors. Another important fact to consider is that all competing methods actually use ImageNet images when training. Although they do not make use of the labels, the underlying image statistics are the same: objects are fairly central in the image, and the networks have seen, for example, abundant images of 120 breads of dogs and thus potentially learnt their distinguishing features. In contrast, we use a completely separate source of training data in the form of frames from Flickr videos -here the objects are in general not centred, it is likely that the network has never seen a "Tibetan terrier" nor the majority of other fine-grained categories. Furthermore, video frames have vastly different low-level statistics to still images, with strong artefacts such as motion blur. With these factors hampering our network, it is impressive that our visual features L 3 -Net-trained on Flickr videos perform on par with self-supervised state-of-the-art trained on ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Qualitative analysis</head><p>In this section we analyse what is it that the network has learnt. We visualize the results on the test set of the Kinetics-Sounds and Flickr-SoundNet datasets, so the network has not seen the videos during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">Vision features</head><p>To probe what the vision subnetwork has learnt, we pick a particular 'unit' in pool4 (i.e. a component of the 512 dimensional pool4 vector) and rank the test images by its magnitude. <ref type="figure">Figure 3</ref> shows the images from Kinetics-Sounds that activate particular units in pool4 the most (i.e. are ranked highest by its magnitude). As can be seen, the vision subnetwork has automatically learnt, without any explicit supervision, to recognize semantic entities such as guitars, accordions, keyboards, clarinets, bowling alleys, lawns or lawnmowers, etc. Furthermore, it has learnt finergrained categories as well as it is able to distinguish between acoustic and bass guitars ("fingerpicking" is mostly associated with acoustic guitars). <ref type="figure" target="#fig_1">Figure 4</ref> shows heatmaps for the Kinetics-Sounds images in <ref type="figure">Figure 3</ref>, obtained by simply displaying the spatial activations of the corresponding vision unit (i.e. if the k component of pool4 is chosen, then the k channel of conv4 2 is displayed -since the k component is just the spatial max over this channel (after ReLU)). Objects are successfully detected despite significant clutter and occlusions. It is interesting to observe the type of cues that the network decides to use, e.g. the "playing clarinet" unit, in-stead of trying to detect the entire clarinet, seems to mostly activate on the interface between the player's face and the clarinet. <ref type="figure">Figures 5 and 6</ref> show visual concepts learnt by the L 3 -Net on the Flickr-SoundNet dataset. It can be seen that the network learns to recognize many scene categories ( <ref type="figure">Figure  5</ref>), such as outdoors, concert, water, sky, crowd, text, railway, etc. These are useful for the AVC task as, for example, crowds indicate a large event that is associated with a distinctive sound as well (e.g. a football game), text indicates narration, and outdoors scenes are likely to be accompanied with wind sounds. It should be noted that though at first sight some categories seem trivially detectable, it is not the case; for example, "sky" detector is not equivalent to the "blueness" detector as it only fires on "sky" and not on "water", and furthermore there are separate units sensitive to "water surface" and to "underwater" scenes. The network also learns to detect people as user uploaded content is substantially people-oriented - <ref type="figure">Figure 6</ref> shows the network has learnt to distinguish between babies, adults and crowds. <ref type="figure">Figure 7</ref> shows what particular audio units are sensitive to in the Kinetics-Sounds dataset. For visualization purposes, instead of showing the sound form, we display the video frame that corresponds to the sound. It can be seen that the audio subnetwork, again without any supervision, manages to learn various semantic entities, as well as perform fine-grained classification ("fingerpicking" vs "playing bass guitar"). Note that some units are naturally confused -the "tap dancing" unit also responds to "pen tapping", while the "saxophone" unit is sometimes confused with a "trombone". These are reasonable mistakes, especially when taking into account that the sound input is only one second in length. The audio concepts learnt on the Flickr-SoundNet dataset <ref type="figure" target="#fig_2">(Figure 8</ref>) follow the same pattern as the visual ones -the network learns to distinguish various scene categories such as water, underwater, outdoors and windy scenes, as well as human-related concepts like baby and human voices, crowds, etc. <ref type="figure" target="#fig_4">Figure 9</ref> shows spectrograms and their semantic heatmaps, illustrating that our L 3 -Net learns to detect audio events. For example, it shows clear preference for low frequencies when detecting bass guitars, attention to wide frequency range when detecting lawnmowers, and temporal 'steps' when detecting fingerpicking and tap dancing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">Audio features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.3">Versus random features</head><p>Could the results in <ref type="figure" target="#fig_1">Figures 3, 4</ref>, 5, 6, 7 8, and 9 simply be obtained by chance due to examining a large number of units, as colourfully illustrated by the dead salmon experiment <ref type="bibr" target="#b2">[3]</ref>? It is unlikely as there are only 512 units in</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Outdoor</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concert</head><p>Outdoor sport</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cloudy sky Sky Water surface Underwater</head><p>Horizon Railway Crowd Text <ref type="figure">Figure 5</ref>. Learnt visual concepts and semantic heatmaps (Flickr-SoundNet). Each mini-column shows five images that most activate a particular unit of the 512 in pool4 of the vision subnetwork, and the corresponding heatmap (for more details see <ref type="figure" target="#fig_1">Figures 3 and 4)</ref>. Column titles are a subjective names of concepts the units respond to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baby</head><p>Face Head Crowd <ref type="figure">Figure 6</ref>. Learnt human-related visual concepts and semantic heatmaps (Flickr-SoundNet). Each mini-column shows five images that most activate a particular unit of the 512 in pool4 of the vision subnetwork, and the corresponding heatmap (for more details see <ref type="figure" target="#fig_1">Figures  3 and 4)</ref>. Column titles are a subjective names of concepts the units respond to.</p><p>Fingerpicking Lawn mowing P. accordion P. bass guitar P. saxophone Typing P. xylophone Tap dancing Tickling <ref type="figure">Figure 7</ref>. Learnt audio concepts (Kinetics-Sounds). Each column shows five sounds that most activate a particular unit in pool4 of the audio subnetwork. Purely for visualization purposes, as it is hard to display sound, the frame of the video that is aligned with the sound is shown instead of the actual sound form, but we stress that no vision is used in this experiment. Videos come from the Kinetics-Sounds test set and the network was trained on the Kinetics-Sounds train set. The top row shows the dominant action label for the unit ("P." stands for "playing").  . Each mini-column shows sounds that most activate a particular unit of the 512 in pool4 of the audio subnetwork. Purely for visualization purposes, as it is hard to display sound, the frame of the video that is aligned with the sound is shown instead of the actual sound form, but we stress that no vision is used in this experiment. Column titles are a subjective names of concepts the units respond to. Note that for the "Human voice", "Male voice", "Crowd", " pool4 to choose from, and many of those were found to be highly correlated with a semantic concept. Nevertheless, we repeated the same experiment with a random network (i.e. a network that has not been trained), and have failed to find such correlation. In more detail, we examined how many out of the action classes in Kinetics-Sounds have a unit in pool4 which shows high preference for the class. For the vision subnetwork the preference is determined by ranking all images by their unit activation, and retaining the top 5; if 4 out of these 5 images correspond to one class, then that class is deemed to have a high-preference for the unit (a similar procedure is carried out for the audio subnetwork using spectrograms). Our trained vision and audio networks have high-preference units for 10 and 11 out of a possible 34 action classes, respectively, compared to 1 and 1 for the random vision and audio networks. Furthermore, if the threshold for deeming a unit to be high-preference is reduced to 3, our trained vision and audio subnetworks cover 23 and 20 classes, respectively, compared to the 4 and 3 of a random network, respectively. These results confirm that our network has indeed learnt semantic features. Furthermore, <ref type="figure">Figure 10</ref> shows the comparison between the trained and the non-trained (i.e. network with random weights) L 3 -Net representations for the visual and the audio modalities, on the Kinetics-Sounds dataset, using the t-SNE visualization <ref type="bibr" target="#b33">[34]</ref>. It is clear that training for the audio-visual correspondence task produces representations that have a semantic meaning, as videos containing the same action classes often cluster together, while the random network's representations do not exhibit any clustering. There is still a fair amount of confusion in the representations, but this is expected as no class-level supervision is provided and classes can be very alike. For example, an organ and a piano are quite visually similar as they contain keyboards, and the visual difference between a bass guitar and an acoustic guitar is also quite fine; these similarities are reflected in the closeness or overlap of respective clusters in <ref type="figure">Figure 10</ref>(c) (e.g. as noted earlier, "fingerpicking" is mostly associated with acoustic guitars).</p><p>We also evaluate the quality of the L 3 -Net embeddings by clustering them with k-means into 64 clusters and reporting the Normalized Mutual Information (NMI) score between the clusters and the ground truth action classes. Results in <ref type="table">Table 4</ref>   the L 3 -Net embeddings outperform the best random baselines by 50-100%. The t-SNE visualization also shows some interesting features, such as the "typing" class being divided into two clusters in the visual domain. Further investigation reveals that all frames in one cluster show both a keyboard and hands, while the second cluster contains much fewer hands. Separating these two cases can be a good indication of whether the typing action is happening at the moment captured by the (frame, 1 second sound clip) pair, and thus whether the typing sound is expected to be heard. Furthermore, we found that the "typing" audio samples appear in three clusters -the two fairly pure clusters (outlined in <ref type="figure">Figure 10</ref>(a)) correspond to strong typing sounds and talking while typing, respectively, and the remaining cluster, which is very impure and intermingled with other action classes, mostly corresponds to silence and background noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion</head><p>We have shown that the network trained for the AVC task achieves superior results on sound classification to recent methods that pre-train and fix the visual networks (one each for ImageNet and Scenes), and we conjecture that the reason for this is that the additional freedom of the visual network allows the learning to better take advantage of the opportunities offered by the variety of visual information in the video (rather than be restricted to seeing only through the eyes of the pre-trained network). Also, the visual features that emerge from the L 3 -Net are on par with the stateof-the-art among self-supervised approaches. Furthermore, it has been demonstrated that the network automatically learns, in both modalities, fine-grained distinctions such as bass versus acoustic guitar or saxophone versus clarinet.</p><p>The localization visualization results are reminiscent of the classic highlighted pixels in <ref type="bibr" target="#b13">[14]</ref>, except in our case we do not just learn the few pixels that move (concurrent with the sound) but instead are able to learn extended regions corresponding to the instrument.</p><p>We motivated this work by considering correlation of video and audio events. However, we believe there is additional information in concurrency of the two streams, as concurrency is stronger than correlation because the events need to be synchronised (of course, if events are concurrent then they will correlate, but not vice versa). Training for concurrency will require video (multiple frames) as input, rather than a single video frame, but it would be interesting to explore what more is gained from this stronger condition.</p><p>In the future, it would be interesting to learn from the recently released large dataset of videos curated according to audio, rather than visual, events <ref type="bibr" target="#b9">[10]</ref> and see what subtle visual semantic categories are discovered.  <ref type="figure">Figure 10</ref>. t-SNE visualization <ref type="bibr" target="#b33">[34]</ref> of learnt representations (Kinetics-Sounds). The (a,c) and (b,d) show the two-dimensional t-SNE embeddings for the trained versus non-trained (i.e. network with random weights) L 3 -Net, respectively. For visualization purposes only, we colour the t-SNE embeddings using the Kinetics-Sounds labels, but no labels were used for training the L 3 -Net. For clarity and reduced clutter, only a subset of actions (13 classes out of 34) is shown. Some clearly noticeable clusters are manually highlighted by enclosing them with ellipses. Best viewed in colour.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Visual semantic heatmap (Kinetics-Sounds). Examples correspond to the ones in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 8 .</head><label>8</label><figDesc>Learnt audio concepts (Flickr-SoundNet)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Music" and "Concert" examples, the respective clips do contain the relevant audio despite the frame looking as if it is unrelated, e.g. the third example in the "Concert" column does contain loud music sounds. Audio clips containing the five concatenated 1s samples corresponding to each mini-column are hosted on YouTube and can be reached by clicking on the respective mini-columns; this YouTube playlist (https://goo.gl/ohDGtJ) contains all 16 examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 .</head><label>9</label><figDesc>Audio semantic heatmaps (Kinetics-Sounds). Each pair of columns shows a single action class (top, "P." stands for "playing"), five log-spectrograms (left) and spectrogram semantic heatmaps (right) for the class. Horizontal and vertical axes correspond to the time and frequency dimensions, respectively. A semantic heatmap is obtained as a slice of activations of the unit from conv4 2 of the audio subnetwork which shows preference for the considered class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .Table 3</head><label>23</label><figDesc>Sound classification. "Ours random" is an additional baseline which shows the performance of our network without L 3training. Our L 3 -training sets the new state-of-the-art by a large margin on both benchmarks.</figDesc><table><row><cell>Method</cell><cell>Top 1 accuracy</cell></row><row><cell>Random</cell><cell>18.3%</cell></row><row><cell>Pathak et al. [24]</cell><cell>22.3%</cell></row><row><cell>Kr?henb?hl et al. [16]</cell><cell>24.5%</cell></row><row><cell>Donahue et al. [7]</cell><cell>31.0%</cell></row><row><cell>Doersch et al. [6]</cell><cell>31.7%</cell></row><row><cell>Zhang et al. [36] (init: [16])</cell><cell>32.6%</cell></row><row><cell>Noroozi and Favaro [21]</cell><cell>34.7%</cell></row><row><cell>Ours random</cell><cell>12.9%</cell></row><row><cell>Ours</cell><cell>32.3%</cell></row></table><note>. Visual classification on ImageNet. Following [36], our features are evaluated by training a linear classifier on the Ima- geNet training set and measuring the classification accuracy on the validation set. For more details and discussions see Section 3.4. All performance numbers apart from ours are provided by au- thors of [36], showing only the best performance for each method over all parameter choices (e.g. Donahue et al. [7] achieve 27.1% instead of 31.0% when taking features from pool5 instead of conv3).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>confirm the emergence of semantics as</figDesc><table><row><cell>Method</cell><cell cols="2">Vision Audio</cell></row><row><cell>Random assignments</cell><cell>0.165</cell><cell>0.165</cell></row><row><cell cols="2">Ours random (L 3 -Net without training) 0.204</cell><cell>0.219</cell></row><row><cell>Ours (L 3 -Net self-supervised training)</cell><cell>0.409</cell><cell>0.330</cell></row></table><note>Table 4. Clustering quality. Normalized Mutual Information (NMI) score between the unsupervised clusterings of feature em- beddings and the Kinetics-Sounds labels.</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to see by moving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SoundNet: Learning sound representations from unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural correlates of interspecies perspective taking in the post-mortem Atlantic salmon: An argument for multiple comparisons correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Wolford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational learning theory</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Out of time: Automated lip sync in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Multi-view Lip-reading, ACCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Audio Set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised learning of spoken language with visual context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Harwath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The Kinetics human action video dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1705.06950</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pixels that sound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kidron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Schechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Datadependent initializations of convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Predicting deep zero-shot convolutional neural networks using textual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Auditory scene classification using machine learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Toub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Shuffle and learn: Unsupervised learning using temporal order verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Herbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visually indicated sounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ambient sound provides supervision for visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiajun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Environmental sound classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Piczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Machine Learning for Signal processing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">ESC: Dataset for environmental sound classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Piczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACMM</title>
		<meeting>ACMM</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Histogram of gradients of time-frequency representations for audio scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rakotomamonjy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gasso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recurrence quantification analysis features for environmental sound recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Roma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Zero-shot learning through cross-modal transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ganjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Detection and classification of acoustic scenes and events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stowell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Giannoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Benetos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lagrange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Rethinking the Inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Kinetics-Sounds The 34 action classes taken from the Kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">chopping wood, ripping paper, shuffling cards, singing, tapping pen, typing, blowing out, dribbling ball, laughing, mowing the lawn by pushing lawnmower, shoveling snow, stomping, tap dancing, tapping guitar, tickling, fingerpicking, patting, playing accordion, playing bagpipes, playing bass guitar, playing clarinet, playing drums, playing guitar, playing harmonica, playing keyboard, playing organ, playing piano, playing saxophone</title>
	</analytic>
	<monogr>
		<title level="m">Sounds dataset 3.1 are: blowing nose, bowling</title>
		<imprint/>
	</monogr>
	<note>playing trombone, playing trumpet, playing violin, playing xylophone</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
