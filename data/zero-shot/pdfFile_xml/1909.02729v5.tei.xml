<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A BASELINE FOR FEW-SHOT IMAGE CLASSIFICATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guneet</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Amazon Web Services</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratik</forename><surname>Chaudhari</surname></persName>
							<email>pratikac@seas.upenn.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Amazon Web Services</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
							<email>soattos@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="department">Amazon Web Services</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A BASELINE FOR FEW-SHOT IMAGE CLASSIFICATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2020</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fine-tuning a deep network trained with the standard cross-entropy loss is a strong baseline for few-shot learning. When fine-tuned transductively, this outperforms the current state-of-the-art on standard datasets such as Mini-ImageNet, Tiered-ImageNet, CIFAR-FS and FC-100 with the same hyper-parameters. The simplicity of this approach enables us to demonstrate the first few-shot learning results on the ImageNet-21k dataset. We find that using a large number of meta-training classes results in high few-shot accuracies even for a large number of few-shot classes. We do not advocate our approach as the solution for few-shot learning, but simply use the results to highlight limitations of current benchmarks and few-shot protocols. We perform extensive studies on benchmark datasets to propose a metric that quantifies the "hardness" of a few-shot episode. This metric can be used to report the performance of few-shot algorithms in a more systematic way.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Are we making progress? The box-plot illustrates the performance of state-of-the-art few-shot algorithms on the Mini-ImageNet <ref type="bibr" target="#b51">(Vinyals et al., 2016)</ref> dataset for the 1-shot 5-way protocol. The boxes show the ? 25% quantiles of the accuracy while the notches indicate the median and its 95% confidence interval. Whiskers denote the 1.5? interquartile range which captures 99.3% of the probability mass for a normal distribution. The spread of the box-plots are large, indicating that the standard deviations of the few-shot accuracies is large too. This suggests that progress may be illusory, especially considering that none outperform the simple transductive fine-tuning baseline discussed in this paper (rightmost).</p><p>As image classification systems begin to tackle more and more classes, the cost of annotating a massive number of images and the difficulty of procuring images of rare categories increases. This has fueled interest in few-shot learning, where only few labeled samples per class are available for training. <ref type="figure">Fig. 1</ref> displays a snapshot of the state-of-the-art. We estimated this plot by using published numbers for the estimate of the mean accuracy, the 95% confidence interval of this estimate and the number of few-shot episodes. For MAML <ref type="bibr" target="#b9">(Finn et al., 2017)</ref> and MetaOpt SVM , we use the number of episodes in the author's Github implementation.</p><p>The field appears to be progressing steadily albeit slowly based on <ref type="figure">Fig. 1</ref>. However, the variance of the estimate of the mean accuracy is not the same as the variance of the accuracy. The former can be zero (e.g., asymptotically for an unbiased estimator), yet the latter could be arbitrarily large. The variance of the accuracies is extremely large in <ref type="figure">Fig. 1</ref>. This suggests that progress in the past few years may be less significant than it seems if one only looks at the mean accuracies. To compound the problem, many algorithms report results using different models for different number of ways (classes) and shots (number of labeled samples per class), with aggressive hyper-parameter optimization. 1 Our goal is to develop a simple baseline for few-shot learning, one that does not require specialized training depending on the number of ways or shots, nor hyper-parameter tuning for different protocols.</p><p>The simplest baseline we can think of is to pre-train a model on the meta-training dataset using the standard cross-entropy loss, and then fine-tune on the few-shot dataset. Although this approach is basic and has been considered before <ref type="bibr" target="#b51">(Vinyals et al., 2016;</ref><ref type="bibr" target="#b5">Chen et al., 2018)</ref>, it has gone unnoticed that it outperforms many sophisticated few-shot algorithms. Indeed, with a small twist of performing fine-tuning transductively, this baseline outperforms all state-of-the-art algorithms on all standard benchmarks and few-shot protocols (cf. <ref type="table" target="#tab_1">Table 1</ref>).</p><p>Our contribution is to develop a transductive fine-tuning baseline for few-shot learning, our approach works even for a single labeled example and a single test datum per class. Our baseline outperforms the state-of-the-art on a variety of benchmark datasets such as Mini-ImageNet <ref type="bibr" target="#b51">(Vinyals et al., 2016)</ref>, Tiered-ImageNet <ref type="bibr" target="#b40">(Ren et al., 2018)</ref>, CIFAR-FS  and FC-100 , all with the same hyper-parameters. Current approaches to few-shot learning are hard to scale to large datasets. We report the first few-shot learning results on the ImageNet-21k dataset <ref type="bibr" target="#b8">(Deng et al., 2009</ref>) which contains 14.2 million images across 21,814 classes. The rare classes in ImageNet-21k form a natural benchmark for few-shot learning.</p><p>The empirical performance of this baseline, should not be understood as us suggesting that this is the right way of performing few-shot learning. We believe that sophisticated meta-training, understanding taxonomies and meronomies, transfer learning, and domain adaptation are necessary for effective few-shot learning. The performance of the simple baseline however indicates that we need to interpret existing results 2 with a grain of salt, and be wary of methods that tailor to the benchmark. To facilitate that, we propose a metric to quantify the hardness of few-shot episodes and a way to systematically report performance for different few-shot protocols.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PROBLEM DEFINITION AND RELATED WORK</head><p>We first introduce some notation and formalize the few-shot image classification problem. Let (x, y) denote an image and its ground-truth label respectively. The training and test datasets are</p><formula xml:id="formula_0">D s = {(x i , y i )} Ns i=1 and D q = {(x i , y i )} Nq i=1</formula><p>respectively, where y i ? C t for some set of classes C t . In the few-shot learning literature, training and test datasets are referred to as support and query datasets respectively, and are collectively called a few-shot episode. The number of ways, or classes, is |C t |. The set {x i | y i = k, (x i , y i ) ? D s } is the support of class k and its cardinality is s support shots (this is non-zero and is generally shortened to shots). The number s is small in the few-shot setting. The set {x i | y i = k, (x i , y i ) ? D q } is the query of class k and its cardinality is q query shots. The goal is to learn a function F to exploit the training set D s to predict the label of a test datum x, 1 For instance,  tune for different few-shot protocols, with parameters changing by up to six orders of magnitude;  use a different query shot for different few-shot protocols.</p><p>2 For instance, <ref type="bibr" target="#b51">Vinyals et al. (2016)</ref>; <ref type="bibr" target="#b38">Ravi &amp; Larochelle (2016)</ref> use different versions of Mini-ImageNet;  report results for meta-training on the training set while  use both the training and validation sets; <ref type="bibr" target="#b5">Chen et al. (2018)</ref> use full-sized images from the parent ImageNet-1k dataset <ref type="bibr" target="#b8">(Deng et al., 2009)</ref> where (x, y) ? D q , by? = F (x; D s ).</p><p>(1)</p><p>Typical approaches for supervised learning replace D s above with a statistic, ? * = ? * (D s ) that is, ideally, sufficient to classify D s , as measured by, say, the cross-entropy loss</p><formula xml:id="formula_1">? * (D s ) = arg min ? 1 N s (x,y)?Ds ? log p ? (y|x),<label>(2)</label></formula><p>where p ? (?|x) is the probability distribution on C t as predicted by the model in response to input x.</p><p>When presented with a test datum, the classification rule is typically chosen to be of the form</p><formula xml:id="formula_2">F ? * (x; D s ) arg max k p ? * (k|x),<label>(3)</label></formula><p>where D s is represented by ? * . This form of the classifier entails a loss of generality unless ? * is a sufficient statistic, p ? * (y|x) = p(y|x), which is of course never the case, especially given few labeled data in D s . However, it conveniently separates training and inference phases, never having to revisit the training set. This might be desirable in ordinary image classification, but not in few-shot learning.</p><p>We therefore adopt the more general form of F in (1).</p><p>If we call the test datum x = x Ns +1 , then we can obtain the general form of the classifier b?</p><formula xml:id="formula_3">y = F (x; D s ) = arg min y Ns +1 min ? 1 N s + 1 Ns+1 i=1 ? log p ? (y i |x i ).<label>(4)</label></formula><p>In addition to the training set, one typically also has a meta-training set,</p><formula xml:id="formula_4">D m = {(x i , y i )} Nm i=1 , where y i ? C m , with set of classes C m disjoint from C t .</formula><p>The goal of meta-training is to use D m to infer the parameters of the few-shot learning model:?(D m ; (D s , D q )) = arg min ? 1 Nm (x,y)?Dm (y, F ? (x; (D s , D q ))), where meta-training loss depends on the method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">RELATED WORK</head><p>Learning to learn: The meta-training loss is designed to make few-shot training efficient <ref type="bibr" target="#b49">(Utgoff, 1986;</ref><ref type="bibr" target="#b43">Schmidhuber, 1987;</ref><ref type="bibr" target="#b1">Baxter, 1995;</ref><ref type="bibr" target="#b47">Thrun, 1998)</ref>. This approach partitions the problem into a base-level that performs standard supervised learning and a meta-level that accrues information from the base-level. Two main approaches have emerged to do so.</p><p>Gradient-based approaches: These approaches treat the updates of the base-level as a learnable mapping <ref type="bibr" target="#b2">(Bengio et al., 1992)</ref>. This mapping can be learnt using temporal models <ref type="bibr" target="#b16">(Hochreiter et al., 2001;</ref><ref type="bibr" target="#b38">Ravi &amp; Larochelle, 2016)</ref>, or one can back-propagate the gradients across the base-level updates <ref type="bibr" target="#b30">(Maclaurin et al., 2015;</ref><ref type="bibr" target="#b9">Finn et al., 2017)</ref>. It is challenging to perform this dual or bi-level optimization, respectively. These approaches have not been shown to be competitive on large datasets. Recent approaches learn the base-level in closed-form using SVMs  which restricts the capacity of the base-level although it alleviates the optimization problem.</p><p>Metric-based approaches: A majority of the state-of-the-art algorithms are metric-based approaches. These approaches learn an embedding that can be used to compare <ref type="bibr" target="#b4">(Bromley et al., 1994;</ref><ref type="bibr" target="#b6">Chopra et al., 2005)</ref> or cluster <ref type="bibr" target="#b51">(Vinyals et al., 2016;</ref> query samples. Recent approaches build upon this idea with increasing levels of sophistication in learning the embedding <ref type="bibr" target="#b51">(Vinyals et al., 2016;</ref><ref type="bibr" target="#b12">Gidaris &amp; Komodakis, 2018;</ref>, creating exemplars from the support set and picking a metric for the embedding <ref type="bibr" target="#b12">(Gidaris &amp; Komodakis, 2018;</ref><ref type="bibr" target="#b0">Allen et al., 2018;</ref>. There are numerous hyper-parameters involved in implementing these approaches which makes it hard to evaluate them systematically <ref type="bibr" target="#b5">(Chen et al., 2018)</ref>.</p><p>Transductive learning: This approach is more efficient at using few labeled data than supervised learning <ref type="bibr" target="#b21">(Joachims, 1999;</ref><ref type="bibr" target="#b55">Zhou et al., 2004;</ref><ref type="bibr" target="#b50">Vapnik, 2013)</ref>. The idea is to use information from the test datum x to restrict the hypothesis space while searching for the classifier F (x, D s ) at test time. Our approach is closest to this line of work. We train a model on the meta-training set D m and initialize a classifier using the support set D s . The parameters are then fine-tuned to adapt to the new test datum x.</p><p>There are recent papers in few-shot learning such as ;  that are motivated from transductive learning and exploit the unlabeled query samples. The former updates batch-normalization parameters using query samples while the latter uses label propagation to estimate labels of all query samples at once.</p><p>Semi-supervised learning: We penalize the Shannon Entropy of the predictions on the query samples at test time. This is a simple technique in the semi-supervised learning literature, closest to <ref type="bibr" target="#b13">Grandvalet &amp; Bengio (2005)</ref>. Modern augmentation techniques such as <ref type="bibr" target="#b32">Miyato et al. (2015)</ref>; <ref type="bibr" target="#b42">Sajjadi et al. (2016)</ref>; <ref type="bibr" target="#b7">Dai et al. (2017)</ref> or graph-based approaches <ref type="bibr" target="#b23">(Kipf &amp; Welling, 2016)</ref> can also be used with our approach; we used the entropic penalty for the sake of simplicity.</p><p>Semi-supervised few-shot learning is typically formulated as having access to extra unlabeled data during meta-training or few-shot training <ref type="bibr" target="#b11">(Garcia &amp; Bruna, 2017;</ref><ref type="bibr" target="#b40">Ren et al., 2018)</ref>. This is different from our approach which uses the unlabeled query samples for transductive learning.</p><p>Initialization for fine-tuning: We use recent ideas from the deep metric learning literature <ref type="bibr" target="#b18">(Hu et al., 2015;</ref><ref type="bibr" target="#b33">Movshovitz-Attias et al., 2017;</ref><ref type="bibr" target="#b36">Qi et al., 2018;</ref><ref type="bibr" target="#b5">Chen et al., 2018;</ref><ref type="bibr" target="#b12">Gidaris &amp; Komodakis, 2018)</ref> to initialize the meta-trained model for fine-tuning. These works connect the softmax cross-entropy loss with cosine distance and are discussed further in Section 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">APPROACH</head><p>The simplest form of meta-training is pre-training with the cross-entropy loss, which yield?</p><formula xml:id="formula_5">? = arg min ? 1 N m (x,y)?Dm ? log p ? (y|x) + R(?),<label>(5)</label></formula><p>where the second term denotes a regularizer, say weight decay R(?) = ? 2 /2. The model predicts logits z k (x; ?) for k ? C m and the distribution p ? (?|x) is computed from these logits using the softmax operator. This loss is typically minimized by stochastic gradient descent-based algorithms.</p><p>If few-shot training is performed according to the general form in (4), then the optimization is identical to that above and amounts to fine-tuning the pre-trained model. However, the model needs to be modified to account for the new classes. Careful initialization can make this process efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SUPPORT-BASED INITIALIZATION</head><p>Given the pre-trained model (called the "backbone"), p ? (dropping the hat from?), we append a new fully-connected "classifier" layer that takes the logits of the backbone as input and predicts the labels in C t . For a support sample (x, y), denote the logits of the backbone by z(x; ?) ? R |Cm| ; the weights and biases of the classifier by w ? R |Ct|?|Cm| and b ? R |Ct| respectively; and the k th row of w and b by w k and b k respectively. The ReLU non-linearity is denoted by (?) + .</p><p>If the classifier's logits are z = wz(x; ?) + + b, the first term in the cross-entropy loss: ? log p ? (y|x) = ?w y z(x; ?) + ? b y + log k e w k z(x;?)++b k would be the cosine distance between w y and z(x; ?) + if both were normalized to unit 2 norm and bias b y = 0. This suggests</p><formula xml:id="formula_6">w y = z(x; ?) + z(x; ?) + and b y = 0<label>(6)</label></formula><p>as a candidate for initializing the classifier, along with normalizing z(x; ?) + to unit 2 norm. It is easy to see that this maximizes the cosine similarity between features z(x; ?) + and weights w y . For multiple support samples per class, we take the Euclidean average of features z(x; ?) + for each class in C t , before 2 normalization in <ref type="bibr" target="#b69">(6)</ref>. The logits of the classifier are thus given by</p><formula xml:id="formula_7">R |Ct| z(x; ?) = w z(x; ?) + z(x; ?) + + b,<label>(7)</label></formula><p>where ? = {?, w, b}, the combined parameters of the backbone and the classifier. Note that we have added a ReLU non-linearity between the backbone and the classifier, before the 2 normalization. All the parameters ? are trainable in the fine-tuning phase.</p><p>Remark 1 (Relation to weight imprinting). The support-based initialization is motivated from previous papers <ref type="bibr" target="#b18">(Hu et al., 2015;</ref><ref type="bibr" target="#b33">Movshovitz-Attias et al., 2017;</ref><ref type="bibr" target="#b5">Chen et al., 2018;</ref><ref type="bibr" target="#b12">Gidaris &amp; Komodakis, 2018)</ref>. In particular, <ref type="bibr" target="#b36">Qi et al. (2018)</ref> use a similar technique, with minor differences, to expand the size of the final fully-connected layer (classifier) for low-shot continual learning. The authors call their technique "weight imprinting" because w k can be thought of as a template for class k. In our case, we are only interested in performing well on the few-shot classes.</p><p>Remark 2 (Using logits of the backbone instead of features as input to the classifier). A natural way to adapt the backbone to predict new classes is to re-initialize its final fully-connected layer (classifier). We instead append a new classifier after the logits of the backbone. This is motivated from <ref type="bibr" target="#b10">Frosst et al. (2019)</ref> who show that for a trained backbone, outputs of all layers are entangled, without class-specific clusters; but the logits are peaked on the correct class, and are therefore well-clustered. The logits are thus better inputs to the classifier as compared to the features. We explore this choice via an experiment in Appendix C.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">TRANSDUCTIVE FINE-TUNING</head><p>In <ref type="formula" target="#formula_3">(4)</ref>, we assumed that there is a single query sample. However, we can also process multiple query samples together, and perform the minimization over all unknown query labels. We introduce a regularizer, similar to <ref type="bibr" target="#b13">Grandvalet &amp; Bengio (2005)</ref>, as we seek outputs with a peaked posterior, or low Shannon Entropy H. So the transductive fine-tuning phase solves for</p><formula xml:id="formula_8">? * = arg min ? 1 N s (x,y)?Ds ? log p ? (y | x) + 1 N q (x,y)?Dq H(p ? (? | x)).<label>(8)</label></formula><p>Note that the data fitting term uses the labeled support samples whereas the regularizer uses the unlabeled query samples. The two terms can be highly imbalanced (due to the varying range of values for the two quantities, or due to the variance in their estimates which depend on N s and N q ). To allow finer control on this imbalance, one can use a coefficient for the entropic term and/or a temperature in the softmax distribution of the query samples. Tuning these hyper-parameters per dataset and few-shot protocol leads to uniform improvements in the results in Section 4 by 1-2%. However, we wish to keep in line with our goal of developing a simple baseline and refrain from optimizing these hyper-parameters, and set them equal to 1 for all experiments on benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL RESULTS</head><p>We show results of transductive fine-tuning on benchmark datasets in few-shot learning, namely Mini-ImageNet <ref type="bibr" target="#b51">(Vinyals et al., 2016)</ref>, Tiered-ImageNet <ref type="bibr" target="#b40">(Ren et al., 2018)</ref>, CIFAR-FS  and FC-100 , in Section 4.1. We also show large-scale experiments on the ImageNet-21k dataset <ref type="bibr" target="#b8">(Deng et al., 2009)</ref> in Section 4.2. Along with the analysis in Section 4.3, these help us design a metric that measures the hardness of an episode in Section 4.4. We sketch key points of the experimental setup here; see Appendix A for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-training:</head><p>We use the WRN-28-10 (Zagoruyko &amp; Komodakis, 2016) model as the backbone. We pre-train using standard data augmentation, cross-entropy loss with label smoothing <ref type="bibr" target="#b46">(Szegedy et al., 2016)</ref> of =0.1, mixup regularization <ref type="bibr" target="#b54">(Zhang et al., 2017)</ref> of ?=0.25, SGD with batch-size of 256, Nesterov's momentum of 0.9, weight-decay of 10 ?4 and no dropout. We use batch-normalization <ref type="bibr" target="#b19">(Ioffe &amp; Szegedy, 2015)</ref> but exclude its parameters from weight decay . We use cyclic learning rates (Smith, 2017) and half-precision distributed training on 8 GPUs <ref type="bibr" target="#b17">(Howard et al., 2018)</ref> to reduce training time.</p><p>Each dataset has a training, validation and test set consisting of disjoint sets of classes. Some algorithms use only the training set as the meta-training set , while others use both training and validation sets . For completeness we report results using both methodologies; the former is denoted as (train) while the latter is denoted as (train + val). All experiments in Sections 4.3 and 4.4 use the (train + val) setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-tuning:</head><p>We perform fine-tuning on one GPU in full-precision for 25 epochs and a fixed learning rate of 5 ? 10 ?5 with Adam (Kingma &amp; Ba, 2014) without any regularization. We make two weight updates in each epoch: one for the cross-entropy term using support samples and one for the Shannon Entropy term using query samples (cf. <ref type="formula" target="#formula_8">(8)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyper-parameters:</head><p>We used images from ImageNet-1k belonging to the training classes of Mini-ImageNet as the validation set for pre-training the backbone for Mini-ImageNet. We used the validation set of Mini-ImageNet to choose hyper-parameters for fine-tuning. All hyper-parameters are kept constant for experiments on benchmark datasets.</p><p>Evaluation: Few-shot episodes contain classes sampled uniformly from classes in the test sets of the respective datasets; support and query samples are further sampled uniformly for each class; the query shot is fixed to 15 for all experiments unless noted otherwise. All networks are evaluated over 1,000 few-shot episodes unless noted otherwise. To enable easy comparison with existing literature, we report an estimate of the mean accuracy and the 95% confidence interval of this estimate. However, we encourage reporting the standard deviation in light of Section 1 and <ref type="figure">Fig. 1</ref>.  <ref type="table" target="#tab_1">Table 1</ref> shows the results of transductive fine-tuning on benchmark datasets for standard few-shot protocols. We see that this simple baseline is uniformly better than state-of-the-art algorithms. We include results for support-based initialization, which does no fine-tuning; and for fine-tuning, which involves optimizing only the cross-entropy term in <ref type="formula" target="#formula_8">(8)</ref> using the labeled support samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">RESULTS ON BENCHMARK DATASETS</head><p>The support-based initialization is sometimes better than or comparable to state-of-the-art algorithms (marked ? ). The few-shot literature has gravitated towards larger backbones . Our results indicate that for large backbones even standard cross-entropy pre-training and support-based initialization work well, similar to observation made by <ref type="bibr" target="#b5">Chen et al. (2018)</ref>.</p><p>For the 1-shot 5-way setting, fine-tuning using only the labeled support examples leads to minor improvement over the initialization, and sometimes marginal degradation. However, for the 5-shot 5-way setting non-transductive fine-tuning is better than the state-of-the-art.</p><p>In both (train) and (train + val) settings, transductive fine-tuning leads to 2-7% improvement for 1-shot 5-way setting over the state-of-the-art for all datasets. It results in an increase of 1.5-4% for the 5-shot 5-way setting except for the Mini-ImageNet dataset, where the performance is matched. This suggests that the use of the unlabeled query samples is vital for the few-shot setting.</p><p>For the Mini-ImageNet, CIFAR-FS and FC-100 datasets, using additional data from the validation set to pre-train the backbone results in 2-8% improvements; the improvement is smaller for Tiered-ImageNet. This suggests that having more pre-training classes leads to improved few-shot performance as a consequence of a better embedding. See Appendix C.5 for more experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">LARGE-SCALE FEW-SHOT LEARNING</head><p>The ImageNet-21k dataset <ref type="bibr" target="#b8">(Deng et al., 2009</ref>) with 14.2M images across 21,814 classes is an ideal large-scale few-shot learning benchmark due to the high class imbalance. The simplicity of our approach allows us to present the first few-shot learning results on this large dataset. We use the 7,491 classes having more than 1,000 images each as the meta-training set and the next 13,007 classes with at least 10 images each for constructing few-shot episodes. See Appendix B for details.  <ref type="table" target="#tab_2">Table 2</ref> shows the mean accuracy of transductive fine-tuning evaluated over 80 few-shot episodes on ImageNet-21k. The accuracy is extremely high as compared to corresponding results in <ref type="table" target="#tab_1">Table 1</ref> even for large way. E.g., the 1-shot 5-way accuracy on Tiered-ImageNet is 72.87 ? 0.71% while it is 89 ? 1.86% here. This corroborates the results in Section 4.1 and indicates that pre-training with a large number of classes may be an effective strategy to build large-scale few-shot learning systems.</p><p>The improvements of transductive fine-tuning are minor for ImageNet-21k because the support-based initialization accuracies are extremely high. We noticed a slight degradation of accuracies due to transductive fine-tuning at high ways because the entropic term in <ref type="formula" target="#formula_8">(8)</ref> is much larger than the the cross-entropy loss. The experiments for ImageNet-21k therefore scale down the entropic term by log |C t | and forego the ReLU in <ref type="formula" target="#formula_6">(6)</ref> and <ref type="formula" target="#formula_7">(7)</ref>. This reduces the difference in accuracies at high ways.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">ANALYSIS</head><p>This section presents a comprehensive analysis of transductive fine-tuning on the Mini-ImageNet, Tiered-ImageNet and ImageNet-21k datasets.</p><p>Robustness of transductive fine-tuning to query shot: <ref type="figure" target="#fig_1">Fig. 2a</ref> shows the effect of changing the query shot on the mean accuracy. For the 1-shot 5-way setting, the entropic penalty in (8) helps as the query shot increases. This effect is minor in the 5-shot 5-way setting as more labeled data is available. Query shot of 1 achieves a relatively high mean accuracy because transductive fine-tuning can adapt to those few queries. One query shot is enough to benefit from transductive fine-tuning: for Mini-ImageNet, the 1-shot 5-way accuracy with query shot of 1 is 66.94 ? 1.55% which is better than non-transductive fine-tuning (59.62 ? 0.66% in <ref type="table" target="#tab_1">Table 1</ref>) and higher than other approaches.</p><p>Performance for different way and support shot: A few-shot system should be able to robustly handle different few-shot scenarios. <ref type="figure" target="#fig_1">Figs. 2b and 2c</ref>, show the performance of transductive fine-tuning  <ref type="figure" target="#fig_1">Fig. 2a</ref> shows that the mean accuracy improves with query shot if the support shot is low; this effect is minor for Tiered-ImageNet. The mean accuracy for query shot of 1 is high because transductive fine-tuning can specialize to those queries. <ref type="figure" target="#fig_1">Fig. 2b</ref> shows that the mean accuracy degrades logarithmically with way for fixed support shot and query shot (15). <ref type="figure" target="#fig_1">Fig. 2c</ref> suggests that the mean accuracy improves logarithmically with the support shot for fixed way and query shot (15). These trends suggest thumb rules for building few-shot systems.</p><p>with changing way and support shot. The mean accuracy changes logarithmically with the way and support shot which provides thumb rules for building few-shot systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Different backbone architectures:</head><p>We include experiments using conv (64) ?4 <ref type="bibr" target="#b51">(Vinyals et al., 2016)</ref> and ResNet-12 <ref type="bibr" target="#b14">(He et al., 2016a;</ref> in <ref type="table">Table 3</ref>, in order to facilitate comparisons for different backbone architectures. The results for transductive fine-tuning are comparable or better than state-of-the-art for a given backbone architecture, except for those in  who use a more sophisticated transductive algorithm using graph propagation, with conv (64) ?4 . In line with our goal for simplicity, we kept the hyper-parameters for pre-training and fine-tuning the same as the ones used for WRN-28-10 (cf. Sections 3 and 4). These results show that transductive fine-tuning is a sound baseline for a variety of backbone architectures.</p><p>Computational complexity: There is no free lunch and our advocated baseline has its limitations. It performs gradient updates during the fine-tuning phase which makes it slow at inference time. Specifically, transductive fine-tuning is about 300? slower (20.8 vs. 0.07 seconds) for a 1-shot 5-way episode with 15 query shot as compared to  with the same backbone architecture (prototypical networks ) do not update model parameters at inference time). The latency factor reduces with higher support shot. Interestingly, for a single query shot, the former takes 4 seconds vs. 0.07 seconds. This is a more reasonable factor of 50?, especially considering that the mean accuracy of the former is 66.2% compared to about 58% of the latter in our implementation. Experiments in Appendix C.3 suggest that using a smaller backbone architecture partially compensates for the latency with some degradation of accuracy. A number of approaches such as Ravi &amp; Larochelle <ref type="formula" target="#formula_1">(2016)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">A PROPOSAL FOR REPORTING FEW-SHOT CLASSIFICATION PERFORMANCE</head><p>As discussed in Section 1, we need better metrics to report the performance of few-shot algorithms. There are two main issues: (i) standard deviation of the few-shot accuracy across different sampled episodes for a given algorithm, dataset and few-shot protocol is very high (cf. <ref type="figure">Fig. 1</ref>), and (ii) different models and hyper-parameters for different few-shot protocols makes evaluating algorithmic contributions difficult (cf. <ref type="table" target="#tab_1">Table 1</ref>). This section takes a step towards resolving these issues.</p><p>Hardness of an episode: Classification performance on a few-shot episode is determined by the relative location of the features corresponding to labeled and unlabeled samples. If the unlabeled features are close to the labeled features from the same class, the classifier can distinguish between the classes easily to obtain a high accuracy. Otherwise, the accuracy would be low. The following definition characterizes this intuition.</p><p>For training (support) set D s and test (query) set D q , we will define the hardness ? ? as the average log-odds of a test datum being classified incorrectly. More precisely,</p><formula xml:id="formula_9">? ? (D q ; D s ) = 1 N q (x,y)?Dq log 1 ? p(y | x) p(y | x) ,<label>(9)</label></formula><p>where p(?| x) is a softmax distribution with logits z y = w?(x). w is the weight matrix constructed using <ref type="formula" target="#formula_6">(6)</ref> and D s ; and ? is the 2 normalized logits computed using a rich-enough feature generator, say a deep network trained for standard image classification. This is a clustering loss where the labeled support samples form class-specific cluster centers. The cluster affinities are calculated using cosine-similarities, followed by the softmax operator to get the probability distribution p(?| x).</p><p>Note that ? ? does not depend on the few-shot learner and gives a measure of how difficult the classification problem is for any few-shot episode, using a generic feature extractor.   <ref type="bibr">(5, 10, 20, 40, 80 and 160)</ref> and support shots (1 and 5). Abscissae are computed using (9) and a Resnet-152 <ref type="bibr" target="#b15">(He et al., 2016b)</ref> network trained for standard image classification on the ImageNet-1k dataset. Each marker indicates the accuracy of transductive fine-tuning on a few-shot episode; markers for support-based initialization are hidden to avoid clutter. Shape of the markers denotes different ways; ways increase from left to right <ref type="bibr">(5, 10, 20, 40, 80 and 160)</ref>. Size of the markers denotes different support shot (1 and 5); it increases from the bottom to the top. E.g., the ellipse contains accuracies of different 5-shot 10-way episodes for ImageNet-21k. Regression lines are drawn for each algorithm and dataset by combining the episodes of all few-shot protocols. This plot is akin to a precision-recall curve and allows comparing two algorithms for different few-shot scenarios. The areas in the first quadrant under the fitted regression lines are 295 vs. 284 (CIFAR-FS), 167 vs. 149 (FC-100), 208 vs. 194 (Mini-ImageNet), 280 vs. 270 (Tiered-ImageNet) and 475 vs. 484 (ImageNet-21k) for transductive fine-tuning and support-based initialization. <ref type="figure" target="#fig_4">Fig. 3</ref> demonstrates how to use the hardness metric. Few-shot accuracy degrades linearly with hardness. Performance for all hardness can thus be estimated by testing for two different ways. We advocate selecting hyper-parameters using the area under the fitted curve as a metric instead of tuning them specifically for each few-shot protocol. The advantage of such a test methodology is that it predicts the performance of the model across multiple few-shot protocols systematically.</p><p>Different algorithms can be compared directly, e.g., transductive fine-tuning (solid lines) and support-based initialization (dotted lines). For instance, the former leads to large improvements on easy episodes, the performance is similar for hard episodes, especially for Tiered-ImageNet and ImageNet-21k.</p><p>The high standard deviation of accuracy of few-shot learning algorithms in <ref type="figure">Fig. 1</ref> can be seen as the spread of the cluster corresponding to each few-shot protocol, e.g., the ellipse in <ref type="figure" target="#fig_4">Fig. 3</ref> denotes the 5-shot 10-way protocol for ImageNet-21k. It is the nature of few-shot learning that episodes have varying hardness even if the way and shot are fixed. However, episodes within the ellipse lie on a different line (with a large negative slope) which indicates that given a few-shot protocol, hardness is a good indicator of accuracy. <ref type="figure" target="#fig_4">Fig. 3</ref> also shows that due to fewer test classes, CIFAR-FS, FC-100 and Mini-ImageNet have less diversity in the hardness of episodes while Tiered-ImageNet and ImageNet-21k allow sampling of both very hard and very easy diverse episodes. For a given few-shot protocol, the hardness of episodes in the former three is almost the same as that of the latter two datasets. This indicates that CIFAR-FS, FC-100 and Mini-ImageNet may be good benchmarks for applications with few classes.</p><p>The hardness metric in <ref type="bibr" target="#b72">(9)</ref> naturally builds upon existing ideas in deep metric learning <ref type="bibr" target="#b36">(Qi et al., 2018)</ref>. We propose it as a means to evaluate few-shot learning algorithms uniformly across different few-shot protocols for different datasets; ascertaining its efficacy and comparisons to other metrics will be part of future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION</head><p>Our aim is to provide grounding to the practice of few-shot learning. The current literature is in the spirit of increasingly sophisticated approaches for modest improvements in mean accuracy using an inadequate evaluation methodology. This is why we set out to establish a baseline, namely transductive fine-tuning, and a systematic evaluation methodology, namely the hardness metric. We would like to emphasize that our advocated baseline, namely transductive fine-tuning, is not novel and yet performs better than existing algorithms on all standard benchmarks. This is indeed surprising and indicates that we need to take a step back and re-evaluate the status quo in few-shot learning. We hope to use the results in this paper as guidelines for the development of new algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A SETUP A.1 DATASETS</head><p>We use the following datasets for our benchmarking experiments.</p><p>? The Mini-ImageNet dataset <ref type="bibr" target="#b51">(Vinyals et al., 2016)</ref> which is a subset of ImageNet-1k <ref type="bibr" target="#b8">(Deng et al., 2009)</ref>  ? We also consider two smaller CIFAR-100 <ref type="bibr" target="#b24">(Krizhevsky &amp; Hinton, 2009</ref>) derivatives, both with 32 ? 32 sized images and 600 images per class. The first is the CIFAR-FS dataset  which splits classes randomly into 64 training, 16 validation and 20 test. The second is the FC-100 dataset  which splits CIFAR-100 into 60 training, 20 validation and 20 test classes with minimal semantic overlap.</p><p>Each dataset has a training, validation and test set. The set of classes for each of these sets are disjoint from each other. For meta-training, we ran two sets of experiments: the first, where we only use the training set as the meta-training dataset, denoted by (train); the second, where we use both the training and validation sets as the meta-training dataset, denoted by (train + val). We use the test set to construct few-shot episodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 PRE-TRAINING</head><p>We use a wide residual network <ref type="bibr" target="#b53">(Zagoruyko &amp; Komodakis, 2016;</ref> with a widening factor of 10 and a depth of 28 which we denote as WRN-28-10. The smaller networks: conv (64) ?4 <ref type="bibr" target="#b51">(Vinyals et al., 2016;</ref>, ResNet-12 <ref type="bibr" target="#b14">(He et al., 2016a;</ref> and <ref type="bibr">WRN-16-4 (Zagoruyko &amp; Komodakis, 2016)</ref>, are used for analysis in Appendix C. All networks are trained using SGD with a batch-size of 256, Nesterov's momentum set to 0.9, no dropout, weight decay of 10 ?4 . We use batch-normalization <ref type="bibr" target="#b19">(Ioffe &amp; Szegedy, 2015)</ref>. We use two-cycles of learning rate annealing (Smith, 2017), these are 40 and 80 epochs each for all datasets except ImageNet-21k, which uses cycles of 8 and 16 epochs each. The learning rate is set to 10 ?i at the beginning of the i th cycle and decreased to 10 ?6 by the end of that cycle with a cosine schedule <ref type="bibr" target="#b28">(Loshchilov &amp; Hutter, 2016)</ref>. We use data parallelism across 8 Nvidia V100 GPUs and half-precision training using techniques from <ref type="bibr" target="#b31">Micikevicius et al. (2017)</ref>; <ref type="bibr" target="#b17">Howard et al. (2018)</ref>.</p><p>We use the following regularization techniques that have been discovered in the non-few-shot, standard image classification literature  for pre-training the backbone.</p><p>? Mixup <ref type="bibr" target="#b54">(Zhang et al., 2017)</ref>: This augments data by a linear interpolation between input images and their one-hot labels. If (x 1 , y 1 ), (x 2 , y 2 ) ? D are two samples, mixup creates a new sample (x,?) wherex = ?x 1 + (1 ? ?)x 2 and its label? = ?e y1 + (1 ? ?)e y2 ; here e k is the one-hot vector with a non-zero k th entry and ? ? [0, 1] is sampled from Beta(?, ?) for a hyper-parameter ?.</p><p>? Label smoothing <ref type="bibr" target="#b46">(Szegedy et al., 2016)</ref>: When using a softmax operator, the logits can increase or decrease in an unbounded manner causing numerical instabilities while training. Label smoothing sets p ? (k|x) = 1 ? if k = y and /(K ? 1) otherwise, for a small constant &gt; 0 and number of classes K. The ratio between the largest and smallest output neuron is thus fixed which helps large-scale training.</p><p>? We exclude the batch-normalization parameters from weight-decay .</p><p>We set =0.1 for label smoothing cross-entroy loss and ?=0.25 for mixup regularization for all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 FINE-TUNING HYPER-PARAMETERS</head><p>We used 1-shot 5-way episodes on the validation set of Mini-ImageNet to manually tune hyperparameters. Fine-tuning is done for 25 epochs with a fixed learning rate of 5 ? 10 ?5 with Adam <ref type="figure" target="#fig_1">(Kingma &amp; Ba, 2014)</ref>. Adam is used here as it is more robust to large changes in the magnitude of the loss and gradients which occurs if the number of classes in the few-shot episode (ways) is large. We do not use any regularization (weight-decay, mixup, dropout, or label smoothing) in the fine-tuning phase. These hyper-parameters are kept constant on all benchmark datasets, namely Mini-ImageNet, Tiered-ImageNet, CIFAR-FS and FC-100.</p><p>All fine-tuning and evaluation is performed on a single GPU in full-precision. We update the parameters sequentially by computing the gradient of the two terms in <ref type="formula" target="#formula_8">(8)</ref> independently. This updates both the weights of the model and the batch-normalization parameters.</p><p>A.4 DATA AUGMENTATION Input images are normalized using the mean and standard-deviation computed on ImageNet-1k. Our Data augmentation consists of left-right flips with probability of 0.5, padding the image with 4px and adding brightness and contrast changes of ? 40%. The augmentation is kept the same for both pre-training and fine-tuning.</p><p>We explored augmentation using affine transforms of the images but found that adding this has minor effect with no particular trend on the numerical results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 EVALUATION PROCEDURE</head><p>The few-shot episode contains classes that are uniformly sampled from the test classes of corresponding datasets. Support and query samples are further uniformly sampled for each class. The query shot is fixed to 15 for all experiments unless noted otherwise. We evaluate all networks over 1,000 episodes unless noted otherwise. For ease of comparison, we report the mean accuracy and the 95% confidence interval of the estimate of the mean accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B SETUP FOR IMAGENET-21K</head><p>The ImageNet-21k dataset <ref type="bibr" target="#b8">(Deng et al., 2009</ref>) has 14.2M images across 21,814 classes. The blue region in <ref type="figure" target="#fig_5">Fig. 4</ref> denotes our meta-training set with 7,491 classes, each with more than 1,000 images. The green region shows 13,007 classes with at least 10 images each, the set used to construct few-shot episodes. We do not use the red region consisting of 1,343 classes with less than 10 images each. We train the same backbone (WRN-28-10) with the same procedure as that in Appendix A on 84 ? 84 resized images, albeit for only 24 epochs. Since we use the same hyper-parameters as the other benchmark datasets, we did not create validation sets for pre-training or the fine-tuning phases. The few-shot episodes are constructed in the same way as Appendix A. We evaluate using fewer few-shot episodes (80) on this dataset because we would like to demonstrate the performance across a large number of different ways.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ADDITIONAL ANALYSIS</head><p>This section contains additional experiments and analysis, complementing Section 4.3. All experiments use the (train + val) setting, pre-training on both the training and validation sets of the corresponding datasets, unless noted otherwise.  Colors denote the ground-truth labels; crosses denote the support samples; circles denote the query samples; translucent markers and opaque markers denote the embeddings before and after transductive fine-tuning respectively. Even though query samples are far away from their respective supports in the beginning, they move towards the supports by the end of transductive fine-tuning. Logits of support samples are relatively unchanged which suggests that the support-based initialization is effective. <ref type="figure" target="#fig_6">Fig. 5</ref> demonstrates this effect. The logits for query samples are far from those of their respective support samples and metric-based loss functions, e.g., those for prototypical networks  would have a high loss on this episode; indeed the accuracy after the support-based initialization is 64%. Logits for the query samples change dramatically during transductive fine-tuning and majority of the query samples cluster around their respective supports. The post transductive fine-tuning accuracy of this episode is 73.3%. This suggests that modifying the embedding using the query samples is crucial to obtaining good performance on new classes. This example also demonstrates that the support-based initialization is efficient, logits of the support samples are relatively unchanged during the transductive fine-tuning phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 TRANSDUCTIVE FINE-TUNING CHANGES THE EMBEDDING DRAMATICALLY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 LARGE VS. SMALL BACKBONES</head><p>The expressive power of the backbone plays an important role in the efficacy of fine-tuning. We observed that a WRN-16-4 architecture (2.7M parameters) performs worse than WRN-28-10 (36M parameters). The former obtains 63.28 ? 0.68% and 77.39 ? 0.5% accuracy on Mini-ImageNet and 69.04 ? 0.69% and 83.55 ? 0.51% accuracy on Tiered-ImageNet on 1-shot 5-way and 5-shot 5-way protocols respectively. While these numbers are comparable to those of state-of-the-art algorithms, they are lower than their counterparts for WRN-28-10 in <ref type="table" target="#tab_1">Table 1</ref>. This suggests that a larger network is effective in learning richer features from the meta-training classes, and fine-tuning is effective in taking advantage of this to further improve performance on samples belonging to few-shot classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 LATENCY WITH A SMALLER BACKBONES</head><p>The WRN-16-4 architecture (2.7M parameters) is much smaller than WRN-28-10 (36M parameters) and transductive fine-tuning on the former is much faster. As compared to our implementation of  with the same backbone, WRN-16-4 is 20-70? slower (0.87 vs. 0.04 seconds for a query shot of 1, and 2.85 vs. 0.04 seconds for a query shot of 15) for the 1-shot 5-way scenario. Compare this to the computational complexity experiment in Section 4.3.</p><p>As discussed in Appendix C.2, the accuracy of WRN-16-4 is 63.28 ? 0.68% and 77.39 ? 0.5% for 1-shot 5-way and 5-shot 5-way on Mini-ImageNet respectively. As compared to this, our implementation of  using a WRN-16-4 backbone obtains 57.29 ? 0.40% and 75.34 ? 0.32% accuracies for the same settings respectively; the former number in particular is significantly worse than its transductive fine-tuning counterpart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 COMPARISONS AGAINST BACKBONES IN THE CURRENT LITERATURE</head><p>We include experiments using conv (64) ?4 and ResNet-12 in <ref type="table">Table 3</ref>, in addition to WRN-28-10 in Section 4, in order to facilitate comparisons of the proposed baseline for different backbone architectures. Our results are comparable or better than existing results for a given backbone architecture, except for those in  who use a graph-based transduction algorithm, for conv (64) ?4 on Mini-ImageNet. In line with our goal for simplicity, we kept the hyper-parameters for pre-training and fine-tuning the same as the ones used for WRN-28-10 (cf. Sections 3 and 4). These results suggest that transductive fine-tuning is a sound baseline for a variety of backbone architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 USING MORE META-TRAINING CLASSES</head><p>In Section 4.1 we observed that having more pre-training classes improves few-shot performance. But since we append a classifier on top of a pre-trained backbone and use the logits of the backbone as inputs to the classifier, a backbone pre-trained on more classes would also have more parameters as compared to one pre-trained on fewer classes. However, this difference is not large: WRN-28-10 for Mini-ImageNet has 0.03% more parameters for (train + val) as compared to (train). However, in order to facilitate a fair comparison, we ran an experiment where we use the features of the backbone, instead of the logits, as inputs to the classifier. By doing so, the number of parameters in the pre-trained backbone that are used for few-shot classification remain the same for both the (train) and (train + val) settings. For Mini-ImageNet, (train + val) obtains 64.20 ? 0.65% and 81.26 ? 0.45%, and (train) obtains 62.55 ? 0.65% and 78.89 ? 0.46%, for 1-shot 5-way and 5-shot 5-way respectively. These results corroborate the original statement that more pre-training classes improves few-shot performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.6 USING FEATURES OF THE BACKBONE AS INPUT TO THE CLASSIFIER</head><p>Instead of re-initializing the final fully-connected layer of the backbone to classify new classes, we simply append the classifier on top of it. We implemented the former, more common, approach and found that it achieves an accuracy of 64.20 ? 0.65% and 81.26 ? 0.45% for 1-shot 5-way and 5-shot 5-way respectively on Mini-ImageNet, while the accuracy on Tiered-ImageNet is 67.14 ? <ref type="table">Table 3</ref>: Few-shot accuracies on benchmark datasets for 5-way few-shot episodes. The notation conv (64 k )?4 denotes a CNN with 4 layers and 64 k channels in the k th layer. The rows are grouped by the backbone architectures. Best results in each column and for a given backbone architecture are shown in bold. Results where the support-based initialization is better than or comparable to existing algorithms are denoted by ? . The notation (train + val) indicates that the backbone was pre-trained on both training and validation sets of the datasets; the backbone is trained only on the training set otherwise.   0.74% and 86.67 ? 0.46% for 1-shot 5-way and 5-shot 5-way respectively. These numbers are significantly lower for the 1-shot 5-way protocol on both datasets compared to their counterparts in <ref type="table" target="#tab_1">Table 1</ref>. However, the 5-shot 5-way accuracy is marginally higher in this experiment than that in <ref type="table" target="#tab_1">Table 1</ref>. As noted in Remark 2, logits of the backbone are well-clustered and that is why they work better for few-shot scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.7 FREEZING THE BACKBONE RESTRICTS PERFORMANCE</head><p>The previous observation suggests that the network changes a lot in the fine-tuning phase. Freezing the backbone severely restricts the changes in the network to only changes to the classifier. As a consequence, the accuracy of freezing the backbone is 58.38 ? 0.66 % and 75.46 ? 0.52% on Mini-ImageNet and 67.06 ? 0.69% and 83.20 ? 0.51% on Tiered-ImageNet for 1-shot 5-way and 5-shot 5-way respectively. While the 1-shot 5-way accuracies are much lower than their counterparts in <ref type="table" target="#tab_1">Table 1</ref>, the gap in the 5-shot 5-way scenario is smaller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.8 USING MIXUP DURING PRE-TRAINING</head><p>Mixup improves the few-shot accuracy by about 1%; the accuracy for WRN-28-10 trained without mixup is 67.06 ? 0.71% and 79.29 ? 0.51% on Mini-ImageNet for 1-shot 5-way and 5-shot 5-way respectively.</p><p>C.9 MORE FEW-SHOT EPISODES <ref type="figure">Fig. 1</ref> suggests that the standard deviation of the accuracies achieved by few-shot algorithms is high. Considering this randomness, evaluations were done over 10,000 few-shot episodes as well. The accuracies on Mini-ImageNet are 67.77 ? 0.21 % and 80.24 ? 0.16 % and on Tiered-ImageNet are 72.36 ? 0.23 % and 85.70 ? 0.16 % for 1-shot 5-way and 5-shot 5-way respectively. The numbers are consistent with the ones for 1,000 few-shot episodes in <ref type="table" target="#tab_1">Table 1</ref>, though the confidence intervals decreased as the number of episodes sampled increased.</p><p>C.10 EVALUATION ON META-DATASET <ref type="table">Table 4</ref>: Few-shot accuracies on Meta-Dataset. Best results in each row are shown in bold. 600 few-shot episodes were used to compare to the results reported in . Results where the supportbased initialization is better than or comparable to existing algorithms are denoted by ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D FREQUENTLY ASKED QUESTIONS</head><p>1. Why has it not been noticed yet that this simple approach works so well? Non-transductive fine-tuning as a baseline has been considered before <ref type="bibr" target="#b51">(Vinyals et al., 2016;</ref><ref type="bibr" target="#b5">Chen et al., 2018)</ref>. The fact that this is comparable to state-of-the-art has probably gone unnoticed because of the following reasons:</p><p>? Given that there are only a few labeled support samples provided in the few-shot setting, initializing the classifier becomes important. The support-based initialization (cf. Section 3.1) motivated from the deep metric learning literature <ref type="bibr" target="#b18">(Hu et al., 2015;</ref><ref type="bibr" target="#b33">Movshovitz-Attias et al., 2017;</ref><ref type="bibr" target="#b36">Qi et al., 2018;</ref><ref type="bibr" target="#b12">Gidaris &amp; Komodakis, 2018)</ref> classifies support samples correctly (for a support shot of 1, this may not be true for higher support shots). This initialization, as opposed to initializing the weights of the classifier randomly, was critical to performance in our experiments. ? In our experience, existing meta-training methods, both gradient-based ones and metricbased ones, are difficult to tune for larger architectures. We speculate that this is the reason a large part of the existing literature focuses on smaller backbone architectures. The few-shot learning literature has only recently started to move towards bigger backbone architectures . From <ref type="table">Table 3</ref> we see that non-tranductive finetuning gets better with a deeper backbone architecture. A similar observation was made by <ref type="bibr" target="#b5">(Chen et al., 2018)</ref>. The observation that we can use "simple" well-understood training techniques from standard supervised learning that scale up to large backbone architectures for few-shot classification is a key contribution of our paper.</p><p>Transductive methods have recently started to become popular in the few-shot learning literature . Because of the scarcity of labeled support samples, it is crucial to make use of the unlabeled query samples in the few-shot regime. Our advocated baseline makes use of both a good initialization and transduction, relatively new in the few-shot learning literature, which makes this simplistic approach go unrecognized till now.</p><p>2. Transductive fine-tuning works better than existing algorithms because of a big backbone architecture. One should compare on the same backbone architectures as the existing algorithms for a fair comparison. The current literature is in the spirit of increasingly sophisticated approaches for modest performance gains, often with different architectures (cf. <ref type="table" target="#tab_1">Table 1</ref>). This is why we set out to establish a baseline. Our simple baseline is comparable or better than existing approaches. The backbone we have used is common in the recent few-shot learning literature ) (cf <ref type="table" target="#tab_1">. Table 1</ref>). Additionally, we have included results on smaller common backbone architectures, namely conv (64) ?4 and ResNet-12 in Appendix C.4, and some additional experiments in Appendix C.2. These experiments suggest that transductive fine-tuning is a sound baseline for a variety of different backbone architectures. This indicates that we should take results on existing benchmarks with a grain of salt. Also see the response to question 1 above.</p><p>3. There are missing entries in <ref type="table" target="#tab_1">Tables 1 and 3</ref>. Is it still a fair comparison? <ref type="table" target="#tab_1">Tables 1 and 3</ref> show all relevant published results by the original authors. Re-implementing existing algorithms to fill missing entries without access to original code is impractical and often yields results inferior to those published, which may be judged as unfair. The purpose of a benchmark is to enable others to test their method easily. This does not exist today due to myriad performance-critical design choices often not detailed in the papers. In fact, missing entries in the table indicate the inadequate state of the current literature. Our work enables benchmarking relative to a simple, systematic baseline.</p><p>4. Fine-tuning for few-shot learning is not novel. We do not claim novelty in this paper. Transductive fine-tuning is our advocated baseline for few-shot classification. It is a combination of different techniques that are not novel. Yet, it performs better than existing algorithms on all few-shot protocols with fixed hyper-parameters. We emphasize that this indicates the need to re-interpret existing results on benchmarks and re-evaluate the status quo in the literature.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>; Snell et al. (2017); Finn et al. (2017); Oreshkin et al. (2018); Rusu et al. (2018) use different model architectures of varying sizes, which makes it difficult to disentangle the effect of their algorithmic contributions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Mean accuracy of transductive fine-tuning for different query shot, way and support shot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>;</head><label></label><figDesc><ref type="bibr" target="#b9">Finn et al. (2017)</ref>;; also perform additional processing at inference time and are expected to be slow, along with other transductive approaches. Additionally, support-based initialization has the same inference time as.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Comparing the accuracy of transductive fine-tuning (solid lines) vs. support-based initialization (dotted lines) for different datasets, ways</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>ImageNet-21k is a highly imbalanced dataset. The most frequent class has about 3K images while the rarest class has a single image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>t-SNE<ref type="bibr" target="#b29">(Maaten &amp; Hinton, 2008)</ref> embedding of the logits for 1-shot 5-way few-shot episode of Mini-ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Few-shot accuracies on benchmark datasets for 5-way few-shot episodes. The notation conv (64 k )?4 denotes a CNN with 4 layers and 64 k channels in the k th layer. Best results in each column are shown in bold. Results where the support-based initialization is better than or comparable to existing algorithms are denoted by ? . The notation (train + val) indicates that the backbone was pre-trained on both training and validation sets of the datasets; the backbone is trained only on the training set otherwise.) uses a 1.25? wider ResNet-12 which we denote as ResNet-12 * . ?4 43.44 ? 0.77 60.60 ? 0.71 Prototypical Networks conv (64) ?4 49.42 ? 0.78 68.20 ? 0.66 MAML<ref type="bibr" target="#b9">(Finn et al., 2017)</ref> conv(32) ?4 48.70 ? 1.84 63.11 ? 0.92 ?4 55.51 ? 0.86 69.86 ? 0.65 59.91 ? 0.94 73.30 ? 0.75 62.64 ? 0.61 78.63 ? 0.46 65.99 ? 0.72 81.56 ? 0.53 72.0 ? 0.7 84.2 ? 0.5 41.1 ? 0.6 55.5 ? 0.6 Support-based initialization (train) WRN-28-10 56.17 ? 0.64 73.31 ? 0.53 67.45 ? 0.70 ? 82.88 ? 0.53 ? 70.26 ? 0.70 83.82 ? 0.49 ? 36.82 ? 0.51 49.72 ? 0.55 Fine-tuning (train) WRN-28-10 57.73 ? 0.62 78.17 ? 0.49 66.58 ? 0.70 85.55 ? 0.48 68.72 ? 0.67 86.11 ? 0.47 38.25 ? 0.52 57.19 ? 0.57 Transductive fine-tuning (train) WRN-28-10 65.73 ? 0.68 78.40 ? 0.52 73.34 ? 0.71 85.50 ? 0.50 76.58 ? 0.68 85.79 ? 0.50 43.16 ? 0.59 57.57 ? 0.55 64.09 ? 0.62 80.00 ? 0.45 65.81 ? 0.74 81.75 ? 0.53 72.8 ? 0.7 85.0 ? 0.5 47.2 ? 0.6 62.5 ? 0.6 Support-based initialization (train + val) WRN-28-10 58.47 ? 0.66 75.56 ? 0.52 67.34 ? 0.69 ? 83.32 ? 0.51 ? 72.14 ? 0.69 ? 85.21 ? 0.49 ? 45.08 ? 0.61 60.05 ? 0.60 Fine-tuning (train + val) WRN-28-10 59.62 ? 0.66 79.93 ? 0.47 66.23 ? 0.68 86.08 ? 0.47 70.07 ? 0.67 87.26 ? 0.45 43.80 ? 0.58 64.40 ? 0.58 Transductive fine-tuning (train + val) WRN-28-10 68.11 ? 0.69 80.36 ? 0.50 72.87 ? 0.71 86.15 ? 0.50 78.36 ? 0.70 87.54 ? 0.49 50.44 ? 0.68 65.74 ? 0.60</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Mini-ImageNet</cell><cell cols="2">Tiered-ImageNet</cell><cell cols="2">CIFAR-FS</cell><cell cols="2">FC-100</cell></row><row><cell>Algorithm</cell><cell>Architecture</cell><cell>1-shot (%)</cell><cell>5-shot (%)</cell><cell>1-shot (%)</cell><cell>5-shot (%)</cell><cell>1-shot (%)</cell><cell>5-shot (%)</cell><cell>1-shot (%)</cell><cell>5-shot (%)</cell></row><row><cell cols="2">Matching networks (Vinyals et al., 2016) conv (64) ?4</cell><cell>46.6</cell><cell>60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">LSTM meta-learner (Ravi &amp; Larochelle, 2016) conv (64) R2D2 (Bertinetto et al., 2018) conv (96 k ) ?4</cell><cell cols="2">51.8 ? 0.2 68.4 ? 0.2</cell><cell></cell><cell></cell><cell>65.4 ? 0.2</cell><cell>79.4 ? 0.2</cell><cell></cell><cell></cell></row><row><cell>TADAM (Oreshkin et al., 2018)</cell><cell>ResNet-12</cell><cell cols="2">58.5 ? 0.3 76.7 ? 0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">40.1 ? 0.4 56.1 ? 0.4</cell></row><row><cell cols="2">Transductive Propagation (Liu et al., 2018b) conv (64) Transductive Propagation (Liu et al., ResNet-12</cell><cell>59.46</cell><cell>75.64</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2018b)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">MetaOpt SVM (Lee et al., 2019) ResNet-12  Activation to Parameter (Qiao et al., 2018) WRN-28-10 59.60 ? 0.41 73.74 ? 0.19</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(train + val)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LEO (Rusu et al., 2018) (train + val)</cell><cell cols="5">WRN-28-10 61.76 ? 0.08 77.59 ? 0.12 66.33 ? 0.05 81.44 ? 0.09</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MetaOpt SVM (Lee et al., 2019) (train +</cell><cell>ResNet-12</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>val)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>**</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>? 0.84 91.00 ? 1.09 84.77 ? 1.04 78.10 ? 0.79 70.09 ? 0.71 61.93 ? 0.45 ? 0.94 90.61 ? 1.03 84.21 ? 1.09 77.13 ? 0.82 68.94 ? 0.75 60.11 ? 0.48</figDesc><table><row><cell>Way</cell></row></table><note>Accuracy (%) on the few-shot data of ImageNet-21k. The confidence intervals are large because we compute statistics only over 80 few-shot episodes so as to test for large number of ways.20 ? 1.72 78.71 ? 1.63 69.48 ? 1.30 60.55 ? 1.03 49.15 ? 0.68 40.57 ? 0.42 Transductive fine-tuning WRN-28-10 1 89.00 ? 1.86 79.88 ? 1.70 69.66 ? 1.30 60.72 ? 1.04 48.88 ? 0.66 40.46 ? 0.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>and consists of 84 ? 84 sized images with 600 images per class. There are 64 training, 16 validation and 20 test classes. There are multiple versions of this dataset in the literature; we obtained the dataset from the authors of Gidaris &amp; Komodakis (2018) 3 .</figDesc><table><row><cell>? The Tiered-ImageNet dataset (Ren et al., 2018) is a larger subset of ImageNet-1k with 608</cell></row><row><cell>classes split as 351 training, 97 validation and 160 testing classes, each with about 1300</cell></row><row><cell>images of size 84 ? 84. This dataset ensures that training, validation and test classes do not</cell></row><row><cell>have a semantic overlap and is a potentially harder few-shot learning dataset.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>uses a 1.25? wider ResNet-12 which we denote as ResNet-12 * . WRN-28-10 58.47 ? 0.66 75.56 ? 0.52 67.34 ? 0.69 ? 83.32 ? 0.51 ? 72.14 ? 0.69 85.21 ? 0.49 45.08 ? 0.61 60.05 ? 0.60 Fine-tuning (train + val) WRN-28-10 59.62 ? 0.66 79.93 ? 0.47 66.23 ? 0.68 86.08 ? 0.47 70.07 ? 0.67 87.26 ? 0.45 43.80 ? 0.58 64.40 ? 0.58 Transductive fine-tuning (train + val) WRN-28-10 68.11 ? 0.69 80.36 ? 0.50 72.87 ? 0.71 86.15 ? 0.50 78.36 ? 0.70 87.54 ? 0.49 50.44 ? 0.68 65.74 ? 0.60</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/gidariss/FewShotWithoutForgetting</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mini-ImageNet</head><p>Tiered-ImageNet CIFAR-FS FC-100</p><p>MAML <ref type="bibr" target="#b9">(Finn et al., 2017)</ref> conv <ref type="formula">(</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Variadic learning by bayesian nonparametric deep embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanul</forename><surname>Kelsey R Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><forename type="middle">B</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning internal representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Baxter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
		<respStmt>
			<orgName>Flinders University of S. Aust.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the optimization of a synaptic learning rule</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jocelyn</forename><surname>Cloutier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Gecsei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Preprints Conf. Optimality in Artificial and Biological Neural Networks</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="6" to="8" />
		</imprint>
		<respStmt>
			<orgName>Univ. of Texas</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jo?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08136</idno>
		<title level="m">Meta-learning with differentiable closed-form solvers</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Signature verification using a&quot; siamese&quot; time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>S?ckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roopak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="737" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (1)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Good semi-supervised learning that requires a bad gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan R</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6510" to="6520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Analyzing and improving representations with the soft nearest neighbor loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.01889</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04043</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4367" to="4375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="529" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05027</idno>
		<title level="m">Identity mappings in deep residual networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to learn using gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Younger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter R Conwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="87" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<ptr target="https://github.com/fastai/fastai" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep transfer metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junlin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yap-Peng</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="325" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianyan</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shutao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangzihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haidong</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feihu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhou</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.11205</idno>
		<title level="m">Highly scalable deep learning training system with mixed-precision: Training imagenet in four minutes</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Transductive inference for text classification using support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Icml</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="200" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwonjoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03758</idno>
		<title level="m">Meta-learning with differentiable convex optimization</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning to propagate labels: Transductive propagation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseop</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saehoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunho</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Transductive propagation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseop</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saehoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10002</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Gradient-based hyperparameter optimization through reversible learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2113" to="2122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulius</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonah</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksii</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Venkatesh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.03740</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>et al. Mixed precision training</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Shin-Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Nakae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishii</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.00677</idno>
		<title level="m">Distributional smoothing with virtual adversarial training</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">No fuss distance metric learning using proxies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="360" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">On first-order meta-learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Tadam: Task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Pau Rodr?guez L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="719" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Low-shot learning with imprinted weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5822" to="5830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Few-shot image recognition by predicting parameters from activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7229" to="7238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Few-shot learning with embedded class models and shot-free meta training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Bhotika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Meta-learning for semi-supervised few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00676</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Andrei A Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hadsell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.05960</idno>
		<title level="m">Meta-learning with latent embedding optimization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Regularization with stochastic transformations and perturbations for deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehran</forename><surname>Javanmardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Tasdizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1163" to="1171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Evolutionary principles in self-referential learning. On learning how to learn: The meta-meta-... hook.) Diploma thesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
		<respStmt>
			<orgName>Institut f. Informatik, Tech. Univ. Munich</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Cyclical learning rates for training neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="464" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Lifelong learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning to learn</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="181" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carles</forename><surname>Gelada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.03096</idno>
		<title level="m">Meta-dataset: A dataset of datasets for learning to learn from few examples</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Shift of bias for inductive concept learning. Machine learning: An artificial intelligence approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Utgoff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="107" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The nature of statistical learning theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Springer science &amp; business media</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01187</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">LSTM meta-learner (Ravi &amp; Larochelle, 2016) conv (64)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Lal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sch?lkopf</surname></persName>
		</author>
		<idno>?4 43.44 ? 0.77 60.60 ? 0.71</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
	<note>Learning with local and global consistency</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prototypical</forename><surname>Networks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Snell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<title level="m">Transductive Propagation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>R2d2 (bertinetto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note>96 k</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tadam (oreshkin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<title level="m">Transductive Propagation</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svm (</forename><surname>Metaopt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<idno>ResNet-12 * 62.64 ? 0.61 78.63 ? 0.46 65.99 ? 0.72 81.56 ? 0.53 72.0 ? 0.7 84.2 ? 0.5 41.1 ? 0.6 55.5 ? 0.6</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svm (</forename><surname>Metaopt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>train + val</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qiao</surname></persName>
		</author>
		<idno>train + val) WRN-28-10 59.60 ? 0.41 73.74 ? 0.19</idno>
		<title level="m">Activation to Parameter</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leo (rusu</surname></persName>
		</author>
		<idno>train + val) WRN-28-10 61.76 ? 0.08 77.59 ? 0.12 66.33 ? 0.05 81.44 ? 0.09</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Support-based initialization (train + val) Best in Triantafillou et al. (2019) Support-based initialization Fine-tuning Transductive fine-tuning</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">and compared the performance of support-based initialization, fine-tuning and transductive fine-tuning to the best results in Triantafillou et al. (2019) for meta-training done on ImageNet-1k (ILSVRC) in Table 4. We observe that supportbased initialization is better than or comparable to state-of-the-art on 8 out of 10 tasks. Additionally, transductive fine-tuning is better, most times significantly, than state-of-the-art on 8 out of 10 tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Triantafillou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">We ran experiments on Meta-Dataset</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>it trails the state-of-the-art closely on the remaining tasks. The few-shot episode sampling was done the same way as described in Triantafillou et al.</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">use a hierarchical sampling technique to sample classes that are far from each other in the hierarchy, and hence easier to distinguish between). The hyper-parameters used for meta-training and few-shot fine-tuning are kept the same as the ones in Section 4 and are not tuned for these experiments. Images of size 84 ? 84 are used. Similar to the ImageNet-21k experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Triantafillou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">except for the few-shot class sampling for ImageNet-1k (ILSVRC) and Omniglot, which was done uniformly over all few-shot classes</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>these experiments scale down the entropic term in (8) by log |C t | and forego the ReLU in (6) and</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Our goal is to establish a systematic baseline for accuracy, which might help judge the accuracy of few-shot learning algorithms in the future. The question of test-time latency is indeed important but we have not focused on it in this paper. Appendix C.3 provides results using a smaller backbone where we see that the WRN-16-4 network is about 20-70x slower than metric-based approaches employing the same backbone while having significantly better accuracy. The latencies with WRN-28-10 are larger (see the computational complexity section in Section 4.3) but with a bigger advantage in terms of accuracy. There are other transductive methods used for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nichol</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Transductive fine-tuning has a very high latency at inference time, this is not practical. that are expected to be slow as well</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">We explore a similar scenario in Section 4.3 and Fig. 2a, which discuss the performance of transductive fine-tuning with a query shot of 1 (this means 5 query samples one from each class for 5-way evaluation). Note that the loss function in (8) leverages multiple query samples when available. It does not require that the query samples be balanced in terms of their ground-truth classes</title>
		<imprint/>
	</monogr>
	<note>Transductive fine-tuning can be performed even with a single test datum. Indeed, the network can specialize itself completely to classify this one datum. In particular, the loss function in (8) is well-defined even for a single test datum. For concerns about latency, see the question 5 above</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">and it is a fundamental property of the transductive paradigm to be dependent on the query set, in addition to the support set. In order to prevent query set hacking, we will make the test episodes public which will enable consistent benchmarking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nichol</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Having transductive approaches will incentivize hacking the query set. There are already published methods that use transductive methods</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>even for transductive methods</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Having one model for each different scenario is unreasonable in the real-world, as the number of different scenarios is, in principle, infinite. Current algorithms do not handle this well. A single model which can handle any few</title>
		<imprint/>
	</monogr>
	<note>Why is having the same hyper-parameters for different few-shot protocols so important? A practical few-shot learning algorithm should be able to handle any few-shot protocol. shot scenario is thus desirable</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Is this over-fitting to the test datum? No, label of the test datum is not used in the loss function</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Can you give some intuition about the hardness metric? How did you come up with the formula? The hardness metric is the clustering loss where the labeled support samples form the centers of the class-specific clusters. The special form, namely, E (x,y)?Dq log 1?p(y|x) p(y|x) (cf. (9)) allows an interpretation of log-odds. We used this form because it is sensitive to the number of few-shot classes (cf</title>
		<imprint/>
	</monogr>
	<note>Similar metrics, e.g., E (x,y)?Dq</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">We compared two algorithms in Fig. 3, namely transductive fine-tuning and support-based initialization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">?</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">How does Fig. 3 look for algorithm</title>
		<imprint/>
	</monogr>
	<note>Section 4.4 and the caption of Fig. 3 explains how the former algorithm is better. We will consider adding comparisons to other algorithms to this plot in the future</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
