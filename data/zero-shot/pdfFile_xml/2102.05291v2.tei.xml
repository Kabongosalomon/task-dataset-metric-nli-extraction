<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Clusterability as an Alternative to Anchor Points When Learning with Noisy Labels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwen</forename><surname>Song</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
						</author>
						<title level="a" type="main">Clusterability as an Alternative to Anchor Points When Learning with Noisy Labels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The label noise transition matrix, characterizing the probabilities of a training instance being wrongly annotated, is crucial to designing popular solutions to learning with noisy labels. Existing works heavily rely on finding "anchor points" or their approximates, defined as instances belonging to a particular class almost surely. Nonetheless, finding anchor points remains a non-trivial task, and the estimation accuracy is also often throttled by the number of available anchor points. In this paper, we propose an alternative option to the above task. Our main contribution is the discovery of an efficient estimation procedure based on a clusterability condition. We prove that with clusterable representations of features, using up to third-order consensuses of noisy labels among neighbor representations is sufficient to estimate a unique transition matrix. Compared with methods using anchor points, our approach uses substantially more instances and benefits from a much better sample complexity. We demonstrate the estimation accuracy and advantages of our estimates using both synthetic noisy labels (on CIFAR-10/100) and real human-level noisy labels (on Clothing1M and our self-collected humanannotated CIFAR-10). Our code and human-level noisy CIFAR-10 labels are available at https: //github.com/UCSC-REAL/HOC.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Training deep neural networks (DNNs) relies on the largescale labeled datasets while they often include a nonnegligible fraction of wrongly annotated instances. The corrupted patterns tend to be memorized by the over-A variety of approaches were proposed to address the problem of learning with noisy labels. The implementations of a major line of them, e.g., <ref type="bibr">Patrini et al. (2017)</ref>; <ref type="bibr">Xiao et al. (2015)</ref>; <ref type="bibr">Xia et al. (2020b)</ref>; <ref type="bibr">Berthon et al. (2021)</ref>; <ref type="bibr">Xia et al. (2019)</ref>; <ref type="bibr">Yao et al. (2020b)</ref>; <ref type="bibr" target="#b21">Li et al. (2021)</ref>, depend on accurate knowledge of the noise transition matrix T , which characterizes the probabilities of a training example being wrongly annotated. It has been show that <ref type="bibr" target="#b23">(Liu &amp; Tao, 2015;</ref><ref type="bibr">Patrini et al., 2017)</ref>, with perfect knowledge of T , the minimizer of a corrected or reweighted expected risk (loss) defined on the noisy distribution is the same as the minimizer of the true expected risk (loss) of the clean distribution. These results clearly established the power and benefits of knowing T .</p><p>Estimating T is challenging without accessing clean labels. Existing works on estimating T often rely on finding a number of high-quality anchor points <ref type="bibr">(Scott, 2015;</ref><ref type="bibr" target="#b23">Liu &amp; Tao, 2015;</ref><ref type="bibr">Patrini et al., 2017)</ref>, or approximate anchor points <ref type="bibr">(Xia et al., 2019)</ref>, which are defined as the training examples that belong to a particular class almost surely. To find the anchor point, a model needs to be trained to accurately characterize the noisy label distribution. This model will help inform the selection of anchor points. Again relying on this model, T is then estimated using posterior noisy label distributions of the anchor points.</p><p>While the anchor point approach observes a significant amount of successes, it suffers from several limitations: 1) accurately fitting noisy distributions is challenging when the number of label classes is high; 2) the number of anchor points restricts the estimation accuracy; and 3) it lacks the flexibility to extend to more complicated noise settings. Other methods such as confident learning <ref type="bibr">(Northcutt et al., 2017;</ref><ref type="bibr" target="#b24">2021)</ref> may not explicitly identify anchor points, but they still need to fit the noisy distributions and find some "confident points", thus suffer from the above limitations.</p><p>In this paper, we provide an alternative to estimate T without resolving to anchor points. The only requirement we need is clusterability, i.e., the two nearest-neighbor representations of a training example and the example itself arXiv:2102.05291v2 <ref type="bibr">[cs.</ref>LG] 13 Jul 2021 belong to the same true label class. Our main contributions summarize as follows: ? Based on the clusterability condition, we propose a novel T estimator by exploiting a set of high-order consensuses information among neighbor representations' noisy labels. Compared with the methods using anchor points, our estimator uses a much larger set of training examples and benefits from a much better sample complexity. <ref type="bibr">?</ref> We prove that using up to third-order consensuses is sufficient to identify the true noise transition matrix uniquely. ? Extensive empirical studies on CIFAR-10/100 datasets with synthetic noisy labels, the Clothing1M dataset with real-world human noise, and the CIFAR-10 dataset with our self-collected human annotations, demonstrate the advantage of our estimator. ? Open-source contribution and flexible extension: we will contribute to the community 1) a generically applicable and light tool for fast estimation of the noise transition matrix. This flexible tool has the potential to be applied to more sophisticated noise settings, including instancedependent ones (Section 3.4). 2) A noisy version of the CIFAR-10 dataset with human-level label noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related Works</head><p>In the literature of learning with label noise, a major set of works focus on designing risk-consistent methods, i.e., performing empirical risk minimization (ERM) with specially designed loss functions on noisy distributions leads to the same minimizer as if performing ERM over the corresponding unobservable clean distribution. The noise transition matrix is a crucial component for implementing risk-consistent methods, e.g., loss correction <ref type="bibr">(Patrini et al., 2017)</ref>, loss reweighting <ref type="bibr" target="#b23">(Liu &amp; Tao, 2015)</ref>, label correction <ref type="bibr">(Xiao et al., 2015)</ref> and unbiased loss <ref type="bibr">(Natarajan et al., 2013)</ref>. To a certain degree, the knowledge of it also helps tune hyperparameters in other approaches, e.g., label smoothing <ref type="bibr">(Lukasik et al., 2020)</ref>. As introduced previously, anchor points are critical for estimating the transition matrix in above mentioned existing methods -we further elaborate this in Section 2.2.</p><p>Some recently proposed risk-consistent approaches do not require the knowledge of transition matrix, including: L DMI (Xu et al., 2019) based on an information theoretical measure, peer loss <ref type="bibr">(Liu &amp; Guo, 2020)</ref> by punishing overagreements with noisy labels, robust f -divergence <ref type="bibr">(Wei &amp; Liu, 2021)</ref>, and CORES 2 <ref type="bibr" target="#b5">(Cheng et al., 2020)</ref> built on a confidence-regularizer. However, to principally handle a more complicated case when the noise transition matrix depends on each feature locally, i.e., instance-dependent noise, the ability to estimate local transition matrices remains a significant and favorable property. Examples include the potential of applying local transition matrices to different groups of data <ref type="bibr">(Xia et al., 2020b)</ref>, using confidence scores to revise transition matrices <ref type="bibr">(Berthon et al., 2021)</ref>, and estimating the second-order information of local transition matrices <ref type="bibr">(Zhu et al., 2021)</ref>. Thus we need an estimation approach that scales and generalizes well to these situations.</p><p>As a growing literature, we are aware of other promising approaches that do not rely on the estimation of T , e.g., focusing on the numerical property of loss functions and designing bounded loss functions <ref type="bibr" target="#b0">(Amid et al., 2019a;</ref><ref type="bibr">Zhang &amp; Sabuncu, 2018;</ref><ref type="bibr" target="#b12">Wang et al., 2019;</ref><ref type="bibr" target="#b9">Gong et al., 2018;</ref><ref type="bibr" target="#b8">Ghosh et al., 2017;</ref><ref type="bibr">Shu et al., 2020)</ref>, using sample selection to pick up reliable instances from the dataset <ref type="bibr">(Jiang et al., 2018;</ref><ref type="bibr" target="#b10">Han et al., 2018;</ref><ref type="bibr">Yu et al., 2019;</ref><ref type="bibr">Yao et al., 2020a;</ref><ref type="bibr">Wei et al., 2020)</ref>, among many more. We compare to some of the popular ones using experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head><p>This section introduces the preliminaries, including problem formulation, anchor points, and the clusterability condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Our Setup</head><p>We summarize the important definitions as follows.</p><p>Clean/Noisy distribution The traditional classification problem with clean labels often builds on a set of N training examples denoted by D := {(x n , y n )} n? <ref type="bibr">[N ]</ref> , where [N ] := {1, 2, ? ? ? , N }. Each example (x n , y n ) could be seen as a snapshot of random variable (X, Y ) drawn from a clean distribution D. Let X and Y denote the space of feature X and label Y , respectively. In our considered weakly-supervised classification problem, instead of having access to the clean dataset D, the learner could only obtain a noisy dataset D := {(x n ,? n )} n? <ref type="bibr">[N ]</ref> , where the noisy label? n may or may not be the same as y n . Noisy examples (x n ,? n ) are generated according to random variables (X, Y ) drawn from a distribution D.</p><p>Noise transition matrix We model the relationship between (X, Y ) and (X, Y ) using a noise transition matrix T (X), where each element T ij (X) represents the probability of mislabeling a clean label Y = i to the noisy label Y = j, i.e. T ij (X) := P( Y = j|Y = i, X). We call T (X) the local transition matrix in this paper since it is defined for a particular feature X. Most of the literature would focus on the case where the noise is independent of feature X: T (X) ? T . The knowledge of T enables a variety of learning with noisy label solutions. Below we illustrate solutions with the celebrated loss correction approach <ref type="bibr">(Natarajan et al., 2013;</ref><ref type="bibr">Patrini et al., 2017)</ref>.</p><p>The learning task The classification task aims to identify a classifier f : X ? Y that maps X to Y accurately. We focus on minimizing the empirical risk using DNNs with respect to the cross-entropy (CE) loss defined</p><formula xml:id="formula_0">as (f (X), Y ) = ? ln(f X [Y ]), Y ? [K], where f X [Y ]</formula><p>denotes the Y -th component of column vector f (X) and K is the number of classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Loss Correction and Estimating T</head><p>In the popular loss correction approach <ref type="bibr">(Patrini et al., 2017)</ref>, when the noise transition matrix is known, forward or backward loss correction can be applied to design a corrected loss. For example, the forward loss correction function can be designed as:</p><formula xml:id="formula_1">? (f (X), Y ) := (T f (X), Y ),</formula><p>where T denotes the transpose of matrix T . If T is perfectly known in advance, it can be shown that the minimizer of the corrected loss under the noisy distribution is the same as the minimizer of the original loss under the clean distribution <ref type="bibr">(Patrini et al., 2017)</ref>.</p><p>We would like to emphasize that in addition to loss correction, the knowledge of noise transition matrices is potentially useful in other approaches, especially when dealing with the challenging instance-dependent label noise where T (X) differs for different X. For example, it was shown that knowing T (X) helps improve the robustness of peer loss when the noise transition matrix differs across instances <ref type="bibr">(Zhu et al., 2021)</ref>, and can help improve fairness guarantees when label noise is group-dependent <ref type="bibr">(Wang et al., 2021)</ref>. Knowing T also tends to be helpful in setting hyperparameters in sample selection <ref type="bibr" target="#b10">(Han et al., 2018)</ref> and label smoothing <ref type="bibr">(Lukasik et al., 2020;</ref><ref type="bibr">Wei et al., 2021)</ref>.</p><p>Estimating T with anchor points The traditional approach for estimating T relies on anchor points <ref type="bibr">(Scott, 2015;</ref><ref type="bibr" target="#b23">Liu &amp; Tao, 2015)</ref>, which are defined as the training examples (Xs) that belong to a specific class almost surely. Formally, an x is an anchor point for the class i if P(Y = i|X = x) is equal to one or close to one <ref type="bibr">(Xia et al., 2019)</ref>. Further, if P(Y = i|X = x) = 1, we have</p><formula xml:id="formula_2">P( Y = j|X = x) = k?[K] T kj P(Y = k|X = x) = T ij .</formula><p>The matrix T can be obtained via estimating the noisy class posterior probabilities for anchor points heuristically <ref type="bibr">(Patrini et al., 2017)</ref> or theoretically <ref type="bibr" target="#b23">(Liu &amp; Tao, 2015)</ref>.</p><p>While the anchor point approach observes a significant amount of successes, this method suffers from three major limitations:</p><p>? The implementation of it requires that the trained model can perfectly predict the probability of the noisy labels, which is challenging when the number of classes is high, and when the number of training instances is limited. ? The number of available and identifiable anchor points can become a bottleneck even if the posterior distribution can be perfectly learned. ? The lack of flexibility to zoom into a subset of training data also limits its potential to be applied to estimate local transition matrices for more challenging instance- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Clusterability</head><p>The alternative we are seeking builds on the notion of clusterability. Intuitively, clusterability implies that two instances are likely to have the same labels if they are close to each other <ref type="bibr" target="#b7">(Gao et al., 2016)</ref>. To facilitate the discovery of closeby instances, our solution will resolve to representation learning <ref type="bibr" target="#b2">(Bengio et al., 2013)</ref>. Recent literature shows, even though label noise makes the model generalizes poorly, it still induces good representations . Formally, for a neural network with both convolutional layers and linear layers, e.g., ResNet <ref type="bibr" target="#b13">(He et al., 2016)</ref>, we denote the convolution layers by function f conv and the representations byX := f conv (X). With the above, we define k-Nearest-Neighbor (k-NN) label clusterability 1 as: Definition 1 (k-NN label clusterability). We call a dataset D satisfies k-NN label clusterability if ?n ? [N ], the representationx n and its k-Nearest-Neighborx n1 , ? ? ? ,x n k belong to the same true class.</p><p>See <ref type="figure" target="#fig_0">Figure 1</ref> for an illustration of the k-NN clusterability.</p><p>There are three primary properties of the definition:</p><p>? The k 1 -NN label clusterability condition is harder to satisfy than k 2 -NN label clusterability when k 1 &gt; k 2 ;</p><p>? The cluster containing the same clean labels is not required to be a continuum, e.g., in <ref type="figure" target="#fig_0">Figure 1</ref>, two clusters of class "1" can be far away;</p><p>? Our k-NN label clusterability only requires the existence of these feasible points, i.e., specifying the true class is not necessary.</p><p>The k-NN label clusterability likely holds in many tasks, such as image classification when features are well-extracted by convolutional layers <ref type="bibr" target="#b12">(Han et al., 2019;</ref><ref type="bibr" target="#b16">Ji et al., 2019;</ref><ref type="bibr" target="#b18">Kolesnikov et al., 2019)</ref> and each feature belongs to a unique true class. The high-level intuition is that similar representations should belong to the same label class. One can consider a label generation process <ref type="bibr" target="#b6">(Feldman, 2020;</ref><ref type="bibr" target="#b24">Liu, 2021)</ref> where the feature distribution is modeled as a mixture of many disjoint sub-distributions, and the labeling function maps each sub-distribution to a unique label class. Therefore, samples from the same sub-distribution have the same true label. In this paper, instead of requiring identical labels for a big cluster defined by a large k, we will only require the 2 nearest neighbors to have the same clean labels with the example itself, i.e., 2-NN label clusterability. Its feasibility will be demonstrated in Section 5.3.</p><p>Comparison to anchor points The anchor point approach relies on training a classifier to identify anchor points and the corresponding true class. Our label clusterability definition does not require the knowledge of true label class as claimed in the third property. Moreover, if good representations are available apriori, our method is model-free.</p><p>Next, we will elaborate our proposed T estimator leveraging 2-NN label clusterability. Relaxation of 2-NN label clusterability is discussed in Appendix C.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Power of High-Order Consensuses</head><p>We now present our alternative to estimate T . Our idea builds around the concept of using high-order consensuses of the noisy labels Y s among each training instance and its 2-NN. In this section, we consider the case when T (X) is the same for different X, i.e., T (X) ? T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Warm-up: A Binary Example</head><p>For a gentle start, consider binary cases (K = 2) with classes {1, 2}. Short-hand error rates e 1 := T 12 := P( Y = 2|Y = 1), e 2 := T 21 := P( Y = 1|Y = 2). p 1 := P(Y = 1) denotes the clean prior probability of class-1.</p><p>We are inspired by the matching mechanism for binary error rates estimation <ref type="bibr">(Liu &amp; Chen, 2017;</ref>. Intuitively, with 1-NN label clusterability, for two representations in the same dataset with minimal distance, their labels should be identical. Otherwise, we know there must be exactly one example with the corrupted label. Similarly, if k-NN label clusterability holds, by comparing the noisy label of one representation with its k-NN, we can write down the probability of the k + 1 noisy label consensuses (including agreements and disagreements) as a function of e 1 , e 2 , p 1 .</p><p>Going beyond votes from k-NN noisy labels To infer whether the label of an instance is clean or corrupted, one could use the 2-NN of this instance and take a majority vote. For example, if the considered instance has the label "1" and the other two neighbors have the label "2", it can be inferred that the label of the considered instance is corrupted since "2" is in the majority. Nonetheless, this inference would be wrong when the 2-NN are corrupted. Increasing accuracy of the naive majority vote <ref type="bibr" target="#b23">(Liu &amp; Liu, 2015)</ref> or other inference approaches <ref type="bibr" target="#b22">(Liu et al., 2012)</ref> requires stronger clusterability that more neighbor representations should belong to the same clean class. Our approach goes beyond simply using the votes among k-NNs. Instead, we will rely on the statistics of high-order consensuses among the k-NN noisy labels. As a result, our method enjoys a robust implementation with only requiring 2-NN label clusterability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consensuses in binary cases</head><p>We now derive our approach for the binary case to deliver our main idea. We present the general form of our estimator in the next subsection. Let Y 1 be the noisy label of one particular instance, Y 2 and Y 3 be the noisy labels of its nearest neighbor and second nearest neighbor. With 2-NN label clusterability, their clean</p><formula xml:id="formula_3">labels are identical, i.e. Y 1 = Y 2 = Y 3 . For Y 1 , noting P( Y 1 = j) = i?[K] P( Y 1 = j|Y 1 = i) ? P(Y 1 = i),</formula><p>we have the following two first-order equations:</p><formula xml:id="formula_4">P( Y 1 = 1) = p 1 (1 ? e 1 ) + (1 ? p 1 )e 2 , P( Y 1 = 2) = p 1 e 1 + (1 ? p 1 )(1 ? e 2 ).</formula><p>For the second-order consensuses, we have</p><formula xml:id="formula_5">P( Y1 = j1, Y2 = j2) (a) = i?[K] P( Y1 = j1, Y2 = j2|Y1 = i, Y2 = i) ? P(Y1 = i) (b) = i?[K] P( Y1 = j1|Y1 = i) ? P( Y2 = j2|Y2 = i) ? P(Y1 = i),</formula><p>where equality (a) holds due to the 2-NN label clusterability, i.e., Y 1 = Y 2 (= Y 3 ) w.p. 1, and equality (b) holds due to the conditional independency between Y 1 and Y 2 given their clean labels. In total, there are four second-order equations for different combinations of Y 1 , Y 2 , e.g.,</p><formula xml:id="formula_6">P( Y 1 = 1, Y 2 = 1) = p 1 (1 ? e 1 ) 2 + (1 ? p 1 )e 2 2 , P( Y 1 = 1, Y 2 = 2) = p 1 (1 ? e 1 )e 1 + (1 ? p 1 )e 2 (1 ? e 2 ). Similarly, given Y 1 = Y 2 = Y 3 , there are eight third-order equations defined for consensuses among Y 1 , Y 2 , Y 3 , e.g., P( Y 1 = 1, Y 2 = 1, Y 3 = 1) = p 1 (1 ? e 1 ) 3 + (1 ? p 1 )e 3 2 .</formula><p>Figure 2 illustrates the above consensus checking process. We leave more details and full derivations to Appendix A. The left-hand side of each above equation is the probability of a particular first-, second-, or third-order consensus pattern of Y , which could be estimated given the noisy dataset D. These consensus patterns encode the high-order information of T . Later in Section 4.1, we will prove that given the consensus probability (LHS), the first three order consensus equations we presented above are sufficient to jointly identify a unique solution to T , which indeed corresponds to the true T . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Estimating T : The General Form</head><p>We generalize this idea to classifications with multiple classes. For a K-class classification problem, define p :</p><formula xml:id="formula_7">= [P(Y = i), i ? [K]] and T r := T ? S r , ?r ? [K],<label>(1)</label></formula><p>where S r := [e r+1 , e r+2 , ? ? ? , e K , e 1 , e 2 , ? ? ? e r ] is a cyclic permutation matrix, and e r is the K ? 1 column vector of which the r-th element is 1 and 0 otherwise. The matrix S r cyclically shifts each column of T to its left side by r units. Similar to the previous binary example, the LHS of the equation is the probability of different distributions of Y s among each instance and its 2-NN. Let</p><formula xml:id="formula_8">(i + r) K := [(i + r ? 1) mod K] + 1.</formula><p>For the first-, second-, and third-order consensuses, we can respectively denote them in vector forms as follows (?r</p><formula xml:id="formula_9">? [K], s ? [K]). c [1] = [P( Y1 = i), i ? [K]] , c [2] r = [P( Y1 = i, Y2 = (i + r)K ), i ? [K]] , c [3] r,s =[P( Y1 = i, Y2 = (i + r)K , Y3 = (i + s)K ), i ? [K]] .</formula><p>Denote by ? the Hadamard product of two matrices. We now present the system of consensus equations for estimating T and p in the general form:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consensus Equations</head><p>? First-order (K equations):</p><formula xml:id="formula_10">c [1] := T p,<label>(2)</label></formula><p>? Second-order (K 2 equations):</p><formula xml:id="formula_11">c [2] r := (T ? T r ) p, r ? [K],<label>(3)</label></formula><p>? Third-order (K 3 equations):</p><formula xml:id="formula_12">c [3] r,s := (T ? T r ? T s ) p, r, s ? [K]. (4)</formula><p>While we leave the full details of derivation to Appendix A, we show one second-order consensus below for an example:</p><formula xml:id="formula_13">e j c [2] r = P( Y1 = j, Y2 = (j + r)K ) (a) = i?[K] P( Y1 = j|Y1 = i)P( Y2 = (j + r)K |Y2 = i)P(Y1 = i) = i?[K] Ti,j ? T i,(j+r) K ? pi (b) = e j (T ? Tr) p,</formula><p>where equality (a) holds again due to the 2-NN label clusterability and the conditional independency (similar to binary cases), and equality (b) holds due to T r [i, j] = T i,(j+r) K .</p><p>We note that although there are higher-order consensuses according to this rule, we only consider up to third-order consensuses of Y as shown in Eqns.</p><p>(2)-(4). For ease of notation, we define two stacked vector-forms for c</p><formula xml:id="formula_14">[2] r,s , c [3] r,s : c [2] : = [(c [2] r ) , ?r ? [K]] ,<label>(5)</label></formula><formula xml:id="formula_15">c [3] : = [(c [3] r,s ) , ?r, s ? [K]] .<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">The HOC Estimator</head><p>Solving the consensus equations requires estimating the consensus probabilities c <ref type="bibr">[1]</ref> , c <ref type="bibr">[2]</ref> , and c <ref type="bibr">[3]</ref> . In this subsection, we will first show the procedures for estimating these probabilities and then formulate an efficient optimization problem for T and p. To summarize, there are three steps:</p><p>? Step 1: Find 2-NN for eachx n from the noisy dataset D. <ref type="bibr">?</ref> Step 2: Compute each? <ref type="bibr">[?]</ref> usingx n and their 2-NN. <ref type="bibr">?</ref> Step 3: Formulate the optimization problem in <ref type="formula" target="#formula_7">(10)</ref>.</p><p>Denote by E ? [N ]. We elaborate on each step as follows.</p><p>Step 1: Find 2-NN Given the noisy dataset {(x n ,? n ), n ? E}, for each representationx n = f conv (x n ), we can find its 2-NNx n1 ,x n2 as:</p><formula xml:id="formula_16">n1 = arg min n ?E,n =n Dist(xn,x n ), n2 = arg min n ?E,n =n =n 1 Dist(xn,x n ),</formula><p>and the corresponding noisy labels? n1 ,? n2 . Dist(A, B) measures the distance between A and B -we will use Dist as the negative cosine similarity in our experiment.</p><p>Step 2: Empirical mean Denote by 1{?} the indicator function taking value 1 when the specified condition is met and 0 otherwise. Let E be a set of indices and |E| be the number of them. The probability of each high-order consensus could be estimated by the empirical mean using a particular set of sampled examples in E: {(? n ,? n1 ,? n2 ), n ? E} as follows (?i).</p><formula xml:id="formula_17">c [1] [i] = 1 |E| n?E 1{?n = i}, c [2] r [i] = 1 |E| n?E 1{?n = i,?n 1 = (i + r)K },<label>(7)</label></formula><formula xml:id="formula_18">c [3] r,s [i] = 1 |E| n?E 1{?n = i,?n 1 = (i + r)K ,?n 2 = (i + s)K }.</formula><p>The motivation of identifying a subset E for the estimators is due to the desired provable convergence to the expectation. Each 3-tuple in the sample should be independent and identically distributed (i.i.d.) so that each? <ref type="bibr">[?]</ref> is consistent. However, the existence of nearest neighbors, e.g., when both n and n 1 belong to E and n is a 2-NN of n 1 , may violate the i.i.d. property of these 3-tuples. Denote by</p><formula xml:id="formula_19">E * 3 = arg max E?[N ]</formula><p>|E|, s.t. |{n, n 1 , n 2 , ?n ? E}| = 3|E|.</p><p>Then any subset E ? E * 3 guarantees the i.i.d. property. Note it is generally time-consuming to find the best E. For an efficient solution (with empirical approximation), we randomly sample |E| center indices from [N ] and repeat Step 1 and Step 2 multiple times with different E (as Line 3 -Line 8 in Algorithm 2). We will further discuss the magnitude of |E| in Section 4.2 and Appendix B.3.</p><p>Step 3: Optimization With? <ref type="bibr">[1]</ref> ,? <ref type="bibr">[2]</ref> , and? <ref type="bibr">[3]</ref> , we formulate the optimization problem in (8) to jointly solve for T , p.</p><formula xml:id="formula_20">minimize T ,p 3 ?=1 ? [?] ? c [?] 2 (8a) subject to Eqns. (1) -(6) (8b) p i ? 0, T ij ? 0, i, j ? [K] (8c) i?[K] p i = 1, j?[K] T ij = 1, i ? [K]. (8d)</formula><p>The crucial components in <ref type="formula">(8)</ref>   <ref type="formula">(8c)</ref> and <ref type="formula">(8d)</ref>: feasibility of a solution.</p><p>Challenges for solving the constrained optimization problem The problem in (8) is a constrained optimization problem with K(K + 1) variables, K(K + 1) inequality constraints, and (K + 1) equality constraints, and it is generally hard to guarantee its convexity. Directly solving this problem using the Lagrangian-dual method may take a long time to converge <ref type="bibr" target="#b4">(Boyd et al., 2004)</ref>.</p><p>Unconstrained soft approximation Notice that both p and each row of T are probability measures. Instead of directly solving for T and p, we seek to relax the constraints by introducing auxiliary and unconstrained variables to represent T and p. Particularly, we turn to optimizing variables T ? R K?K andp ? R K that are associated with T and p by T := ? T (T ), p := ? p (p), where ? T (?) and ? p (?) are softmax functions such that</p><formula xml:id="formula_21">T ij := exp(T ij ) k?[K] exp(T ik ) , p i := exp(p i ) k?[K] exp(p k )</formula><p>. <ref type="formula">(9)</ref> Therefore, we can drop all the constraints in (8) and focus on solving the unconstrained optimization problem with K(K + 1) variables. Our new optimization problem is given as follows:</p><formula xml:id="formula_22">minimiz? T ,p 3 ?=1 ? [?] ? c [?] 2<label>(10a)</label></formula><p>subject to Eqns.</p><p>(1) -(6), Eqn. (9).</p><p>Algorithm 1 The HOC Estimator 1: Input: Rounds: G. Sample size: |E|. Noisy dataset:</p><formula xml:id="formula_24">D = {(x n ,? n )} n?[N ] . Representation extractor: f conv . 2: Initialization: Set? [1] ,? [2] ,? [3] to 0. Extract represen- tations x n ? f conv (x n ), ?n ? [N ].T = KI ? 11 . p = 1/K. // I: identity matrix, 1: all-ones column vector. 3: repeat 4: E ? RndSmp([N ], |E|); // sample |E| center indices 5: {(? n ,? n1 ,? n2 ), n ? [E]} ? Get2NN( D, E); // find the noisy labels of the 2-NN of xn, n ? [E] 6: (? [1] tmp ,? [2] tmp ,? [3] tmp ) ? CountFreq(E) // as Eqn. (7) 7:? [?] ?? [?] +? [?] tmp , ? ? {1, 2, 3}; 8: until G times 9:? [?] ?? [?] /G, ? ? {1, 2, 3}; // estimate c [?] G times 10: Solve the unconstrained problem in (10) with (? [1] ,? [2] ,? [3] ) by gradient decent, getT andp 11: Output: EstimatesT ? ? T (T ),p ? ? p (p).</formula><p>Equations in <ref type="formula" target="#formula_7">(10b)</ref> are presented only for a clear objective function. Given the solution of problem <ref type="formula" target="#formula_7">(10)</ref>, we can calculate T and p according to Eqn. (9). Note the search space of T before and after soft approximation differs only in corner cases (before: T ij ? 0, after: T ij &gt; 0). For each original and non-corner T , there exists a soft approximated T that leads to the same transition probabilities. Thus the soft approximation preserves the property of T , e.g. the uniqueness in Theorem 1. Algorithm 1 summarizes our High-Order-Consensus (HOC) estimator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Flexible Extensions to Instance-Dependent Noise</head><p>Algorithm 1 provides a generically applicable and light tool for fast estimation of T . The flexibility makes it possible to be applied to more sophisticated instance-dependent label noise. We briefly discuss possible applications to estimating the local noise transition matrix T (X).</p><p>Locally homogeneous label noise Intuitively, by considering a local dataset in which every representation shares the same T (X), the method in Section 3.2 can then be applied locally to estimate the local T (X). Specially, using a "waypoint"x n , we build a local dataset D n that includes the M -</p><formula xml:id="formula_25">NN ofx n , i.e., D n = {(x n ,? n )} ? {(x ni ,? ni ), ?i ? [M ]}, where {n i , i ? [M ]} are the indices of the M -NN ofx n .</formula><p>We introduce the following definitions: Definition 2 (M -NN noise clusterability). We call D n satisfies M -NN noise clusterability if the M -NN ofx n have the same noise transition matrix as x n , i.e., T (</p><formula xml:id="formula_26">x n ) = T (x ni ), ?i ? [M ]. Definition 3 ((H, M )-coverage). We call D satisfies (H, M )-coverage if there exist H instancesx h(n) , n ? [H] such that D = ? H n=1 D h(n) , where each D h(n) satisfies M -NN noise clusterability.</formula><p>Note Dentition 2 focuses on the clusterability of noise transition matrices, which is different from the clusterability of the true classes of labels. When M -NN noise clusterability holds forx n , the label noise in local dataset D n is effectively homogeneous. If D further satisfies (H, M )coverage, we can divide the training data D to H local sub-datasets D h(n) , n ? [H] and separately apply Algorithm 1 on each of them. The local estimates allow us to apply loss correction separately using different T (X) at different parts of the training data. Besides, when there is no M -NN noise clusterability, we may require knowing properly constructed sub-spaces to separate the data, with each part of them sharing similar noise rates <ref type="bibr">(Xia et al., 2020a;</ref>. We leave more detailed discussions in Appendix C.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Theoretical Guarantees</head><p>We will prove that our consensus equations are sufficient for estimating a unique T , and show the advantage of our approach in terms of a better sample complexity than the anchor point approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Uniqueness of Solution</head><p>Before formally presenting the uniqueness guarantee, we introduce two assumptions as we will need.</p><p>Assumption 1 (Nonsingular T ). The noise transition matrix is non-singular, i.e., Rank(T ) = K.</p><p>Assumption 2 (Informative T ). The diagonal elements of T are dominant, i.e., T ii &gt; T ij , ?i ? [K], j ? [K], j = i.</p><p>Assumption 1 is commonly made in the literature and ensures the effect of label noise is invertible <ref type="bibr">(Van Rooyen &amp; Williamson, 2017)</ref>. Assumption 2 characterizes a particular permutation of row vectors in T . See more discussions on their feasibility in Appendix C.3. The uniqueness is formally stated in Theorem 1. The proof is sketched at the end of main paper and detailed in Appendix B.1.</p><p>Theorem 1. When D satisfies the 2-NN label clusterability and T is nonsingular and informative, with a perfect knowledge of c <ref type="bibr">[?]</ref> , ? = 1, 2, 3, the solution of consensus equations (2) -(4) returns the true T uniquely.</p><p>Challenges Proving Theorem 1 is challenging due to: 1) The coupling effect between T and p makes the structure of solution T unclear; 2) Naively replacing p, e.g., using p = (T ) ?1 c <ref type="bibr">[1]</ref> , will introduce matrix inverse, which cannot be canceled with the Hadamard product; 3) A system of third-order equations with K 2 variables will have up to 3 K 2 solutions and the closed-form is not explicit.</p><p>Local estimates Our next corollary 1 extends Theorem 1 to local datasets, when T can be heterogeneous.</p><p>Corollary 1. When D satisfies (H, M )-coverage, each D h(n) satisfies 2-NN label clusterability, and T (x h(n) ) is nonsingular and informative, with a perfect knowledge of the local c <ref type="bibr">[?]</ref> , ? = 1, 2, 3, the solution of consensus equations (2) -(4) is unique and recovers T (x h(n) ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Sample Complexity</head><p>We next show that with the estimates? <ref type="bibr">[1]</ref> ,? <ref type="bibr">[2]</ref> , and? <ref type="bibr">[3]</ref> , HOC returns a reasonably well solution.</p><p>Recall that, in Section 3.3, Step 2 requires a particular E ? E * 3 to guarantee the i.i.d. property of the sample {(? n ,? n1 ,? n2 ), n ? E}. For a tractable sample complexity, we focus on a particular dataset D and feature extractor f conv such that 1) |E * 3 | = ?(N ) and 2)</p><formula xml:id="formula_27">T ij = 1?Tii K?1 , ?j = i, i ? [N ], j ? [N ].</formula><p>Supposing each tuple is drawn from non-overlapping balls, condition 1) is satisfied when the number of these non-overlapping balls covering the representation space is ?(N ). See Appendix B.2 for a detailed example when the representations are uniformly distributed. Lemma 1 shows the error upper bound of our estimate? c <ref type="bibr">[?]</ref> , ? = 1, 2, 3. See Appendix B.3 for the proof. Lemma 1. With probability 1??, ??, l, the estimation error |? <ref type="bibr">[?]</ref> [l]?c <ref type="bibr">[?]</ref> [l]| is bounded at the order of O( ln(1/?)/N ).</p><p>Lemma 1 is effectively the sample complexity of estimating |E * 3 | i.i.d. random variables by the sample mean. Due to assuming a uniform diagonal T , we only need to consider the estimation error ofT ii . For each i ? [K], see the result in Theorem 2 and the proof in Appendix B.4.</p><formula xml:id="formula_28">Theorem 2. When T ii &gt; 1?P(Y =i)+(K?1)P( Y =i) K(K?1)P(Y =i) , w.p. 1 ? 2?, |T ii ? T ii | is bounded at the order of O( ln(1/?)/N ).</formula><p>Theorem 2 indicates the sample complexity of our solution has the same order in terms of N compared to a standard empirical mean estimation in Lemma 1. Remark 1 shows our approach is better than using a set of anchor points in the sample complexity. Remark 1 (Comparison). The methods based on anchor points estimate T with N AC &lt; N (N AC N in many cases) anchor points. Thus w.p. 1 ? ?, the estimation error is at the order of O( ln(1/?)/N AC ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We present experiment settings as follows.</p><p>Datasets and models HOC is evaluated on three benchmark datasets: CIFAR-10, CIFAR-100 <ref type="bibr" target="#b19">(Krizhevsky et al., 2009) and</ref><ref type="bibr">Clothing1M (Xiao et al., 2015)</ref>. For the standard training step, we use ResNet34 for CIFAR-10 and CIFAR-100, and ResNet50 for Clothing1M. The representations come from the outputs before the final fully-connected layer of ResNet34/50. The distance between different representations is measured by the negative cosine similarity.  <ref type="bibr" target="#b23">(Liu &amp; Tao, 2015)</ref>. Symmetric noise is applied. Noise type HOC is tested on both synthetic label noise and real-world human label noise. The synthetic label noise includes two regimes: symmetric noise and instancedependent noise. For both regimes, the noise rate ? is the overall ratio of instances with a corrupted label in the whole dataset. The symmetric noise is generated by randomly flipping a clean label to the other possible classes w.p. ? (Xia et al., 2019). The basic idea of generating instancedependent noise is to randomly generate one vector for each class (K vectors in total) and project each incoming feature onto these K vectors (Xia et al., 2020b). The label noise is added by jointly considering the clean label and the projection results. See Appendix D.1 for more details. The real-world human noise comes from human annotations. Particularly, for the 50, 000 training images in CIFAR-10, we re-collect human annotations 2 from Amazon Mechanical Turk (MTurk) in February 2020. For the Clothing1M dataset, we train on 1 million noisy training instances reflecting the real-world human noise. <ref type="bibr">2</ref> We only collect one annotation for each image with a cost of ?10 per image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Performance of Estimating T</head><p>We compare HOC with T-revision (Xia et al., 2019) following the flow: 1) Estimation ? 2) Training ? 3) Revision. For a fair comparison, we follow their training framework and parameter settings to get representations. Particularly, we obtain the same model as the one that T-revision adopts before revision. As illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>, compared with the dynamical revision adopted in T-revision, HOC does not need to change or adapt in different epochs and still achieves lower estimation errors no matter the model is trained with forward corrected loss or reweighted loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Performance of Classification Accuracy</head><p>To test the classification performance, we adopt the flow: 1) Pre-training ? 2) Global Training ? 3) Local Training. Our HOC estimator is applied once at the beginning of each above step. In Stage-1, we load the standard ResNet50 model pre-trained on ImageNet to obtain basic representations. At the beginning of Stage-2 and Stage-3, we use the representations given by the current model. All experiments are repeated three times. HOC Global only employs  <ref type="bibr">, 2017)</ref>. We report the performance on synthetic instance-dependent label noise in <ref type="table" target="#tab_1">Table 1</ref> and real-world human-level label noise in <ref type="table" target="#tab_2">Table 2</ref>.</p><p>Comparing with these baselines (with similar data augmentations), both global estimates and local estimates given by HOC achieve satisfying performance, and the local estimates indeed provide sufficient performance improvement on CIFAR-10. When there are 100 classes, T contains 10k variables thus local estimates with only 10k instances may not be accurate, which leads to a slight performance drop in HOC Local on CIFAR-100 (but it still outperforms other methods).</p><p>Real human-level noise On CIFAR-10 with our selfcollected human-level noisy labels, HOC achieves a 0.097 estimation error in the global T and a 0.110 ? 0.027 error in estimating 300 local transition matrices. See more details in Appendix D.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Feasibility of 2-NN label clusterability</head><p>We show the ratio of feasible 2-NN tuples in <ref type="table" target="#tab_3">Table 3</ref>. One 2-NN tuple is called feasible ifx n and its 2-NN belong to the same true class. The feature extractors are obtained from overfitting CIFAR-10/100 with different noise levels. For example, CIFAR-10 Inst. ? = 0.2 indicates that we use the standard CE loss to train ResNet34 on CIFAR-10 with 20% instance-dependent label noise. The convolution layers when the model approaches nearly 100% training accuracy are selected as the feature extractor f conv (X). <ref type="table" target="#tab_3">Table 3</ref> shows, with a standard feature extractor, there are more than 2/3 of the feasible 2-NN tuples in most cases. Besides, reducing the sample size from 50k to 5k will not substantially reduce the ratio of feasible 2-NN tuples. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>This paper has proposed a new and flexible estimator of the noise transition matrix relying on the first-, second-, and third-order consensuses checking among an example and its' 2-NN's noisy labels. Future directions of this work include extending our estimator to collaborate with other learning with noisy label techniques. We are also interested in developing algorithms to identify critical masses of instances that share similar noise rates such that our estimator can be applied to local estimation more efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof Sketch for Theorem 1</head><p>The high-level idea of the proof is to connect the Hadamard products to matrix products, and prove that any linear combination of two or more rows of T does not exist in T .</p><p>Step I: Transform the second-order equations. By exploiting the relation between Hadamard products and matrix products, the second-order equations can be transformed to</p><formula xml:id="formula_29">T D p T = T ? , where T ? is fixed given c [2]</formula><p>r , ?r ? [K], and D p is a diagonal matrix with p as its main diagonals,</p><p>Step II: Transform the third-order equations. Following the idea in Step I, we can also transform the third-order</p><formula xml:id="formula_30">equations to (T ? T s ) = T T ?1 ? T ?,s , ?s ? [K], where T ?,s is fixed given c [3]</formula><p>r,s , ?r, s.</p><p>Step III: From matrices to vectors We analyze the rows u of T and transform the equations in Step II to (e.g.</p><formula xml:id="formula_31">s = 0) Au = u ? u, where A = T ? (T ?1 ? ) .</formula><p>Then we need to find the number of feasible vectors u.</p><p>Step IV: Construct the (K + 1)-th vector When T is non-singular, we prove the (K + 1)-th solution u K+1 must be identical u k , k ? [K].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wrapping-up: Unique T</head><p>Step IV shows T only contains K different feasible rows. The informativeness of T ensures the unique order of these K rows. Thus T is unique. Zhang, Z. and Sabuncu, M. Generalized cross entropy loss for training deep neural networks with noisy labels. In Advances in neural information processing systems, pp. 8778-8788, 2018.</p><p>Zhu, Z., Liu, T., and Liu, Y. A second-order approach to learning with instance-dependent label noise. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2021.</p><p>The Appendix is organized as follows.</p><p>? Section A presents the detailed examples and derivations of consensus equations.</p><p>? Section B includes proofs and other details about our theoretical results. Particularly, -Section B.1 proves the uniqueness of T .</p><p>-Section B.2 justifies the feasibility of assumption |E * 3 | = ?(N ) -Section B.3 shows the proof for Lemma 1 -Section B.4 shows the proof for Theorem 2.</p><p>? Section C presents more discussions, e.g., the soft 2-NN label clusterability, more details on local T (X), and the feasibility of our Assumption 1 &amp; 2 to guarantee the uniqueness of T .</p><p>? Section D shows more experimental settings and results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Derivation of Consensus Equations</head><p>For the first-order consensuses, we have</p><formula xml:id="formula_32">P( Y 1 = j 1 ) = i?[K] P( Y 1 = j 1 |Y 1 = i)P(Y 1 = i).</formula><p>For the second-order consensuses, we have</p><formula xml:id="formula_33">P( Y 1 = j 1 , Y 2 = j 2 ) = i?[K] P( Y 1 = j 1 , Y 2 = j 2 |Y 1 = i, Y 2 = i)P(Y 1 = Y 2 = i) (a) = i?[K] P( Y 1 = j 1 , Y 2 = j 2 |Y 1 = i, Y 2 = i) ? P(Y 1 = i) (b) = i?[K] P( Y 1 = j 1 |Y 1 = i) ? P( Y 2 = j 2 |Y 2 = i) ? P(Y 1 = i),</formula><p>where equality (a) holds due to the 2-NN label clusterability, i.e., Y 1 = Y 2 (= Y 3 ) w.p. 1, and equality (b) holds due to the conditional independency between Y 1 and Y 2 given their clean labels.</p><p>For the third-order consensuses, we have</p><formula xml:id="formula_34">P( Y 1 = j 1 , Y 2 = j 2 , Y 3 = j 3 ) = i?[K] P( Y 1 = j 1 , Y 2 = j 2 , Y 3 = j 3 |Y 1 = i, Y 2 = i, Y 3 = i)P(Y 1 = Y 2 = Y 3 = i) (a) = i?[K] P( Y 1 = j 1 , Y 2 = j 2 , Y 3 = j 3 |Y 1 = i, Y 2 = i, Y 3 = i)P(Y 1 = i) (b) = i?[K] P( Y 1 = j 1 |Y 1 = i)P( Y 2 = j 2 |Y 2 = i)P( Y 3 = j 3 |Y 3 = i)P(Y 1 = i).</formula><p>where equality (a) holds due to the 3-NN label clusterability, i.e., Y 1 = Y 2 = Y 3 w.p. 1, and equality (b) holds due to the conditional independency between Y 1 , Y 2 and Y 3 given their clean labels.</p><p>With the above analyses, there are 2 first-order equations, P( Y 1 = 1) = p 1 (1 ? e 1 ) + (1 ? p 1 )e 2 , P( Y 1 = 2) = p 1 e 1 + (1 ? p 1 )(1 ? e 2 ).</p><p>There are 4 second-order equations for different combinations of Y 1 , Y 2 , e.g., P( Y 1 = 1, Y 2 = 1) = p 1 (1 ? e 1 ) 2 + (1 ? p 1 )e 2 2 , P( Y 1 = 1, Y 2 = 2) = p 1 (1 ? e 1 )e 1 + (1 ? p 1 )e 2 (1 ? e 2 ), P( Y 1 = 2, Y 2 = 1) = p 1 (1 ? e 1 )e 1 + (1 ? p 1 )e 2 (1 ? e 2 ),</p><formula xml:id="formula_35">P( Y 1 = 1, Y 2 = 1) = p 1 e 2 1 + (1 ? p 1 )(1 ? e 2 ) 2 .</formula><p>There are 8 third-order equations for different combinations of Y 1 , Y 2 , Y 3 , e.g.,</p><formula xml:id="formula_36">P( Y 1 = 1, Y 2 = 1, Y 3 = 1) = p 1 (1 ? e 1 ) 3 + (1 ? p 1 )e 3 2 ,</formula><formula xml:id="formula_37">P( Y 1 = 1, Y 2 = 1, Y 3 = 2) = p 1 (1 ? e 1 ) 2 e 1 + (1 ? p 1 )e 2 2 (1 ? e 2 ),</formula><formula xml:id="formula_38">P( Y 1 = 1, Y 2 = 2, Y 3 = 1) = p 1 (1 ? e 1 ) 2 e 1 + (1 ? p 1 )e 2 2 (1 ? e 2 ),</formula><formula xml:id="formula_39">P( Y 1 = 1, Y 2 = 2, Y 3 = 2) = p 1 (1 ? e 1 )e 2 1 + (1 ? p 1 )e 2 (1 ? e 2 ) 2 , P( Y 1 = 2, Y 2 = 1, Y 3 = 1) = p 1 (1 ? e 1 ) 2 e 1 + (1 ? p 1 )e 2 2 (1 ? e 2 ), P( Y 1 = 2, Y 2 = 1, Y 3 = 2) = p 1 (1 ? e 1 )e 2 1 + (1 ? p 1 )e 2 (1 ? e 2 ) 2 , P( Y 1 = 2, Y 2 = 2, Y 3 = 1) = p 1 (1 ? e 1 )e 2 1 + (1 ? p 1 )e 2 (1 ? e 2 ) 2 , P( Y 1 = 2, Y 2 = 2, Y 3 = 2) = p 1 e 3 1 + (1 ? p 1 )(1 ? e 2 ) 3 .</formula><p>For a general K-class classification problem, we show one first-order consensus below:</p><formula xml:id="formula_40">e j c [1] = P( Y 1 = j) = i?[K] P( Y 1 = j|Y 1 = i)P(Y 1 = i) = i?[K]</formula><p>T ij ? p i =e j T p.</p><p>The second-order consensus follows the example below:</p><formula xml:id="formula_41">e j c [2] r = P( Y 1 = j, Y 2 = (j + r) K ) (a) = i?[K] P( Y 1 = j|Y 1 = i)P( Y 2 = (j + r) K |Y 2 = i)P(Y 1 = i) = i?[K] T i,j ? T i,(j+r) K ? p i (b) = e j (T ? T r ) p,</formula><p>where equality (a) holds again due to the 2-NN label clusterability the conditional independency (similar to binary cases), and equality (b) holds due to T r [i, j] = T i,(j+r) K . We also show one third-order consensus below:</p><formula xml:id="formula_42">e j c [3] r = P( Y 1 = j, Y 2 = (j + r) K , Y 3 = (j + s) K ) (a) = i?[K] P( Y 1 = j|Y 1 = i)P( Y 2 = (j + r) K |Y 2 = i)P( Y 3 = (j + s) K |Y 3 = i)P(Y 1 = i) = i?[K] T i,j ? T i,(j+r) K ? T i,(j+s) K ? p i (b) = e j (T ? T r ? T s ) p,</formula><p>where equality (a) holds again due to the 3-NN label clusterability the conditional independency (similar to binary cases), and equality (b) holds due to T r [i, j] = T i,(j+r) K , T s [i, j] = T i,(j+s) K .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Theoretical Guarantees</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Uniqueness of T</head><p>We need to prove the following equations have a unique solution when T is non-singular and informative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consensus Equations</head><p>? First-order (K equations):</p><formula xml:id="formula_43">c [1] := T p,</formula><p>? Second-order (K 2 equations):</p><formula xml:id="formula_44">c [2] r := (T ? T r ) p, r ? [K],</formula><p>? Third-order (K 3 equations):</p><formula xml:id="formula_45">c [3] r,s := (T ? T r ? T s ) p, r, s ? [K].</formula><p>Firstly, we need the following Lemma for the Hadamard product of matrices:</p><p>Lemma 2. <ref type="bibr" target="#b15">(Horn &amp; Johnson, 2012)</ref> For column vectors x and y, and corresponding diagonal matrices D x and D y with these vectors as their main diagonals, the following identity holds:</p><formula xml:id="formula_46">x * (A ? B)y = tr D * x AD y B ,</formula><p>where x * denotes the conjugate transpose of x.</p><p>The following proof focuses on the second and third-order consensuses. It is worth noting that, although the first-order consensus is not necessary for the derivation of the unique solution, it still helps improve the stability of solving for T and p numerically.</p><p>Step I: Transform the second-order equations. Denoted by T r = T S r , where S r permutes particular columns of T . Let e i be the column vector with only the i-th element being 1 and 0 otherwise. With Lemma 2, the second-order consensus can be transformed as e i c [2] r = e i (T ? T r ) p = tr D ei T D p T S r Then the (i, (i + r) K )-th element of matrix T D p T is</p><formula xml:id="formula_47">(T D p T )[i, (i + r) K ] = e i c [2]</formula><p>r .</p><p>With a fixed e i c</p><formula xml:id="formula_48">[2] r , ?i, r ? [K], denote by T D p T = T ? ,<label>(11)</label></formula><p>where</p><formula xml:id="formula_49">T ? [i, (i + r) K ] = e i c [2] r . Note T ? is fixed given c [2] r , ?r ? [K].</formula><p>Step II: Transform the third-order equations. Following the idea in Step I, we can also transform the third-order equations. First, notice that</p><formula xml:id="formula_50">e i c [3] r,s = e i [(T ? T s ) ? T r ] p = tr D ei (T ? T s ) D p T S r .</formula><p>Then the (i,</p><formula xml:id="formula_51">(i + r) K )-th element of matrix (T ? T s ) D p T is ((T ? T s ) D p T )[i, (i + r) K ] = e i c [3] r,s .</formula><p>With a fixed e i c <ref type="bibr">[3]</ref> r,s , ?i, r ? [K], denote by</p><formula xml:id="formula_52">(T ? T s ) D p T = T ?,s ? T D p (T ? T s ) = T ?,s ,<label>(12)</label></formula><p>where T ?,s [i, (i + r) K ] = e i c <ref type="bibr">[3]</ref> r,s . According to Eqn. (11), we have</p><formula xml:id="formula_53">T D p (T ? T s ) = T D p T T ?1 (T ? T s ) = T ? T ?1 (T ? T s ) = T ?,s . Thus (T ? T s ) = T T ?1 ? T ?,s , ?s ? [K].<label>(13)</label></formula><p>Step III: From matrices to vectors With Step I and Step II, we could transform the equations formulated by the second and the third-order consensuses to a particular system of multivariate quadratic equations of T in Eqn. (13). Generally, these equations could have up to 2 K 2 solutions introduced by different combinations of each element in T . To prove the uniqueness of T , we need to exploit the structure of the equations in (13).</p><p>For a clear representation of the structure of equations and solutions, we first consider one subset of the equations in (13). Specifically, let s = 0 we have</p><formula xml:id="formula_54">(T ? T ) = T T ?1 ? T ? .<label>(14)</label></formula><p>Then we need to study the number of feasible T satisfying Eqn. <ref type="bibr">(14)</ref>. Denote by A = T ? (T ?1 ? ) . Then each row of T , denoted by u , is a solution to the equation</p><formula xml:id="formula_55">Au = D u u (a.k.a. Au = u ? u).<label>(15)</label></formula><p>Till now, in Step III, we split the matrix T to several vectors u, and transform our target from finding a matrix solution T for (13) to a set of vector solutions u for <ref type="formula" target="#formula_7">(15)</ref>.</p><p>Assume there are M feasible u vectors. We collect all the possible u and define U :</p><formula xml:id="formula_56">= [u 1 , u 2 , ? ? ? , u M ], u i = u i , ?i, i ? [M ]. If M = K,</formula><p>we know there exists at most K! different T (considering all the possible permutations of u) that Eqn. <ref type="formula" target="#formula_7">(14)</ref> holds. Further, by considering an informative T as Assumption 2, we can identify a particular permutation. Therefore, if M = K and T is informative, we know there exists and only exists one unique T that Eqn. (14) holds.</p><p>Step IV: Constructing the M -th vector Supposing M &gt; K, we have</p><formula xml:id="formula_57">AU = A[u 1 , u 2 , ? ? ? , u K , ? ? ? u M ] = [D u1 u 1 , D u2 u 2 , ? ? ? , D u K u K , ? ? ? D u M u M ].</formula><p>With a non-singular T (Assumption 1), without loss of generality, we will assume the first K columns are fullrank. Then u M must be a linear combination of the first K columns, i.e., u M = i?[K] ? i u i = U ? 0 , where ? 0 = [? 1 , ? 2 , ? ? ? , ? K , 0, ? ? ? , 0]. According to the equation Au = D u u = u ? u, we have</p><formula xml:id="formula_58">Au M = D u M u M = D U ?0 U ? 0 , and Au M = i?[M ] ? 0 [i]Au i = i?[M ] ? 0 [i]u i ? u i = (U ? U )? 0 . Thus (U ? U )? 0 = D U ?0 U ? 0 = (U ? 0 ) ? (U ? 0 ).</formula><p>Note that, the matrix U can be written as U = [U K , U M ?K ], and the vector ? 0 can be written as ? 0 = [? , 0, ? ? ? , 0] , where ? := [? 1 , ? ? ? , ? K ] . Then the above equation can be transformed as follows:</p><formula xml:id="formula_59">(U K ? U K )? = u M ? u M , and U K ? = u M .</formula><p>Similarly, ?s ? [K], we have </p><formula xml:id="formula_60">(U K ? (S s U K ))? = u M ? (S s u M ), and U K ? = u M ,<label>whereS</label></formula><formula xml:id="formula_61">tr(D ei U K D ? U KS s ) = tr(D ei U K D ? U K S s ) = (u M ? (S s u M ))[i]</formula><p>Then the (i, (i + s) K )-th element of matrix U K D ? U K is</p><formula xml:id="formula_62">(U K D ? U K )[i, (i + s) K ] = (u M ? (S s u M ))[i] = u M [i] ? u M [(i + s) K ].</formula><p>Then we have</p><formula xml:id="formula_63">U K D ? U K = Q, and Q = u M u M .</formula><p>When T is non-singular, we know U is invertible (full-rank), then</p><formula xml:id="formula_64">D ? = (U ?1 K u M )(U ?1 K u M )</formula><p>. Thus Rank(D ? ) = 1. Recalling 1 ? = 1, the vector ? could only be one-hot vectors, i.e. e i , ?i ? [K]. This proves u M must be the same as one of u i , i ? [K].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wrapping-up: Unique T From</head><p>Step III, we know that, if M = K, we have a unique T under the assumption that T is informative and non-singular.</p><p>Step IV proves the M -th (M &gt; K) vector u must be identical to one of u i , i ? [K], indicating we only have M = K non-repetitive u vectors. Therefore, our consensus equations are sufficient for guaranteeing a unique T . Besides, note there is no approximation applied during the whole proof. Thus with a perfect knowledge of c <ref type="bibr">[?]</ref> , ? = 1, 2, 3, the unique T satisfying the consensus equations is indeed the true noise transition matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Feasibility of Assumption |E *</head><p>3 | = ?(N ) We discuss the feasibility of our assumption on the number of 3-tuples. According to the definition of E * 3 , we know there are no more than |E * 3 | ? N/3 feasible 3-tuples. Strictly deriving the lower bound for |E * 3 | is challenging due to the unknown distributions of representations. To roughly estimate the order of |E * 3 | (i.e., the maximum number of non-overlapping 3-tuples), we consider a special scenario where those high-dimensional representations could be mapped to a 2-D square of width N/3, each grid of width 1 has exactly 3 mapped representations, and one mapped representation is at the center of each grid (also the center of each circle). Consider a particular construction of feasible 3-tuples as illustrated in <ref type="figure" target="#fig_3">Figure 4</ref>. We require that, for each grid, the 2-NN fall in the corresponding circle. Otherwise, they may become the 2-NN of representations in other nearby girds. Assume the 2-NN are independently and uniformly distributed in the unit square, thus the probability of both 2-NN falling in the circle is (?/4) 2 . Noting there are N/3 grids in the big square illustrated in <ref type="figure" target="#fig_3">Figure 4</ref>, the expected number of feasible 3-tuples in this case is ? 2 48 ? N = ?(N ). Although this example only considers a special case, it demonstrates the order of |E * 3 | could be ?(N ) with appropriate representations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Proof for Lemma 1</head><p>Then we present the proof for Lemma 1.</p><p>Proof. Recall in Eqn. <ref type="formula" target="#formula_17">(7)</ref>, each high-order consensus pattern could be estimated by the sample mean of |E * 3 | independent and identically distributed random variables, thus according to Hoeffding's inequality <ref type="bibr" target="#b14">(Hoeffding, 1963)</ref>, w.p. 1 ? ?, we have</p><formula xml:id="formula_65">|? [i] [j] ? c [i] [j]| ? ln 2 ? 2|E * 3 | , i = 1, 2, 3, ?j,</formula><p>which is at the order of O( ln(1/?)/N ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Proof for Theorem 2</head><p>Consider a particular uniform off-diagonal matrix T , where the off-diagonal elements are T ij = 1?Tii K?1 . Recall the clean prior probability for the i-th class is p i . To find the upper bound for the sample complexity, we can only consider a subset of our consensus equations. Specifically, we consider the equations related to the i-th element of Eqn. (2) and Eqn. (3) when r = 0. Then a solution to our consensus equations will need to satisfy at least the following two equations:</p><formula xml:id="formula_66">p iTii + (1 ?p i ) 1 ?T ii K ? 1 =? 1 ,<label>(16)</label></formula><formula xml:id="formula_67">p iT 2 ii + (1 ?p i ) (1 ?T ii ) 2 (K ? 1) 2 =? 2 ,<label>(17)</label></formula><p>wherep i andT ii denote the estimated clean prior probability and noisy transition matrix,? 1 and? 2 denote the corresponding estimates of first-and second-order statistics. Lemma 1 shows, with probability 1 ? ?:</p><formula xml:id="formula_68">|? i ? c i | ? O ln(1/?) N .</formula><p>Multiplying both sides of Eqn. (16) by T ii and adding Eqn. (17), we have</p><formula xml:id="formula_69">K(K ? 1)p iT 2 ii + (1 ?p i )(1 ?T ii ) = (K ? 1)? 1Tii + (K ? 1) 2? 2 .</formula><p>Note the above equality also holds for the true values p i , T ii , c 1 , c 2 . Taking the difference we have</p><formula xml:id="formula_70">(T ii ? T ii )(K(K ? 1)p i (T ii +T ii ) ? (1 ? p i ) ? (K ? 1)c 1 ) =(K ? 1) 2 (? 2 ? c 2 ) + (K ? 1)(? 1 ? c 1 )T ii ? K(K ? 1)T 2 ii (p i ? p i ) ? (T ii ? 1)(p i ? p i ).</formula><p>Taking the absolute value for both sides yields</p><formula xml:id="formula_71">|T ii ? T ii | ? |K(K ? 1)p i (T ii +T ii ) ? (1 ? p i ) ? (K ? 1)c 1 | ?(K ? 1) 2 |? 2 ? c 2 | + (K ? 1)|? 1 ? c 1 | + (K(K ? 1) + 1)|p i ? p i | From Eqn. (16), we havep i = K ? 1 K? 1 ? 1/K T ii ? 1/K + 1 K .</formula><p>Thus</p><formula xml:id="formula_72">|p i ? p i | ? K ? 1 K |? 1 ? c 1 | min(T ii , T ii ) ? 1/K , indicating |p i ? p i | is at the order of |? 1 ? c 1 |. Note that K(K ? 1)p i (T ii +T ii ) ? (1 ? p i ) ? (K ? 1)c 1 ? K(K ? 1)p i T ii ? (1 ? p i ) ? (K ? 1)c 1 . When K(K ? 1)p i T ii ? (1 ? p i ) ? (K ? 1)c 1 &gt; 0, we have |T ii ? T ii | ? (K ? 1) 2 |? 2 ? c 2 | + (K ? 1)|? 1 ? c 1 | + (K(K ? 1) + 1) K?1 K |?1?c1| min(Tii,Tii)?1/K K(K ? 1)p i T ii ? (1 ? p i ) ? (K ? 1)c 1 .</formula><p>Then by union bound we know, w.p. 1 ? 2?, the estimation error |T ii ? T ii | is at the same order as |? i ? c i |, i.e. O( ln(1/?) N ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. More Discussions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Soft 2-NN Label Clusterability</head><p>The soft 2-NN label clusterability means one's 2-NN may have a certain (but small) probability of belonging to different clean classes. Statistically, if we use a new matrix T soft to characterize the probability of getting a different nearest neighbor, i.e. T soft ij = P(Y 2 = j|Y 1 = i) = P(Y 3 = j|Y 1 = i), the second-order consensuses become c <ref type="bibr">[2]</ref> r := (T ? (T soft T r )) p and the third-order consensuses become c <ref type="bibr">[3]</ref> r,s := (T ? (T soft T r ) ? (T soft T s )) p. Specifically, if T soft ij = e, ?i = j and T soft ii = 1 ? (K ? 1)e, 0 ? e &lt; 1/K, where e captures the small perturbation of the 2-NN assumption, our solution will likely output a transition matrix that affects the label noise between the effects of T soft T and T . The above observation informs us that our estimation will be away from the true T by at most a factor e. When e = 0, we recover the original 2-NN label clusterability condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Local T (X)</head><p>Sparse regularizer Compared with estimating one global T using the whole dataset of size N , each local estimation will have access to only M instances, where M N . Thus the feasibility of returning an accurate T (x n ) requires more consideration. In some particular cases, e.g., HOC Local in <ref type="table" target="#tab_1">Table 1</ref>, when p is sparse due to the local datasets, we usually add a regularizer to ensure a sparse p, such as i? <ref type="bibr">[K]</ref> ln(c i + ?), ? ? 0 + , where c i is the i-th element of p. Note the standard sparse regularizer, i.e. 1 -norm p 1 , could not be applied here since p 1 = 1. Therefore, with a regularizer that shrinks the search space and fewer variables, we could get an accurate estimate of T (X) with a small M .</p><p>Other extensions Even with M -NN noise clusterability, estimating T (X) for the whole dataset requires executing Algorithm 1 a numerous number of times (? N/M ). If equipped with prior knowledge that the label noise can be divided into several groups and T = T (X) within each group <ref type="bibr">(Xia et al., 2020b;</ref><ref type="bibr">Wang et al., 2021)</ref>, we only need to estimate T for each group by treating instances in each group as a local dataset and directly apply Algorithm 1. As a preliminary work on estimating T relying on clusterability, the focus of this paper is to provide a generic method for estimating T given a dataset. Designing efficient algorithms to split the original dataset into a tractable number of local datasets is interesting for future investigation.</p><p>C.3. Feasibility of Assumption 1 and Assumption 2 1. Denote the confusion matrix by C[h], where each element is C ij [h] := P(Y = i, h(X) = j) and h(X) = j represents the event that the classifier predicts j given feature X. Then the noisy confusion matrix could be written as C[h] := T C[h]. If T is non-singular (a.k.a. invertible), statistically, we can always find the inverse matrix T ?1 such that the clean confusion matrix could be recovered as C[h] = (T ?1 ) C[h]. Otherwise, we may think the label noise is too "much" such that the clean confusion matrix is not recoverable by T . Then learning T may not be meaningful anymore. Therefore, Assumption 1 is effectively ensuring the necessity of estimating T .</p><p>2. We require T ii &gt; T ij in Assumption 2 to ensure instances from observed class i (observed from noisy labels) are informative <ref type="bibr">(Liu &amp; Chen, 2017)</ref>. Intuitively, this assumption characterizes a particular permutation of row vectors in T . Otherwise, there may exist K! possible solutions by considering all the permutations of K rows .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. More Detailed Experiment Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Generating the Instance-Dependent Label Noise</head><p>In this section, we introduce how to generate instance-based label noise, which is illustrated in Algorithm 2. Note this algorithm follows the state-of-the-art method <ref type="bibr">(Xia et al., 2020b;</ref><ref type="bibr">Zhu et al., 2021)</ref>. Define the noise rate (the global flipping rate) as ?. To calculate the probability of x n mapping to each class under certain noise conditions, we set sample instance flip rates q n and sample parameters W . The size of W is S ? K, where S denotes the length of each feature.</p><p>First, we sample instance flip rates q n from a truncated normal distribution N(?, 0.1 2 , [0, 1]) in Line 2. The average flipping rate (a.k.a. average noise rate) is ?. q n avoids all the instances having the same flip rate. Then, in Line 3, we sample parameters W from the standard normal distribution for generating the instance-dependent label noise. Each column of W acts as a projection vector. After acquiring q n and W , we can calculate the probability of getting a wrong label for each works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Global and Local Estimation Errors on CIFAR-10 with Human Noise</head><p>Algorithm 3 details the generation of local datasets. Notice the fact that the i-th row of T (x n ) could be any feasible values when p i = 0, so as the estimatesT local . In such case, we need to refer to T to complete the information. Particularly, we calculate the weighted average value with the correspondingT a?</p><formula xml:id="formula_73">T local [i] = (1 ? ? +p i )T local [i] + (? ?p i )T [i],</formula><p>whereT local [i] andT [i] denote the i-th row of estimatesT local andT ,p i denotes the estimated clean prior probability of class-i given the local dataset. We use ? = 1 for local estimates of CIFAR-10, and ? = 0.5 for local estimate of CIFAR-100. <ref type="figure" target="#fig_4">Figure 5</ref> illustrates the variation of local estimation errors on CIFAR-10 with human noise using HOC.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Illustration of k-NN label clusterability. dependent settings (Xia et al., 2019).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of high-order consensuses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Comparison of estimation errors of T given byT-Revision (Xia et al., 2019)  and our HOC estimator. The error is measured by the matrix L1,1-norm with a normalization factor K, i.e. T ? T 1,1/K . Forward: Using the forward corrected loss(Patrini et al., 2017)   Reweight: Using the reweighted loss</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Illustration of a special case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Illustration of the global and local estimation errors. Global estimation error: 0.0970. Local estimation errors: mean = 0.1103, standard deviation = 0.0278.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Variable definitions (8b): the closed-form relationship between intermediate variables (such as c [?] and T r ) and the optimized variables (T and p).</figDesc><table><row><cell>are:</cell></row></table><note>? Objective (8a): the sum of errors from each order of consensus, where the error is defined in 2 -norm.?? Constraints</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>The best epoch (clean) test accuracy (%) with synthetic label noise. 66?0.62 76.89?0.93 60.29?1.17 57.26?1.33 41.33?0.89 25.08?1.85 Peer Loss (Liu &amp; Guo, 2020) 89.52?0.22 83.44?0.30 75.15?0.82 61.13?0.48 48.01?0.12 33.00?1.47 L DMI (Xu et al., 2019) 88.67?0.70 83.65?1.13 69.82?1.72 57.36?1.18 43.06?0.97 26.13?2.39 L q (Zhang &amp; Sabuncu, 2018) 85.66?1.09 75.24?1.07 61.30?3.35 56.92?0.24 40.17?1.52 25.58?3.12 Co-teaching (Han et al., 2018) 88.84?0.20 72.61?1.35 63.76?1.11 43.37?0.47 23.20?0.44 12.43?0.50 Co-teaching+ (Yu et al., 2019) 89.82?0.39 73.44?0.38 63.61?1.78 41.62?1.05 24.73?0.85 12.25?0.35 JoCoR (Wei et al., 2020) 88.82?0.20 71.13?1.94 63.88?2.05 44.55?0.62 23.92?0.32 13.05?1.10 Forward (Patrini et al., 2017) 87.87?0.96 79.81?2.58 68.32?1.68 57.69?1.55 42.62?0.92 27.35?3.42 T-Revision (Xia et al., 2019) 90.31?0.37 84.99?0.81 72.06?3.40 58.00?0.20 40.01?0.32 40.88?7.57 HOC Global 89.71?0.51 84.62?1.02 70.67?3.38 68.82?0.26 62.29?1.11 52.96?1.85 HOC Local 90.03?0.15 85.49?0.80 77.40?0.47 67.47?0.85 61.20?1.04 49.84?1.81</figDesc><table><row><cell>Method</cell><cell>? = 0.2</cell><cell>Inst. CIFAR-10 ? = 0.4</cell><cell>? = 0.6</cell><cell>? = 0.2</cell><cell>Inst. CIFAR-100 ? = 0.4</cell><cell>? = 0.6</cell></row><row><cell>CE (Standard)</cell><cell>85.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>The best epoch test accuracy (%) with human noise.</figDesc><table><row><cell>Method</cell><cell cols="2">Clothing1M Human CIFAR-10</cell></row><row><cell>CE (standard)</cell><cell>68.94</cell><cell>83.50</cell></row><row><cell>CORES 2 (Cheng et al., 2020)</cell><cell>73.24</cell><cell>89.98</cell></row><row><cell>L DMI (Xu et al., 2019)</cell><cell>72.46</cell><cell>86.33</cell></row><row><cell>Co-teaching (Han et al., 2018)</cell><cell>69.21</cell><cell>90.39</cell></row><row><cell>JoCoR (Wei et al., 2020)</cell><cell>70.30</cell><cell>90.10</cell></row><row><cell>Forward (Patrini et al., 2017)</cell><cell>70.83</cell><cell>86.82</cell></row><row><cell>PTD-R-V(Xia et al., 2020b)</cell><cell>71.67</cell><cell>85.92</cell></row><row><cell>HOC</cell><cell>73.39</cell><cell>90.62</cell></row><row><cell cols="3">one global T with G = 50 and |E| = 15k as inputs of</cell></row><row><cell cols="3">Algorithm 2. HOC Local uses 300 local matrices (250-NN</cell></row><row><cell cols="3">noise clusterability, G = 30, |E| = 100) for CIFAR-10</cell></row><row><cell cols="3">and 5 local matrices (10k-NN noise clusterability, G = 30,</cell></row><row><cell cols="3">|E| = 5k) for CIFAR-100. 3 See more details in Appendix D.</cell></row><row><cell cols="3">Without sophisticated learning techniques, we simply feed</cell></row></table><note>the estimated transition matrices given by HOC into for- ward loss correction (Patrini et al.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>The ratio of feasible 2-NN tuples with different feature extractors. |E| = 5k: Sample 5k examples from the whole dataset in each round, and average over 10 rounds. |E| = 50k: Check the feasibility of all 2-NN tuples.</figDesc><table><row><cell>Feature Extractor</cell><cell cols="4">CIFAR-10 |E| = 5k |E| = 50k |E| = 5k |E| = 50k CIFAR-100</cell></row><row><cell>Clean</cell><cell>99.99</cell><cell>99.99</cell><cell>99.88</cell><cell>99.90</cell></row><row><cell>Inst. ? = 0.2</cell><cell>87.88</cell><cell>89.06</cell><cell>82.82</cell><cell>84.33</cell></row><row><cell>Inst. ? = 0.4</cell><cell>78.15</cell><cell>79.85</cell><cell>64.88</cell><cell>68.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Liu, Y. and Chen, Y. Machine-learning aided peer prediction. In Proceedings of the 2017 ACM Conference on Economics and Computation, pp. 63-80, 2017. Liu, Y. and Guo, H. Peer loss functions: Learning from noisy labels without knowing noise rates. In Proceedings of the 37th International Conference on Machine Learning, ICML '20, 2020. Wei, H., Feng, L., Chen, X., and An, B. Combating noisy labels by agreement: A joint training method with coregularization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13726-13735, 2020. In Proceedings of the 36th International Conference on Machine Learning, volume 97, pp. 7164-7173. PMLR, 09-15 Jun 2019.</figDesc><table><row><cell>label corruption?</cell><cell></cell></row><row><cell></cell><cell>Wei, J. and Liu, Y. When optimizing f-divergence is robust</cell></row><row><cell></cell><cell>with label noise. In International Conference on Learning</cell></row><row><cell>Liu, Y. and Liu, M. An online learning approach to improv-</cell><cell>Representations, 2021.</cell></row><row><cell>ing the quality of crowd-sourcing. ACM SIGMETRICS</cell><cell>Wei, J., Liu, H., Liu, T., Niu, G., and Liu, Y. Understanding</cell></row><row><cell>Performance Evaluation Review, 43(1):217-230, 2015.</cell><cell>(generalized) label smoothing when learning with noisy</cell></row><row><cell>Liu, Y., Wang, J., and Chen, Y. Surrogate scoring rules. In</cell><cell>labels. 2021.</cell></row><row><cell>Proceedings of the 21st ACM Conference on Economics and Computation, pp. 853-871, 2020.</cell><cell>Xia, X., Liu, T., Wang, N., Han, B., Gong, C., Niu, G., and Sugiyama, M. Are anchor points really indispensable in</cell></row><row><cell>Lukasik, M., Bhojanapalli, S., Menon, A., and Kumar, S.</cell><cell>label-noise learning? In Advances in Neural Information</cell></row><row><cell>Does label smoothing mitigate label noise? In Interna-</cell><cell>Processing Systems, pp. 6838-6849, 2019.</cell></row><row><cell>tional Conference on Machine Learning, pp. 6448-6458. PMLR, 2020.</cell><cell>Xia, X., Liu, T., Han, B., Wang, N., Deng, J., Li, J., and Mao, Y. Extended T: Learning with mixed closed-set and</cell></row><row><cell>Natarajan, N., Dhillon, I. S., Ravikumar, P. K., and Tewari,</cell><cell>open-set noisy labels. arXiv preprint arXiv:2012.00932,</cell></row><row><cell>A. Learning with noisy labels. In Advances in neural</cell><cell>2020a.</cell></row><row><cell>information processing systems, pp. 1196-1204, 2013.</cell><cell>Xia, X., Liu, T., Han, B., Wang, N., Gong, M., Liu, H., Niu,</cell></row><row><cell>Northcutt, C., Jiang, L., and Chuang, I. Confident learn-</cell><cell>G., Tao, D., and Sugiyama, M. Part-dependent label noise:</cell></row><row><cell>ing: Estimating uncertainty in dataset labels. Journal of</cell><cell>Towards instance-dependent label noise. In Advances in</cell></row><row><cell>Artificial Intelligence Research, 70:1373-1411, 2021.</cell><cell>Neural Information Processing Systems, volume 33, pp.</cell></row><row><cell>Northcutt, C. G., Wu, T., and Chuang, I. L. Learning with</cell><cell>7597-7610, 2020b.</cell></row><row><cell>confident examples: Rank pruning for robust classifica-</cell><cell>Xia, X., Liu, T., Han, B., Gong, C., Wang, N., Ge, Z., and</cell></row><row><cell>tion with noisy labels. UAI, 2017.</cell><cell>Chang, Y. Robust early-learning: Hindering the memo-</cell></row><row><cell>Patrini, G., Rozza, A., Krishna Menon, A., Nock, R., and Qu, L. Making deep neural networks robust to label noise:</cell><cell>rization of noisy labels. In International Conference on Learning Representations, 2021.</cell></row><row><cell>A loss correction approach. In Proceedings of the IEEE</cell><cell>Xiao, T., Xia, T., Yang, Y., Huang, C., and Wang, X. Learn-</cell></row><row><cell>Conference on Computer Vision and Pattern Recognition,</cell><cell>ing from massive noisy labeled data for image classifica-</cell></row><row><cell>pp. 1944-1952, 2017.</cell><cell>tion. In Proceedings of the IEEE Conference on Computer</cell></row><row><cell>Scott, C. A rate of convergence for mixture proportion</cell><cell>Vision and Pattern Recognition, pp. 2691-2699, 2015.</cell></row><row><cell>estimation, with application to learning from noisy labels.</cell><cell>Xu, Y., Cao, P., Kong, Y., and Wang, Y. L dmi: A novel</cell></row><row><cell>In AISTATS, 2015.</cell><cell>information-theoretic loss function for training deep nets</cell></row><row><cell>Shu, J., Zhao, Q., Chen, K., Xu, Z., and Meng, D. Learning adaptive loss for robust learning with noisy labels. arXiv</cell><cell>robust to label noise. In Advances in Neural Information Processing Systems, volume 32, 2019.</cell></row><row><cell>preprint arXiv:2002.06482, 2020.</cell><cell>Yao, Q., Yang, H., Han, B., Niu, G., and Kwok, J. T. Search-</cell></row><row><cell>Van Rooyen, B. and Williamson, R. C. A theory of learning</cell><cell>ing to exploit memorization effect in learning with noisy</cell></row><row><cell>with corrupted labels. J. Mach. Learn. Res., 18(1):8501-</cell><cell>labels. In Proceedings of the 37th International Confer-</cell></row><row><cell>8550, 2017.</cell><cell>ence on Machine Learning, ICML '20, 2020a.</cell></row><row><cell>Wang, J., Liu, Y., and Levy, C. Fair classification with</cell><cell>Yao, Y., Liu, T., Han, B., Gong, M., Deng, J., Niu, G., and</cell></row><row><cell>group-dependent label noise. FAccT, pp. 526-536, New</cell><cell>Sugiyama, M. Dual t: Reducing estimation error for</cell></row><row><cell>York, NY, USA, 2021.</cell><cell>transition matrix in label-noise learning. In Advances in</cell></row><row><cell></cell><cell>Neural Information Processing Systems, volume 33, pp.</cell></row><row><cell></cell><cell>7260-7271, 2020b.</cell></row><row><cell></cell><cell>Yu, X., Han, B., Yao, J., Niu, G., Tsang, I., and Sugiyama,</cell></row><row><cell></cell><cell>M. How does disagreement help generalization against</cell></row></table><note>Wang, Y., Ma, X., Chen, Z., Luo, Y., Yi, J., and Bailey, J. Symmetric cross entropy for robust learning with noisy labels. In Proceedings of the IEEE International Confer- ence on Computer Vision, pp. 322-330, 2019.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Distances are measured between representations. Feature xn and its representationxn refer to the same data point in different views.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Our unconstrained transformation provides much better convergence such that running HOC Local on CIFAR will at most double the running time of a standard training with CE.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>Algorithm 2 Instance-Dependent Label Noise Generation Input:</p><p>1: Clean examples (x n , y n ) N n=1 ; Noise rate: ?; Size of feature: 1 ? S; Number of classes: K. Iteration:</p><p>2: Sample instance flip rates q n from the truncated normal distribution N (?, 0.1 2 , [0, 1]); 3: Sample W ? R S?K from the standard normal distribution N (0, 1 2 ); for n = 1 to N do 4: p = x n ? W // Generate instance dependent flip rates. The size of p is 1 ? K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>p yn = ?? // Only consider entries that are different from the true label 6: p = q n ? SoftMax(p) // Let qn be the probability of getting a wrong label 7:</p><p>p yn = 1 ? q n // Keep clean w.p. 1 ? qn 8:</p><p>Randomly choose a label from the label space as noisy label? n according to p; end for Output:</p><p>instance(x n , y n ) in Lines 4 -6. Note that in Line 5, we set p yn = ??, which ensures that x n will not be mapped to its own true label. In addition, Line 7 ensures the sum of all the entries of p is 1. Suppose there are two features: x i and x j where x i = x j . Then the possibility p of these two features, calculated by x ? W , from the Algorithm 2, would be exactly the same. Thus the label noise is strongly instance-dependent.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust bi-tempered logistic loss based on bregman divergences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Amid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Warmuth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koren</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="14987" to="14996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Twotemperature logistic regression based on the tsallis divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Amid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Warmuth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 22nd International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2388" to="2396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Confidence scores make instance-dependent label-noise learning possible</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berthon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML</title>
		<meeting>the 38th International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning with instance-dependent label noise: A sample sieve approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Does learning require memorization? a short tale about a long tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Feldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing</title>
		<meeting>the 52nd Annual ACM SIGACT Symposium on Theory of Computing</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="954" to="959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.07526</idno>
		<title level="m">On the resistance of nearest neighbor to random noisy labels</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Robust loss functions under label noise for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Decomposition-based evolutionary multiobjective optimization to self-paced learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="288" to="302" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Co-teaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8527" to="8537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A survey of label-noise representation learning: Past, present and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.04406</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep self-learning from noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5138" to="5147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Probability inequalities for sums of bounded random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hoeffding</surname></persName>
		</author>
		<idno>01621459</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">301</biblScope>
			<biblScope unit="page" from="13" to="30" />
			<date type="published" when="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
		<title level="m">Matrix analysis</title>
		<imprint>
			<publisher>Cambridge university press</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Invariant information clustering for unsupervised image classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9865" to="9874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning data-driven curriculum for very deep neural networks on corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mentornet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="2304" to="2313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Revisiting selfsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1920" to="1929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Dickerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12896</idno>
		<title level="m">Noisy labels can induce good representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Provably end-to-end label-noise learning without anchor points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.02400</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Variational inference for crowdsourcing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ihler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Neural Information Processing Systems</title>
		<meeting>the 25th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="692" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Classification with noisy labels by importance reweighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="447" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The importance of understanding instance-level noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML &apos;21</title>
		<meeting>the 38th International Conference on Machine Learning, ICML &apos;21</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Note Algorithm 2 cannot ensure T ii (X) &gt; T ij (X) when ? &gt; 0.5. To generate an informative dataset, we set 0.9 ? T ii (X) as the upper bound of T ij (X) and distribute the remaining probability to other classes</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">All experiments are repeated three times. HOC Global only employs one global T with G = 50 and |E| = 15k as inputs of Algorithm 2. HOC Local uses 300 local matrices (250-NN noise clusterability, |D h(n) | = 250, G = 30, |E| = 100) for CIFAR-10 and 5 local matrices (10k-NN noise clusterability, |D h(n) | = 10k, G = 30, |E| = 5k) for CIFAR-100. Note the local matrices may not cover the whole dataset. For those uncovered instances, we simply apply T</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Basic Hyper-Parameters To testify the classification performance, we adopt the flow: 1) Pre-training ? 2) Global Training ? 3) Local Training. Our HOC estimator is applied once at the beginning of each above step. Each training stage re-trains the model. In Stage-1, we load the standard ResNet50 model pre-trained on ImageNet to obtain basic representations</title>
		<imprint/>
	</monogr>
	<note>Other hyperparameters: ? Batch size: 128 (CIFAR), 32 (Clothing1M) ? Learning rate: ? CIFAR-10: Pre-training: 0.1. for 20 epochs ? 0.01 for 20 epochs. Global Training: 0.1 for 20 epochs ? 0.01 for 20 epochs. Local Training: 0.1 for 60 epochs ? 0.01 for 60 epochs ? 0.001 for 60 epochs</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">for 30 epochs ? 0.01 for 30 epochs. Global Training: 0.1 for 30 epochs ? 0.01 for 30 epochs. Local Training: 0.1 for 30 epochs ? 0.01 for 30 epochs ? 0.001 for 30 epochs</title>
		<idno>? CIFAR-100: Pre-training: 0.1</idno>
		<imprint/>
	</monogr>
	<note>15 epochs (Pre-training, Global training, and local training</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Clothing1M) ? Optimizer: SGD (Model training) and Adam with initial a learning rate of 0.1 (solving for T )</title>
		<idno>? Momentum: 0.9 ? Weight decay: 0.0005 (CIFAR) and 0.001</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Estimating local transition matrices using HOC on Clothing1M is feasible, e.g., assuming M -NN noise clusterability, but it may be time-consuming to tune M . Noting our current performance is already satisfying, and the focus of this paper is on the ability to estimate T , we leave the combination of T (X) with loss correction or other advanced techniques for future Algorithm 3 Local Datasets Generation Input: 1: Maximal rounds: G . Local dataset size: L</title>
	</analytic>
	<monogr>
		<title level="m">Clothing1M, we sample 1000 mini-batches from the training data while ensuring the (noisy) labels are balanced. The global T is obtained by an average of T from 5 random epochs</title>
		<imprint/>
	</monogr>
	<note>We only use T (X) = T in local training. Noisy dataset: D = {(x n ,? n )} n?[N ] . Noisy dataset size: |D|. Iteration: 2: Initialize the |D|-dimensional index list: S = 1 for k = 1 to G do if(size(S[S &gt; 0]) &gt; 0) then 3: Idx selected = random.choice(S[S &gt; 0. index randomly from the unselected index of D. else 4: Idx selected = random.randint(0, |D|</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">// If the selected index has covered D, we choose local center randomly. end if 5: Idx local = SelectbyDist(Idx selected , L) // Select the index of L features closest to Idx selected</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note>Idx local ] = ?1</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">// Mark the state of the selected index in S to avoid duplicate selection. 7: D k = D[Idx local ] // Build a local dataset by selecting (xi,?i), i ? Idx local . end for Output: 8: Local Datasets D k = {(x n ,? n )} ? {(x n1 ,? n1 ), ? ? ?</title>
		<imprint/>
	</monogr>
	<note>n i , k ? [L], i ? [M</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
