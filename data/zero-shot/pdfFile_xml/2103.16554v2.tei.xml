<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Shiyang Cheng 1 , Jing Yang 2</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
							<email>adrian@adrianbulat.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Cambridge</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Nottingham</orgName>
								<address>
									<settlement>Nottingham</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Queen Mary University London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Shiyang Cheng 1 , Jing Yang 2</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Face recognition</term>
					<term>face alignment</term>
					<term>emotion recognition</term>
					<term>3D face reconstruction</term>
					<term>representation learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>What is the best way to learn a universal face representation? Recent work on Deep Learning in the area of face analysis has focused on supervised learning for specific tasks of interest (e.g. face recognition, facial landmark localization etc.) but has overlooked the overarching question of how to find a facial representation that can be readily adapted to several facial analysis tasks and datasets. To this end, we make the following 4 contributions: (a) we introduce, for the first time, a comprehensive evaluation benchmark for facial representation learning consisting of 5 important face analysis tasks. (b) We systematically investigate two ways of large-scale representation learning applied to faces: supervised and unsupervised pre-training. Importantly, we focus our evaluations on the case of few-shot facial learning. (c) We investigate important properties of the training datasets including their size and quality (labelled, unlabelled or even uncurated). (d) To draw our conclusions, we conducted a very large number of experiments. Our main two findings are: (1) Unsupervised pre-training on completely in-thewild, uncurated data provides consistent and, in some cases, significant accuracy improvements for all facial tasks considered. (2) Many existing facial video datasets seem to have a large amount of redundancy. We will release code, and pre-trained models to facilitate future research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Supervised learning with Deep Neural Networks has been the standard approach to solving several Computer Vision problems over the recent past years <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b40">57,</ref><ref type="bibr" target="#b48">65,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr">41</ref>]. Among others, this approach has been very successfully applied to several face Large scale unlabelled facial dataset</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Universal Facial Representation</head><p>Behaviour &amp; Emotion</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Face Recognition</head><p>Facial Landmarks Fine-tuning <ref type="figure">Fig. 1</ref>: We advocate for a new paradigm to solving face analysis based on the following pipeline: <ref type="bibr" target="#b0">(1)</ref> collection of large-scale unlabelled facial dataset, (2) (task agnostic) network pre-training for universal facial representation learning, and (3) facial task-specific fine-tuning. Our main result is that even when training on a completely in-the-wild, uncurated dataset downloaded from Flickr, this generic pipeline provides consistent and, in some cases, significant accuracy improvements for all facial tasks considered.</p><p>analysis tasks including face detection <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b71">88,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr">38]</ref>, recognition <ref type="bibr" target="#b46">[63,</ref><ref type="bibr" target="#b57">74,</ref><ref type="bibr" target="#b58">75,</ref><ref type="bibr" target="#b67">84,</ref><ref type="bibr" target="#b16">17]</ref> and landmark localization <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr">94,</ref><ref type="bibr" target="#b59">76]</ref>. For example, face recognition was one of the domains where even very early attempts in the area of deep learning demonstrated performance of super-human accuracy <ref type="bibr" target="#b36">[53,</ref><ref type="bibr" target="#b51">68]</ref>. Beyond deep learning, this success can be largely attributed to the fact that for most face-related application domains, large scale datasets could be readily collected and annotated, see for example <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>There are several concerns related to the above approach. Firstly, from a practical perspective, collecting and annotating new large scale face datasets is still necessary; examples of this are context-dependent domains like emotion recognition <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b52">69,</ref><ref type="bibr" target="#b53">70]</ref> and surveillance <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">22]</ref>, or new considerations of existing problems like fair face recognition <ref type="bibr" target="#b42">[59,</ref><ref type="bibr" target="#b49">66]</ref>. Secondly, from a methodological point of view, it is unsatisfactory for each application to require its own large-scale dataset, although there is only one object of interest -the human face.</p><p>To this end, we investigate, for the first time to our knowledge, the task of large-scale learning universal facial representation in a principled and systematic manner. In particular, we shed light to the following research questions:</p><p>-"What is the best way to learn a universal facial representation that can be readily adapted to new tasks and datasets? Which facial representation is more amenable to few-shot facial learning?" -"What is the importance of different training dataset properties (including size and quality) in learning this representation? Can we learn powerful facial feature representations from uncurated facial data as well?"</p><p>To address this, we make the following 4 contributions:</p><p>1. We introduce, for the first time, a comprehensive and principled evaluation benchmark for facial representation learning consisting of 5 important face analysis tasks, namely face recognition, AU recognition, emotion recognition, landmark localization and 3D reconstruction.</p><p>2. Within this benchmark, and for the first time, we systematically evaluate 2 ways of large-scale representation learning applied to faces: supervised and unsupervised pre-training. Importantly, we focus our evaluations on the case of few-shot facial learning where only a limited amount of data is available for the downstream tasks. 3. We systematically evaluate the role of datasets in learning the facial feature presentations by constructing training datasets of varying size and quality.</p><p>To this end, we considered ImageNet, several existing curated face datasets but also a new in-the-wild, uncurated face dataset downloaded from Flickr. 4. We conducted extensive experiments to answer the aforementioned research questions and from them we were able to draw several interesting observations and conclusions.</p><p>Our main findings are: (a) Even when training on a completely in-the-wild, uncurated dataset downloaded from Flickr, unsupervised pre-training pipeline provides consistent and, in some cases, significant accuracy improvements for all facial tasks considered. (b) We found that many existing facial video datasets seem to have a large amount of redundancy. Given that unsupervised pre-training is cheap and that the cost of annotating facial datasets is often significant, some of our findings could be particularly important for researchers when collecting new facial datasets is under consideration. Finally, we will release code and pretrained models to facilitate future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Facial transfer learning: Transfer learning in Computer Vision typically consists of ImageNet pre-training followed by fine-tuning on the downstream task <ref type="bibr" target="#b40">[57,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b10">11]</ref>. Because most recent face-related works are based on the collection of larger and larger facial datasets <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr">47]</ref>, the importance of transfer learning has been overlooked in face analysis and, especially, the face recognition literature. ImageNet pre-training has been applied to face analysis when training on small datasets is required, for example for emotion recognition [48], face anti-spoofing [51] and facial landmark localization <ref type="bibr" target="#b59">[76]</ref>. Furthermore, the VGG-Face [50] or other large face datasets (e.g. [47]) have been identified as better alternatives by several works, see for example <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b66">83,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">56,</ref><ref type="bibr" target="#b38">55,</ref><ref type="bibr">51,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr">39]</ref>. To our knowledge, we are the first to systematically evaluate supervised network pre-training using both ImageNet and VGG-Face datasets on several face analysis tasks. Facial datasets: The general trend is to collect larger and larger facial datasets for the face-related task in hand <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr">47]</ref>. Also it is known that label noise can severely impact accuracy (e.g. see <ref type="table" target="#tab_5">Table 6</ref> of <ref type="bibr" target="#b16">[17]</ref>). Beyond faces, the work of [43] presents a study which shows the benefit of weakly supervised pre-training on much larger datasets for general image classification and object detection. Similarly, we also investigate the impact of the size of facial datasets on unsupervised pre-training for facial representation learning. Furthermore, one of our main results is to show that a high-quality facial representation can be learned even when a completely uncurated face dataset is used.</p><p>Few-shot face analysis: Few-shot refers to both low data and label regime. There is very little work in this area. To our knowledge, there is no prior work on few-shot face recognition where the trend is to collect large-scale datasets with millions of samples (e.g. <ref type="bibr" target="#b24">[25]</ref>). There is no systematic study for the task of emotion recognition, too. There is only one work on few-shot learning for facial landmark localization, namely that of <ref type="bibr" target="#b1">[2]</ref> which, different to our approach, proposes an auto-encoder approach for network pre-training. To our knowledge, our evaluation framework provides the very first comprehensive attempt to evaluate the transferability of facial representations for few-shot learning for several face analysis tasks. Semi-supervised face analysis: Semi-supervised learning has been applied to the domain of Action Unit recognition where data labelling is extremely laborious <ref type="bibr">[90,</ref><ref type="bibr">92,</ref><ref type="bibr">91]</ref>. Although these methods work with few labels, they are domain specific (as opposed to our work), assuming also that extra annotations are available in terms of "peak" and "valley" frames which is also an expensive operation. Unsupervised learning: There is a very large number of recently proposed unsupervised/self-supervised learning methods, see for example <ref type="bibr" target="#b63">[80,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b69">86,</ref><ref type="bibr">46,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">23]</ref>. To our knowledge, only very few attempts from this line of research have been applied to faces so far. The authors of <ref type="bibr" target="#b61">[78]</ref> learn face embeddings in a selfsupervised manner by predicting the motion field between two facial images. The authors of <ref type="bibr" target="#b55">[72]</ref> propose to combine several facial representations learned using an autoencoding framework. In this work, we explore learning facial representations in an unsupervised manner using the state-of-the-art method of <ref type="bibr" target="#b9">[10]</ref> and show how to effectively fine-tune the learned representations to the various face analysis tasks of our benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Supervised deep learning directly applied to large labelled datasets is the de facto approach to solving the most important face analysis tasks. In this section, we propose to take a different path to solving face analysis based on the following 2stage pipeline: (task agnostic) network pre-training followed by task adaptation. Importantly, we argue that network pre-training should be actually considered as part of the method and not just a simple initialization step. We explore two important aspects of network pre-training: (1) the method used, and (2) the dataset used. Likewise, we highlight hyper-parameter optimization for task adaptation as an absolutely crucial component of the proposed pipeline. Finally, we emphasize the importance of evaluating face analysis on low data regimes, too. We describe important aspects of the pipeline in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network Pre-training</head><p>Supervised pre-training of face networks on ImageNet or VGG datasets is not new. We use these networks as strong baselines. For the first time, we comprehensively evaluate their impact on the most important face analysis tasks.</p><p>Unsupervised pre-training: Inspired by <ref type="bibr" target="#b22">[23,</ref><ref type="bibr">46,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b9">10]</ref>, we explore, for the first time in literature, large-scale unsupervised learning on facial images to learn a universal, task-agnostic facial representation. To this end, we adopt the recently proposed SwAV <ref type="bibr" target="#b9">[10]</ref> which simultaneously clusters the data while enforcing consistency between the cluster assignments produced for different augmentations of the same image. The pretext task is defined as a "swapped" prediction problem where the code of one view is predicted from the representation of another: L(z 0 , z 1 ) = ?(z 0 , q 1 ) + ?(z 1 , q 0 ), where z 0 , z 1 are the features produced by the network for two different views of the same image and q 0 , q 1 their corresponding codes computed by matching these feature using a set of prototypes. ? is a cross-entropy (with temperature) loss. See supplementary material for training details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pre-training Datasets</head><p>With pre-training being now an important part of the face analysis pipeline, it is important to investigate what datasets can be used to this end. We argue that supervised pre-training is sub-optimal due to two main reasons: (a) the resulting models may be overly specialized to the source domain and task (e.g. face recognition pre-training) or be too generic (e.g. ImageNet pre-training), and (b) the amount of labeled data may be limited and/or certain parts of the natural data distribution may not be covered. To alleviate this, for the first time, we propose to explore large scale unsupervised pre-training on 4 facial datasets of interest, under two settings: using curated and uncurated data. The later departs from the common paradigm that uses carefully collected data that already includes some forms of explicit annotations and post-processing. In contrast, in the later case, all acquired facial images are used.</p><p>Curated Datasets For unsupervised pre-training we explore 3 curated datasets, collected for various facial analysis tasks: (a) Full VGG-Face (? 3.4M ), (b) Small VGG-Face (? 1M ) and (c) Large-Scale-Face (&gt; 5.0M ), consisting of VGG-Face2 <ref type="bibr" target="#b7">[8]</ref>, 300W-LP [93], IMDb-face <ref type="bibr" target="#b56">[73]</ref>, AffectNet [47] and WiderFace <ref type="bibr" target="#b68">[85]</ref>. During unsupervised pre-training we drop all labels using only the facial images. See supplementary material for more details.</p><p>Uncurated Datasets For a more realistic and practical scenario, we go beyond sanitized datasets, by creating a completely uncurated, in-the-wild, dataset, coined Flickr-Face, of ? 1.5M facial images by simply downloading images from Flickr (using standard search keywords like "faces", "humans", etc.) and filtering them with a face detector <ref type="bibr" target="#b17">[18]</ref> (the dataset will be made available). In total we collected 1.793.119 facial images. For more details, see supp. material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Facial Task Adaptation</head><p>End facial tasks: To draw as safe conclusions as possible, we used a large variety of face tasks (5 in total) including face recognition (classification), facial Action Unit intensity estimation (regression), emotion recognition in terms of valence and arousal (regression), 2D facial landmark localization (pixel-wise regression), and 3D face reconstruction (GCN regression). For these tasks, we used, in total, 10 datasets for evaluation purposes. Each circle on the radar plot denotes a constant error level. Points located closer to the center correspond to lower error levels. Accuracy greatly varies for different hyperparameters.</p><p>Adaptation methods: We are given a pre-trained model on task m, composed of a backbone g(.) and a network head h m (.). The model follows the ResNet-50 <ref type="bibr" target="#b27">[28]</ref> architecture. We considered two widely-used methods for task adaptation: (a) Network finetuning adapts the weights of g(.) to the new task m i . The previous head is replaced with a task-specific head h mi (.) that is trained from scratch. (b) Linear layer adaptation keeps the weights of g(.) fixed and trains only the new head h mi (.). Depending on the task, the structure of the head varies. This will be defined for each task in the corresponding section. See also Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyper-parameter optimization:</head><p>We find that, without a proper hyperparameters selection for each task and setting, the produced results are often misleading. In order to alleviate this and ensure a fair comparison, we search for the following optimal hyper-parameters: (a) learning rate, (b) scheduler duration and (c) backbone learning rate for the pre-trained ResNet-50. This search is repeated for each data point defined by the tuple (task, dataset, pre-training method and % of training data). In total, this yields in an extraordinary number of experiments for discovering the optimal hyperparameters. <ref type="figure" target="#fig_0">Fig. 2</ref> shows the importance of hyperparameters on accuracy for the task of facial landmark localization. In particular, for 1 specific value of learning rate, about 40 different combinations of scheduler duration and backbone relative's learning rate are evaluated. 24 of those combinations are placed on the perimeter of the figure. The 3 closed curves represent the Normalized Mean Error (NME) for each hyperparameter combination for each pre-training method. We observe that accuracy greatly varies for different hyperparameters. We explore, for the first time, evaluating the face models using a varying percentage of training data for each face analysis task. Specifically, beyond the standard evaluation using 100% of training data, we emphasize the importance of the low data regime, in particular 10% and 2%, which has a clear impact when new datasets are to be collected and annotated. The purpose of the proposed evaluation is not only to show which method works the best for this setting but also to draw interesting conclusions about the redundancy of existing facial datasets. See also Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Few-shot Learning-based Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Self-distillation for Semi-supervised Learning</head><p>The low data regime of the previous section refers to having both few data and few labels. We further propose to investigate the case of semi-supervised learning [37, <ref type="bibr" target="#b65">82,</ref><ref type="bibr" target="#b64">81,</ref><ref type="bibr" target="#b12">13]</ref> where a full facial dataset has been collected but only few labels are provided. To this end, we propose a simple self-distillation technique which fully utilizes network pre-training: we use the fine-tuned network to generate in an online manner new labels for training an identically sized student model on unlabeled data. The student is initialized from a pre-trained model trained in a fully unsupervised manner. The self-distillation process is repeated iteratively for T steps, where, at each step, the previously trained model becomes the teacher. Formally, the knowledge transfer is defined as argmin ?t L((f (x, ? t?1 ), f (x, ? t ))), where x is the input sample, ? t?1 and ? t are the parameters of the teacher and the student, respectively, and L is the task loss (e.g. pixel-wise ? 2 loss for facial landmark localization).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Ablation Studies</head><p>In this section, we study and answer key questions related to our approach. Fine-tuning vs. linear adaptation: Our results, provided in <ref type="table" target="#tab_6">Table 7</ref>, show that linear adaptation results in significant performance degradation. As our ultimate goal is high accuracy for the end facial task, linear adaptation is not considered for the rest of our experiments. How much facial data is required? Unlike supervised, unsupervised pretraining does not require labels and hence it can be applied easily to all types of combinations of facial datasets. Then, a natural question arising is how much data is needed to learn a high-quality representation. To this end, we used 3 datasets of varying size. The first one, comprising ? 3.3M images, is the original VGG-Face dataset (VGG-Face). The second comprises ? 1M images randomly selected from VGGFace2 (VGG-Face-small). The last one, coined as Large-Scale-Face, comprises over 5M images, and is obtained by combining VGG-Face, 300W-LP [93], IMDb-face <ref type="bibr" target="#b56">[73]</ref>, AffectNet [47] and WiderFace <ref type="bibr" target="#b68">[85]</ref>. For more details regarding the datasets see Section 3.2. We trained 3 models on these datasets and evaluated them for the tasks of facial landmark localization, AU intensity estimation and face recognition. As the results from <ref type="table" target="#tab_1">Table 2</ref> show, VGG-Face vs. VGG-Face-small yields small yet noticeable improvements especially for the case of 2% of labelled data. We did not observe further gains by training on Large-Scale-Face. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Curated vs. uncurated datasets:</head><p>While the previous section investigated the quantity of data required, it did not explore the question of data quality. While we did not use any labels during the unsupervised pretraining phase, one may argue that all datasets considered are sanitized as they were collected by human annotators with a specific task in mind.</p><p>In this section, we go beyond sanitized datasets, by experimenting with the newly completely uncurated, inthe-wild, dataset, coined Flickr-Face, introduced in Section 3.2. We trained a model on it and evaluated it on the same tasks/datasets of the previous section. <ref type="table" target="#tab_1">Table 2</ref> shows some remarkable results: the resulting model is on par with the one trained on the full VGG-Face dataset (Section 5 shows that it outperforms all other pre-training methods, too). We believe that this result can pave a whole new way to how practitioners, both in industry and academia, collect and label facial datasets for new tasks and applications. Pre-training task or data? In order to fully understand whether the aforementioned gains are coming from the unsupervised task alone, the data, or both, we pre-trained a model on ImageNet dataset using both supervised and unsupervised pre-training. Our experiments showed that both models performed similarly (e.g. 4.97% vs 5.1% on 300W@2% of data) and significantly more poorly than models trained on face datasets. We conclude that both unsupervised pre-training and data are required for high accuracy.</p><p>Effect of unsupervised method: Herein, we compare the results obtained by changing the unsupervised pre-training method from SwAV to Moco-v2 <ref type="bibr" target="#b26">[27]</ref>. <ref type="table" target="#tab_0">Table 1</ref> shows that SwAV largely outperforms Moco-v2, emphasizing the importance of utilizing the most powerful available unsupervised method. Note, that better representation learning as measured on imagenet, doesn't equate with better representation in general <ref type="bibr" target="#b13">[14]</ref>, hence way it's important to validate the performance of different methods for faces too. Furthermore, we evaluated SwAV models using different batch-sizes which is shown to be an important hyper-parameter. We found both models to perform similarly. See SwAV (256) in <ref type="table" target="#tab_0">Table 1</ref> for the model trained with batch-size 256. With small batch-size training requires less resources, yet we found that it was prolonged by 2?. Self-distillation for semi-supervised learning: Herein, we evaluate the effectiveness of network pre-training on self-distillation (see Section 3.5) for the task of semi-supervised facial landmark localization (300W).</p><p>We compare unsupervised vs. supervised pre-training on VGG-Face as well as training from scratch. These networks are fine-tuned on 300W using 100% and, the most interesting, 10% and 2% of the data. Then, they are used as students for self-distillation. <ref type="figure" target="#fig_2">Fig. 4</ref> clearly shows the effectiveness of unsupervised student pre-training. Furthermore, a large pool of unlabelled data was formed by 300W, AFLW <ref type="bibr" target="#b33">[34]</ref>, WFLW <ref type="bibr" target="#b62">[79]</ref> and COFW <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21]</ref>), and then used for self-distillation. <ref type="figure" target="#fig_1">Fig. 3 (left)</ref> shows the impact on the accuracy of the final model by adding more and more unlabelled data to the self-distillation process. Clearly, self-distillation based on network pretraining is capable of effectively utilizing a large amount of unlabelled data. Finally, <ref type="figure" target="#fig_1">Fig. 3 (right)</ref> shows the impact of the number of self-distillation steps on accuracy.</p><p>Other supervised pre-training: Our best supervised pre-trained network is that based on training CosFace <ref type="bibr" target="#b58">[75]</ref> on VGG-Face. Herein, for completeness, we compare this to supervised pre-training on another task/dataset, namely facial landmark localization. As <ref type="table" target="#tab_2">Table 3</ref> shows, the supervised pre-trained model on VGG-Face outperforms it by large margin. This is expected due to the massive size of VGG-Face. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Main Results</head><p>In this section, we thoroughly test the generalizability of the universal facial representations by adapting the resulting models to the most important facial analysis tasks. The full training and implementation details for each of this tasks is detailed in the corresponding sub-section. Training code will be made available.</p><p>Data &amp; label regime: For all datasets and tasks, we used fine-tuning for network adaptation using 3 data and label regimes: full (100%), low (10%) and very low (2% or less). For all low data scenarios, we randomly sub-sampled a set of annotated images without accounting for the labels (i.e. we don't attempt to balance the classes). Once formed, the same subset is used for all subsequent experiments to avoid noise induced by different sets of images. For face recognition, we deviated slightly from the above setting by enforcing that at least 1/4 of the identities are preserved for the very low data regime of 2%. This is a consequence of the training objective used for face recognition that is sensitive to both the number of identities and samples per identity.</p><p>Models compared: For unsupervised network pre-training, we report the results of two models, one trained on the full VGG-Face and one on Flickr-Face. These models are denoted as Ours (VGG-F) and Ours (Flickr-F). These models are compared with supervised pre-training on ImageNet and VGG-Face (denoted as VGG-F), as well as the model trained from scratch. Comparison with SOTA: Where possible, we also present the results reported by state-of-the-art methods for each task on the few-shot setting. Finally, for each task, and, to put our results into perspective, we report the accuracy of a stateof-the-art method for the given task. We note however, that the results are not directly comparable, due to different networks, losses, training procedure, and even training datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Face Recognition</head><p>For face recognition, we fine-tuned the models on the VGGFace <ref type="bibr" target="#b7">[8]</ref> and tested them on the IJB-B <ref type="bibr" target="#b60">[77]</ref> and IJB-C [45] datasets. The task specific head h(.) consists of a linear layer. The whole network was optimized using the CosFace loss <ref type="bibr" target="#b58">[75]</ref>. Note that, for this experiment, since training was done on VGGFace <ref type="bibr" target="#b7">[8]</ref>, the results of supervised pre-training on VGG-Face are omitted (as meaningless).</p><p>For training details, see supplementary material.</p><p>Results are shown in <ref type="table" target="#tab_3">Table 4</ref>. Both Ours (VGG-F) and Ours (Flickr-F) perform similarly and both they outperform the other baselines by large margin for the low (10%) and very low (2%) data regimes. For the latter case, the accuracy drops significantly for all cases. We fine-tuned the pre-trained models for facial landmark localization on 300W <ref type="bibr" target="#b43">[60]</ref>, AFLW-19 <ref type="bibr" target="#b33">[34]</ref>, WFLW <ref type="bibr" target="#b62">[79]</ref> and COFW-68 <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21]</ref> reporting results in terms of NME i-o <ref type="bibr" target="#b43">[60]</ref> or NME diag <ref type="bibr" target="#b33">[34]</ref>. We followed the current best practices based on heatmap regression <ref type="bibr" target="#b3">[4]</ref>. In order to accommodate for the pixel-wise nature of the task, the task specific head h(.) is defined as a set of 3 1 ? 1 conv. layers with 256 channels, each interleaved with bilinear upsampling operations for recovering part of the lost resolution. Additional high resolution information is brought up via skip connections and summation from the lower part of the network. Despite the simple and un-optimized architecture we found that the network performs very well, thanks to the strong facial representation learned. All models were trained using a pixel-wise MSE loss. For full training details, see supp. material. Results are shown in <ref type="table" target="#tab_5">Table 6</ref>: unsupervised pre-training (both models) outperform the other baselines for all data regimes, especially for the low and very low cases. For the latter case, Ours (VGG-F) outperforms Ours (Flickr-F) probably because Ours (VGG-F) contains a more balanced distribution of facial poses. The best supervised pre-training method is VGG-F showing the importance of pre-training on facial datasets. Furthermore, <ref type="table" target="#tab_4">Table 5</ref> shows comparison with few very recent works on fewshot face alignment. Our method scores significantly higher across all data regimes and datasets tested setting a new state-of-the-art despite the straightforward network architecture and the generic nature of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Facial Landmark Localization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Action Unit (AU) Intensity Estimation</head><p>We fine-tuned and evaluated the pre-trained models for AU intensity estimation on the corresponding partitions of BP4D <ref type="bibr" target="#b54">[71,</ref><ref type="bibr">89]</ref> and DISFA [44] datasets. The network head h(.) is implemented using a linear layer. The whole network is trained to regress the intensity value of each AU using an ? 2 loss. We report results in terms of intra-class correlation (ICC) <ref type="bibr" target="#b47">[64]</ref>. For training details, see supplementary material. Results are shown in <ref type="table" target="#tab_6">Table 7</ref>: unsupervised pre-training (both models) outperform the other baselines for all data regimes. Notably, our models achieve very high accuracy even for the case when 2% of data was used. Supervised pre-training on VGG-F also works well.</p><p>Furthermore, <ref type="table" target="#tab_7">Table 8</ref> shows comparison with very recent works on semisupervised AU intensity estimation. We note that these methods had access to all training data; only the amount of labels was varied. Our methods, although trained under both very low data and label regimes, outperformed them by a significant margin.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Emotion Recognition</head><p>We observe similar behaviour on the well-established AffectNet [47] for emotion recognition. For details and results, see supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">3D Face Reconstruction</head><p>We fine-tuned all models on the 300W-LP [93] dataset and tested them on AFLW2000-3D [93]. Our task specific head is implemented with a GCN based on spiral convolutions <ref type="bibr">[40]</ref>. The network was trained to minimise the ? 1 distance between the predicted and the ground truth vertices.</p><p>Training details: Since 300W-LP has a small number of identities, during training we randomly augment the data using the following transformations: scaling(0.85 ? ?1.15?), in-plane rotation (?45 o ), and random 10% translation w.r.t image width and height. Depending on the setting, we trained the model between 120 and 360 epochs using a learning rate of 0.05, a weight decay of 10 ?4 and SGD with momentum (set to 0.9). All models were trained using 2 GPUs.</p><p>Results are shown in <ref type="table" target="#tab_8">Table 9</ref>: it can be seen that, for all data regimes, our unsupervised models outperform the supervised baselines. Supervised pre-training on VGG-F also works well. For more results, see supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion and Conclusions</head><p>Several conclusions can be drawn from our results: Unsupervised pre-training followed by task-specific fine-tuning provides very strong baselines for face analysis. For example, we showed that such generically built baselines outperformed recently proposed methods for few-shot/semi-supervised learning (e.g. for facial landmark localization and AU intensity estimation) some of which are based on quite sophisticated techniques. Moreover, we showed that unsupervised pretraining largely boosts self-distillation. Hence, it might be useful for newlyproposed task-specific methods to consider such a pipeline for both development and evaluation especially when newly-achieved accuracy improvements are to be reported. Furthermore, these results can be achieved even by simply training on uncurated facial datasets that can be readily downloaded from image repositories. The excellent results obtained by pre-training on Flickr-Face are particularly encouraging. Note that we could have probably created a better and more balanced dataset in terms of facial pose by running a method for facial pose estimation.</p><p>When new datasets are to be collected, such powerful pre-trained networks can be potentially used for minimizing data collection and label annotation labour. Our results show that many existing datasets (e.g. AFLW, DISFA, BP4D, even AffectNet) seem to have a large amount of redundancy. This is more evident for video datasets (e.g. DISFA, BP4D).</p><p>Note that by no means our results imply or suggest that all face analysis can be solved with small labelled datasets. For example, for face recognition, it was absolutely necessary to fine-tune on the whole VGG-Face in order to get high accuracy. For the unsupervised pretraining, similarly with <ref type="bibr" target="#b9">[10]</ref> we trained our model on 64 GPUs using a batch size of 4096 and Synchronized Batch Normalization. The network was trained for 200 epochs using a weight decay of 10 ?6 and learning rate of 4.8 that was decayed toward 0.045 using a Cosine Scheduler <ref type="bibr">[42]</ref>. During the first 10 epochs the learning rate is increased toward the target value using a linear scheduler. In all experiments, unless otherwise specified, we kept the temperature parameter to 0.1 and the Sinkhorn regularization parameters to 0.05. Each input sample was augmented into 2 views at a resolution of 224?224px and 6 at a resolution of 96 ? 96px. The model was trained using the LARS <ref type="bibr" target="#b70">[87]</ref> optimizer and was implemented in PyTorch <ref type="bibr">[52]</ref>. Datasets and data preparation: All images are detected using <ref type="bibr" target="#b17">[18]</ref> and then cropped based on the produced bounding-box so that the face will take approx. 190px on a 256 ? 256px image. Unless otherwise specified all the data used for unsupervised pre-training were processed in the same manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Downstream task implementation details</head><p>Herein, we present the implementation details for each downstream task used in the main body to evaluate the efficacy of the facial representation learned. We note that in all cases the images were normalized in accordance with the training procedure of the pre-trained backbone model used as initialization.</p><p>Face recognition Following the best practices <ref type="bibr" target="#b58">[75,</ref><ref type="bibr" target="#b16">17]</ref>, all images were normalized and aligned using the provided 5 landmarks. During training, the only augmentation applied was random horizontal flipping. Depending on the data regime, the models were trained between 18 and 54 epochs using a batch size of 512 and learning rate of 0.1. The weight decay was set to 0.0005 and the models were optimized using SGD with momentum (set to 0.9). For the cosface loss, the margin was set to 0.35. All models were trained on 8 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Facial Landmark Localization</head><p>The facial landmark localization pipeline was implemented following <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b50">67]</ref>. During training, we applied the following augmentations randomly: rotation (between ?30 o ), horizontal flipping, scaling (0.85 ? ?1.15?) and color jittering. Depending on the data regime, dataset and pretrained model, as detailed in the main body of the work, we trained the models between 60 and 480 epochs using a learning rate of 0.0001, a batch size of 24, a weight decay of 10 ?5 and Adam optimizer <ref type="bibr" target="#b31">[32]</ref> (? 1 = 0.5, ? 2 = 0.99). All the models were trained using a pixel-wise ? 2 on a single GPU.</p><p>Action Unit (AU) Intensity Estimation For AU intensity estimation, we adopted a similar augmentation strategy with the one used for face alignment, mainly we applied random rotation (?30 o ), random horizontal flipping and scale jittering (0.85 ? ?1.15?), Gaussian blurring with a kernel size between 5 and 10px and a probability of 0.4 and colour jittering. Depending on the setting, the models were trained between 60 and 320 epochs. The learning rate was typically set to 0.0001, the weight decay to 0.000005 and the batch size to 48. The models were optimized using Adam (? 1 = 0.5, ? 2 = 0.99) and trained on 2 GPUs.</p><p>Emotion recognition For valence and arousal estimation, we applied the same augmentation strategies as for AU Intensity Estimation with the exception of Gaussian blurring. Depending on the setting, the models were trained between 60 and 240 epochs using a batch size of 32, a learning rate of 0.1, weight decay of 10 ?4 and Adam optimizer(? 1 = 0.5, ? 2 = 0.99). All models were trained on a single GPU.</p><p>3D Face reconstruction Since 300W-LP has a small number of identities, during training we randomly augment the data using the following transformations: scaling(0.85 ? ?1.15?), in-plane rotation (?45 o ), and random 10% translation w.r.t image width and height. Depending on the setting, we trained the model between 120 and 360 epochs using a learning rate of 0.05, a weight decay of 10 ?4 and SGD with momentum (set to 0.9). All models were trained using 2 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Data sampling</head><p>For all low data scenarios, we randomly subsampled a set of annotated images without accounting for the labels (i.e. we don't attempt to balance the classes).</p><p>Once formed, the same subset is used for all subsequent experiments to avoid noise induced by different sets of images. For face recognition where the loss attempts to minimize the intra-class while maximising the inter-class distance and its sensitivity to both the number of identities and samples per identity, we deviated slightly from the above setting by enforcing that at least 1/4 of the identities are preserved for the very low data regime of 2%.  <ref type="bibr" target="#b68">[85]</ref>. Therefore, the dataset combines a set of datasets originally collected for facial recognition, face alignment, emotion recognition and face detection:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Curated Datasets</head><p>300W-LP [93] is a face alignment dataset constructed by warping into large poses, from ?90 o to 90 o , the ? 4000 near-frontal images from the 300W <ref type="bibr" target="#b44">[61]</ref> dataset. IMDb-face <ref type="bibr" target="#b56">[73]</ref> is a large-scale noise-controlled dataset for face recognition, originally containing 1.7M faces with 59,000 identities which were manually cleaned by the authors from 2.0M raw images. All images were obtained by downloading data from the IMDb website. AffectNet [47] is a in-the-wild facial expression dataset consisting of more than 1M images collected by queering results from the internet using 1250 emotion related keywords. Out of this, 440,000 images were manually annotated with 7 discrete facial expressions and the intensity of valence and arousal. WiderFace <ref type="bibr" target="#b68">[85]</ref> is a face detection benchmarking dataset consisting of 393,703 faces sourced from 32,203 images. The faces exhibit a high degree of variability in terms of scale, pose and occlusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Uncurated Flick-Face dataset</head><p>Herein we provide additional details regarding the collected uncurated, in-thewild, Flickr-Face dataset. The dataset was constructed by downloading a set of images from Flickr. The facial images were then automatically localized and cropped using a face detector <ref type="bibr" target="#b17">[18]</ref>. In order to increase the likelihood of finding a face in the image we downloaded images that have one of the following 100 tags: human, people, person, face, fashion, portrait, emotion, expression, affect, happy, sad, anger, angry, smile, laugh, joy, surprise, disgust, confused, fear, horror, adult, lady, ladies, beauty, gentleman, gentlemen, man, men, woman, women, baby, infant, toddler, kid, child, children, senior, father, mother, dad, mom, elderly, grandfather, grandmother, grandpa, grandma, grandparent, ancestor, 40s, 50s, 60s, 70s, 80s, 90s, couple, family, brother, sister, sibling, cousin, wedding, marriage, funeral, party, formal, boy, girl, teen, teenager, youth, friend, classmate, group photo, team, gathering, teacher, professor, lecturer, coach, tutor, worker, boss, celebrity, sport, self, selfie, photoshoot, concert, gigs, band, dance, marathon, passenger, army, soldier, marching, military, protest, crowds.</p><p>In total we collected 1.793.119 facial images with a bounding box size that follows the distribution shown in <ref type="figure" target="#fig_4">Fig. 5</ref>. We release the code used to download the images from Flickr thus allowing reproducing the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional results</head><p>Herein, we report results for AU intensity estimation and emotion recognition (see Section D.1 and Tables 10, 11 and 12).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Emotion Recognition</head><p>We fine-tuned the models for valence and arousal estimation on the well-established AffectNet <ref type="bibr">[47]</ref>. We report results in terms of RMSE and CCC <ref type="bibr" target="#b41">[58]</ref>, SAGR and PCC. The task specific head h(.) is a linear layer that regresses the valence and arousal values and also predicts the basic emotion classes. The network was trained to jointly minimise the RMSE and CCC losses for valence and arousal, and the cross-entropy loss for classification.</p><p>Results are shown in <ref type="table" target="#tab_0">Table 12</ref> again, for all data regimes, our unsupervised models outperform the supervised baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Additional 3D Face Reconstruction results</head><p>Furthermore, in <ref type="figure" target="#fig_6">Fig. 6</ref> we report results on the Florence dataset for the task of 3D face reconstruction.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Facial landmark localization accuracy in terms of NME (%) of 3 different pre-training methods for selected combinations of hyperparameters. The labels on the figure's perimeter show the scheduler length (first value) and backbone relative's learning rate (second value) separated by an underscore.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Self-distillation accuracy for facial landmark vs. (left) amount of unlabeled data (100% corresponds to 300W), and (right) number of distillation steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Effectiveness of network pretraining on self-distillation for the tasks of facial landmark localization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>37. Lee, D.H.: Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In: Workshop on challenges in representation learning, ICML. vol. 3 (2013) 38. Li, J., Wang, Y., Wang, C., Tai, Y., Qian, J., Yang, J., Wang, C., Li, J.,Huang, F.: Dsfd: dual shot face detector. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 5060-5069 (2019) 39. Li, S., Deng, W.: Deep facial expression recognition: A survey. IEEE Transactions on Affective Computing (2020) 40. Lim, I., Dielen, A., Campen, M., Kobbelt, L.: A simple approach to intrinsic correspondence learning on unstructured 3d meshes. In: Proceedings of the European Conference on Computer Vision (ECCV). pp. 0-0 (2018) 41. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic segmentation. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 3431-3440 (2015) 42. Loshchilov, I., Hutter, F.: Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983 (2016) 43. Mahajan, D., Girshick, R., Ramanathan, V., He, K., Paluri, M., Li, Y., Bharambe, A., van der Maaten, L.: Exploring the limits of weakly supervised pretraining. In: ECCV (2018) 44. Mavadati, S.M., Mahoor, M.H., Bartlett, K., Trinh, P., Cohn, J.F.: Disfa: A spontaneous facial action intensity database. IEEE Transactions on Affective Computing 4(2), 151-160 (2013) 45. Maze, B., Adams, J., Duncan, J.A., Kalka, N., Miller, T., Otto, C., Jain, A.K., Niggel, W.T., Anderson, J., Cheney, J., et al.: Iarpa janus benchmark-c: Face dataset and protocol. In: IEEE International Conference on Biometrics (ICB) (2018) 46. Misra, I., van der Maaten, L.: Self-supervised learning of pretext-invariant representations. arXiv (2019) 47. Mollahosseini, A., Hasani, B., Mahoor, M.H.: Affectnet: A database for facial expression, valence, and arousal computing in the wild. IEEE Transactions on Affective Computing 10(1), 18-31 (2017) 48. Ng, H.W., Nguyen, V.D., Vonikakis, V., Winkler, S.: Deep learning for emotion recognition on small datasets using transfer learning. In: Proceedings of the 2015 ACM on international conference on multimodal interaction. pp. 443-449 (2015) 49. Ntinou, I., Sanchez, E., Bulat, A., Valstar, M., Tzimiropoulos, G.: A transfer learning approach to heatmap regression for action unit intensity estimation. IEEE Transactions on Affective Computing (2021) 50. Parkhi, O.M., Vedaldi, A., Zisserman, A.: Deep face recognition (2015) 51. Parkin, A., Grinchuk, O.: Recognizing multi-modal face spoofing with face recognition networks. In: CVPR-W (2019) 52. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., Chintala, S.: Pytorch: An imperative style, high-performance deep learning library. In: Wallach, H., Larochelle, H., Beygelzimer, A., d'Alch?-Buc, F., Fox, E., Garnett, R. (eds.) Advances in Neural Information Processing Systems 32, pp. 8024-8035. Curran Associates, Inc. (2019), http://papers.neurips.cc/paper/ 9015-pytorch-an-imperative-style-high-performance-deep-learning-library. pdf 89. Zhang, X., Yin, L., Cohn, J.F., Canavan, S., Reale, M., Horowitz, A., Liu, P., Girard, J.M.: Bp4d-spontaneous: a high-resolution spontaneous 3d dynamic facial expression database. Image and Vision Computing 32(10), 692-706 (2014) 90. Zhang, Y., Dong, W., Hu, B.G., Ji, Q.: Weakly-supervised deep convolutional neural network learning for facial action unit intensity estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 2314-2323 (2018) 91. Zhang, Y., Jiang, H., Wu, B., Fan, Y., Ji, Q.: Context-aware feature and label fusion for facial action unit intensity estimation with partially labeled data. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 733-742 (2019) 92. Zhang, Y., Wu, B., Dong, W., Li, Z., Liu, W., Hu, B.G., Ji, Q.: Joint representation and estimator learning for facial action unit intensity estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 3457-3466 (2019) 93. Zhu, X., Lei, Z., Liu, X., Shi, H., Li, S.Z.: Face alignment across large poses: A 3d solution. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 146-155 (2016) 94. Zhu, X., Liu, X., Lei, Z., Li, S.Z.: Face alignment in full pose range: A 3d total solution. IEEE transactions on pattern analysis and machine intelligence 41(1),78-92 (2017)    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Distribution on face sizes in the uncurated Flickr-Face dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a) Trained on 100% of the data.(b) Trained on 10% of the data.(c) Trained on 2% of the data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>Cumulative 3D reconstruction error curves on the Florence [1] dataset for 3 different supervised data regimes: (a) using 100%, (b) 10% and (c) 2%. All models were trained on the 300W-LP dataset as detailed in the main body.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison between the facial representations learned by MoCov2 and SwAV, by fine-tuning the models on 2% of 300W and DISFA.</figDesc><table><row><cell>Method</cell><cell cols="2">300W DISFA</cell></row><row><cell></cell><cell cols="2">NME (%) ICC</cell></row><row><cell>Scratch</cell><cell>13.5</cell><cell>.237</cell></row><row><cell>MoCov2</cell><cell>11.9</cell><cell>.280</cell></row><row><cell>SwAV</cell><cell>4.97</cell><cell>.560</cell></row><row><cell>SwAV (256)</cell><cell>5.00</cell><cell>.549</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">Impact of different datasets on</cell></row><row><cell cols="4">the facial representations learned in an</cell></row><row><cell cols="4">unsupervised manner for the tasks of fa-</cell></row><row><cell cols="4">cial landmark localization (300W), AU</cell></row><row><cell cols="4">intensity estimation (DISFA) and face</cell></row><row><cell cols="3">recognition (IJB-B).</cell></row><row><cell>Data</cell><cell></cell><cell>Unsup.</cell><cell>300W DISFA IJBB</cell></row><row><cell cols="2">amount</cell><cell>Data</cell><cell>NME ICC 10 ?4</cell></row><row><cell></cell><cell cols="3">VGG-Face-small 3.91</cell><cell>.583 0.910</cell></row><row><cell>100%</cell><cell cols="3">VGG-Face Large-Scale-Face 3.83 3.85</cell><cell>.598 0.912 .593 0.912</cell></row><row><cell></cell><cell></cell><cell>Flickr-Face</cell><cell>3.86</cell><cell>.590 0.911</cell></row><row><cell></cell><cell cols="3">VGG-Face-small 4.37</cell><cell>.572 0.887</cell></row><row><cell>10%</cell><cell cols="3">VGG-Face Large-Scale-Face 4.30 4.25</cell><cell>.592 0.889 .597 0.892</cell></row><row><cell></cell><cell></cell><cell>Flickr-Face</cell><cell>4.31</cell><cell>.581 0.887</cell></row><row><cell></cell><cell cols="3">VGG-Face-small 5.46</cell><cell>.550 0.729</cell></row><row><cell>2%</cell><cell cols="3">VGG-Face Large-Scale-Face 4.98 4.97</cell><cell>.560 0.744 .551 0.743</cell></row><row><cell></cell><cell></cell><cell>Flickr-Face</cell><cell>5.05</cell><cell>.571 0.740</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Supervised pre-training applied to different datasets. The models are evaluated for AU intensity estimation on DISFA.</figDesc><table><row><cell>Data</cell><cell></cell><cell>Pretrain. method</cell><cell></cell></row><row><cell>amount</cell><cell cols="3">Sup. (ImageNet) Sup. (VGG-F) Sup. (300W)</cell></row><row><cell>100%</cell><cell>.560</cell><cell>.575</cell><cell>.463</cell></row><row><cell>10%</cell><cell>.556</cell><cell>.560</cell><cell>.460</cell></row><row><cell>1%</cell><cell>.453</cell><cell>.542</cell><cell>.414</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Face recognition results in terms of TAR on IJB-B and IJB-C.</figDesc><table><row><cell>Data</cell><cell>Pretrain.</cell><cell></cell><cell></cell><cell>IJB-B</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>IJB-C</cell><cell></cell><cell></cell></row><row><cell>amount</cell><cell>method</cell><cell>10 ?6</cell><cell>10 ?5</cell><cell>10 ?4</cell><cell>10 ?3</cell><cell>10 ?2</cell><cell>10 ?6</cell><cell>10 ?5</cell><cell>10 ?4</cell><cell>10 ?3</cell><cell>10 ?2</cell></row><row><cell></cell><cell>Scratch</cell><cell>0.389</cell><cell>0.835</cell><cell>0.912</cell><cell>0.950</cell><cell>0.975</cell><cell>0.778</cell><cell>0.883</cell><cell>0.931</cell><cell>0.961</cell><cell>0.981</cell></row><row><cell>100%</cell><cell>Sup. (ImageNet)</cell><cell>0.390</cell><cell cols="2">0.843 0.912</cell><cell>0.950</cell><cell>0.975</cell><cell>0.831</cell><cell>0.891</cell><cell>0.931</cell><cell>0.961</cell><cell>0.981</cell></row><row><cell></cell><cell>Ours (Flickr-F)</cell><cell>0.406</cell><cell>0.834</cell><cell>0.911</cell><cell>0.951</cell><cell>0.975</cell><cell>0.807</cell><cell>0.880</cell><cell>0.932</cell><cell cols="2">0.962 0.982</cell></row><row><cell></cell><cell>Ours (VGG-F)</cell><cell>0.432</cell><cell>0.835</cell><cell>0.912</cell><cell>0.950</cell><cell>0.976</cell><cell>0.882</cell><cell>0.882</cell><cell>0.932</cell><cell>0.961</cell><cell>0.981</cell></row><row><cell></cell><cell>Scratch</cell><cell>0.326</cell><cell>0.645</cell><cell>0.848</cell><cell>0.926</cell><cell>0.965</cell><cell>0.506</cell><cell cols="2">0.7671 0.8840</cell><cell>0.940</cell><cell>0.721</cell></row><row><cell>10%</cell><cell>Sup. (ImageNet)</cell><cell>0.320</cell><cell>0.653</cell><cell>0.858</cell><cell>0.926</cell><cell>0.966</cell><cell>0.503</cell><cell>0.779</cell><cell>0.891</cell><cell>0.941</cell><cell>0.973</cell></row><row><cell></cell><cell>Ours (Flickr-F)</cell><cell>0.334</cell><cell>0.758</cell><cell>0.887</cell><cell>0.940</cell><cell>0.970</cell><cell>0.715</cell><cell>0.834</cell><cell>0.909</cell><cell>0.952</cell><cell>0.978</cell></row><row><cell></cell><cell>Ours (VGG-F)</cell><cell cols="5">0.392 0.784 0.889 0.941 0.972</cell><cell>0.733</cell><cell>0.847</cell><cell>0.911</cell><cell>0.953</cell><cell>0.977</cell></row><row><cell></cell><cell>Scratch</cell><cell>0.086</cell><cell>0.479</cell><cell>0.672</cell><cell>0.800</cell><cell>0.909</cell><cell>0.400</cell><cell>0.570</cell><cell>0.706</cell><cell>0.829</cell><cell>0.922</cell></row><row><cell>2%</cell><cell>Sup. (ImageNet)</cell><cell>0.264</cell><cell>0.553</cell><cell>0.694</cell><cell>0.820</cell><cell>0.915</cell><cell>0.493</cell><cell>0.599</cell><cell>0.723</cell><cell>0.841</cell><cell>0.928</cell></row><row><cell></cell><cell>Ours (Flickr-F)</cell><cell>0.282</cell><cell>0.558</cell><cell>0.740</cell><cell>0.870</cell><cell>0.944</cell><cell>0.486</cell><cell>0.649</cell><cell>0.786</cell><cell>0.891</cell><cell>0.954</cell></row><row><cell></cell><cell>Ours (VGG-F)</cell><cell>0.333</cell><cell>0.547</cell><cell cols="3">0.744 0.873 0.948</cell><cell>0.455</cell><cell>0.637</cell><cell>0.786</cell><cell cols="2">0.893 0.956</cell></row><row><cell cols="2">SOTA (from paper) [17]</cell><cell>0.401</cell><cell>0.821</cell><cell>0.907</cell><cell>0.950</cell><cell>0.978</cell><cell>0.0.767</cell><cell>0.879</cell><cell>0.929</cell><cell>0.964</cell><cell>0.984</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table><row><cell cols="4">Comparison against state-</cell></row><row><cell cols="4">of-the-art in few-shot facial land-</cell></row><row><cell cols="2">mark localization.</cell><cell></cell><cell></cell></row><row><cell>300W</cell><cell cols="3">100% 10% 1.5%</cell></row><row><cell>RCN+ [29]</cell><cell>3.46</cell><cell>4.47</cell><cell>-</cell></row><row><cell>TS 3 [19]</cell><cell>3.49</cell><cell>5.03</cell><cell>-</cell></row><row><cell>3FabRec [2]</cell><cell>3.82</cell><cell>4.47</cell><cell>5.10</cell></row><row><cell cols="4">Ours (VGG-F) 3.20 3.48 4.13</cell></row><row><cell>AFLW</cell><cell cols="2">100% 10%</cell><cell>1%</cell></row><row><cell>RCN+ [29]</cell><cell>1.61</cell><cell>-</cell><cell>2.88</cell></row><row><cell>TS 3 [19]</cell><cell>-</cell><cell>2.14</cell><cell>-</cell></row><row><cell>3FabRec [2]</cell><cell>1.87</cell><cell>2.03</cell><cell>2.38</cell></row><row><cell cols="4">Ours (VGG-F) 1.54 1.70 1.91</cell></row><row><cell>WFLW</cell><cell cols="3">100% 10% 0.7%</cell></row><row><cell>SA [54]</cell><cell>4.39</cell><cell>7.20</cell><cell>-</cell></row><row><cell>3FabRec [2]</cell><cell>5.62</cell><cell>6.73</cell><cell>8.39</cell></row><row><cell cols="4">Ours (VGG-F) 4.57 5.44 7.11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Facial landmark localization results on 300W (test set), COFW, WFLW and AFLW in terms of NME inter-ocular , except for AFLW where NME diag is used.</figDesc><table><row><cell>Data amount</cell><cell>Pretrain. method</cell><cell cols="4">300W COFW WFLW AFLW</cell></row><row><cell></cell><cell>Scratch</cell><cell>4.50</cell><cell>4.10</cell><cell>5.10</cell><cell>1.59</cell></row><row><cell></cell><cell>Sup. (ImageNet)</cell><cell>4.16</cell><cell>3.63</cell><cell>4.80</cell><cell>1.59</cell></row><row><cell>100%</cell><cell>Sup. (VGG-F)</cell><cell>3.97</cell><cell>3.51</cell><cell>4.70</cell><cell>1.58</cell></row><row><cell></cell><cell>Ours (Flickr-F)</cell><cell>3.86</cell><cell>3.45</cell><cell>4.65</cell><cell>1.57</cell></row><row><cell></cell><cell>Ours (VGG-F)</cell><cell>3.85</cell><cell>3.32</cell><cell>4.57</cell><cell>1.55</cell></row><row><cell></cell><cell>Scratch</cell><cell>6.61</cell><cell>5.63</cell><cell>6.82</cell><cell>1.84</cell></row><row><cell></cell><cell>Sup. (ImageNet)</cell><cell>5.15</cell><cell>5.32</cell><cell>6.56</cell><cell>1.81</cell></row><row><cell>10%</cell><cell>Sup. (VGG-F)</cell><cell>4.55</cell><cell>4.46</cell><cell>5.87</cell><cell>1.77</cell></row><row><cell></cell><cell>Ours (Flickr-F)</cell><cell>4.31</cell><cell>4.27</cell><cell>5.45</cell><cell>1.73</cell></row><row><cell></cell><cell>Ours (VGG-F)</cell><cell>4.25</cell><cell>3.95</cell><cell>5.44</cell><cell>1.74</cell></row><row><cell></cell><cell>Scratch</cell><cell>13.52</cell><cell>14.7</cell><cell>10.43</cell><cell>2.23</cell></row><row><cell></cell><cell>Sup. (ImageNet)</cell><cell>8.04</cell><cell>8.05</cell><cell>8.99</cell><cell>2.09</cell></row><row><cell>2%</cell><cell>Sup. (VGG-F)</cell><cell>5.45</cell><cell>5.55</cell><cell>6.94</cell><cell>2.00</cell></row><row><cell></cell><cell>Ours (Flickr-F)</cell><cell>5.05</cell><cell>5.18</cell><cell>6.53</cell><cell>1.86</cell></row><row><cell></cell><cell>Ours (VGG-F)</cell><cell>4.97</cell><cell>4.70</cell><cell>6.29</cell><cell>1.88</cell></row><row><cell cols="2">SOTA (from paper) [76]</cell><cell>3.85</cell><cell>3.45</cell><cell>4.60</cell><cell>1.57</cell></row><row><cell cols="2">SOTA (from paper) [36]</cell><cell>-</cell><cell>-</cell><cell>4.37</cell><cell>1.39</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>AU intensity estimation results in terms of ICC on BP4D and DISFA.</figDesc><table><row><cell>Data</cell><cell>Pretrain.</cell><cell cols="2">DISFA</cell><cell>BP4D</cell><cell></cell></row><row><cell>amount</cell><cell>method</cell><cell cols="4">finetune linear finetune linear</cell></row><row><cell></cell><cell>Scratch</cell><cell>.318</cell><cell>-</cell><cell>.617</cell><cell>-</cell></row><row><cell></cell><cell>Sup. (ImageNet)</cell><cell>.560</cell><cell>.316</cell><cell>.708</cell><cell>.587</cell></row><row><cell>100%</cell><cell>Sup. (VGG-F)</cell><cell>.575</cell><cell>.235</cell><cell>.700</cell><cell>.564</cell></row><row><cell></cell><cell>Ours (Flickr-F)</cell><cell>.590</cell><cell>.373</cell><cell>.715</cell><cell>.599</cell></row><row><cell></cell><cell>Ours (VGG-F)</cell><cell>.598</cell><cell>.342</cell><cell>.719</cell><cell>.610</cell></row><row><cell></cell><cell>Scratch</cell><cell>.313</cell><cell>-</cell><cell>.622</cell><cell>-</cell></row><row><cell></cell><cell>Sup. (ImageNet)</cell><cell>.556</cell><cell>.300</cell><cell>.698</cell><cell>.573</cell></row><row><cell>10%</cell><cell>Sup. (VGG-F)</cell><cell>.560</cell><cell>.232</cell><cell>.692</cell><cell>.564</cell></row><row><cell></cell><cell>Ours (Flickr-F)</cell><cell>.581</cell><cell>.352</cell><cell>.699</cell><cell>.603</cell></row><row><cell></cell><cell>Ours (VGG-F)</cell><cell>.592</cell><cell>.340</cell><cell>.706</cell><cell>.604</cell></row><row><cell></cell><cell>Scratch</cell><cell>.237</cell><cell>-</cell><cell>.586</cell><cell>-</cell></row><row><cell></cell><cell>Sup. (ImageNet)</cell><cell>.453</cell><cell>.301</cell><cell>.689</cell><cell>.564</cell></row><row><cell>1%</cell><cell>Sup. (VGG-F)</cell><cell>.542</cell><cell>.187</cell><cell>.690</cell><cell>.562</cell></row><row><cell></cell><cell>Ours (Flickr-F)</cell><cell>.571</cell><cell>.321</cell><cell>.695</cell><cell>.596</cell></row><row><cell></cell><cell>Ours (VGG-F)</cell><cell>.560</cell><cell>.326</cell><cell>.694</cell><cell>.592</cell></row><row><cell cols="2">SOTA (from paper) [49]</cell><cell>0.57</cell><cell>-</cell><cell>0.72</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Comparison against state-of-the-art on few-shot Facial AU intensity estimation on the BP4D dataset.</figDesc><table><row><cell>Method</cell><cell>Data amount</cell><cell>6</cell><cell>10</cell><cell>AU 12</cell><cell>14</cell><cell>17</cell><cell>Avg.</cell></row><row><cell>KBSS [90]</cell><cell>1%</cell><cell>.760</cell><cell>.725</cell><cell>.840</cell><cell>.445</cell><cell>.454</cell><cell>.645</cell></row><row><cell>KJRE [92]</cell><cell>6%</cell><cell>.710</cell><cell>.610</cell><cell>.870</cell><cell>.390</cell><cell>.420</cell><cell>.600</cell></row><row><cell>CLFL [91]</cell><cell>1%</cell><cell>.766</cell><cell>.703</cell><cell>.827</cell><cell>.411</cell><cell>.600</cell><cell>.680</cell></row><row><cell>SSCFL [62]</cell><cell>2%</cell><cell>.766</cell><cell>.749</cell><cell>.857</cell><cell>.475</cell><cell>.553</cell><cell>.680</cell></row><row><cell>Ours</cell><cell>1%</cell><cell cols="4">.789 .756 .882 .529</cell><cell>.578</cell><cell>.707</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>3D face reconstruction reconstruction in terms of NME (68 points) on AFLW2000-3D.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Pretrain. method</cell><cell></cell></row><row><cell>Data</cell><cell>Scratch</cell><cell>Sup.</cell><cell>Sup.</cell><cell>Ours</cell><cell>Ours</cell></row><row><cell></cell><cell></cell><cell>(Imagenet)</cell><cell>(VGG-F)</cell><cell>(Flickr-F)</cell><cell>(VGG-F)</cell></row><row><cell>100%</cell><cell>3.70</cell><cell>3.58</cell><cell>3.51</cell><cell>3.53</cell><cell>3.42</cell></row><row><cell>10%</cell><cell>4.72</cell><cell>4.06</cell><cell>3.82</cell><cell>3.81</cell><cell>3.72</cell></row><row><cell>2%</cell><cell>7.11</cell><cell>6.15</cell><cell>4.42</cell><cell>4.50</cell><cell>4.31</cell></row><row><cell></cell><cell></cell><cell cols="3">SOTA (from paper) [15]: 3.39</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Full VGG-Face denotes the entirety of the VGG-Face2 dataset<ref type="bibr" target="#b7">[8]</ref>, consisting of ? 3.4M facial images of 9131 identities, with an average of 362.6 images for each subject. Images are downloaded from Google Image Search and have large variations in pose, age, illumination, ethnicity and profession, although they typically depict celebrities.</figDesc><table><row><cell>For unsupervised pre-training we explore 3 curated datasets, collected for various</cell></row><row><cell>facial analysis tasks: (a) Full VGG-Face (? 3.4M ), (b) Small VGG-Face (?</cell></row><row><cell>1M ) and (c) Large-Scale-Face (&gt; 5.0M ), consisting of VGG-Face2 [8], 300W-</cell></row><row><cell>LP [93], IMDb-face [73], AffectNet [47] and WiderFace [85]. During unsupervised</cell></row><row><cell>pre-training we drop all labels using only the facial images. See supplementary</cell></row><row><cell>material for more details.</cell></row><row><cell>a) images selected</cell></row><row><cell>from VGG-Face2.</cell></row><row><cell>c) Large-Scale-Face is constructed by combining the facial images from VGG-</cell></row><row><cell>Face2 [8], 300W-LP [93], IMDb-face [73], AffectNet [47] and WiderFace</cell></row></table><note>b) Small VGG-Face is a randomly sampled subset of 1M</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Comparison against state-of-the-art on few-shot Facial AU intensity estimation on the DISFA dataset.</figDesc><table><row><cell>Method</cell><cell>Data amount</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>9</cell><cell cols="2">AU</cell><cell>12</cell><cell>15</cell><cell>17</cell><cell>20</cell><cell>25</cell><cell>26</cell><cell>Avg.</cell></row><row><cell>KBSS [90]</cell><cell>1%</cell><cell>.136</cell><cell>.116</cell><cell>.480</cell><cell>.169</cell><cell>.433</cell><cell cols="2">.353</cell><cell cols="2">.710</cell><cell>.154</cell><cell>.248</cell><cell>.085</cell><cell>.778</cell><cell>.536</cell><cell>.350</cell></row><row><cell>KJRE [92]</cell><cell>6%</cell><cell>.270</cell><cell>.350</cell><cell>.250</cell><cell>.330</cell><cell>.510</cell><cell cols="2">.310</cell><cell cols="2">.670</cell><cell>.140</cell><cell>.170</cell><cell>.200</cell><cell>.740</cell><cell>.250</cell><cell>.350</cell></row><row><cell>CLFL [91]</cell><cell>1%</cell><cell>.263</cell><cell>.194</cell><cell>.459</cell><cell>.354</cell><cell>.516</cell><cell cols="2">.356</cell><cell cols="2">.707</cell><cell>.183</cell><cell>.340</cell><cell>.206</cell><cell>.811</cell><cell>.510</cell><cell>.408</cell></row><row><cell>SSCFL [62]</cell><cell>2%</cell><cell>.327</cell><cell>.328</cell><cell>.645</cell><cell>.024</cell><cell>.601</cell><cell cols="2">.335</cell><cell cols="2">.783</cell><cell>.181</cell><cell>.243</cell><cell>.078</cell><cell>.882</cell><cell>.578</cell><cell>.413</cell></row><row><cell>Ours</cell><cell>1%</cell><cell cols="4">.636 .667 .754 .367</cell><cell>.549</cell><cell cols="6">.535 .820 .313 .541</cell><cell>.199</cell><cell cols="2">.928 .608</cell><cell>.574</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Comparison against state-of-the-art on few-shot Facial AU intensity estimation on the BU4D dataset.</figDesc><table><row><cell>Method</cell><cell>Data amount</cell><cell>6</cell><cell>10</cell><cell>AU 12</cell><cell>14</cell><cell>17</cell><cell>Avg.</cell></row><row><cell>KBSS [90]</cell><cell>1%</cell><cell>.760</cell><cell>.725</cell><cell>.840</cell><cell>.445</cell><cell>.454</cell><cell>.645</cell></row><row><cell>KJRE [92]</cell><cell>6%</cell><cell>.710</cell><cell>.610</cell><cell>.870</cell><cell>.390</cell><cell>.420</cell><cell>.600</cell></row><row><cell>CLFL [91]</cell><cell>1%</cell><cell>.766</cell><cell>.703</cell><cell>.827</cell><cell>.411</cell><cell>.600</cell><cell>.680</cell></row><row><cell>SSCFL [62]</cell><cell>2%</cell><cell>.766</cell><cell>.749</cell><cell>.857</cell><cell>.475</cell><cell>.553</cell><cell>.680</cell></row><row><cell>Ours</cell><cell>1%</cell><cell cols="4">.789 .756 .882 .529</cell><cell>.578</cell><cell>.707</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 :</head><label>12</label><figDesc>Results on the emotion recogntion task on the AffectNet dataset.</figDesc><table><row><cell>Data amount</cell><cell>Init. method</cell><cell>Acc.</cell><cell cols="3">Valence RMSE SAGR PCC</cell><cell cols="4">Arousal CCC RMSE SAGR PCC</cell><cell>CCC</cell></row><row><cell></cell><cell>random</cell><cell>0.590</cell><cell>0.370</cell><cell>0.790</cell><cell>0.696</cell><cell>0.695</cell><cell>0.339</cell><cell>0.781</cell><cell>0.613</cell><cell>0.611</cell></row><row><cell>100%</cell><cell>imagenet</cell><cell>0.592</cell><cell>0.360</cell><cell>0.789</cell><cell>0.705</cell><cell>0.705</cell><cell>0.327</cell><cell>0.792</cell><cell>0.624</cell><cell>0.620</cell></row><row><cell></cell><cell>vggface</cell><cell>0.601</cell><cell>0.369</cell><cell>0.798</cell><cell>0.707</cell><cell>0.706</cell><cell>0.330</cell><cell>0.796</cell><cell>0.625</cell><cell>0.624</cell></row><row><cell></cell><cell>ours</cell><cell>0.602</cell><cell>0.356</cell><cell>0.793</cell><cell cols="2">0.711 0.710</cell><cell>0.328</cell><cell>0.793</cell><cell cols="2">0.634 0.629</cell></row><row><cell></cell><cell>random</cell><cell>0.493</cell><cell>0.402</cell><cell>0.752</cell><cell>0.626</cell><cell>0.625</cell><cell>0.366</cell><cell>0.753</cell><cell>0.536</cell><cell>0.536</cell></row><row><cell>10%</cell><cell>imagenet</cell><cell>0.548</cell><cell>0.383</cell><cell>0.784</cell><cell>0.655</cell><cell>0.654</cell><cell>0.351</cell><cell>0.767</cell><cell>0.569</cell><cell>0.566</cell></row><row><cell></cell><cell>vggface</cell><cell>0.529</cell><cell>0.401</cell><cell>0.755</cell><cell>0.636</cell><cell>0.634</cell><cell>0.372</cell><cell>0.750</cell><cell>0.532</cell><cell>0.526</cell></row><row><cell></cell><cell>ours</cell><cell>0.562</cell><cell>0.382</cell><cell>0.780</cell><cell cols="2">0.678 0.678</cell><cell>0.344</cell><cell>0.803</cell><cell cols="2">0.600 0.599</cell></row><row><cell></cell><cell>random</cell><cell>0.419</cell><cell>0.453</cell><cell>0.727</cell><cell>0.515</cell><cell>0.515</cell><cell>0.400</cell><cell>0.747</cell><cell>0.423</cell><cell>0.422</cell></row><row><cell>2%</cell><cell>imagenet</cell><cell>0.479</cell><cell>0.411</cell><cell>0.740</cell><cell>0.562</cell><cell>0.557</cell><cell>0.362</cell><cell>0.769</cell><cell>0.465</cell><cell>0.456</cell></row><row><cell></cell><cell>vggface</cell><cell>0.511</cell><cell>0.416</cell><cell>0.778</cell><cell>0.610</cell><cell>0.607</cell><cell>0.384</cell><cell>0.768</cell><cell>0.485</cell><cell>0.485</cell></row><row><cell></cell><cell>ours</cell><cell>0.495</cell><cell>0.370</cell><cell>0.763</cell><cell>0.620</cell><cell>0.593</cell><cell>0.338</cell><cell>0.794</cell><cell>0.500</cell><cell>0.471</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The florence 2d/3d hybrid face dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Del Bimbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 joint ACM workshop on Human gesture and behavior understanding</title>
		<meeting>the 2011 joint ACM workshop on Human gesture and behavior understanding</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="79" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">3fabrec: Fast few-shot face alignment by reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Browatzki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wallraven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6110" to="6120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Two-stage convolutional part heatmap regression for the 1st 3d face alignment in the wild (3dfaw) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="616" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2d &amp; 3d face alignment problem?(and a dataset of 230,000 3d facial landmarks)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1021" to="1030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Robust face landmark estimation under occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">P</forename><surname>Burgos-Artizzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1513" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Face recognition in poor-quality video: Evidence from security surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Burton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bruce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="243" to="248" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A unified multi-scale deep convolutional neural network for fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="354" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Vggface2: A dataset for recognising faces across pose and age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th IEEE International Conference on Automatic Face &amp; Gesture Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="67" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09882</idno>
		<title level="m">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<title level="m">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">A simple framework for contrastive learning of visual representations. arXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10029</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10566</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Faster, better and more detailed: 3d face reconstruction with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4690" to="4699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00641</idno>
		<title level="m">Retinaface: Single-stage dense face localisation in the wild</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Teacher supervises students how to learn from partially labeled images for facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="783" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Video-based emotion recognition using cnn-rnn and c3d hybrid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimodal Interaction</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.08347</idno>
		<title level="m">Occlusion coherence: Detecting and localizing occluded faces</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Scface-surveillance cameras face database. Multimedia tools and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Delac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Grgic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="863" to="879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<title level="m">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automatic, dimensional and continuous emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gunes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Synthetic Emotions (IJSE)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="68" to="99" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Ms-celeb-1m: A dataset and benchmark for large-scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<title level="m">Momentum contrast for unsupervised visual representation learning. arXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improving landmark localization with semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1546" to="1555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Video-based emotion recognition in the wild using deep transfer learning and score fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>G?rp?nar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Salah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="66" to="75" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Knyazev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shvetsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Efremova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuharenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04598</idno>
		<title level="m">Convolutional neural networks pretrained on large face recognition datasets for emotion classification from video</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Annotated facial landmarks in the wild: A large-scale, real-world database for facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE international conference on computer vision workshops (ICCV workshops)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2144" to="2151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Factorized higher-order cnns with an application to spatio-temporal emotion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toisoul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Panagakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Luvli face alignment: Estimating landmarks&apos; location, uncertainty, and visibility likelihood</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Koike-Akino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8236" to="8246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Face recognition accuracy of forensic examiners, superrecognizers, and face recognition algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Noyes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Cavazos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jeckeln</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page" from="6171" to="6176" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Aggregation via separation: Boosting facial landmark detector with semi-supervised style translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10153" to="10163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hyperface: A deep multi-task learning framework for face detection, landmark localization, pose estimation, and gender recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="121" to="135" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">An all-in-one convolutional neural network for face analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE FG 2017</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Avec 2015: The 5th international audio/visual emotion challenge and workshop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ringeval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM international conference on Multimedia</title>
		<meeting>the 23rd ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1335" to="1336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Face recognition: too bias, or not too bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Livitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Henon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Timoner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="0" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">300 faces in-the-wild challenge: Database and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and vision computing</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="3" to="18" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">300 faces in-the-wild challenge: The first facial landmark localization challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="397" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zaganidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.01864</idno>
		<title level="m">Semi-supervised au intensity estimation with contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Shrout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Fleiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intraclass correlations: uses in assessing rater reliability</title>
		<imprint>
			<date type="published" when="1979" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page">420</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sixta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Junior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jacques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Buch-Cardona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.07838</idno>
		<title level="m">Fairface challenge at eccv 2020: Analyzing bias in face recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04514</idno>
		<title level="m">High-resolution representations for labeling pixels and regions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to humanlevel performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">End-toend multimodal emotion recognition using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tzirakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1301" to="1309" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Avec 2016: Depression, mood, and emotion recognition workshop and challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gratch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ringeval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lalanne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Torres Torres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Stratou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th interna</title>
		<meeting>the 6th interna</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Fera 2015-second facial expression recognition and analysis challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Almaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mehu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Towards a general model of knowledge for facial analysis by multi-source transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vielzeuf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lechervy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pateux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">The devil of face recognition is in the noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="765" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Additive margin softmax for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="926" to="930" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Cosface: Large margin cosine loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5265" to="5274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Iarpa janus benchmark-b face dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Whitelam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Taborsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Maze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="90" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Self-supervised learning of a facial attribute embedding from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wiles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Koepke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Look at boundary: A boundary-aware face alignment algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2129" to="2138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via nonparametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Billion-scale semisupervised learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">Z</forename><surname>Yalniz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00546</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Neural aggregation network for video face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Fan-face: a simple orthogonal improvement to deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
			<biblScope unit="page" from="12621" to="12628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Wider face: A face detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Unsupervised embedding learning via invariant and spreading instance feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6210" to="6219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03888</idno>
		<title level="m">Large batch training of convolutional networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">S3fd: Single shot scaleinvariant face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="192" to="201" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
