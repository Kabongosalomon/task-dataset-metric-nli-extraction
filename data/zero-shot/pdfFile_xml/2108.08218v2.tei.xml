<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">OUT-OF-DISTRIBUTION DETECTION USING OUTLIER DETECTION METHODS A PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-01-25">January 25, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Diers</surname></persName>
							<email>jan.diers@uni-jena.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Friedrich-Schiller-University Jena</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Pigorsch</surname></persName>
							<email>christian.pigorsch@uni-jena.de</email>
							<affiliation key="aff1">
								<orgName type="department">Friedrich-Schiller-University Jena</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">OUT-OF-DISTRIBUTION DETECTION USING OUTLIER DETECTION METHODS A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-01-25">January 25, 2022</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Out-of-distribution Detection ? Outlier Detection ? Isolation Forest</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Out-of-distribution detection (OOD) deals with anomalous input to neural networks. In the past, specialized methods have been proposed to reject predictions on anomalous input. Similarly, it was shown that feature extraction models in combination with outlier detection algorithms are well suited to detect anomalous input. We use outlier detection algorithms to detect anomalous input as reliable as specialized methods from the field of OOD. No neural network adaptation is required; detection is based on the model's softmax score. Our approach works unsupervised using an Isolation Forest and can be further improved by using a supervised learning method such as Gradient Boosting.</p><p>Keywords Out-of-distribution Detection ? Outlier Detection ? Isolation Forest Recently, the research field of out-of-distribution detection (OOD) has emerged, which deals with processing data points that do not match the distribution of the training data. The goal is to detect anomalous input and deny an invalid prediction. The raw softmax score is not suitable in these cases for neural networks <ref type="bibr" target="#b6">[7]</ref> as well as for other machine learning methods [27] -although it is supposed to represent the confidence of the decision.</p><p>For image classification, the problem is formulated as follows. A model was trained to distinguish between M different classes K 1 , K 2 , ..., K M . At inference time, data appears which originates from a different domain, i.e. not from the M classes already known before, but from a new class. The model can therefore only misclassify, because the true label arXiv:2108.08218v2 [cs.</p><p>LG] 24 Jan 2022 A PREPRINT -JANUARY 25, 2022 K M +1 is unknown to the model. So how do we need to change models to let them handle anomalous input? How can we find out if an input is from an unknown class?</p><p>Hendrycks and Gimpel [14] provide a baseline against which misclassified and out-of-domain input can be detected. They empirically find that the predicted confidence of a neural network is lower when the input contains a foreign class K M +1 .</p><p>It has been shown that extracted features of a neural network trained on ImageNet, provide an effective way to detect anomalous images in the input <ref type="bibr" target="#b2">[3]</ref>. In addition, there is evidence that anomalous input in image data can also be reliably detected with supervised learning methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b15">16]</ref>. The label for the supervised methods is generated automatically based on arbitrary images that represent the distributions different to the normal distribution.</p><p>We follow up this work and show that outlier detection algorithms also reliably detect anomalous input when the neural network has been specifically trained for a particular task. The detection of out-of-distribution data becomes even more reliable when a supervised learning method is used instead of the unsupervised outlier-detection algorithm.</p><p>Our proposed methodology does not require re-training of models and can be applied to any existing models. We build on the work of <ref type="bibr" target="#b13">[14]</ref> and detect OOD input based solely on predicted class probabilities. To do this, we fit an Isolation Forest that separates expected class probabilities from abnormal class probabilities. This enables the detection of the OOD input. Validation data, which should be available for every model anyway, is sufficient for fitting the Isolation Forest.</p><p>Our contributions to the research area are:</p><p>1. We propose an unsupervised method to distinguish in-distribution from out-of-distribution input. The results indicate that the assumptions and methods of outlier and deep anomaly detection are also relevant to the field of out-of-distribution detection.</p><p>2. The method works on the basis of an Isolation Forest. It can be applied to existing models, requires no special training and no special architecture of the model. The employed loss function also does not need to be adapted. Classification accuracy does not suffer since no change is made to the model.</p><p>3. We show empirically, using common benchmark datasets, that our approach leads to a better detection of out-of-domain input than with current OOD detection techniques. This is especially the case when the classification accuracy of the network is already low.</p><p>4. We present that the results can be further improved if a supervised learning method is used instead of the Isolation Forest to detect the anomalies. The supervised learning method generalizes well to previously unknown OOD data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Machine learning methods and in particular neural networks are the backbone of many modern applications in research and industry. To apply these methods, at first, training data is collected, then models are trained and evaluated on validation or test data. If the error is low and the model makes reliable predictions, the model is released to be used in software. In the software, for example, it controls autonomous systems, detects production errors, or analyzes text.</p><p>But what happens if the data from the real environment does not match the training data well? What happens if the real data does not match the training data, e.g. if targets have changed? The model will not refuse to predict, but will fail the task undetected. It cannot signal the user that the new input is an unknown class or from a different distribution of the class.</p><p>Most of the modern models have no way to detect this change of environment (called "domain adaptation" or "concept drift"). Depending on how the distribution of the test data changes, it is possible to reweight the training data in the model's objective function to reflect the domain change <ref type="bibr" target="#b19">[20]</ref>. However, this does not work if the data and labels change arbitrarily, i. e., no knowledge about the distribution of the test data exists in advance. In practice, unfortunately, this is the case because it is impossible to know in advance what data points the model will be queried on. It is an open challenge for AI safety to teach models not to make a decision if they are unsure about the decision <ref type="bibr" target="#b1">[2]</ref>. Current models lack this capability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Literature</head><p>The related literature for this work is primarily guided by two research areas. The first is the field of out-of-distribution detection. The second research field covers the area of outlier detection and has received little attention in the field of out-of-distribution detection. We will give a brief overview of the two research fields in the following section. A comprehensive overview is provided by Saikiran Bulusu et al. <ref type="bibr" target="#b29">[30]</ref> in their survey of the field of OOD detection.</p><p>In the following, we assume that a model f has been trained to predict the probability f (x in ) = p(y|x in ). The estimation of the likelihood will succeed if x ? X in , i.e., the input x corresponds to the distribution of the training data X in .</p><p>Out-of-distribution detection considers the case where p(y|x out ) is estimated, where x out is taken from a distribution that has no correspondence with X in . In particular, the class probabilities p(y|x out ) must be false, since none of the y in X out exists.</p><p>Consider the example, that f was trained to distinguish cats from dogs. X in then includes images of cats and dogs. It follows that X out represents all other images, that neither contain cats nor dogs. The target y ? {0, 1} indicates whether the image contains a cat or a dog. The predicted probability p(y|x out ) is then false in all cases, since X out does not include images of cats or dogs.</p><p>An approach to determine whether p(y|?) was estimated based on x in or x out is provided by <ref type="bibr" target="#b13">[14]</ref>. The paper finds that the overall estimated confidence for X out is lower than the estimated confidence for X in . This allows to define a threshold that detects out-of-domain input: If p(y|?) is too low, then assume x out , otherwise it is x in . Liang et al. <ref type="bibr" target="#b21">[22]</ref> follow up on this work and propose their method called ODIN. ODIN uses temperature scaling to obtain a better calibrated model that estimates p(y|?) more reliably. This further increases the difference between predictions on X in and X out . In addition, the approach also makes changes to the input to obtain a more robust estimate. To achieve this, the gradient w.r.t. input is calculated and the input is changed so that p(y|x) increases:</p><formula xml:id="formula_0">x = x ? ? sign(?? x log(f (x) y )).<label>(1)</label></formula><p>These studies rely on the difference of the confidences to be sufficiently large for detecting OOD input. In addition, there are other approaches that do not rely on the predicted confidences, such as the one proposed by DeVries and Taylor <ref type="bibr" target="#b7">[8]</ref>. They add an additional output to the model to represent the confidence of the decision. This gives the model the ability to output a confidence for which it expects the estimated probabilities to be correct.</p><p>Ren et al. <ref type="bibr" target="#b27">[28]</ref> work with two different models. For this purpose, they decompose p(x) into a semantic part p(x S ), which contains the relevant information for the class membership y. The irrelevant part (noise), which is not necessary for estimating p(y|x), is subsumed under p(x B ). The joint occurrence is then modeled as p(x) = p(x S ) ? p(x B ). The decomposition is used to estimate two different models. The models allow to separate between semantics and noise, which enables the detection of OOD input.</p><p>Another line of research has emerged around the name Outlier Exposure. This refers to methods that have already been trained on data from X out . It turns out that neural networks usually generalize to other, previously unknown data from X out and thus provide reliable detection.</p><p>Hendrycks et al. <ref type="bibr" target="#b15">[16]</ref> use Outlier Exposure to adjust the loss function L of the model so that the entropy p(y|x out ) is high. To achieve this, they introduce an additional term into the loss function for the classification task:</p><formula xml:id="formula_1">min E (x,y)?Xin [L(f (x), y)] + ?E x?Xout [H(f (x), U )],</formula><p>where H corresponds to the cross entropy and U corresponds to the uniform distribution over M classes. The change of the loss function encourages the model to predict a uniform distribution when there is input from x out . The authors also present ways to apply the method when the problem is not a classification task.</p><p>Previous work uses datasets X out , which are semantically different from X in , to evaluate their methods. Chen et al. <ref type="bibr" target="#b3">[4]</ref> note that this evaluation is incomplete. They show that most methods inadequately recognize when OOD is input generated based on adversarial attacks. The authors therefore extend the approach of <ref type="bibr" target="#b15">[16]</ref> to include data that was generated using adversarial attacks. This produces more robust detection of X out when X out is minimally modified data from X in . They use the following adjusted loss function:</p><formula xml:id="formula_2">min E (x,y)?Xin [ max ??B(x, ) [? log(f (x + ?) y )]] + ?E x?Xout max ??B(x, ) [H(f (x), U )]].</formula><p>The set of changes to the input that deviate at most by from the original input is denoted by B(x, ) = {? ? R n : |?|| ? ? ? x + ? is valid}. x + ? is valid if the minimum and maximum pixel values of the image are maintained. The generation is thus similar tox from (1), with the change in maximum distance from the original .</p><p>Our work uses outlier detection methods to detect the OOD input of neural networks. A fixed definition of outliers does not exist, however, the community has widely agreed on the definition of Hawkins <ref type="bibr" target="#b12">[13]</ref>: "An outlier is an observation which deviates so much from the other observations as to arouse suspicions that it was generated by a different mechanism". The distinction between X in and X out thus fulfills all requirements to be an application of outlier detection.</p><p>Outlier detection methods search for points that are located in areas of low density. If the data generating distribution is known, it is easy to calculate p(x). For empirical data, methods must be found to estimate p(x).</p><p>Common methods for outlier detection are based on kernel density estimates <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b32">33]</ref>, k-nearest neighbor methods (kNN) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b32">33]</ref>, dimension reduction <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b16">17]</ref> or support vector machines <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b36">37]</ref>, among others. Methods from the domain of neural networks rely, among others, on generative adversarial networks <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b42">43]</ref> or autoencoders <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b43">44]</ref> for this purpose. The usage of k-nearest neighbors is an indirect estimation of the local density, since large distances to neighbors correspond to low density in the area.</p><p>Successful concepts from other areas of machine learning are also transferred to outlier detection. For example, there are approaches for ensemble learning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b44">45]</ref>, active learning in the case of partially labeled data <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref> or self-supervised learning to form a supervised task from the unlabeled data <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>In our work, we use the Isolation Forest <ref type="bibr" target="#b22">[23]</ref> as a method for outlier detection. As a tree-based method, it has similarity to the implicit density estimation of kNN-based methods with the difference that it is a model-driven approach. The Isolation Forest scales linearly with the size of the data set, which is a major advantage over instance-based kNN methods.</p><p>The Isolation Forest is composed of multiple Isolation Trees that perform random splits to features. The objective is to isolate individual data points in the nodes. The fewer splits are required before a point can be isolated into a terminal node, the larger is the outlier factor of that point. The outlier factor is defined as the expected path length of a point in all trees of the forest.</p><p>Combining outlier detection methods with neural network features has already been used in the community. For example, <ref type="bibr" target="#b2">[3]</ref> use the features of a network trained on ImageNet to detect anomalies using kNN. Our approach is similar. However, unlike <ref type="bibr" target="#b2">[3]</ref>, we do not use features based on ImageNet but train each of the networks for the dataset. Furthermore, we do not use intermediate features, but rather the output of the network. Both of these are motivated by the fact that we are interested in detecting OOD input when the network has been trained for a specific task.</p><p>The traditional methods of outlier detection work unsupervised. The reason is that outliers from the past are not necessarily representative for outliers in the future. This prohibits the reliable use of supervised learning methods. However, it is not true for every anomaly detection task that the distribution of outliers may change over time. There are some anomalies, e.g. in medicine, whose origin is well understood and therefore it can be assumed with a high degree of certainty that anomalies will not change in the future.</p><p>If this is the case, then anomaly detection can be transformed into a supervised problem. The task then corresponds to an ordinary classification problem. Usually, the data is highly unbalanced, since the outliers represent, by definition, a minority of the data points.</p><p>For OOD detection, there is evidence that anomalies also do not change over time. For example, Ruff et al. <ref type="bibr" target="#b28">[29]</ref> and Hendrycks et al. <ref type="bibr" target="#b15">[16]</ref> report that neural networks also detect previously unknown OOD data well when trained with arbitrary, other OOD data. This also justifies the use of outlier exposure models for the task and suggests that supervised learning methods may be well-suited for OOD detection. In our experiments, we use Gradient Boosting <ref type="bibr" target="#b11">[12]</ref> for this purpose and confirm that reliable OOD detection is possible with supervised learning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>One of the disadvantages of popular outlier detection methods is that they are designed for tabular data. For this reason, the algorithms cannot be applied to raw image data, but the data must be transformed properly.</p><p>For the transformation, we use the mapping of the function f to the output?.? represents the softmax values of a neural network f . The values in? follow a distribution, which we denote as D in , so that f (X in ) ? D in . A key assumption in our approach is that the softmax values of the neural network are informative with respect to the question whether a given input value is out of distribution or not. This is a reasonable presumption since a well trained neural network will assign elements from the trained distribution, i.e. f (X in ) ? D in , to a given class whereas elements out of distribution, i.e. f (X out ) ? D out , will result in values that do not allow for a clear decision, e.g. the softmax values are very close together. We will come back to this topic in the discussion of our empirical results in Chapter 6.</p><p>We use different approaches to distinguish elements of D in from elements of D out . Our first approach is based on an unsupervised Isolation Forest. For this, we use the Isolation Forest to estimate the normal distribution D in , which we can use to estimate p(f (x)). If the Isolation Forest signals that p(f (x)) is low, we assume that x does not come from X in and instead represents OOD input.</p><p>We can determine the distribution D in by splitting a validation set from the training data. On the validation set, we can apply f to obtain data that follow the distribution D in . We then fit the Isolation Forest on this data. See also Algorithm 1. As a second approach, we propose to use supervised learning to distinguish D in and D out . To do this, we also use the validation data from the training set and take completely different data to simulate D out . In our case, the Food101 dataset <ref type="bibr" target="#b25">[26]</ref> is the basis for determining D out . The Food101 dataset contains images from different food categories and has no similarity to the other datasets we use. <ref type="bibr" target="#b0">1</ref> This procedure is identical to the outlier exposure models. We then fit a Gradient Boosting classifier that separates between points f (X in ) and f (X out ). The choice of Gradient Boosting is not of particular importance; many classification models are suitable for this purpose. The use of supervised anomaly detection seems feasible because known OOD input generalizes well to unknown OOD input <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b15">16]</ref>. That is, although we use Food101 as an exemplary OOD dataset to train the model, we can also detect OOD input from other datasets. The details are shown by Algorithm 2. For our experiments, we use 8 different datasets as in-and OOD data. The images are all cropped to a size of 224 ? 224 pixels and enriched with smaller augmentation steps (flip and contrast). The details of the datasets can be found in <ref type="table" target="#tab_2">Table 1</ref>. We use EfficientNets <ref type="bibr" target="#b35">[36]</ref> pre-trained on ImageNet data to perform the transfer learning on the respective dataset. We set the label smoothing parameter <ref type="bibr" target="#b34">[35]</ref> to 0.2. We first train the front layers for three epochs with a learning rate of 0.01, before further fine-tuning the model starting from the second block with a learning rate of 0.001. Adam <ref type="bibr" target="#b18">[19]</ref> is used as optimizer. We apply early stopping and reduce the learning rate by a factor of 0.6 if the validation loss could not be reduced 3 epochs in a row. The classification error of the models on the different datasets is shown in <ref type="table" target="#tab_3">Table  2</ref>. Note that the methods Baseline, Isolation Forest and Gradient Boosting Classifier provide the same results by design.</p><p>In the benchmark we compare 5 different methods: the Baseline approach <ref type="bibr" target="#b14">[15]</ref>, ODIN <ref type="bibr" target="#b21">[22]</ref>, Outlier Exposure <ref type="bibr" target="#b15">[16]</ref> and our proposed methods based on Isolation Forest and Gradient Boosting.   <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b21">[22]</ref>. For the Isolation Forest and Gradient Boosting, we use the implementations and hyperparameters given by scikit-learn <ref type="bibr" target="#b9">[10]</ref> in version 0.24. We do not perform any further optimization of the hyperparameters. Our code is publicly available on GitHub. <ref type="bibr" target="#b1">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>For the metrics, we stay consistent with other work in the field. We consider the out-of-distribution error (OOD error), the area-under-curve value (AUC), and the false precision rate when the true precision rate is 95% (FPR at 95% TPR). We would like to point out that OOD detection is a topic with a high level of practical relevance. The system must make a binary decision whether the input is anomalous or not -the score of a point is not sufficient for this task. Therefore, we emphasize the importance of the OOD error, which is the number of misclassified OOD images. In practice, we would like to minimize this error. The AUC is a metric that does not require a threshold but works based on scores.</p><p>The results with an Isolation Forest provide comparable results to the methods designed specifically for out-ofdistribution detection. The advantage of Isolation Forests is that no adjustment to the model is necessary. If an existing model is already in use, only a validation set is needed to apply our method. ODIN can also be applied to existing models, but this requires two runs: In the first run, the gradient to the input data must be computed, and then in the second run, the confidence on the modified images must be obtained. This significantly increases the runtime of the inference and thus complicates the practical use of the method. The Baseline approach can also be used without modification, but falls behind the results of Isolation Forest and Gradient Boosting. Outlier exposure approaches cannot be applied to existing models because special loss functions must be used during training.</p><p>The supervised method based on Gradient Boosting works particularly reliably. In all metrics this approach performs best, especially with respect to the OOD error, the superiority of the approach is clear. On average 25% misclassified OOD input from ODIN can be reduced by Gradient Boosting to an error rate of 14%, which is an improvement of 44%. <ref type="table">Table 3</ref>: Results averaged over all out-of-distribution datasets and all in-distribution datasets. The supervised OODmethod based on Gradient Boosting classification outperforms all other methods in all metrics. Especially in terms of OOD-error, which is the most important metric when OOD is applied in practice, the supervised methods is clearly the best. Also the OOD detection based on Isolation Forests works well. Note that Isolation Forests work completely unsupervised and therefore solve a much more difficult task to detect OOD-Input. The AUC tests different thresholds for classification. Most models yield similar AUC values with different OOD error values. This suggests that the threshold for binary classification (OOD vs. in-distribution) is difficult to choose for many methods. Gradient Boosting is again the superior methodology, but by a smaller margin compared to the other metrics.</p><p>The third metric of FPR at 95% TPR is also borrowed from common literature in this research field. It measures the False Positive Rate when the True Positive Rate is 95%. A lower value is better here. In this metric, other methods perform slightly better than Isolation Forest. Again, the best model by far is the supervised OOD detection via Gradient Boosting Classifier. <ref type="table" target="#tab_5">Table 4</ref> summarizes the results. Shown are the metrics, each as an average over 6 out-of-distribution datasets. For example, the Isolation Forest has an OOD error of 18% on the Cifar10 dataset. This means on average 18% of the images were misidentified as Cifar10 images, even though they were from a different dataset. The baseline approach also has an error rate of 18% on Cifar10. The error rate for ODIN is 21%, outlier exposure has 15%, Gradient Boosting has 20%.</p><p>We list the detailed results for each dataset in the appendix of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>An open question is why the anomalous input can be detected in the softmax values of the neural network. Although it is consistent with previous research that the softmax values allow good generalization to other OOD input <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b28">29]</ref>, there is a lack of arguments to support these observations. In the introduction of our method, we stated that f (X in ) ? D in and consequently f (X out ) ? D out holds and that the corresponding softmax values allow for a distinction between the elements of these distributions. To support this assumption and to visualize the distributions we compute the t-SNE visualization of the softmax activations of in-distribution data (D in ) and plot the softmax activations of out-ofdistribution data (D out ) against it.</p><p>Taking Cifar10 as X in and Food101 or SVHN as X out as examples, we can see in <ref type="figure">Figure 6</ref> that the predicted classes of Cifar10 form well-formed clusters (leftmost figure). Between these clusters is a wide set of points that do not belong to any cluster. These are the predictions on Food101 (center <ref type="figure">figure) and SVHN (right figure)</ref>, respectively. It can be observed that even previously unknown OOD data is projected where already known OOD data is located. While t-SNE does not reflect a global topology, it is a clear indication that the clusters of in-distribution and OOD data are distinctly separated. This explains the absence of the distribution shift in the neural network output and enables supervised detection of OOD input. The performance of the Isolation Forest is explained by the well-defined clusters. The cluster structure represents a fundamental assumption of outlier detection that is exploited by the Isolation Forest. In existing clusters (in-distribution clusters), it is difficult for the Isolation Forest to isolate individual points. In widely spreaded clusters, this is easier (out-of-distribution data).</p><p>When looking at the results, it is also noticeable that Gradient Boosting is superior to the other methods. One reason might be, that datasets that are difficult to classify (Cassava, Cars196) do not form a good precondition for existing OOD methods. If general classification accuracy already suffers, then OOD detection is particularly difficult. It also turns out that using a model on the entire softmax distribution is beneficial. The input to the Baseline approach is exactly the same as for Gradient Boosting and Isolation Forest, however these methods are superior to the simple Baseline.  <ref type="figure">Figure 1</ref>: The softmax activations of the neural network reduced to 2 dimensions. As expected, on the left, the predicted classes form clearly separated clusters. Between these clusters lie the Food101 and SVHN OOD datasets, respectively. SVHN is projected at a similar location as Food101, which explains the good results of the supervised outlier detection. This does not mean that points are also projected onto each other in the full dimension, however there is a clear separation from the in-distribution data. Gradient Boosting learns to distinguish between Food101 and Cifar10, but can also distinguish SVHN from Cifar10 with it.</p><p>For future research it is of interest to combine performant approaches for OOD detection. ODIN or Outlier Exposure can be used as a basis to apply Isolation Forests or Gradient Boosting on the softmax values. It is expected that this will further improve performance. Furthermore, it seems challenging to include datasets in the benchmark comparisons that contain real-world OOD data. In OOD detection, autonomously acting systems are the focus of interest. Thus, for future research, we plan to use images of autonomously acting agents. By doing so, we would like to test whether OOD methods can contribute to agents' ability to better orient and act in previously unknown environments. <ref type="table" target="#tab_2">Table A1</ref>: Detailed results for the OOD-detection using CIFAR10 as in-distribution data. The OOD threshold defines the minimum confidence that is required to assign a data point to the in-distribution. If the confidence is lower than the score, then the data point is assigned to the OOD distribution. The score is calculated as the 0.05 quantile of the in-distribution scores, which means that 95% of all in-distribution scores are greater than the threshold.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OOD</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>in ? make predictions on validation set; 2.? out ? make predictions on OOD set; 3. fit the GBM to learn difference between? in and? out ; 4. apply GBM to new predictions; end</figDesc><table><row><cell>Algorithm 2: OOD-Detection with a Gradient Boosting Classifier</cell></row><row><cell>Result: Gradient Boosting Classifier for OOD-Detection</cell></row><row><cell>Input: Trained neural network,</cell></row><row><cell>validation set,</cell></row><row><cell>any OOD-dataset (e.g. Food101),</cell></row><row><cell>Gradient Boosting Classifier (GBM)</cell></row><row><cell>begin</cell></row><row><cell>1.? 4 Experiment Setup</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Details for the datasets used in this study. Textures and SVHNCropped are only used as out-of-distribution datasets and therefore do not have a training or validation split. The Food101 dataset is only used as training and validation set to train Outlier Exposure and Gradient Boosting models. It is not used for out of distribution detection on test images.</figDesc><table><row><cell></cell><cell></cell><cell>number of</cell><cell></cell></row><row><cell></cell><cell cols="3">classes train samples val samples test samples</cell></row><row><cell>name</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Cars196</cell><cell>196 6.515</cell><cell>1.629</cell><cell>8.041</cell></row><row><cell>Cassava</cell><cell>5 5.656</cell><cell>1.889</cell><cell>1.885</cell></row><row><cell>CatsVsDogs</cell><cell>2 13.957</cell><cell>4.653</cell><cell>4.652</cell></row><row><cell>Cifar10</cell><cell>10 40.000</cell><cell>10.000</cell><cell>10.000</cell></row><row><cell>Cifar100</cell><cell>100 40.000</cell><cell>10.000</cell><cell>10.000</cell></row><row><cell>Food101</cell><cell>101 60.600</cell><cell>15.150</cell><cell>-</cell></row><row><cell>Textures</cell><cell>47 -</cell><cell>-</cell><cell>5.640</cell></row><row><cell>SVHNCropped</cell><cell>10 -</cell><cell>-</cell><cell>10.000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Classification error on datasets for various methods.</figDesc><table><row><cell>classification error</cell></row></table><note>The neural networks are implemented in Tensorflow based on the publications of</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Results for out of distribution detection based on 6 different datasets. Values represent averages. While ODIN and Outlier Exposure require explicit optimization for out of distribution detection, our approach only requires a validation set to learn the outlier distribution. It works with any existing classifier without modification.</figDesc><table><row><cell cols="2">in-distribution method</cell><cell cols="2">average over 6 OOD datasets</cell><cell></cell></row><row><cell>dataset</cell><cell></cell><cell cols="3">OOD error OOD AUC FPR at 95% TPR</cell></row><row><cell>CatsVsDogs</cell><cell>Baseline</cell><cell>0.04</cell><cell>0.98</cell><cell>0.02</cell></row><row><cell></cell><cell>ODIN</cell><cell>0.05</cell><cell>0.98</cell><cell>0.04</cell></row><row><cell></cell><cell>Outlier Exposure</cell><cell>0.03</cell><cell>0.99</cell><cell>0.02</cell></row><row><cell></cell><cell>Isolation Forest</cell><cell>0.09</cell><cell>0.98</cell><cell>0.03</cell></row><row><cell></cell><cell>Gradient Boosting Classifier</cell><cell>0.04</cell><cell>0.98</cell><cell>0.07</cell></row><row><cell>Cifar10</cell><cell>Baseline</cell><cell>0.18</cell><cell>0.89</cell><cell>0.37</cell></row><row><cell></cell><cell>ODIN</cell><cell>0.21</cell><cell>0.81</cell><cell>0.42</cell></row><row><cell></cell><cell>Outlier Exposure</cell><cell>0.15</cell><cell>0.93</cell><cell>0.29</cell></row><row><cell></cell><cell>Isolation Forest</cell><cell>0.18</cell><cell>0.92</cell><cell>0.30</cell></row><row><cell></cell><cell>Gradient Boosting Classifier</cell><cell>0.20</cell><cell>0.91</cell><cell>0.29</cell></row><row><cell>Cifar100</cell><cell>Baseline</cell><cell>0.25</cell><cell>0.86</cell><cell>0.50</cell></row><row><cell></cell><cell>ODIN</cell><cell>0.21</cell><cell>0.86</cell><cell>0.40</cell></row><row><cell></cell><cell>Outlier Exposure</cell><cell>0.23</cell><cell>0.88</cell><cell>0.47</cell></row><row><cell></cell><cell>Isolation Forest</cell><cell>0.38</cell><cell>0.71</cell><cell>0.94</cell></row><row><cell></cell><cell>Gradient Boosting Classifier</cell><cell>0.27</cell><cell>0.87</cell><cell>0.43</cell></row><row><cell>Cars196</cell><cell>Baseline</cell><cell>0.45</cell><cell>0.66</cell><cell>0.96</cell></row><row><cell></cell><cell>ODIN</cell><cell>0.43</cell><cell>0.61</cell><cell>0.93</cell></row><row><cell></cell><cell>Outlier Exposure</cell><cell>0.45</cell><cell>0.68</cell><cell>0.95</cell></row><row><cell></cell><cell>Isolation Forest</cell><cell>0.44</cell><cell>0.78</cell><cell>0.75</cell></row><row><cell></cell><cell>Gradient Boosting Classifier</cell><cell>0.05</cell><cell>0.99</cell><cell>0.04</cell></row><row><cell>Cassava</cell><cell>Baseline</cell><cell>0.57</cell><cell>0.85</cell><cell>0.69</cell></row><row><cell></cell><cell>ODIN</cell><cell>0.35</cell><cell>0.86</cell><cell>0.43</cell></row><row><cell></cell><cell>Outlier Exposure</cell><cell>0.62</cell><cell>0.80</cell><cell>0.76</cell></row><row><cell></cell><cell>Isolation Forest</cell><cell>0.22</cell><cell>0.86</cell><cell>0.77</cell></row><row><cell></cell><cell>Gradient Boosting Classifier</cell><cell>0.12</cell><cell>0.88</cell><cell>0.40</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table A2 :</head><label>A2</label><figDesc>Detailed results for the OOD-detection using CIFAR100 as in-distribution data. For details on how the OOD threshold is defined, seeTable A1.</figDesc><table><row><cell>OOD data</cell><cell>method</cell><cell cols="4">OOD threshold OOD error OOD AUC FPR at 95% TPR</cell></row><row><cell>CatsVsDogs</cell><cell>Baseline</cell><cell>0.2129</cell><cell>0.1349</cell><cell>0.9392</cell><cell>0.3173</cell></row><row><cell></cell><cell>ODIN</cell><cell>0.1437</cell><cell>0.1244</cell><cell>0.9139</cell><cell>0.2844</cell></row><row><cell></cell><cell>Outlier Exposure</cell><cell>0.2145</cell><cell>0.1141</cell><cell>0.9563</cell><cell>0.2519</cell></row><row><cell></cell><cell>Isolation Forest</cell><cell>-</cell><cell>0.3175</cell><cell>0.7291</cell><cell>0.9323</cell></row><row><cell></cell><cell cols="2">Gradient Boosting Classifier -</cell><cell>0.1461</cell><cell>0.9625</cell><cell>0.1347</cell></row><row><cell>Cifar10</cell><cell>Baseline</cell><cell>0.2129</cell><cell>0.3412</cell><cell>0.8389</cell><cell>0.6324</cell></row><row><cell></cell><cell>ODIN</cell><cell>0.1437</cell><cell>0.3601</cell><cell>0.7856</cell><cell>0.6703</cell></row><row><cell></cell><cell>Outlier Exposure</cell><cell>0.2145</cell><cell>0.347</cell><cell>0.8324</cell><cell>0.644</cell></row><row><cell></cell><cell>Isolation Forest</cell><cell>-</cell><cell>0.5</cell><cell>0.6351</cell><cell>0.9439</cell></row><row><cell></cell><cell cols="2">Gradient Boosting Classifier -</cell><cell>0.46</cell><cell>0.6961</cell><cell>0.9485</cell></row><row><cell>Cars196</cell><cell>Baseline</cell><cell>0.2129</cell><cell>0.3771</cell><cell>0.8212</cell><cell>0.7839</cell></row><row><cell></cell><cell>ODIN</cell><cell>0.1437</cell><cell>0.1531</cell><cell>0.9417</cell><cell>0.2813</cell></row><row><cell></cell><cell>Outlier Exposure</cell><cell>0.2145</cell><cell>0.4039</cell><cell>0.7975</cell><cell>0.844</cell></row><row><cell></cell><cell>Isolation Forest</cell><cell>-</cell><cell>0.4457</cell><cell>0.6698</cell><cell>0.9442</cell></row><row><cell></cell><cell cols="2">Gradient Boosting Classifier -</cell><cell>0.3336</cell><cell>0.8917</cell><cell>0.4353</cell></row><row><cell>Cassava</cell><cell>Baseline</cell><cell>0.2129</cell><cell>0.0708</cell><cell>0.9611</cell><cell>0.1809</cell></row><row><cell></cell><cell>ODIN</cell><cell>0.1437</cell><cell>0.0611</cell><cell>0.9687</cell><cell>0.1199</cell></row><row><cell></cell><cell>Outlier Exposure</cell><cell>0.2145</cell><cell>0.067</cell><cell>0.9733</cell><cell>0.157</cell></row><row><cell></cell><cell>Isolation Forest</cell><cell>-</cell><cell>0.1586</cell><cell>0.7393</cell><cell>0.9406</cell></row><row><cell></cell><cell cols="2">Gradient Boosting Classifier -</cell><cell>0.0639</cell><cell>0.9683</cell><cell>0.1331</cell></row><row><cell>Textures</cell><cell>Baseline</cell><cell>0.2129</cell><cell>0.1782</cell><cell>0.9116</cell><cell>0.4055</cell></row><row><cell></cell><cell>ODIN</cell><cell>0.1437</cell><cell>0.1402</cell><cell>0.9035</cell><cell>0.3</cell></row><row><cell></cell><cell>Outlier Exposure</cell><cell>0.2145</cell><cell>0.1426</cell><cell>0.9353</cell><cell>0.3067</cell></row><row><cell></cell><cell>Isolation Forest</cell><cell>-</cell><cell>0.3606</cell><cell>0.7681</cell><cell>0.8787</cell></row><row><cell></cell><cell cols="2">Gradient Boosting Classifier -</cell><cell>0.1659</cell><cell>0.9548</cell><cell>0.1757</cell></row><row><cell cols="2">SVHNCropped Baseline</cell><cell>0.2129</cell><cell>0.3718</cell><cell>0.7078</cell><cell>0.6936</cell></row><row><cell></cell><cell>ODIN</cell><cell>0.1437</cell><cell>0.4001</cell><cell>0.6703</cell><cell>0.7502</cell></row><row><cell></cell><cell>Outlier Exposure</cell><cell>0.2145</cell><cell>0.3216</cell><cell>0.8068</cell><cell>0.5933</cell></row><row><cell></cell><cell>Isolation Forest</cell><cell>-</cell><cell>0.5</cell><cell>0.6957</cell><cell>0.9725</cell></row><row><cell></cell><cell cols="2">Gradient Boosting Classifier -</cell><cell>0.4721</cell><cell>0.7182</cell><cell>0.8016</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table A3 :</head><label>A3</label><figDesc>Detailed results for the OOD-detection using CatsVsDogs as in-distribution data. For details on how the OOD threshold is defined, seeTable A1.</figDesc><table><row><cell>OOD data</cell><cell>method</cell><cell cols="4">OOD threshold OOD error OOD AUC FPR at 95% TPR</cell></row><row><cell>Cifar10</cell><cell>Baseline</cell><cell>0.862</cell><cell>0.0842</cell><cell>0.958</cell><cell>0.1004</cell></row><row><cell></cell><cell>ODIN</cell><cell>0.7616</cell><cell>0.1181</cell><cell>0.9427</cell><cell>0.1503</cell></row><row><cell></cell><cell>Outlier Exposure</cell><cell>0.8836</cell><cell>0.0754</cell><cell>0.9615</cell><cell>0.0873</cell></row><row><cell></cell><cell>Isolation Forest</cell><cell>-</cell><cell>0.1007</cell><cell>0.9536</cell><cell>0.1266</cell></row><row><cell></cell><cell cols="2">Gradient Boosting Classifier -</cell><cell>0.1193</cell><cell>0.9554</cell><cell>0.3356</cell></row><row><cell>Cifar100</cell><cell>Baseline</cell><cell>0.862</cell><cell>0.0366</cell><cell>0.9839</cell><cell>0.0304</cell></row><row><cell></cell><cell>ODIN</cell><cell>0.7616</cell><cell>0.0599</cell><cell>0.977</cell><cell>0.0648</cell></row><row><cell></cell><cell>Outlier Exposure</cell><cell>0.8836</cell><cell>0.0308</cell><cell>0.9882</cell><cell>0.0218</cell></row><row><cell></cell><cell>Isolation Forest</cell><cell>-</cell><cell>0.0718</cell><cell>0.9806</cell><cell>0.0428</cell></row><row><cell></cell><cell cols="2">Gradient Boosting Classifier -</cell><cell>0.0581</cell><cell>0.9815</cell><cell>0.043</cell></row><row><cell>Cars196</cell><cell>Baseline</cell><cell>0.862</cell><cell>0.0187</cell><cell>0.9841</cell><cell>0.0005</cell></row><row><cell></cell><cell>ODIN</cell><cell>0.7616</cell><cell>0.0187</cell><cell>0.9972</cell><cell>0.0005</cell></row><row><cell></cell><cell>Outlier Exposure</cell><cell>0.8836</cell><cell>0.0194</cell><cell>0.9838</cell><cell>0.0016</cell></row><row><cell></cell><cell>Isolation Forest</cell><cell>-</cell><cell>0.0719</cell><cell>0.9812</cell><cell>0.0039</cell></row><row><cell></cell><cell cols="2">Gradient Boosting Classifier -</cell><cell>0.0144</cell><cell>0.9903</cell><cell>0.0181</cell></row><row><cell>Cassava</cell><cell>Baseline</cell><cell>0.862</cell><cell>0.0358</cell><cell>0.9879</cell><cell>0.0005</cell></row><row><cell></cell><cell>ODIN</cell><cell>0.7616</cell><cell>0.0363</cell><cell>0.997</cell><cell>0.0021</cell></row><row><cell></cell><cell>Outlier Exposure</cell><cell>0.8836</cell><cell>0.0356</cell><cell>0.9904</cell><cell>0.0</cell></row><row><cell></cell><cell>Isolation Forest</cell><cell>-</cell><cell>0.1395</cell><cell>0.9858</cell><cell>0.009</cell></row><row><cell></cell><cell cols="2">Gradient Boosting Classifier -</cell><cell>0.0239</cell><cell>0.9915</cell><cell>0.017</cell></row><row><cell>Textures</cell><cell>Baseline</cell><cell>0.862</cell><cell>0.025</cell><cell>0.9908</cell><cell>0.0043</cell></row><row><cell></cell><cell>ODIN</cell><cell>0.7616</cell><cell>0.0271</cell><cell>0.9958</cell><cell>0.0083</cell></row><row><cell></cell><cell>Outlier Exposure</cell><cell>0.8836</cell><cell>0.0247</cell><cell>0.9918</cell><cell>0.0037</cell></row><row><cell></cell><cell>Isolation Forest</cell><cell>-</cell><cell>0.0897</cell><cell>0.9884</cell><cell>0.0087</cell></row><row><cell></cell><cell cols="2">Gradient Boosting Classifier -</cell><cell>0.0204</cell><cell>0.9914</cell><cell>0.0174</cell></row><row><cell cols="2">SVHNCropped Baseline</cell><cell>0.862</cell><cell>0.0159</cell><cell>0.9959</cell><cell>0.0</cell></row><row><cell></cell><cell>ODIN</cell><cell>0.7616</cell><cell>0.0159</cell><cell>0.9996</cell><cell>0.0</cell></row><row><cell></cell><cell>Outlier Exposure</cell><cell>0.8836</cell><cell>0.0159</cell><cell>0.9978</cell><cell>0.0</cell></row><row><cell></cell><cell>Isolation Forest</cell><cell>-</cell><cell>0.0622</cell><cell>0.9925</cell><cell>0.0</cell></row><row><cell></cell><cell cols="2">Gradient Boosting Classifier -</cell><cell>0.0148</cell><cell>0.9884</cell><cell>0.0155</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table A4 :</head><label>A4</label><figDesc>Detailed results for the OOD-detection using Cars196 as in-distribution data. For details on how the OOD threshold is defined, seeTable A1.</figDesc><table><row><cell>OOD data</cell><cell>method</cell><cell cols="4">OOD threshold OOD error OOD AUC FPR at 95% TPR</cell></row><row><cell>CatsVsDogs</cell><cell>Baseline</cell><cell>0.0766</cell><cell>0.3611</cell><cell>0.6999</cell><cell>0.8988</cell></row><row><cell></cell><cell>ODIN</cell><cell>0.0593</cell><cell>0.3651</cell><cell>0.5945</cell><cell>0.9095</cell></row><row><cell></cell><cell>Outlier Exposure</cell><cell>0.0683</cell><cell>0.3409</cell><cell>0.7895</cell><cell>0.8435</cell></row><row><cell></cell><cell>Isolation Forest</cell><cell>-</cell><cell>0.3665</cell><cell>0.7427</cell><cell>0.8738</cell></row><row><cell></cell><cell cols="2">Gradient Boosting Classifier -</cell><cell>0.0295</cell><cell>0.9968</cell><cell>0.0132</cell></row><row><cell>Cifar10</cell><cell>Baseline</cell><cell>0.0766</cell><cell>0.5657</cell><cell>0.6439</cell><cell>0.9802</cell></row><row><cell></cell><cell>ODIN</cell><cell>0.0593</cell><cell>0.566</cell><cell>0.5558</cell><cell>0.9808</cell></row><row><cell></cell><cell>Outlier Exposure</cell><cell>0.0683</cell><cell>0.5668</cell><cell>0.6551</cell><cell>0.9822</cell></row><row><cell></cell><cell>Isolation Forest</cell><cell>-</cell><cell>0.5543</cell><cell>0.7833</cell><cell>0.7742</cell></row><row><cell></cell><cell cols="2">Gradient Boosting Classifier -</cell><cell>0.0851</cell><cell>0.9819</cell><cell>0.0751</cell></row><row><cell>Cifar100</cell><cell>Baseline</cell><cell>0.0766</cell><cell>0.5624</cell><cell>0.6514</cell><cell>0.9743</cell></row><row><cell></cell><cell>ODIN</cell><cell>0.0593</cell><cell>0.5581</cell><cell>0.5935</cell><cell>0.9665</cell></row><row><cell></cell><cell>Outlier Exposure</cell><cell>0.0683</cell><cell>0.568</cell><cell>0.6579</cell><cell>0.9845</cell></row><row><cell></cell><cell>Isolation Forest</cell><cell>-</cell><cell>0.5543</cell><cell>0.8006</cell><cell>0.7523</cell></row><row><cell></cell><cell cols="2">Gradient Boosting Classifier -</cell><cell>0.0423</cell><cell>0.9923</cell><cell>0.0358</cell></row><row><cell>Cassava</cell><cell>Baseline</cell><cell>0.0766</cell><cell>0.2287</cell><cell>0.6356</cell><cell>0.9905</cell></row><row><cell></cell><cell>ODIN</cell><cell>0.0593</cell><cell>0.2293</cell><cell>0.5585</cell><cell>0.9936</cell></row><row><cell></cell><cell>Outlier Exposure</cell><cell>0.0683</cell><cell>0.2173</cell><cell>0.7248</cell><cell>0.9305</cell></row><row><cell></cell><cell>Isolation Forest</cell><cell>-</cell><cell>0.1899</cell><cell>0.7277</cell><cell>0.8631</cell></row><row><cell></cell><cell cols="2">Gradient Boosting Classifier -</cell><cell>0.0339</cell><cell>0.9949</cell><cell>0.023</cell></row><row><cell>Textures</cell><cell>Baseline</cell><cell>0.0766</cell><cell>0.4324</cell><cell>0.6134</cell><cell>0.9773</cell></row><row><cell></cell><cell>ODIN</cell><cell>0.0593</cell><cell>0.4082</cell><cell>0.5752</cell><cell>0.9188</cell></row><row><cell></cell><cell>Outlier Exposure</cell><cell>0.0683</cell><cell>0.4294</cell><cell>0.6213</cell><cell>0.9702</cell></row><row><cell></cell><cell>Isolation Forest</cell><cell>-</cell><cell>0.4123</cell><cell>0.6915</cell><cell>0.8674</cell></row><row><cell></cell><cell cols="2">Gradient Boosting Classifier -</cell><cell>0.1022</cell><cell>0.9728</cell><cell>0.1024</cell></row><row><cell cols="2">SVHNCropped Baseline</cell><cell>0.0766</cell><cell>0.5551</cell><cell>0.7013</cell><cell>0.9612</cell></row><row><cell></cell><cell>ODIN</cell><cell>0.0593</cell><cell>0.4625</cell><cell>0.7632</cell><cell>0.7941</cell></row><row><cell></cell><cell>Outlier Exposure</cell><cell>0.0683</cell><cell>0.5732</cell><cell>0.6336</cell><cell>0.9938</cell></row><row><cell></cell><cell>Isolation Forest</cell><cell>-</cell><cell>0.5543</cell><cell>0.9219</cell><cell>0.3882</cell></row><row><cell></cell><cell cols="2">Gradient Boosting Classifier -</cell><cell>0.0235</cell><cell>0.9968</cell><cell>0.0153</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table A5 :</head><label>A5</label><figDesc>Detailed results for the OOD-detection using Cassava as in-distribution data. For details on how the OOD threshold is defined, seeTable A1.</figDesc><table><row><cell>OOD data</cell><cell>method</cell><cell cols="4">OOD threshold OOD error OOD AUC FPR at 95% TPR</cell></row><row><cell>CatsVsDogs</cell><cell>Baseline</cell><cell>0.408</cell><cell>0.4751</cell><cell>0.8502</cell><cell>0.6475</cell></row><row><cell></cell><cell>ODIN</cell><cell>0.5131</cell><cell>0.4421</cell><cell>0.7382</cell><cell>0.6012</cell></row><row><cell></cell><cell>Outlier Exposure</cell><cell>0.3927</cell><cell>0.5838</cell><cell>0.7907</cell><cell>0.8003</cell></row><row><cell></cell><cell>Isolation Forest</cell><cell>-</cell><cell>0.344</cell><cell>0.7954</cell><cell>0.9643</cell></row><row><cell></cell><cell cols="2">Gradient Boosting Classifier -</cell><cell>0.1851</cell><cell>0.8313</cell><cell>0.4806</cell></row><row><cell>Cifar10</cell><cell>Baseline</cell><cell>0.408</cell><cell>0.6475</cell><cell>0.8172</cell><cell>0.7605</cell></row><row><cell></cell><cell>ODIN</cell><cell>0.5131</cell><cell>0.5017</cell><cell>0.8057</cell><cell>0.5875</cell></row><row><cell></cell><cell>Outlier Exposure</cell><cell>0.3927</cell><cell>0.7058</cell><cell>0.7817</cell><cell>0.8294</cell></row><row><cell></cell><cell>Isolation Forest</cell><cell>-</cell><cell>0.0919</cell><cell>0.9186</cell><cell>0.603</cell></row><row><cell></cell><cell cols="2">Gradient Boosting Classifier -</cell><cell>0.1123</cell><cell>0.8764</cell><cell>0.4265</cell></row><row><cell>Cifar100</cell><cell>Baseline</cell><cell>0.408</cell><cell>0.6246</cell><cell>0.843</cell><cell>0.7329</cell></row><row><cell></cell><cell>ODIN</cell><cell>0.5131</cell><cell>0.4218</cell><cell>0.8563</cell><cell>0.4923</cell></row><row><cell></cell><cell>Outlier Exposure</cell><cell>0.3927</cell><cell>0.6461</cell><cell>0.8023</cell><cell>0.7587</cell></row><row><cell></cell><cell>Isolation Forest</cell><cell>-</cell><cell>0.112</cell><cell>0.9071</cell><cell>0.6451</cell></row><row><cell></cell><cell cols="2">Gradient Boosting Classifier -</cell><cell>0.1141</cell><cell>0.8701</cell><cell>0.4308</cell></row><row><cell>Cars196</cell><cell>Baseline</cell><cell>0.408</cell><cell>0.4654</cell><cell>0.8994</cell><cell>0.563</cell></row><row><cell></cell><cell>ODIN</cell><cell>0.5131</cell><cell>0.2179</cell><cell>0.937</cell><cell>0.2577</cell></row><row><cell></cell><cell>Outlier Exposure</cell><cell>0.3927</cell><cell>0.5629</cell><cell>0.8697</cell><cell>0.6834</cell></row><row><cell></cell><cell>Isolation Forest</cell><cell>-</cell><cell>0.3035</cell><cell>0.8219</cell><cell>0.9799</cell></row><row><cell></cell><cell cols="2">Gradient Boosting Classifier -</cell><cell>0.0901</cell><cell>0.9199</cell><cell>0.2557</cell></row><row><cell>Textures</cell><cell>Baseline</cell><cell>0.408</cell><cell>0.5552</cell><cell>0.8231</cell><cell>0.7246</cell></row><row><cell></cell><cell>ODIN</cell><cell>0.5131</cell><cell>0.3708</cell><cell>0.8267</cell><cell>0.4796</cell></row><row><cell></cell><cell>Outlier Exposure</cell><cell>0.3927</cell><cell>0.5456</cell><cell>0.8142</cell><cell>0.7115</cell></row><row><cell></cell><cell>Isolation Forest</cell><cell>-</cell><cell>0.3935</cell><cell>0.775</cell><cell>0.9257</cell></row><row><cell></cell><cell cols="2">Gradient Boosting Classifier -</cell><cell>0.1559</cell><cell>0.8642</cell><cell>0.4599</cell></row><row><cell cols="2">SVHNCropped Baseline</cell><cell>0.408</cell><cell>0.6308</cell><cell>0.8789</cell><cell>0.7408</cell></row><row><cell></cell><cell>ODIN</cell><cell>0.5131</cell><cell>0.1306</cell><cell>0.9663</cell><cell>0.146</cell></row><row><cell></cell><cell>Outlier Exposure</cell><cell>0.3927</cell><cell>0.6561</cell><cell>0.7666</cell><cell>0.7706</cell></row><row><cell></cell><cell>Isolation Forest</cell><cell>-</cell><cell>0.0829</cell><cell>0.9293</cell><cell>0.4846</cell></row><row><cell></cell><cell cols="2">Gradient Boosting Classifier -</cell><cell>0.0779</cell><cell>0.8933</cell><cell>0.3358</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Previously published work in the area of OOD often uses the Tiny Images dataset<ref type="bibr" target="#b38">[39]</ref> to calibrate the models. This dataset has been withdrawn by the authors<ref type="bibr" target="#b37">[38]</ref>. Of course, we follow the decision and no longer use this data in our work either. Instead, we use the Food101<ref type="bibr" target="#b25">[26]</ref> dataset to train outlier exposure models and the Gradient Boosting classifier.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/jandiers/ood-detection</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Outlier detection by active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Abe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zadrozny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<idno type="DOI">10.1145/1150402.1150459</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<editor>L. Ungar</editor>
		<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Concrete Problems in AI Safety</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Man?</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deep Nearest Neighbor Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Robust out-of-distribution detection for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/2003.09711" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Outlier detection with autoencoder ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sathe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Turaga</surname></persName>
		</author>
		<idno type="DOI">10.1137/1.9781611974973.11</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 SIAM International Conference on Data Mining</title>
		<editor>N. V. Chawla and W. Wang</editor>
		<meeting>the 2017 SIAM International Conference on Data Mining<address><addrLine>SIAM, 3600 Market Street, Floor 6, Philadelphia, PA; Philadelphia, Pennsylvania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="90" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Outlier detection with autoencoder ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sathe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Turaga</surname></persName>
		</author>
		<idno type="DOI">10.1137/1.9781611974973.11</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 SIAM International Conference on Data Mining</title>
		<editor>N. V. Chawla and W. Wang</editor>
		<meeting>the 2017 SIAM International Conference on Data Mining<address><addrLine>SIAM, 3600 Market Street, Floor 6, Philadelphia, PA; Philadelphia, Pennsylvania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="90" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>2640-3498</idno>
		<ptr target="http://proceedings.mlr.press/v70/guo17a.html" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1321" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning Confidence for Out-of-Distribution Detection in Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Self-supervised learning for outlier detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Diers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pigorsch</surname></persName>
		</author>
		<idno type="DOI">10.1002/sta4.322</idno>
	</analytic>
	<monogr>
		<title level="j">Stat</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ga?l</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?douard</forename><surname>Duchesnay</surname></persName>
		</author>
		<idno>1533-7928</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">85</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Fast outlier detection in high dimensional spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Angiulli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Pizzuti</surname></persName>
		</author>
		<idno type="DOI">https:/link.springer.com/chapter/10.1007/3-540-45681-3_2</idno>
		<idno>10.1007/3-540-45681-3_2</idno>
		<ptr target="https://link.springer.com/chapter" />
		<imprint>
			<date type="published" when="2002" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="15" to="27" />
			<pubPlace>Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Stochastic gradient boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0167-9473(01)00065-2</idno>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S0167947301000652" />
	</analytic>
	<monogr>
		<title level="j">Computational Statistics &amp; Data Analysis</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="367" to="378" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Identification of outliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Hawkins</surname></persName>
		</author>
		<idno type="DOI">https:/link.springer.com/content/pdf/10.1007/978-94-015-3994-4.pdf</idno>
		<ptr target="https://link.springer.com/content/pdf/10.1007/978-94-015-3994-4.pdf" />
		<imprint>
			<date type="published" when="1980" />
			<publisher>Chapman and Hall</publisher>
			<biblScope unit="volume">11</biblScope>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep Anomaly Detection with Outlier Exposure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Kernel pca for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoffmann</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2006.07.009</idno>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="863" to="874" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Robust kernel density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jooseuk</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scott</surname></persName>
		</author>
		<idno type="DOI">10.1109/icassp.2008.4518376</idno>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>Piscataway, NJ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1412.6980" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Domain adaptation under target and conditional shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krikamol</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhikun</forename><surname>Wang</surname></persName>
		</author>
		<idno>2640-3498</idno>
		<ptr target="http://proceedings.mlr.press/v28/zhang13d.html" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="819" to="827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">CutPaste: Self-Supervised Learning for Anomaly Detection and Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Isolation forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1109/icdm.2008.17</idno>
	</analytic>
	<monogr>
		<title level="m">Eighth IEEE International Conference on Data Mining</title>
		<editor>F. Giannotti</editor>
		<meeting><address><addrLine>Piscataway, NJ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generative adversarial active learning for unsupervised outlier detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1109/tkde.2019.2905606</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Outlier detection with kernel density functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Longin Jan Latecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragoljub</forename><surname>Lazarevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pokrajac</surname></persName>
		</author>
		<idno type="DOI">https:/link.springer.com/chapter/10.1007/978-3-540-73499-4_6</idno>
		<ptr target="https://link.springer.com/chapter/10.1007/978-3-540-73499-4_6" />
		<imprint>
			<date type="published" when="2007" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="61" to="75" />
			<pubPlace>Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Food-101 -mining discriminative components with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="DOI">https:/link.springer.com/chapter/10.1007/978-3-319-10599-4_29</idno>
		<ptr target="https://link.springer.com/chapter/10.1007/978-3-319-10599-4_29" />
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="446" to="461" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Predicting good probabilities with supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<idno type="DOI">10.1145/1102351.1102430</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on Machine learning</title>
		<editor>S. Dzeroski</editor>
		<meeting>the 22nd international conference on Machine learning<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Likelihood ratios for out-of-distribution detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fertig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Poplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Depristo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1906.02845" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Rethinking assumptions in deep anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Franks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/2006.00339" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Out-of-Distribution Detection in Deep Learning: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saikiran</forename><surname>Bulusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhavya</forename><surname>Kailkhura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<ptr target="https://www.researchgate.net/publication/339972014_Out-of-Distribution_Detection_in_Deep_Learning_A_Survey" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Unsupervised anomaly detection with generative adversarial networks to guide marker discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Seeb?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Langs</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1703.05921" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Support vector method for novelty detection. NIPS, 12 edition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<ptr target="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.675.575&amp;rep=rep1&amp;type=pdf" />
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Generalized outlier detection with flexible kernel density estimates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<idno type="DOI">10.1137/1.9781611973440.63</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 SIAM International Conference on Data Mining</title>
		<editor>M. Zaki, Z. Obradovic, P. N. Tan, A. Banerjee, C. Kamath, and S. Parthasarathy</editor>
		<meeting>the 2014 SIAM International Conference on Data Mining<address><addrLine>Philadelphia, Pennsylvania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Society for Industrial and Applied Mathematics</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">SSD: A Unified Framework for Self-Supervised Outlier Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sehwag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mittal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.308</idno>
	</analytic>
	<monogr>
		<title level="m">29th IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Piscataway, NJ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><forename type="middle">V</forename><surname>Efficientnet</surname></persName>
		</author>
		<idno>2640-3498</idno>
		<ptr target="https://arxiv.org/pdf/1905.11946" />
		<title level="m">Rethinking model scaling for convolutional neural networks. International Conference on Machine Learning</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Support vector data description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Tax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Duin</surname></persName>
		</author>
		<idno type="DOI">10.1023/B:MACH.0000008084.60811.49</idno>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="45" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">80 million tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<idno>01.07.2020</idno>
		<ptr target="https://groups.csail.mit.edu/vision/TinyImages/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">80 million tiny images: a large data set for nonparametric object and scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<idno type="DOI">10.1109/tpami.2008.128</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1958" to="1970" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">One-class active learning for outlier detection with multiple subspaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Trittenbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>B?hm</surname></persName>
		</author>
		<idno type="DOI">10.1145/3357384.3357873</idno>
	</analytic>
	<monogr>
		<title level="m">CIKM&apos;19</title>
		<editor>W. Zhu, D. Tao, and X. Cheng</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">An overview and a benchmark of active learning for outlier detection with one-class classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Trittenbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Englhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>B?hm</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2020.114372</idno>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">168</biblScope>
			<biblScope unit="page">114372</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Robust PCA via Outlier Pursuit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Caramanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sanghavi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Efficient gan-based anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zenati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Foo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lecouat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Manek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1802.06222" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Anomaly detection with robust deep autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Paffenroth</surname></persName>
		</author>
		<idno type="DOI">10.1145/3097983.3098052</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACM</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Subsampling for efficient and effective unsupervised outlier detection ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gaudet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<idno type="DOI">10.1145/2487575.2487676</idno>
		<editor>I. S. Dhillon and R. L. Grossman</editor>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ACM</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Ensembles for unsupervised outlier detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<idno type="DOI">10.1145/2594473.2594476</idno>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD Explorations Newsletter</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="22" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Data perturbation for outlier detection ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><forename type="middle">J G B</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<idno type="DOI">10.1145/2618243.2618257</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Scientific and Statistical Database Management</title>
		<editor>C. S. Jensen</editor>
		<meeting>the 26th International Conference on Scientific and Statistical Database Management<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
